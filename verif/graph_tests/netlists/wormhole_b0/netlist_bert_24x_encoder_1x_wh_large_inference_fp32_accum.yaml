# git checkout e815fd1f
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole-chip1-enc24-large]

devices:
  arch: wormhole

queues:

  # input
  hidden_states:                                                                {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x79cc000]]}
  attention_mask:                                                               {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7d5a060]]}

  # output
  bert_encoder.output_layernorm_3815:                                           {input: layernorm_3815.dc.add.10, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x463c5c0], [3, 0x88bf760], [4, 0x88d54c0], [5, 0x45c4020], [0, 0x46ae1e0], [1, 0x46fd560], [2, 0x467d5e0], [3, 0x8900780]]}
  layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbdac7a0], [4, 0xbdc2d20], [5, 0x77fb5a0], [0, 0x7ae4520]]}
  layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7cabd40], [2, 0x7cd8020], [3, 0xbdb08c0], [4, 0xbdc6e40], [5, 0x77ff6c0], [0, 0x7ae8640], [1, 0x7cecd60], [2, 0x7d19040]]}
  layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbdf18e0], [4, 0xbe07e60], [5, 0x78406e0], [0, 0x7b29660]]}
  layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbe0bf80], [5, 0x7844800], [0, 0x7b2d780], [1, 0x7d2e5c0], [2, 0x7e7e880], [3, 0xbdf6240], [4, 0xbe4cfa0], [5, 0x7885820]]}
  layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7b6e7a0], [1, 0x7d6f5e0], [2, 0x7ebf8a0], [3, 0xbe37260]]}
  layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbe8dfc0], [5, 0x78c6840], [0, 0x7b728c0], [1, 0x7d73700], [2, 0x7ec39c0], [3, 0xbe3b380], [4, 0xbecefe0], [5, 0x7907860]]}
  layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbdbec00], [5, 0x77f7480], [0, 0x7ae0400], [1, 0x7ca7c20]]}
  layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7db4f60]]}
  layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbe7f480]]}
  layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbf130e0], [5, 0x79490c0], [0, 0x7bb4960], [1, 0x7dc5380], [2, 0x7f05a60], [3, 0xbe8f8a0], [4, 0xbf54100], [5, 0x798a0e0], [0, 0x7bf5980], [1, 0x7e063a0], [2, 0x7f46a80], [3, 0xbed08c0], [4, 0xbf95120], [5, 0x79cb100], [0, 0x7c369a0], [1, 0x7e473c0], [2, 0x7f87aa0], [3, 0xbf118e0], [4, 0xbfd6140], [5, 0x7a0c120], [0, 0x7c779c0], [1, 0x7e883e0], [2, 0x7fc8ac0], [3, 0xbf52900], [4, 0xc017160], [5, 0x7a4d140], [0, 0x7cb89e0], [1, 0x7ec9400], [2, 0x8009ae0], [3, 0xbf93920], [4, 0xc058180], [5, 0x7a8e160]]}
  layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7cf9a00], [1, 0x7f0a420], [2, 0x804ab00], [3, 0xbfd4940], [4, 0xc0991a0], [5, 0x7acf180], [0, 0x7d01c20], [1, 0x7f12640]]}
  layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x77b4b20], [3, 0xbb952c0], [4, 0xbbae0e0], [5, 0x75e6960], [0, 0x78d39e0], [1, 0x7a9b200], [2, 0x7836b40], [3, 0xbc172e0], [4, 0xbc30100], [5, 0x7668980], [0, 0x7955a00], [1, 0x7b1d220], [2, 0x78b8b60], [3, 0xbc99300], [4, 0xbcb2120], [5, 0x76ea9a0]]}
  layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbb0e900], [4, 0xbb64620], [5, 0x758eb20], [0, 0x788b780], [1, 0x7a40b20], [2, 0x772d960], [3, 0xbb109a0], [4, 0xbb666c0]]}
  layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7591400]]}
  layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7a45ca0]]}
  layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7732ae0], [3, 0xbb13280], [4, 0xbb68fa0], [5, 0x75a1820], [0, 0x788e8a0], [1, 0x7a560c0], [2, 0x7773b00], [3, 0xbb542a0]]}
  layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbba9fc0], [5, 0x75e2840], [0, 0x78cf8c0], [1, 0x7a970e0]]}
  layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x79beae0], [2, 0x76ab920], [3, 0xbacd8e0], [4, 0xbb23600], [5, 0x754db00], [0, 0x784a760], [1, 0x79ffb00], [2, 0x76ec940]]}
  layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x79d7a20], [1, 0x7b9f240], [2, 0x793ab80], [3, 0xbd1b320]]}
  layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x79dbb40], [1, 0x7ba3360], [2, 0x793eca0], [3, 0xbd1f440], [4, 0xbd34980], [5, 0x776d200], [0, 0x7a1cb60], [1, 0x7be4380]]}
  layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x797fcc0], [3, 0xbd60460], [4, 0xbd759a0], [5, 0x77ae220]]}
  layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7a5db80], [1, 0x7c253a0], [2, 0x7983de0], [3, 0xbd64580], [4, 0xbd79ac0], [5, 0x77b2340], [0, 0x7a9eba0], [1, 0x7c663c0]]}
  layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x79c4e00], [3, 0xbda55a0], [4, 0xbdbaae0], [5, 0x77f3360]]}
  layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x834baa0]]}
  layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc48c4e0]]}
  layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e3dc20], [0, 0x80b3740], [1, 0x82bf020], [2, 0x835bec0], [3, 0xc34ff20], [4, 0xc49c900], [5, 0x7e7ec40], [0, 0x80f4760], [1, 0x8300040], [2, 0x839cee0], [3, 0xc390f40], [4, 0xc4dd920], [5, 0x7ebfc60], [0, 0x8135780], [1, 0x8341060], [2, 0x83ddf00], [3, 0xc3d1f60], [4, 0xc51e940], [5, 0x7f00c80], [0, 0x81767a0], [1, 0x8382080], [2, 0x841ef20], [3, 0xc412f80], [4, 0xc55f960], [5, 0x7f41ca0], [0, 0x81b77c0], [1, 0x83c30a0], [2, 0x845ff40], [3, 0xc453fa0], [4, 0xc5a0980], [5, 0x7f82cc0], [0, 0x81f87e0]]}
  layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x84040c0], [2, 0x84a0f60], [3, 0xc494fc0], [4, 0xc5e19a0], [5, 0x7fc3ce0], [0, 0x8239800], [1, 0x840c2e0], [2, 0x84a9180]]}
  layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc49d1e0], [4, 0xc5e9bc0], [5, 0x7fcbf00], [0, 0x8241a20], [1, 0x8414500], [2, 0x84b13a0], [3, 0xc51f200], [4, 0xc66bbe0], [5, 0x804df20], [0, 0x82c3a40], [1, 0x8496520], [2, 0x85333c0], [3, 0xc5a1220], [4, 0xc6edc00], [5, 0x80cff40], [0, 0x8345a60]]}
  layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8518540], [2, 0x85b53e0], [3, 0xc623240], [4, 0xc76fc20], [5, 0x8151f60], [0, 0x83c7a80], [1, 0x851a5e0], [2, 0x85b7480]]}
  layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x85b9520]]}
  layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc772500]]}
  layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8164420], [0, 0x83ca360], [1, 0x851cec0], [2, 0x85c9940], [3, 0xc626360], [4, 0xc782920], [5, 0x81a5440], [0, 0x840b380]]}
  layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x855dee0], [2, 0x860a960], [3, 0xc667380], [4, 0xc7c3940]]}
  layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x81e6460], [0, 0x844c3a0], [1, 0x8562000], [2, 0x860ea80], [3, 0xc66b4a0], [4, 0xc7c7a60], [5, 0x8227480], [0, 0x848d3c0]]}
  layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x85a3020], [2, 0x864faa0], [3, 0xc6ac4c0], [4, 0xc808a80]]}
  layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbfdcb60], [4, 0xc0a13c0], [5, 0x7ad73a0], [0, 0x7d09e40], [1, 0x7f1a860], [2, 0x8063140], [3, 0xc01db80], [4, 0xc0e23e0]]}
  layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7b183c0], [0, 0x7d4ae60], [1, 0x7f5b880], [2, 0x80a4160]]}
  layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc05eba0], [4, 0xc123400], [5, 0x7b1c4e0], [0, 0x7d4ef80], [1, 0x7f5f9a0], [2, 0x80a8280], [3, 0xc09fbc0], [4, 0xc164420]]}
  layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7b5d500], [0, 0x7d8ffa0], [1, 0x7fa09c0], [2, 0x80e92a0]]}
  layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc0e1420]]}
  layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8052d20]]}
  layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7d971a0], [1, 0x7fa5320], [2, 0x80edc00], [3, 0xc0f1840], [4, 0xc1a64c0], [5, 0x7b64f40], [0, 0x7dd81c0], [1, 0x7fe6340], [2, 0x812ec20], [3, 0xc132860], [4, 0xc1e74e0], [5, 0x7ba5f60], [0, 0x7e191e0], [1, 0x8027360], [2, 0x816fc40], [3, 0xc173880], [4, 0xc228500], [5, 0x7be6f80], [0, 0x7e5a200], [1, 0x8068380], [2, 0x81b0c60], [3, 0xc1b48a0], [4, 0xc269520], [5, 0x7c27fa0], [0, 0x7e9b220], [1, 0x80a93a0], [2, 0x81f1c80], [3, 0xc1f58c0], [4, 0xc2aa540], [5, 0x7c68fc0], [0, 0x7edc240], [1, 0x80ea3c0]]}
  layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8232ca0], [3, 0xc2368e0], [4, 0xc2eb560], [5, 0x7ca9fe0], [0, 0x7f1d260], [1, 0x812b3e0], [2, 0x823aec0], [3, 0xc23eb00]]}
  layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc2f3780], [5, 0x7cb2200], [0, 0x7f25480], [1, 0x8133600], [2, 0x82430e0], [3, 0xc246d20], [4, 0xc3757a0], [5, 0x7d34220], [0, 0x7fa74a0], [1, 0x81b5620], [2, 0x82c5100], [3, 0xc2c8d40], [4, 0xc3f77c0], [5, 0x7db6240], [0, 0x80294c0], [1, 0x8237640]]}
  layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8347120], [3, 0xc34ad60], [4, 0xc4797e0], [5, 0x7e38260], [0, 0x80ab4e0], [1, 0x82b9660], [2, 0x83491c0], [3, 0xc34ce00]]}
  layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc47c0c0]]}
  layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6ee68e0]]}
  layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6baee20], [3, 0xb0019e0], [4, 0xaf8c500], [5, 0x6959b20], [0, 0x6d46280], [1, 0x6ef6d00], [2, 0x6befe40], [3, 0xb042a00]]}
  layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xafcd520], [5, 0x699ab40], [0, 0x6d872a0], [1, 0x6f37d20]]}
  layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c30e60], [3, 0xb083a20], [4, 0xafd1640], [5, 0x699ec60], [0, 0x6d8b3c0], [1, 0x6f3be40], [2, 0x6c71e80], [3, 0xb0c4a40]]}
  layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb012660], [5, 0x69dfc80], [0, 0x6dcc3e0], [1, 0x6f7ce60]]}
  layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb016780], [5, 0x69e3da0], [0, 0x6dd0500], [1, 0x6f80f80], [2, 0x6cb36e0], [3, 0xb1062a0], [4, 0xb0577a0], [5, 0x6a24dc0]]}
  layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e11520], [1, 0x6fc1fa0], [2, 0x6cf4700], [3, 0xb1472c0]]}
  layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb0987c0], [5, 0x6a65de0], [0, 0x6e15640], [1, 0x6fc60c0], [2, 0x6cf8820], [3, 0xb14b3e0], [4, 0xb0d97e0], [5, 0x6aa6e00]]}
  layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e56660], [1, 0x70070e0], [2, 0x6d39840], [3, 0xb18c400]]}
  layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb11b040]]}
  layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e5d860]]}
  layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x700e2e0], [2, 0x6d3e1a0], [3, 0xb190d60], [4, 0xb12b460], [5, 0x6ae8ea0], [0, 0x6e6dc80], [1, 0x704f300], [2, 0x6d7f1c0], [3, 0xb1d1d80], [4, 0xb16c480], [5, 0x6b29ec0], [0, 0x6eaeca0], [1, 0x7090320], [2, 0x6dc01e0], [3, 0xb212da0], [4, 0xb1ad4a0], [5, 0x6b6aee0], [0, 0x6eefcc0], [1, 0x70d1340], [2, 0x6e01200], [3, 0xb253dc0], [4, 0xb1ee4c0], [5, 0x6babf00], [0, 0x6f30ce0], [1, 0x7112360], [2, 0x6e42220], [3, 0xb294de0], [4, 0xb22f4e0], [5, 0x6becf20], [0, 0x6f71d00], [1, 0x7153380], [2, 0x6e83240]]}
  layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb2d5e00], [4, 0xb270500], [5, 0x6c2df40], [0, 0x6fb2d20], [1, 0x71943a0], [2, 0x6ec4260], [3, 0xb2de020], [4, 0xb278720]]}
  layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6c36160], [0, 0x6fbaf40], [1, 0x719c5c0], [2, 0x6ecc480], [3, 0xb2e6240], [4, 0xb280940], [5, 0x6cb8180], [0, 0x703cf60], [1, 0x721e5e0], [2, 0x6f4e4a0], [3, 0xb368260], [4, 0xb302960], [5, 0x6d3a1a0], [0, 0x70bef80], [1, 0x72a0600], [2, 0x6fd04c0]]}
  layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaf70540], [4, 0xaefb060], [5, 0x68cc780], [0, 0x6cb8ee0], [1, 0x6ddff80], [2, 0x6aa84c0], [3, 0xaf725e0], [4, 0xaefd100]]}
  layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xae64ae0]]}
  layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c22960]]}
  layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6cdbf00], [2, 0x69a4440], [3, 0xaeea3e0], [4, 0xae74f00], [5, 0x6846620], [0, 0x6c32d80], [1, 0x6d1cf20], [2, 0x69e5460]]}
  layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaf2b400], [4, 0xaeb5f20], [5, 0x6887640], [0, 0x6c73da0]]}
  layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d5df40], [2, 0x6a26480], [3, 0xaf2f520], [4, 0xaeba040], [5, 0x688b760], [0, 0x6c77ec0], [1, 0x6d9ef60], [2, 0x6a674a0]]}
  layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c1b760], [1, 0x6cd4d00], [2, 0x699fae0], [3, 0xaee5a80]]}
  layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6de2020], [2, 0x6aaa560], [3, 0xaf74680], [4, 0xaeff1a0], [5, 0x68cf060], [0, 0x6cbb7c0], [1, 0x6e23040], [2, 0x6aeb580]]}
  layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xafb56a0], [4, 0xaf401c0], [5, 0x6910080], [0, 0x6cfc7e0]]}
  layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6e64060], [2, 0x6b2c5a0], [3, 0xafb97c0], [4, 0xaf442e0], [5, 0x69141a0], [0, 0x6d00900], [1, 0x6ea5080], [2, 0x6b6d5c0]]}
  layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaffa7e0], [4, 0xaf85300], [5, 0x69551c0], [0, 0x6d41920]]}
  layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x76c65a0]]}
  layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb75ce00]]}
  layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb76faa0], [5, 0x712fd80], [0, 0x74e8000], [1, 0x76d69c0], [2, 0x733b680], [3, 0xb76d220], [4, 0xb7b0ac0], [5, 0x7170da0], [0, 0x7529020], [1, 0x77179e0], [2, 0x737c6a0], [3, 0xb7ae240], [4, 0xb7f1ae0], [5, 0x71b1dc0], [0, 0x756a040], [1, 0x7758a00], [2, 0x73bd6c0], [3, 0xb7ef260], [4, 0xb832b00], [5, 0x71f2de0], [0, 0x75ab060], [1, 0x7799a20], [2, 0x73fe6e0], [3, 0xb830280], [4, 0xb873b20], [5, 0x7233e00], [0, 0x75ec080], [1, 0x77daa40], [2, 0x743f700], [3, 0xb8712a0], [4, 0xb8b4b40], [5, 0x7274e20]]}
  layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x762d0a0], [1, 0x781ba60], [2, 0x7480720], [3, 0xb8b22c0], [4, 0xb8f5b60], [5, 0x72b5e40], [0, 0x76352c0], [1, 0x7823c80]]}
  layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7488940], [3, 0xb8ba4e0], [4, 0xb8fdd80], [5, 0x72be060], [0, 0x763d4e0], [1, 0x782bea0], [2, 0x750a960], [3, 0xb93c500], [4, 0xb97fda0], [5, 0x7340080], [0, 0x76bf500], [1, 0x78adec0], [2, 0x758c980], [3, 0xb9be520], [4, 0xba01dc0], [5, 0x73c20a0]]}
  layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7741520], [1, 0x792fee0], [2, 0x760e9a0], [3, 0xba40540], [4, 0xba83de0], [5, 0x74440c0], [0, 0x77435c0], [1, 0x7931f80]]}
  layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7611280]]}
  layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xba88f60]]}
  layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7449240], [0, 0x7745ea0], [1, 0x7934860], [2, 0x76216a0], [3, 0xba43660], [4, 0xba99380], [5, 0x748a260], [0, 0x7786ec0]]}
  layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7975880], [2, 0x76626c0], [3, 0xba84680], [4, 0xbada3a0]]}
  layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x74cb280], [0, 0x77c7ee0], [1, 0x79799a0], [2, 0x76667e0], [3, 0xba887a0], [4, 0xbade4c0], [5, 0x750c2a0], [0, 0x7808f00]]}
  layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x79ba9c0], [2, 0x76a7800], [3, 0xbac97c0], [4, 0xbb1f4e0]]}
  layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x72d2380], [1, 0x74b1160], [2, 0x71e1020], [3, 0xb602fe0], [4, 0xb58db00], [5, 0x6f5d9c0], [0, 0x73133a0], [1, 0x74f2180]]}
  layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f01260], [0, 0x7286040], [1, 0x74676c0], [2, 0x7197580]]}
  layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb570340], [4, 0xb50aa40], [5, 0x6f05380], [0, 0x728a160], [1, 0x746b7e0], [2, 0x719b6a0], [3, 0xb5b1360], [4, 0xb54ba60]]}
  layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f463a0], [0, 0x72cb180], [1, 0x74ac800], [2, 0x71dc6c0]]}
  layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb5f2bc0]]}
  layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f4d5a0]]}
  layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb3ea280], [4, 0xb384980], [5, 0x6dbc1c0], [0, 0x7140fa0], [1, 0x7322620], [2, 0x70524e0], [3, 0xb42b2a0], [4, 0xb3c59a0], [5, 0x6dfd1e0], [0, 0x7181fc0], [1, 0x7363640], [2, 0x7093500], [3, 0xb46c2c0], [4, 0xb4069c0], [5, 0x6e3e200], [0, 0x71c2fe0], [1, 0x73a4660], [2, 0x70d4520], [3, 0xb4ad2e0], [4, 0xb4479e0], [5, 0x6e7f220], [0, 0x7204000], [1, 0x73e5680], [2, 0x7115540], [3, 0xb4ee300], [4, 0xb488a00], [5, 0x6ec0240], [0, 0x7245020], [1, 0x74266a0], [2, 0x7156560], [3, 0xb52f320], [4, 0xb4c9a20]]}
  layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7222040], [3, 0xb644000], [4, 0xb5ceb20], [5, 0x6f9e9e0], [0, 0x73543c0], [1, 0x75331a0], [2, 0x722a260], [3, 0xb64c220]]}
  layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb5d6d40], [5, 0x6fa6c00], [0, 0x735c5e0], [1, 0x753b3c0], [2, 0x7232480], [3, 0xb654440], [4, 0xb658d60], [5, 0x7028c20], [0, 0x73de600], [1, 0x75bd3e0], [2, 0x72b44a0], [3, 0xb6d6460], [4, 0xb6dad80], [5, 0x70aac40], [0, 0x7460620], [1, 0x763f400]]}
  layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x73364c0], [3, 0xb758480], [4, 0xb75cda0], [5, 0x712cc60], [0, 0x74e2640], [1, 0x76c1420], [2, 0x7338560], [3, 0xb75a520]]}
  layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb75f680]]}
  layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x92d82c0]]}
  layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x908f000], [1, 0x90759c0], [2, 0x9210f00], [3, 0xd376240], [4, 0xd45eb80], [5, 0x8e753a0], [0, 0x90d0020], [1, 0x90b69e0]]}
  layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x94f6040], [1, 0x94b1760], [2, 0x96083c0], [3, 0xd772840]]}
  layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd8abd60], [5, 0x92e86e0], [0, 0x94fa160], [1, 0x94b5880], [2, 0x960c4e0], [3, 0xd776960], [4, 0xd8ecd80], [5, 0x9329700]]}
  layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x953b180], [1, 0x94f68a0], [2, 0x964d500], [3, 0xd7b7980]]}
  layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x953f2a0], [1, 0x94fa9c0], [2, 0x9651620], [3, 0xd7bbaa0], [4, 0xd92e5e0], [5, 0x936af60], [0, 0x95802c0], [1, 0x953b9e0]]}
  layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9692640], [3, 0xd7fcac0], [4, 0xd96f600], [5, 0x93abf80]]}
  layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x95c12e0], [1, 0x957ca00], [2, 0x9696760], [3, 0xd800be0], [4, 0xd973720], [5, 0x93b00a0], [0, 0x9602300], [1, 0x95bda20]]}
  layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x94f1f20], [1, 0x94ad640], [2, 0x96042a0], [3, 0xd76e720]]}
  layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd842440]]}
  layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x93f41a0]]}
  layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9646400], [1, 0x95ff280], [2, 0x96d8800], [3, 0xd852860], [4, 0xd9b57c0], [5, 0x94045c0], [0, 0x9687420], [1, 0x96402a0], [2, 0x9719820], [3, 0xd893880], [4, 0xd9f67e0], [5, 0x94455e0], [0, 0x96c8440], [1, 0x96812c0], [2, 0x975a840], [3, 0xd8d48a0], [4, 0xda37800], [5, 0x9486600], [0, 0x9709460], [1, 0x96c22e0], [2, 0x979b860], [3, 0xd9158c0], [4, 0xda78820], [5, 0x94c7620], [0, 0x974a480], [1, 0x9703300], [2, 0x97dc880], [3, 0xd9568e0], [4, 0xdab9840], [5, 0x9508640], [0, 0x978b4a0], [1, 0x9744320]]}
  layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x981d8a0], [3, 0xd997900], [4, 0xdafa860], [5, 0x9549660], [0, 0x97cc4c0], [1, 0x9785340], [2, 0x9825ac0], [3, 0xd99fb20]]}
  layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd684460], [5, 0x90c3680], [0, 0x92e1400], [1, 0x929cb20], [2, 0x93f7880], [3, 0xd55f460], [4, 0xd706480], [5, 0x91456a0], [0, 0x9363420], [1, 0x931eb40], [2, 0x94798a0], [3, 0xd5e1480], [4, 0xd7884a0], [5, 0x91c76c0], [0, 0x93e5440], [1, 0x93a0b60]]}
  layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x903ccc0], [0, 0x9297940], [1, 0x9244ce0], [2, 0x93af620], [3, 0xd504d80], [4, 0xd5fd2a0], [5, 0x903ed60], [0, 0x92999e0]]}
  layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x92475c0]]}
  layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd509f00]]}
  layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd602420], [5, 0x9041640], [0, 0x929c2c0], [1, 0x92579e0], [2, 0x93b2740], [3, 0xd51a320], [4, 0xd643440], [5, 0x9082660]]}
  layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x92dd2e0], [1, 0x9298a00], [2, 0x93f3760], [3, 0xd55b340]]}
  layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd482d40], [4, 0xd57b260], [5, 0x8ffbca0], [0, 0x9256920], [1, 0x9203cc0], [2, 0x936e600], [3, 0xd4c3d60], [4, 0xd5bc280]]}
  layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x94fb8c0], [3, 0xd6634a0], [4, 0xd80a4c0], [5, 0x92496e0]]}
  layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x94ff9e0], [3, 0xd6675c0], [4, 0xd80e5e0], [5, 0x924d800], [0, 0x9467ca0], [1, 0x94233c0], [2, 0x9540a00], [3, 0xd6a85e0]]}
  layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd84f600], [5, 0x928e820], [0, 0x94a8cc0], [1, 0x94643e0]]}
  layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9581a20], [3, 0xd6e9600], [4, 0xd853720], [5, 0x9292940], [0, 0x94acde0], [1, 0x9468500], [2, 0x95c2a40], [3, 0xd72a620]]}
  layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd894740], [5, 0x92d3960], [0, 0x94ede00], [1, 0x94a9520]]}
  layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9af04c0]]}
  layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xdc5b980]]}
  layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xde6c3a0], [5, 0x98c1320], [0, 0x9b3a720], [1, 0x9b008e0], [2, 0x9ad5e60], [3, 0xdc6bda0], [4, 0xdead3c0], [5, 0x9902340], [0, 0x9b7b740], [1, 0x9b41900], [2, 0x9b16e80], [3, 0xdcacdc0], [4, 0xdeee3e0], [5, 0x9943360], [0, 0x9bbc760], [1, 0x9b82920], [2, 0x9b57ea0], [3, 0xdcedde0], [4, 0xdf2f400], [5, 0x9984380], [0, 0x9bfd780], [1, 0x9bc3940], [2, 0x9b98ec0], [3, 0xdd2ee00], [4, 0xdf70420], [5, 0x99c53a0], [0, 0x9c3e7a0], [1, 0x9c04960], [2, 0x9bd9ee0], [3, 0xdd6fe20], [4, 0xdfb1440], [5, 0x9a063c0]]}
  layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9c7f7c0], [1, 0x9c45980], [2, 0x9c1af00], [3, 0xddb0e40], [4, 0xdff2460], [5, 0x9a473e0], [0, 0x9c879e0], [1, 0x9c4dba0]]}
  layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9c23120], [3, 0xddb9060], [4, 0xdffa680], [5, 0x9a4f600], [0, 0x9c8fc00], [1, 0x9c55dc0], [2, 0x9ca5140], [3, 0xde3b080], [4, 0xe07c6a0], [5, 0x9ad1620], [0, 0x9d11c20], [1, 0x9cd7de0], [2, 0x9d27160], [3, 0xdebd0a0], [4, 0xe0fe6c0], [5, 0x9b53640]]}
  layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9d93c40], [1, 0x9d59e00], [2, 0x9da9180], [3, 0xdf3f0c0], [4, 0xe1806e0], [5, 0x9bd5660], [0, 0x9d95ce0], [1, 0x9d5bea0]]}
  layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e62820]]}
  layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe24e260]]}
  layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9eb33c0], [3, 0xe049300], [4, 0xe20d240], [5, 0x9c61180], [0, 0x9e21800], [1, 0x9de8a00], [2, 0x9ef43e0], [3, 0xe08a320]]}
  layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe209120], [5, 0x9c5d060], [0, 0x9e1d6e0], [1, 0x9de48e0]]}
  layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9e31380], [3, 0xdfc72c0], [4, 0xe1c8100], [5, 0x9c1c040], [0, 0x9ddc6c0], [1, 0x9da38c0], [2, 0x9e723a0], [3, 0xe0082e0]]}
  layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9d9ef60], [2, 0x9e2d260], [3, 0xdfc31a0], [4, 0xe1c3fe0]]}
  layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9dab220], [3, 0xdf41160], [4, 0xe182780], [5, 0x9bd7700], [0, 0x9d97d80], [1, 0x9d5df40], [2, 0x9dec240], [3, 0xdf82180]]}
  layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9919780], [1, 0x98d2600], [2, 0x9972d80], [3, 0xdaecde0]]}
  layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9924aa0], [1, 0x98db080], [2, 0x997b800], [3, 0xdb01b60], [4, 0xdc8a400], [5, 0x96eef60], [0, 0x9965ac0], [1, 0x991c0a0]]}
  layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x96d7940], [0, 0x991d8a0], [1, 0x98d6720], [2, 0x9976ea0]]}
  layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xdaf1740]]}
  layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x96deb40]]}
  layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdb02a80], [5, 0x9551880], [0, 0x97d46e0], [1, 0x978d560], [2, 0x982dce0], [3, 0xd9a7d40], [4, 0xdb43aa0], [5, 0x95928a0], [0, 0x9815700], [1, 0x97ce580], [2, 0x986ed00], [3, 0xd9e8d60], [4, 0xdb84ac0], [5, 0x95d38c0], [0, 0x9856720], [1, 0x980f5a0], [2, 0x98afd20], [3, 0xda29d80], [4, 0xdbc5ae0], [5, 0x96148e0], [0, 0x9897740], [1, 0x98505c0], [2, 0x98f0d40], [3, 0xda6ada0], [4, 0xdc06b00], [5, 0x9655900], [0, 0x98d8760], [1, 0x98915e0], [2, 0x9931d60], [3, 0xdaabdc0], [4, 0xdc47b20], [5, 0x9696920]]}
  layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x99bc820], [3, 0xdb42b80], [4, 0xdccb420], [5, 0x972ff80], [0, 0x99a6ae0], [1, 0x995d0c0], [2, 0x99c4a40], [3, 0xdb4ada0]]}
  layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdcd3640], [5, 0x97381a0], [0, 0x99aed00], [1, 0x99652e0], [2, 0x99ccc60], [3, 0xdb52fc0], [4, 0xdd55660], [5, 0x97ba1c0], [0, 0x9a30d20], [1, 0x99e7300], [2, 0x9a4ec80], [3, 0xdbd4fe0], [4, 0xddd7680], [5, 0x983c1e0], [0, 0x9ab2d40], [1, 0x9a69320]]}
  layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9ad0ca0], [3, 0xdc57000], [4, 0xde596a0], [5, 0x98be200], [0, 0x9b34d60], [1, 0x9aeb340], [2, 0x9ad2d40], [3, 0xdc590a0]]}
  layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xde5bf80]]}
  layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd89b940]]}
  layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x866db80], [1, 0x86bf7a0], [2, 0x877e6a0], [3, 0xc842a40], [4, 0xc99c760], [5, 0x83f88a0], [0, 0x86aeba0], [1, 0x87007c0]]}
  layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x87bf6c0], [3, 0xc883a60], [4, 0xc9dd780], [5, 0x84398c0]]}
  layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x86efbc0], [1, 0x87417e0], [2, 0x87c37e0], [3, 0xc887b80], [4, 0xc9e18a0], [5, 0x843d9e0], [0, 0x8730be0], [1, 0x8782800]]}
  layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8804800], [3, 0xc8c8ba0], [4, 0xca228c0], [5, 0x847ea00]]}
  layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8808920], [3, 0xc8cccc0], [4, 0xca269e0], [5, 0x8482b20], [0, 0x8772440], [1, 0x87c4060], [2, 0x8849940], [3, 0xc90dce0]]}
  layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xca67a00], [5, 0x84c3b40], [0, 0x87b3460], [1, 0x8805080]]}
  layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x888a960], [3, 0xc94ed00], [4, 0xca6bb20], [5, 0x84c7c60], [0, 0x87b7580], [1, 0x88091a0], [2, 0x88cb980], [3, 0xc98fd20]]}
  layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcaacb40], [5, 0x8508c80], [0, 0x87f85a0], [1, 0x884a1c0]]}
  layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x890d1e0]]}
  layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcab3d40]]}
  layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x850fe80], [0, 0x87fcf00], [1, 0x884eb20], [2, 0x891d600], [3, 0xc9e19a0], [4, 0xcac4160], [5, 0x8550ea0], [0, 0x883df20], [1, 0x888fb40], [2, 0x895e620], [3, 0xca229c0], [4, 0xcb05180], [5, 0x8591ec0], [0, 0x887ef40], [1, 0x88d0b60], [2, 0x899f640], [3, 0xca639e0], [4, 0xcb461a0], [5, 0x85d2ee0], [0, 0x88bff60], [1, 0x8911b80], [2, 0x89e0660], [3, 0xcaa4a00], [4, 0xcb871c0], [5, 0x8613f00], [0, 0x8900f80], [1, 0x8952ba0], [2, 0x8a21680], [3, 0xcae5a20], [4, 0xcbc81e0], [5, 0x8654f20], [0, 0x8941fa0]]}
  layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8993bc0], [2, 0x8a626a0], [3, 0xcb26a40], [4, 0xcc09200], [5, 0x8695f40], [0, 0x8982fc0], [1, 0x899bde0], [2, 0x8a6a8c0]]}
  layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xcb2ec60], [4, 0xcc11420], [5, 0x869e160], [0, 0x898b1e0], [1, 0x89a4000], [2, 0x8a72ae0], [3, 0xcbb0c80], [4, 0xcc93440], [5, 0x8720180], [0, 0x8a0d200], [1, 0x8a26020], [2, 0x8af4b00], [3, 0xcc32ca0], [4, 0xcd15460], [5, 0x87a21a0], [0, 0x8a8f220]]}
  layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8aa8040], [2, 0x8b76b20], [3, 0xccb4cc0], [4, 0xcd97480], [5, 0x88241c0], [0, 0x8b11240], [1, 0x8aaa0e0], [2, 0x8b78bc0]]}
  layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x84cec20]]}
  layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8656ca0]]}
  layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc6b36c0], [4, 0xc80d3e0], [5, 0x8269520], [0, 0x84df040], [1, 0x85a81c0], [2, 0x86670c0], [3, 0xc6f46e0], [4, 0xc84e400]]}
  layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82aa540], [0, 0x8520060], [1, 0x85e91e0], [2, 0x86a80e0]]}
  layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc735700], [4, 0xc88f420], [5, 0x82ae660], [0, 0x8524180], [1, 0x85ed300], [2, 0x86ac200], [3, 0xc776720], [4, 0xc8d0440]]}
  layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82ef680], [0, 0x85651a0], [1, 0x862e320], [2, 0x86ed220]]}
  layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82f37a0], [0, 0x85692c0], [1, 0x8632440], [2, 0x86f1340], [3, 0xc7b7f80], [4, 0xc911ca0], [5, 0x83347c0], [0, 0x85aa2e0]]}
  layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8673460], [2, 0x8732360], [3, 0xc7f8fa0], [4, 0xc952cc0]]}
  layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x83757e0], [0, 0x85eb300], [1, 0x8677580], [2, 0x8736480], [3, 0xc7fd0c0], [4, 0xc956de0], [5, 0x83b6800], [0, 0x862c320]]}
  layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x86b85a0], [2, 0x87774a0], [3, 0xc83e0e0], [4, 0xc997e00]]}
  layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc9d0d40]]}
  layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8f2aee0]]}
  layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd05cd80], [4, 0xd1456c0], [5, 0x8c17500], [0, 0x8e319a0], [1, 0x8d9fdc0], [2, 0x8f3b300], [3, 0xd09dda0], [4, 0xd1866e0], [5, 0x8c58520], [0, 0x8e729c0], [1, 0x8de0de0], [2, 0x8f7c320], [3, 0xd0dedc0], [4, 0xd1c7700], [5, 0x8c99540], [0, 0x8eb39e0], [1, 0x8e21e00], [2, 0x8fbd340], [3, 0xd11fde0], [4, 0xd208720], [5, 0x8cda560], [0, 0x8ef4a00], [1, 0x8e62e20], [2, 0x8ffe360], [3, 0xd160e00], [4, 0xd249740], [5, 0x8d1b580], [0, 0x8f35a20], [1, 0x8ea3e40], [2, 0x903f380], [3, 0xd1a1e20], [4, 0xd28a760]]}
  layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8d5c5a0], [0, 0x8f76a40], [1, 0x8ee4e60], [2, 0x90803a0], [3, 0xd1e2e40], [4, 0xd2cb780], [5, 0x8d647c0], [0, 0x8f7ec60]]}
  layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8eed080], [2, 0x90885c0], [3, 0xd1eb060], [4, 0xd2d39a0], [5, 0x8d6c9e0], [0, 0x8f86e80], [1, 0x8f6f0a0], [2, 0x910a5e0], [3, 0xd26d080], [4, 0xd3559c0], [5, 0x8deea00], [0, 0x9008ea0], [1, 0x8ff10c0], [2, 0x918c600], [3, 0xd2ef0a0], [4, 0xd3d79e0]]}
  layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8e70a20], [0, 0x908aec0], [1, 0x90730e0], [2, 0x920e620], [3, 0xd3710c0], [4, 0xd459a00], [5, 0x8e72ac0], [0, 0x908cf60]]}
  layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9251f20]]}
  layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd49fba0]]}
  layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8eb63c0], [0, 0x9111040], [1, 0x90f7a00], [2, 0x9262340], [3, 0xd3b7aa0], [4, 0xd4affc0], [5, 0x8ef73e0], [0, 0x9152060]]}
  layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9138a20], [2, 0x92a3360], [3, 0xd3f8ac0], [4, 0xd4f0fe0]]}
  layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8f38400], [0, 0x9193080], [1, 0x913cb40], [2, 0x92a7480], [3, 0xd3fcbe0], [4, 0xd4f5100], [5, 0x8f79420], [0, 0x91d40a0]]}
  layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x917db60], [2, 0x92e84a0], [3, 0xd43dc00], [4, 0xd536120]]}
  layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9181c80], [2, 0x92ec5c0], [3, 0xd441d20], [4, 0xd53a240], [5, 0x8fbac80], [0, 0x9215900], [1, 0x91c2ca0], [2, 0x932d5e0]]}
  layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcf34380], [5, 0x8a061c0], [0, 0x8ce3660], [1, 0x8c51a80]]}
  layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x882e480], [0, 0x8b1b500], [1, 0x8ab43a0], [2, 0x8b82e80], [3, 0xccc71a0], [4, 0xcda9960], [5, 0x886f4a0], [0, 0x8b5c520]]}
  layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8af53c0], [2, 0x8bc3ea0], [3, 0xcd081c0], [4, 0xcdea980]]}
  layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x88b0d00]]}
  layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8afc5c0]]}
  layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8bcb0a0], [3, 0xcd0cb20], [4, 0xcdef2e0], [5, 0x88c1120], [0, 0x8b9e5c0], [1, 0x8b0c9e0], [2, 0x8c0c0c0], [3, 0xcd4db40], [4, 0xce30300], [5, 0x8902140], [0, 0x8bdf5e0], [1, 0x8b4da00], [2, 0x8c4d0e0], [3, 0xcd8eb60], [4, 0xce71320], [5, 0x8943160], [0, 0x8c20600], [1, 0x8b8ea20], [2, 0x8c8e100], [3, 0xcdcfb80], [4, 0xceb2340], [5, 0x8984180], [0, 0x8c61620], [1, 0x8bcfa40], [2, 0x8ccf120], [3, 0xce10ba0], [4, 0xcef3360], [5, 0x89c51a0], [0, 0x8ca2640], [1, 0x8c10a60], [2, 0x8d10140], [3, 0xce51bc0]]}
  layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xccb6d60], [4, 0xcd99520], [5, 0x8826260], [0, 0x8b132e0], [1, 0x8aac180], [2, 0x8b7ac60], [3, 0xccbef80], [4, 0xcda1740]]}
  layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8d51160], [3, 0xce92be0], [4, 0xcf384a0], [5, 0x8a0a2e0], [0, 0x8ce7780], [1, 0x8c55ba0], [2, 0x8dd3180], [3, 0xcf14c00], [4, 0xcfba4c0], [5, 0x8a8c300], [0, 0x8d697a0], [1, 0x8cd7bc0], [2, 0x8e551a0], [3, 0xcf96c20], [4, 0xd03c4e0], [5, 0x8b0e320]]}
  layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8deb7c0], [1, 0x8d59be0], [2, 0x8ed71c0], [3, 0xd018c40], [4, 0xd0be500], [5, 0x8b90340], [0, 0x8ded860], [1, 0x8d5bc80]]}
  layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8ed9aa0]]}
  layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8154000]]}
  layer.12.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd0c3680], [5, 0x8b954c0], [0, 0x8df0140], [1, 0x8d5e560], [2, 0x8ee9ec0], [3, 0xd01bd60], [4, 0xd1046a0], [5, 0x8bd64e0]]}
  layer.12.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40e7200], [3, 0x83ae460], [4, 0x8347b20], [5, 0x408a320]]}
  layer.12.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x89164e0], [5, 0x4605040], [0, 0x46ef200], [1, 0x473e580], [2, 0x46be600], [3, 0x89417a0], [4, 0x8957500], [5, 0x4646060]]}
  layer.12.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4730220], [1, 0x477f5a0], [2, 0x46ff620], [3, 0x89827c0]]}
  layer.12.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4734340], [1, 0x47836c0], [2, 0x4703740], [3, 0x89868e0], [4, 0x8998d60], [5, 0x46878c0], [0, 0x4775360], [1, 0x47c46e0]]}
  layer.12.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4744760], [3, 0x89c7900], [4, 0x89d9d80], [5, 0x46c88e0]]}
  layer.12.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x47b6380], [1, 0x4805700], [2, 0x4748880], [3, 0x89cba20], [4, 0x89ddea0], [5, 0x46cca00], [0, 0x47f73a0], [1, 0x4846720]]}
  layer.12.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x47898a0], [3, 0x8a0ca40], [4, 0x8a1eec0], [5, 0x470da20]]}
  layer.12.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4838c00]]}
  layer.12.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4790aa0]]}
  layer.12.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8a13c40], [4, 0x8a23820], [5, 0x4712380], [0, 0x4849020], [1, 0x48983a0], [2, 0x47a0ec0], [3, 0x8a54c60], [4, 0x8a64840], [5, 0x47533a0], [0, 0x488a040], [1, 0x48d93c0], [2, 0x47e1ee0], [3, 0x8a95c80], [4, 0x8aa5860], [5, 0x47943c0], [0, 0x48cb060], [1, 0x491a3e0], [2, 0x4822f00], [3, 0x8ad6ca0], [4, 0x8ae6880], [5, 0x47d53e0], [0, 0x490c080], [1, 0x495b400], [2, 0x4863f20], [3, 0x8b17cc0], [4, 0x8b278a0], [5, 0x4816400], [0, 0x494d0a0], [1, 0x499c420], [2, 0x48a4f40], [3, 0x8b58ce0], [4, 0x8b688c0]]}
  layer.12.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4857420], [0, 0x498e0c0], [1, 0x49dd440], [2, 0x48e5f60], [3, 0x8b99d00], [4, 0x8ba98e0], [5, 0x485f640], [0, 0x49962e0]]}
  layer.12.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x49e5660], [2, 0x48ee180], [3, 0x8ba1f20], [4, 0x8bb1b00], [5, 0x4867860], [0, 0x499e500], [1, 0x4a67680], [2, 0x49701a0], [3, 0x8c23f40], [4, 0x8c33b20], [5, 0x48e9880], [0, 0x4a20520], [1, 0x4ae96a0], [2, 0x49f21c0], [3, 0x8ca5f60], [4, 0x8cb5b40]]}
  layer.12.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x496b8a0], [0, 0x4aa2540], [1, 0x4b6b6c0], [2, 0x4a741e0], [3, 0x8d27f80], [4, 0x8d37b60], [5, 0x496d940], [0, 0x4aa45e0]]}
  layer.12.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x449d660]]}
  layer.12.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x87adac0]]}
  layer.13.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4434ca0], [0, 0x451ee60], [1, 0x456e1e0], [2, 0x44ada80], [3, 0x87a8180], [4, 0x87bdee0], [5, 0x4475cc0], [0, 0x455fe80]]}
  layer.13.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x45af200], [2, 0x44eeaa0], [3, 0x87e91a0], [4, 0x87fef00]]}
  layer.13.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x44b6ce0], [0, 0x45a0ea0], [1, 0x45b3320], [2, 0x44f2bc0], [3, 0x87ed2c0], [4, 0x8803020], [5, 0x44f7d00], [0, 0x45e1ec0]]}
  layer.13.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x45f4340], [2, 0x4533be0], [3, 0x882e2e0], [4, 0x8844040]]}
  layer.13.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x45f8460], [2, 0x4537d00], [3, 0x8832400], [4, 0x8848160], [5, 0x4539560], [0, 0x4623720], [1, 0x4639480], [2, 0x4578d20]]}
  layer.13.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8873420], [4, 0x8889180], [5, 0x457a580], [0, 0x4664740]]}
  layer.13.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x467a4a0], [2, 0x45b9d40], [3, 0x8877540], [4, 0x888d2a0], [5, 0x457e6a0], [0, 0x4668860], [1, 0x46bb4c0], [2, 0x45fad60]]}
  layer.13.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88b8560], [4, 0x88ce2c0], [5, 0x45bf6c0], [0, 0x46a9880]]}
  layer.13.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4887740]]}
  layer.13.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4c21dc0]]}
  layer.13.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4e158e0], [1, 0x4ed2760], [2, 0x4db0800], [3, 0x9099ac0], [4, 0x90172a0], [5, 0x4c321e0], [0, 0x4e56900], [1, 0x4f13780], [2, 0x4df1820], [3, 0x90daae0], [4, 0x90582c0], [5, 0x4c73200], [0, 0x4e97920], [1, 0x4f547a0], [2, 0x4e32840], [3, 0x911bb00], [4, 0x90992e0], [5, 0x4cb4220], [0, 0x4ed8940], [1, 0x4f957c0], [2, 0x4e73860], [3, 0x915cb20], [4, 0x90da300], [5, 0x4cf5240], [0, 0x4f19960], [1, 0x4fd67e0], [2, 0x4eb4880], [3, 0x919db40], [4, 0x911b320], [5, 0x4d36260], [0, 0x4f5a980], [1, 0x5017800]]}
  layer.13.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4ef58a0], [3, 0x91deb60], [4, 0x915c340], [5, 0x4d77280], [0, 0x4f9b9a0], [1, 0x5058820], [2, 0x4efdac0], [3, 0x91e6d80]]}
  layer.13.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9164560], [5, 0x4d7f4a0], [0, 0x4fa3bc0], [1, 0x5060a40], [2, 0x4f05ce0], [3, 0x91eefa0], [4, 0x91e6580], [5, 0x4e014c0], [0, 0x5025be0], [1, 0x50e2a60], [2, 0x4f87d00], [3, 0x9270fc0], [4, 0x92685a0], [5, 0x4e834e0], [0, 0x50a7c00], [1, 0x5164a80]]}
  layer.13.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5009d20], [3, 0x92f2fe0], [4, 0x92ea5c0], [5, 0x4f05500], [0, 0x5129c20], [1, 0x51e6aa0], [2, 0x500bdc0], [3, 0x92f5080]]}
  layer.13.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4f48e00]]}
  layer.13.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x522cc40]]}
  layer.14.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x504f6c0], [3, 0x9379160], [4, 0x936eee0], [5, 0x4f59220], [0, 0x5170600], [1, 0x523d060], [2, 0x50906e0], [3, 0x93ba180]]}
  layer.14.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x93aff00], [5, 0x4f9a240], [0, 0x51b1620], [1, 0x527e080]]}
  layer.14.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x50d1700], [3, 0x93fb1a0], [4, 0x93b4020], [5, 0x4f9e360], [0, 0x51b5740], [1, 0x52821a0], [2, 0x5112720], [3, 0x943c1c0]]}
  layer.14.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x93f5040], [5, 0x4fdf380], [0, 0x51f6760], [1, 0x52c31c0]]}
  layer.14.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x93f9160], [5, 0x4fe34a0], [0, 0x51fa880], [1, 0x52c72e0], [2, 0x5153f80], [3, 0x947da20], [4, 0x943a180], [5, 0x50244c0]]}
  layer.14.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4c110e0], [3, 0x8f09f80], [4, 0x8f09f80], [5, 0x4b152e0]]}
  layer.14.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8d32240], [4, 0x8d41e20], [5, 0x4977c00], [0, 0x4aae8a0], [1, 0x4b7dba0], [2, 0x4a866c0], [3, 0x8d73260], [4, 0x8d82e40]]}
  layer.14.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x49b8c20], [0, 0x4aef8c0], [1, 0x4bbebc0], [2, 0x4ac76e0]]}
  layer.14.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8db4ac0]]}
  layer.14.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x49bfe20]]}
  layer.14.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4af6ac0], [1, 0x4bc3520], [2, 0x4acc040], [3, 0x8dc4ee0], [4, 0x8dc4ee0], [5, 0x49d0240], [0, 0x4b37ae0], [1, 0x4c04540], [2, 0x4b0d060], [3, 0x8e05f00], [4, 0x8e05f00], [5, 0x4a11260], [0, 0x4b78b00], [1, 0x4c45560], [2, 0x4b4e080], [3, 0x8e46f20], [4, 0x8e46f20], [5, 0x4a52280], [0, 0x4bb9b20], [1, 0x4c86580], [2, 0x4b8f0a0], [3, 0x8e87f40], [4, 0x8e87f40], [5, 0x4a932a0], [0, 0x4bfab40], [1, 0x4cc75a0], [2, 0x4bd00c0], [3, 0x8ec8f60], [4, 0x8ec8f60], [5, 0x4ad42c0], [0, 0x4c3bb60], [1, 0x4d085c0]]}
  layer.14.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4b6d760], [2, 0x4a76280], [3, 0x8d2a020], [4, 0x8d39c00], [5, 0x496f9e0], [0, 0x4aa6680], [1, 0x4b75980], [2, 0x4a7e4a0]]}
  layer.14.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4c7cb80], [1, 0x4d495e0], [2, 0x4c15200], [3, 0x8f0e0a0], [4, 0x8f0e0a0], [5, 0x4b19400], [0, 0x4cfeba0], [1, 0x4dcb600], [2, 0x4c97220], [3, 0x8f900c0], [4, 0x8f900c0], [5, 0x4b9b420], [0, 0x4d80bc0], [1, 0x4e4d620], [2, 0x4d19240], [3, 0x90120e0]]}
  layer.14.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x90120e0], [5, 0x4c1d440], [0, 0x4e02be0], [1, 0x4ecf640], [2, 0x4d9b260], [3, 0x9094100], [4, 0x9014180], [5, 0x4c1f4e0]]}
  layer.14.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4e054c0]]}
  layer.14.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4da03e0]]}
  layer.15.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92f7120], [4, 0x92ecea0], [5, 0x4f07de0], [0, 0x512eda0], [1, 0x51ebc20], [2, 0x500e6a0], [3, 0x9338140], [4, 0x932dec0]]}
  layer.15.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c43480], [5, 0x3a69480], [0, 0x3a798a0], [1, 0x39f2760]]}
  layer.15.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c475a0], [5, 0x3a6d5a0], [0, 0x3a7d9c0], [1, 0x39f6880], [2, 0x3a05420], [3, 0x7c56980], [4, 0x7c885c0], [5, 0x3aae5c0]]}
  layer.15.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3abe9e0], [1, 0x3a378a0], [2, 0x3a46440], [3, 0x7c979a0]]}
  layer.15.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ac2b00], [1, 0x3a3b9c0], [2, 0x3a4a560], [3, 0x7c9bac0], [4, 0x7cc9e20], [5, 0x3aefe20], [0, 0x3b03b20], [1, 0x3a7c9e0]]}
  layer.15.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a8b580], [3, 0x7cdcae0], [4, 0x7d0ae40], [5, 0x3b30e40]]}
  layer.15.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3b44b40], [1, 0x3abda00], [2, 0x3a8f6a0], [3, 0x7ce0c00], [4, 0x7d0ef60], [5, 0x3b34f60], [0, 0x3b85b60], [1, 0x3afea20]]}
  layer.15.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ad06c0], [3, 0x7d21c20], [4, 0x7d4ff80], [5, 0x3b75f80]]}
  layer.15.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3bc73c0]]}
  layer.15.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ad78c0]]}
  layer.15.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d28e20], [4, 0x7d548e0], [5, 0x3b7a8e0], [0, 0x3bd77e0], [1, 0x3b40ac0], [2, 0x3ae7ce0], [3, 0x7d69e40], [4, 0x7d95900], [5, 0x3bbb900], [0, 0x3c18800], [1, 0x3b81ae0], [2, 0x3b28d00], [3, 0x7daae60], [4, 0x7dd6920], [5, 0x3bfc920], [0, 0x3c59820], [1, 0x3bc2b00], [2, 0x3b69d20], [3, 0x7debe80], [4, 0x7e17940], [5, 0x3c3d940], [0, 0x3c9a840], [1, 0x3c03b20], [2, 0x3baad40], [3, 0x7e2cea0], [4, 0x7e58960], [5, 0x3c7e960], [0, 0x3cdb860], [1, 0x3c44b40], [2, 0x3bebd60], [3, 0x7e6dec0], [4, 0x7e99980]]}
  layer.15.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3cbf980], [0, 0x3d1c880], [1, 0x3c85b60], [2, 0x3c2cd80], [3, 0x7eaeee0], [4, 0x7eda9a0], [5, 0x3cc7ba0], [0, 0x3d24aa0]]}
  layer.15.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c8dd80], [2, 0x3c34fa0], [3, 0x7eb7100], [4, 0x7ee2bc0], [5, 0x3ccfdc0], [0, 0x3d2ccc0], [1, 0x3d0fda0], [2, 0x3cb6fc0], [3, 0x7f39120], [4, 0x7f64be0], [5, 0x3d51de0], [0, 0x3daece0], [1, 0x3d91dc0], [2, 0x3d38fe0], [3, 0x7fbb140], [4, 0x7fe6c00]]}
  layer.15.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3dd3e00], [0, 0x3e30d00], [1, 0x3e13de0], [2, 0x3dbb000], [3, 0x803d160], [4, 0x8068c20], [5, 0x3dd5ea0], [0, 0x3e32da0]]}
  layer.15.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da940]]}
  layer.15.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38dd1e0]]}
  layer.16.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab71e0], [4, 0x7ab4940], [5, 0x38da940], [0, 0x38ead60], [1, 0x38db180], [2, 0x38ed600], [3, 0x7af8200], [4, 0x7af5960]]}
  layer.16.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x391b960], [0, 0x392bd80], [1, 0x391c1a0], [2, 0x392e620]]}
  layer.16.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b39220], [4, 0x7b36980], [5, 0x391fa80], [0, 0x392fea0], [1, 0x39202c0], [2, 0x3932740], [3, 0x7b7a240], [4, 0x7b779a0]]}
  layer.16.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3960aa0], [0, 0x3970ec0], [1, 0x39612e0], [2, 0x3973760]]}
  layer.16.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3964bc0], [0, 0x3974fe0], [1, 0x3965400], [2, 0x3977880], [3, 0x7bbbaa0], [4, 0x7bb9200], [5, 0x39a5be0], [0, 0x39b6000]]}
  layer.16.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39a6420], [2, 0x39b88a0], [3, 0x7bfcac0], [4, 0x7bfa220]]}
  layer.16.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39e6c00], [0, 0x39f7020], [1, 0x39aa540], [2, 0x39bc9c0], [3, 0x7c00be0], [4, 0x7bfe340], [5, 0x3a27c20], [0, 0x3a38040]]}
  layer.16.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39eb560], [2, 0x39fd9e0], [3, 0x7c41c00], [4, 0x7c3f360]]}
  layer.16.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4401f40]]}
  layer.16.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40eb320]]}
  layer.16.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x83b2580], [4, 0x834bc40], [5, 0x408e440], [0, 0x41a48e0], [1, 0x417b6c0], [2, 0x40fb740], [3, 0x83f35a0], [4, 0x838cc60], [5, 0x40cf460], [0, 0x41e5900], [1, 0x41bc6e0], [2, 0x413c760], [3, 0x84345c0], [4, 0x83cdc80], [5, 0x4110480], [0, 0x4226920], [1, 0x41fd700], [2, 0x417d780], [3, 0x84755e0], [4, 0x840eca0], [5, 0x41514a0], [0, 0x4267940], [1, 0x423e720], [2, 0x41be7a0], [3, 0x84b6600], [4, 0x844fcc0], [5, 0x41924c0], [0, 0x42a8960], [1, 0x427f740], [2, 0x41ff7c0], [3, 0x84f7620], [4, 0x8490ce0]]}
  layer.16.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41d34e0], [0, 0x42e9980], [1, 0x42c0760], [2, 0x42407e0], [3, 0x8538640], [4, 0x84d1d00], [5, 0x41db700], [0, 0x42f1ba0]]}
  layer.16.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42c8980], [2, 0x4248a00], [3, 0x8540860], [4, 0x84d9f20], [5, 0x41e3920], [0, 0x42f9dc0], [1, 0x434a9a0], [2, 0x42caa20], [3, 0x85c2880], [4, 0x855bf40], [5, 0x4265940], [0, 0x437bde0], [1, 0x43cc9c0], [2, 0x434ca40], [3, 0x86448a0], [4, 0x85ddf60]]}
  layer.16.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42e7960], [0, 0x43fde00], [1, 0x444e9e0], [2, 0x43cea60], [3, 0x86c68c0], [4, 0x865ff80], [5, 0x42e9a00], [0, 0x43ffea0]]}
  layer.16.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x44512c0]]}
  layer.16.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x86cba40]]}
  layer.17.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8665100], [5, 0x42ec2e0], [0, 0x4412360], [1, 0x44616e0], [2, 0x43d1b80], [3, 0x86dbe60], [4, 0x86a6120], [5, 0x432d300]]}
  layer.17.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4453380], [1, 0x44a2700], [2, 0x4412ba0], [3, 0x871ce80]]}
  layer.17.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x86e7140], [5, 0x436e320], [0, 0x44574a0], [1, 0x44a6820], [2, 0x4416cc0], [3, 0x8720fa0], [4, 0x8728160], [5, 0x43af340]]}
  layer.17.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x44984c0], [1, 0x44e7840], [2, 0x4457ce0], [3, 0x8761fc0]]}
  layer.17.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x449c5e0], [1, 0x44eb960], [2, 0x445be00], [3, 0x87660e0], [4, 0x87699c0], [5, 0x43f0ba0], [0, 0x44dd600], [1, 0x452c980]]}
  layer.17.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f57f00], [3, 0x821f160], [4, 0x823b040], [5, 0x3f7d840]]}
  layer.17.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8047420], [4, 0x8072ee0], [5, 0x3de0160], [0, 0x3e3d060], [1, 0x3e262c0], [2, 0x3dcd4e0], [3, 0x8088440], [4, 0x80b3f00]]}
  layer.17.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e21180], [0, 0x3e7e080], [1, 0x3e672e0], [2, 0x3e0e500]]}
  layer.17.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80c9ca0]]}
  layer.17.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e28380]]}
  layer.17.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e85280], [1, 0x3e6bc40], [2, 0x3e12e60], [3, 0x80da0c0], [4, 0x80f5fa0], [5, 0x3e387a0], [0, 0x3ec62a0], [1, 0x3eacc60], [2, 0x3e53e80], [3, 0x811b0e0], [4, 0x8136fc0], [5, 0x3e797c0], [0, 0x3f072c0], [1, 0x3eedc80], [2, 0x3e94ea0], [3, 0x815c100], [4, 0x8177fe0], [5, 0x3eba7e0], [0, 0x3f482e0], [1, 0x3f2eca0], [2, 0x3ed5ec0], [3, 0x819d120], [4, 0x81b9000], [5, 0x3efb800], [0, 0x3f89300], [1, 0x3f6fcc0], [2, 0x3f16ee0], [3, 0x81de140], [4, 0x81fa020], [5, 0x3f3c820], [0, 0x3fca320], [1, 0x3fb0ce0]]}
  layer.17.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e15e80], [2, 0x3dbd0a0], [3, 0x803f200], [4, 0x806acc0], [5, 0x3dd7f40], [0, 0x3e34e40], [1, 0x3e1e0a0], [2, 0x3dc52c0]]}
  layer.17.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x400b340], [1, 0x3ff1d00], [2, 0x3f5c020], [3, 0x8223280], [4, 0x823f160], [5, 0x3f81960], [0, 0x408d360], [1, 0x4073d20], [2, 0x3fde040], [3, 0x82a52a0], [4, 0x82c1180], [5, 0x4003980], [0, 0x410f380], [1, 0x40f5d40], [2, 0x4060060], [3, 0x83272c0]]}
  layer.17.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x83431a0], [5, 0x40859a0], [0, 0x41913a0], [1, 0x4177d60], [2, 0x40e2080], [3, 0x83a92e0], [4, 0x8345240], [5, 0x4087a40]]}
  layer.17.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4193c80]]}
  layer.17.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c46560]]}
  layer.18.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xad1e160], [4, 0xacd5fa0], [5, 0x66e7ea0], [0, 0x6b0ec60], [1, 0x6bb8620], [2, 0x68191e0], [3, 0xad5f180], [4, 0xad16fc0]]}
  layer.18.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x60cafa0], [1, 0x6106e60], [2, 0x5ec5020], [3, 0xa3646c0]]}
  layer.18.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa2ff5e0], [5, 0x5de5100], [0, 0x60cf0c0], [1, 0x610af80], [2, 0x5ec9140], [3, 0xa3687e0], [4, 0xa340600], [5, 0x5e26120]]}
  layer.18.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x61100e0], [1, 0x614bfa0], [2, 0x5f0a160], [3, 0xa3a9800]]}
  layer.18.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6114200], [1, 0x61500c0], [2, 0x5f0e280], [3, 0xa3ad920], [4, 0xa381e60], [5, 0x5e67980], [0, 0x6155220], [1, 0x61910e0]]}
  layer.18.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5f4f2a0], [3, 0xa3ee940], [4, 0xa3c2e80], [5, 0x5ea89a0]]}
  layer.18.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6196240], [1, 0x61d2100], [2, 0x5f533c0], [3, 0xa3f2a60], [4, 0xa3c6fa0], [5, 0x5eacac0], [0, 0x61d7260], [1, 0x6213120]]}
  layer.18.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5f943e0], [3, 0xa433a80], [4, 0xa407fc0], [5, 0x5eedae0]]}
  layer.18.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6218ac0]]}
  layer.18.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5f9b5e0]]}
  layer.18.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa43ac80], [4, 0xa40c920], [5, 0x5ef2440], [0, 0x6228ee0], [1, 0x6264da0], [2, 0x5faba00], [3, 0xa47bca0], [4, 0xa44d940], [5, 0x5f33460], [0, 0x6269f00], [1, 0x62a5dc0], [2, 0x5feca20], [3, 0xa4bccc0], [4, 0xa48e960], [5, 0x5f74480], [0, 0x62aaf20], [1, 0x62e6de0], [2, 0x602da40], [3, 0xa4fdce0], [4, 0xa4cf980], [5, 0x5fb54a0], [0, 0x62ebf40], [1, 0x6327e00], [2, 0x606ea60], [3, 0xa53ed00], [4, 0xa5109a0], [5, 0x5ff64c0], [0, 0x632cf60], [1, 0x6368e20], [2, 0x60afa80], [3, 0xa57fd20], [4, 0xa5519c0]]}
  layer.18.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x60374e0], [0, 0x636df80], [1, 0x63a9e40], [2, 0x60f0aa0], [3, 0xa5c0d40], [4, 0xa5929e0], [5, 0x603f700], [0, 0x63761a0]]}
  layer.18.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x63b2060], [2, 0x60f8cc0], [3, 0xa5c8f60], [4, 0xa59ac00], [5, 0x6047920], [0, 0x637e3c0], [1, 0x6434080], [2, 0x617ace0], [3, 0xa64af80], [4, 0xa61cc20], [5, 0x60c9940], [0, 0x64003e0], [1, 0x64b60a0], [2, 0x61fcd00], [3, 0xa6ccfa0], [4, 0xa69ec40]]}
  layer.18.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x614b960], [0, 0x6482400], [1, 0x65380c0], [2, 0x627ed20], [3, 0xa74efc0], [4, 0xa720c60], [5, 0x614da00], [0, 0x64844a0]]}
  layer.18.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5f2c040]]}
  layer.18.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5d9d620]]}
  layer.19.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa1d5340], [4, 0xa170260], [5, 0x5c55d80], [0, 0x5f3c460], [1, 0x5fef880], [2, 0x5dada40], [3, 0xa216360], [4, 0xa1b1280]]}
  layer.19.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c96da0], [0, 0x5f7d480], [1, 0x60308a0], [2, 0x5deea60]]}
  layer.19.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa257380], [4, 0xa1f22a0], [5, 0x5c9aec0], [0, 0x5f815a0], [1, 0x60349c0], [2, 0x5df2b80], [3, 0xa2983a0], [4, 0xa2332c0]]}
  layer.19.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5cdbee0], [0, 0x5fc25c0], [1, 0x60759e0], [2, 0x5e33ba0]]}
  layer.19.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ce0000], [0, 0x5fc66e0], [1, 0x6079b00], [2, 0x5e37cc0], [3, 0xa2d9c00], [4, 0xa274b20], [5, 0x5d21020], [0, 0x6007700]]}
  layer.19.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x60bab20], [2, 0x5e78ce0], [3, 0xa31ac20], [4, 0xa2b5b40]]}
  layer.19.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d62040], [0, 0x6048720], [1, 0x60bec40], [2, 0x5e7ce00], [3, 0xa31ed40], [4, 0xa2b9c60], [5, 0x5da3060], [0, 0x6089740]]}
  layer.19.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x60ffc60], [2, 0x5ebde20], [3, 0xa35fd60], [4, 0xa2fac80]]}
  layer.19.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6254140]]}
  layer.19.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6401e80]]}
  layer.19.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x67f57a0], [1, 0x689f160], [2, 0x65bb340], [3, 0xaac0b00], [4, 0xaa003a0], [5, 0x64122a0], [0, 0x68367c0], [1, 0x68e0180], [2, 0x65fc360], [3, 0xab01b20], [4, 0xaa413c0], [5, 0x64532c0], [0, 0x68777e0], [1, 0x69211a0], [2, 0x663d380], [3, 0xab42b40], [4, 0xaa823e0], [5, 0x64942e0], [0, 0x68b8800], [1, 0x69621c0], [2, 0x667e3a0], [3, 0xab83b60], [4, 0xaac3400], [5, 0x64d5300], [0, 0x68f9820], [1, 0x69a31e0], [2, 0x66bf3c0], [3, 0xabc4b80], [4, 0xab04420], [5, 0x6516320], [0, 0x693a840], [1, 0x69e4200]]}
  layer.19.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x67003e0], [3, 0xac05ba0], [4, 0xab45440], [5, 0x6557340], [0, 0x697b860], [1, 0x6a25220], [2, 0x6708600], [3, 0xac0ddc0]]}
  layer.19.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xab4d660], [5, 0x655f560], [0, 0x6983a80], [1, 0x6a2d440], [2, 0x6710820], [3, 0xac15fe0], [4, 0xabcf680], [5, 0x65e1580], [0, 0x6a05aa0], [1, 0x6aaf460], [2, 0x6792840], [3, 0xac98000], [4, 0xac516a0], [5, 0x66635a0], [0, 0x6a87ac0], [1, 0x6b31480]]}
  layer.19.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6814860], [3, 0xad1a020], [4, 0xacd36c0], [5, 0x66e55c0], [0, 0x6b09ae0], [1, 0x6bb34a0], [2, 0x6816900], [3, 0xad1c0c0]]}
  layer.19.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6728ec0]]}
  layer.19.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6bf9640]]}
  layer.20.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x685a200], [3, 0xada01a0], [4, 0xad57fe0], [5, 0x67392e0], [0, 0x6b504c0], [1, 0x6c09a60], [2, 0x689b220], [3, 0xade11c0]]}
  layer.20.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xad99000], [5, 0x677a300], [0, 0x6b914e0], [1, 0x6c4aa80]]}
  layer.20.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x68dc240], [3, 0xae221e0], [4, 0xad9d120], [5, 0x677e420], [0, 0x6b95600], [1, 0x6c4eba0], [2, 0x691d260], [3, 0xae63200]]}
  layer.20.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xadde140], [5, 0x67bf440], [0, 0x6bd6620], [1, 0x6c8fbc0]]}
  layer.20.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xade2260], [5, 0x67c3560], [0, 0x6bda740], [1, 0x6c93ce0], [2, 0x695eac0], [3, 0xaea4a60], [4, 0xae23280], [5, 0x6804580]]}
  layer.20.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x641bc20], [3, 0xa930fc0], [4, 0xa8f3080], [5, 0x62f53a0]]}
  layer.20.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa759280], [4, 0xa72af20], [5, 0x6157cc0], [0, 0x648e760], [1, 0x654a5a0], [2, 0x6291200], [3, 0xa79a2a0], [4, 0xa76bf40]]}
  layer.20.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6198ce0], [0, 0x64cf780], [1, 0x658b5c0], [2, 0x62d2220]]}
  layer.20.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa7dbb00]]}
  layer.20.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x619fee0]]}
  layer.20.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x64d6980], [1, 0x658ff20], [2, 0x62d6b80], [3, 0xa7ebf20], [4, 0xa7adfe0], [5, 0x61b0300], [0, 0x65179a0], [1, 0x65d0f40], [2, 0x6317ba0], [3, 0xa82cf40], [4, 0xa7ef000], [5, 0x61f1320], [0, 0x65589c0], [1, 0x6611f60], [2, 0x6358bc0], [3, 0xa86df60], [4, 0xa830020], [5, 0x6232340], [0, 0x65999e0], [1, 0x6652f80], [2, 0x6399be0], [3, 0xa8aef80], [4, 0xa871040], [5, 0x6273360], [0, 0x65daa00], [1, 0x6693fa0], [2, 0x63dac00], [3, 0xa8effa0], [4, 0xa8b2060], [5, 0x62b4380], [0, 0x661ba20], [1, 0x66d4fc0]]}
  layer.20.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x653a160], [2, 0x6280dc0], [3, 0xa751060], [4, 0xa722d00], [5, 0x614faa0], [0, 0x6486540], [1, 0x6542380], [2, 0x6288fe0]]}
  layer.20.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x665ca40], [1, 0x6715fe0], [2, 0x641fd40], [3, 0xa9350e0], [4, 0xa8f71a0], [5, 0x62f94c0], [0, 0x66dea60], [1, 0x6798000], [2, 0x64a1d60], [3, 0xa9b7100], [4, 0xa9791c0], [5, 0x637b4e0], [0, 0x6760a80], [1, 0x681a020], [2, 0x6523d80], [3, 0xaa39120]]}
  layer.20.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa9fb1e0], [5, 0x63fd500], [0, 0x67e2aa0], [1, 0x689c040], [2, 0x65a5da0], [3, 0xaabb140], [4, 0xa9fd280], [5, 0x63ff5a0]]}
  layer.20.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x67e5380]]}
  layer.20.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x65aaf20]]}
  layer.21.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa00a940], [4, 0x9fe1720], [5, 0x5af7e40], [0, 0x5e1ed00], [1, 0x5ed2120], [2, 0x5c13c40], [3, 0xa04b960], [4, 0xa022740]]}
  layer.21.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53db040], [1, 0x5420960], [2, 0x52bfa80], [3, 0x9650ea0]]}
  layer.21.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x960ad60], [5, 0x51f50a0], [0, 0x53df160], [1, 0x5424a80], [2, 0x52c3ba0], [3, 0x9654fc0], [4, 0x964bd80], [5, 0x52360c0]]}
  layer.21.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5420180], [1, 0x5465aa0], [2, 0x5304bc0], [3, 0x9695fe0]]}
  layer.21.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x54242a0], [1, 0x5469bc0], [2, 0x5308ce0], [3, 0x969a100], [4, 0x968d5e0], [5, 0x5277920], [0, 0x54652c0], [1, 0x54aabe0]]}
  layer.21.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5349d00], [3, 0x96db120], [4, 0x96ce600], [5, 0x52b8940]]}
  layer.21.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x54a62e0], [1, 0x54ebc00], [2, 0x534de20], [3, 0x96df240], [4, 0x96d2720], [5, 0x52bca60], [0, 0x54e7300], [1, 0x552cc20]]}
  layer.21.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x538ee40], [3, 0x9720260], [4, 0x9713740], [5, 0x52fda80]]}
  layer.21.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5528b60]]}
  layer.21.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5396040]]}
  layer.21.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9727460], [4, 0x97180a0], [5, 0x53023e0], [0, 0x5538f80], [1, 0x557e8a0], [2, 0x53a6460], [3, 0x9768480], [4, 0x97590c0], [5, 0x5343400], [0, 0x5579fa0], [1, 0x55bf8c0], [2, 0x53e7480], [3, 0x97a94a0], [4, 0x979a0e0], [5, 0x5384420], [0, 0x55bafc0], [1, 0x56008e0], [2, 0x54284a0], [3, 0x97ea4c0], [4, 0x97db100], [5, 0x53c5440], [0, 0x55fbfe0], [1, 0x5641900], [2, 0x54694c0], [3, 0x982b4e0], [4, 0x981c120], [5, 0x5406460], [0, 0x563d000], [1, 0x5682920], [2, 0x54aa4e0], [3, 0x986c500], [4, 0x985d140]]}
  layer.21.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5447480], [0, 0x567e020], [1, 0x56c3940], [2, 0x54eb500], [3, 0x98ad520], [4, 0x989e160], [5, 0x544f6a0], [0, 0x5686240]]}
  layer.21.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x56cbb60], [2, 0x54f3720], [3, 0x98b5740], [4, 0x98a6380], [5, 0x54578c0], [0, 0x568e460], [1, 0x574db80], [2, 0x5575740], [3, 0x9937760], [4, 0x99283a0], [5, 0x54d98e0], [0, 0x5710480], [1, 0x57cfba0], [2, 0x55f7760], [3, 0x99b9780], [4, 0x99aa3c0]]}
  layer.21.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x555b900], [0, 0x57924a0], [1, 0x5851bc0], [2, 0x5679780], [3, 0x9a3b7a0], [4, 0x9a2c3e0], [5, 0x555d9a0], [0, 0x5794540]]}
  layer.21.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x523c0e0]]}
  layer.21.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5198080]]}
  layer.22.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x94c1b20], [4, 0x947b9e0], [5, 0x5065d20], [0, 0x524c500], [1, 0x5309380], [2, 0x51a84a0], [3, 0x9502b40], [4, 0x94bca00]]}
  layer.22.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x50a6d40], [0, 0x528d520], [1, 0x534a3a0], [2, 0x51e94c0]]}
  layer.22.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9543b60], [4, 0x94fda20], [5, 0x50aae60], [0, 0x5291640], [1, 0x534e4c0], [2, 0x51ed5e0], [3, 0x9584b80], [4, 0x953ea40]]}
  layer.22.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x50ebe80], [0, 0x52d2660], [1, 0x538f4e0], [2, 0x522e600]]}
  layer.22.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x50effa0], [0, 0x52d6780], [1, 0x5393600], [2, 0x5232720], [3, 0x95c63e0], [4, 0x95802a0], [5, 0x5130fc0], [0, 0x53177a0]]}
  layer.22.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x53d4620], [2, 0x5273740], [3, 0x9607400], [4, 0x95c12c0]]}
  layer.22.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5171fe0], [0, 0x53587c0], [1, 0x53d8740], [2, 0x5277860], [3, 0x960b520], [4, 0x95c53e0], [5, 0x51b3000], [0, 0x53997e0]]}
  layer.22.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5419760], [2, 0x52b8880], [3, 0x964c540], [4, 0x9606400]]}
  layer.22.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x556dc40]]}
  layer.22.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5811e20]]}
  layer.22.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5b05840], [1, 0x5bb8c60], [2, 0x59b5da0], [3, 0x9dad2e0], [4, 0x9d0bb20], [5, 0x5822240], [0, 0x5b46860], [1, 0x5bf9c80], [2, 0x59f6dc0], [3, 0x9dee300], [4, 0x9d4cb40], [5, 0x5863260], [0, 0x5b87880], [1, 0x5c3aca0], [2, 0x5a37de0], [3, 0x9e2f320], [4, 0x9d8db60], [5, 0x58a4280], [0, 0x5bc88a0], [1, 0x5c7bcc0], [2, 0x5a78e00], [3, 0x9e70340], [4, 0x9dceb80], [5, 0x58e52a0], [0, 0x5c098c0], [1, 0x5cbcce0], [2, 0x5ab9e20], [3, 0x9eb1360], [4, 0x9e0fba0], [5, 0x59262c0], [0, 0x5c4a8e0], [1, 0x5cfdd00]]}
  layer.22.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5afae40], [3, 0x9ef2380], [4, 0x9e50bc0], [5, 0x59672e0], [0, 0x5c8b900], [1, 0x5d3ed20], [2, 0x5b03060], [3, 0x9efa5a0]]}
  layer.22.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9e58de0], [5, 0x596f500], [0, 0x5c93b20], [1, 0x5d46f40], [2, 0x5b0b280], [3, 0x9f027c0], [4, 0x9edae00], [5, 0x59f1520], [0, 0x5d15b40], [1, 0x5dc8f60], [2, 0x5b8d2a0], [3, 0x9f847e0], [4, 0x9f5ce20], [5, 0x5a73540], [0, 0x5d97b60], [1, 0x5e4af80]]}
  layer.22.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5c0f2c0], [3, 0xa006800], [4, 0x9fdee40], [5, 0x5af5560], [0, 0x5e19b80], [1, 0x5eccfa0], [2, 0x5c11360], [3, 0xa0088a0]]}
  layer.22.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b38e60]]}
  layer.22.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f13140]]}
  layer.23.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5c54c60], [3, 0xa08c980], [4, 0xa063760], [5, 0x5b49280], [0, 0x5e60560], [1, 0x5f23560], [2, 0x5c95c80], [3, 0xa0cd9a0]]}
  layer.23.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa0a4780], [5, 0x5b8a2a0], [0, 0x5ea1580], [1, 0x5f64580]]}
  layer.23.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5cd6ca0], [3, 0xa10e9c0], [4, 0xa0a88a0], [5, 0x5b8e3c0], [0, 0x5ea56a0], [1, 0x5f686a0], [2, 0x5d17cc0], [3, 0xa14f9e0]]}
  layer.23.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa0e98c0], [5, 0x5bcf3e0], [0, 0x5ee66c0], [1, 0x5fa96c0]]}
  layer.23.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa0ed9e0], [5, 0x5bd3500], [0, 0x5eea7e0], [1, 0x5fad7e0], [2, 0x5d59520], [3, 0xa191240], [4, 0xa12ea00], [5, 0x5c14520]]}
  layer.23.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5816680], [3, 0x9c1d7a0], [4, 0x9bfe800], [5, 0x5705340]]}
  layer.23.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9a45a60], [4, 0x9a366a0], [5, 0x5567c60], [0, 0x579e800], [1, 0x58640a0], [2, 0x568bc60], [3, 0x9a86a80], [4, 0x9a776c0]]}
  layer.23.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x55a8c80], [0, 0x57df820], [1, 0x58a50c0], [2, 0x56ccc80]]}
  layer.23.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ac82e0]]}
  layer.23.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x55afe80]]}
  layer.23.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x57e6a20], [1, 0x58a9a20], [2, 0x56d15e0], [3, 0x9ad8700], [4, 0x9ab9760], [5, 0x55c02a0], [0, 0x5827a40], [1, 0x58eaa40], [2, 0x5712600], [3, 0x9b19720], [4, 0x9afa780], [5, 0x56012c0], [0, 0x5868a60], [1, 0x592ba60], [2, 0x5753620], [3, 0x9b5a740], [4, 0x9b3b7a0], [5, 0x56422e0], [0, 0x58a9a80], [1, 0x596ca80], [2, 0x5794640], [3, 0x9b9b760], [4, 0x9b7c7c0], [5, 0x5683300], [0, 0x58eaaa0], [1, 0x59adaa0], [2, 0x57d5660], [3, 0x9bdc780], [4, 0x9bbd7e0], [5, 0x56c4320], [0, 0x592bac0], [1, 0x59eeac0]]}
  layer.23.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5853c60], [2, 0x567b820], [3, 0x9a3d840], [4, 0x9a2e480], [5, 0x555fa40], [0, 0x57965e0], [1, 0x585be80], [2, 0x5683a40]]}
  layer.23.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x596cae0], [1, 0x5a2fae0], [2, 0x581a7a0], [3, 0x9c218c0], [4, 0x9c02920], [5, 0x5709460], [0, 0x59eeb00], [1, 0x5ab1b00], [2, 0x589c7c0], [3, 0x9ca38e0], [4, 0x9c84940], [5, 0x578b480], [0, 0x5a70b20], [1, 0x5b33b20], [2, 0x591e7e0], [3, 0x9d25900]]}
  layer.23.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9d06960], [5, 0x580d4a0], [0, 0x5af2b40], [1, 0x5bb5b40], [2, 0x59a0800], [3, 0x9da7920], [4, 0x9d08a00], [5, 0x580f540]]}
  layer.23.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5af5420]]}
  layer.23.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x59a5980]]}

  # constant
  input_1_multiply_2560_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7d2dd80]]}
  lc.input_tensor.softmax_2562.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbdf5a00]]}
  lc.input_tensor.layernorm_2582.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7db4720]]}
  lc.input_tensor.layernorm_2582.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f049e0]]}
  dc.input_tensor.layernorm_2582.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbe7c3a0], [4, 0xbf10000]]}
  lc.input_tensor.layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7948880]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7bb4120]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7f05220]]}
  lc.input_tensor.layernorm_2596.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7590bc0]]}
  lc.input_tensor.layernorm_2596.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x788d820]]}
  dc.input_tensor.layernorm_2596.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7a42bc0], [2, 0x772fa00]]}
  lc.input_tensor.layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xbb12a40]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbb68760]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x788e060]]}
  input_1_multiply_2613_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbd34140]]}
  lc.input_tensor.softmax_2615.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x776c9c0]]}
  lc.input_tensor.layernorm_2635.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7adfbc0]]}
  lc.input_tensor.layernorm_2635.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7ca73e0]]}
  dc.input_tensor.layernorm_2635.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x79c8f20], [3, 0xbda96c0]]}
  lc.input_tensor.layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7bb38e0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x82be7e0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc34f6e0]]}
  lc.input_tensor.layernorm_2649.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc6252e0]]}
  lc.input_tensor.layernorm_2649.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc771cc0]]}
  dc.input_tensor.layernorm_2649.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e3ab40], [0, 0x80b0660]]}
  lc.input_tensor.layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x83c9b20]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x851c680]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc625b20]]}
  input_1_multiply_2666_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x82684a0]]}
  lc.input_tensor.softmax_2668.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7b64700]]}
  lc.input_tensor.layernorm_2688.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc0e0be0]]}
  lc.input_tensor.layernorm_2688.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc1a5440]]}
  dc.input_tensor.layernorm_2688.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7b61620], [0, 0x7d940c0]]}
  lc.input_tensor.layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7fa4ae0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x80ed3c0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc1a5c80]]}
  lc.input_tensor.layernorm_2702.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc47b880]]}
  lc.input_tensor.layernorm_2702.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x7e3a300]]}
  dc.input_tensor.layernorm_2702.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x80ad580], [1, 0x82bb700]]}
  lc.input_tensor.layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x834b260]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc34eea0]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7745660]]}
  input_1_multiply_2719_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6cb2ea0]]}
  lc.input_tensor.softmax_2721.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb105a60]]}
  lc.input_tensor.layernorm_2741.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6d45a40]]}
  lc.input_tensor.layernorm_2741.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6ae7e20]]}
  dc.input_tensor.layernorm_2741.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6e5a780], [1, 0x700b200]]}
  lc.input_tensor.layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6d3d960]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb190520]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6ae8660]]}
  lc.input_tensor.layernorm_2755.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xae642a0]]}
  lc.input_tensor.layernorm_2755.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x68455a0]]}
  dc.input_tensor.layernorm_2755.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c1f880], [1, 0x6cd8e20]]}
  lc.input_tensor.layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x69a3c00]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaee9ba0]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6845de0]]}
  input_1_multiply_2772_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x68ce820]]}
  lc.input_tensor.softmax_2774.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6cbaf80]]}
  lc.input_tensor.layernorm_2794.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6ee60a0]]}
  lc.input_tensor.layernorm_2794.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6bae5e0]]}
  dc.input_tensor.layernorm_2794.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaffe900], [4, 0xaf89420]]}
  lc.input_tensor.layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x69592e0]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb11a800]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x733ae40]]}
  lc.input_tensor.layernorm_2808.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7610a40]]}
  lc.input_tensor.layernorm_2808.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xba425e0]]}
  dc.input_tensor.layernorm_2808.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xba85e80], [5, 0x7446160]]}
  lc.input_tensor.layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x74e77c0]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x7934020]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xba42e20]]}
  input_1_multiply_2825_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x754d2c0]]}
  lc.input_tensor.softmax_2827.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7849f20]]}
  lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb5f2380]]}
  lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb58ca80]]}
  dc.input_tensor.layernorm_2847.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f4a4c0], [0, 0x72cf2a0]]}
  lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x74b0920]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x71e07e0]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb58d2c0]]}
  lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb75ee40]]}
  lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x712ed00]]}
  dc.input_tensor.layernorm_2861.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x74e46e0], [1, 0x76c34c0]]}
  lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x733a600]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb75c5c0]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x712f540]]}
  input_1_multiply_2878_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd92dda0]]}
  lc.input_tensor.softmax_2880.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x936a720]]}
  lc.input_tensor.layernorm_2900.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd841c00]]}
  lc.input_tensor.layernorm_2900.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd9b4740]]}
  dc.input_tensor.layernorm_2900.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x93f10c0], [0, 0x9643320]]}
  lc.input_tensor.layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x95fea40]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x96d7fc0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd9b4f80]]}
  lc.input_tensor.layernorm_2914.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9246d80]]}
  lc.input_tensor.layernorm_2914.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x93b16c0]]}
  dc.input_tensor.layernorm_2914.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd506e20], [4, 0xd5ff340]]}
  lc.input_tensor.layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9040e00]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x929ba80]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x93b1f00]]}
  input_1_multiply_2931_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9467460]]}
  lc.input_tensor.softmax_2933.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9422b80]]}
  lc.input_tensor.layernorm_2953.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9603a60]]}
  lc.input_tensor.layernorm_2953.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x96d7780]]}
  dc.input_tensor.layernorm_2953.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd76b640], [4, 0xd898860]]}
  lc.input_tensor.layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x92d7a80]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdc88b40]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9ad5620]]}
  lc.input_tensor.layernorm_2967.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9b39ee0]]}
  lc.input_tensor.layernorm_2967.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe1c37a0]]}
  dc.input_tensor.layernorm_2967.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9c18720], [0, 0x9dd8da0]]}
  lc.input_tensor.layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9da3080]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e29a20]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca21a0]]}
  input_1_multiply_2984_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9ddbe80]]}
  lc.input_tensor.softmax_2986.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9c1b800]]}
  lc.input_tensor.layernorm_3006.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xdaf0f00]]}
  lc.input_tensor.layernorm_3006.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdc89380]]}
  dc.input_tensor.layernorm_3006.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x96dba60], [0, 0x99219c0]]}
  lc.input_tensor.layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x98da840]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x997afc0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xdc89bc0]]}
  lc.input_tensor.layernorm_3020.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xde5b740]]}
  lc.input_tensor.layernorm_3020.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x98c02a0]]}
  dc.input_tensor.layernorm_3020.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9b36e00], [1, 0x9aed3e0]]}
  lc.input_tensor.layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9ad4de0]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xdc5b140]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x98c0ae0]]}
  input_1_multiply_3037_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8771c00]]}
  lc.input_tensor.softmax_3039.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x87c3820]]}
  lc.input_tensor.layernorm_3059.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x890c9a0]]}
  lc.input_tensor.layernorm_3059.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x83f8060]]}
  dc.input_tensor.layernorm_3059.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcab0c60], [5, 0x850cda0]]}
  lc.input_tensor.layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x87fc6c0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x884e2e0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc9e1160]]}
  lc.input_tensor.layernorm_3073.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc7b7740]]}
  lc.input_tensor.layernorm_3073.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x85a7140]]}
  dc.input_tensor.layernorm_3073.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8653bc0], [3, 0xc6b05e0]]}
  lc.input_tensor.layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc80cba0]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8268ce0]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x85a7980]]}
  input_1_multiply_3090_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x84ce3e0]]}
  lc.input_tensor.softmax_3092.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc911460]]}
  lc.input_tensor.layernorm_3112.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x83f7820]]}
  lc.input_tensor.layernorm_3112.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x866d340]]}
  dc.input_tensor.layernorm_3112.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x86bc6c0], [2, 0x877b5c0]]}
  lc.input_tensor.layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xc842200]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xc99bf20]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8d9f580]]}
  lc.input_tensor.layernorm_3126.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9075180]]}
  lc.input_tensor.layernorm_3126.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x92106c0]]}
  dc.input_tensor.layernorm_3126.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd373160], [4, 0xd45baa0]]}
  lc.input_tensor.layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8e74b60]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8e31160]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd3b7260]]}
  input_1_multiply_3143_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x8fba440]]}
  lc.input_tensor.softmax_3145.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x92150c0]]}
  lc.input_tensor.layernorm_3165.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x88b04c0]]}
  lc.input_tensor.layernorm_3165.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8b9d540]]}
  dc.input_tensor.layernorm_3165.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8af94e0], [2, 0x8bc7fc0]]}
  lc.input_tensor.layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xcd0c2e0]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xcdeeaa0]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8b9dd80]]}
  lc.input_tensor.layernorm_3179.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x8ed9260]]}
  lc.input_tensor.layernorm_3179.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd01ace0]]}
  dc.input_tensor.layernorm_3179.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xd0c05a0], [5, 0x8b923e0]]}
  lc.input_tensor.layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x8def900]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x8d5dd20]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xd01b520]]}
  input_1_multiply_3196_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8998520]]}
  lc.input_tensor.softmax_3198.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4687080]]}
  lc.input_tensor.layernorm_3218.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x48383c0]]}
  lc.input_tensor.layernorm_3218.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46fcd20]]}
  dc.input_tensor.layernorm_3218.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x478d9c0], [3, 0x8a10b60]]}
  lc.input_tensor.layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8a22fe0]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4711b40]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4897b60]]}
  lc.input_tensor.layernorm_3232.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4538d20]]}
  lc.input_tensor.layernorm_3232.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87a7100]]}
  dc.input_tensor.layernorm_3232.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x87aa9e0], [5, 0x4431bc0]]}
  lc.input_tensor.layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x451e620]]}
  lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x456d9a0]]}
  lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87a7940]]}
  input_1_multiply_3249_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x449ce20]]}
  lc.input_tensor.softmax_3251.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4622ee0]]}
  lc.input_tensor.layernorm_3271.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46fc4e0]]}
  lc.input_tensor.layernorm_3271.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x463bd80]]}
  dc.input_tensor.layernorm_3271.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88bc680], [4, 0x88d23e0]]}
  lc.input_tensor.layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x45c37e0]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x46ad9a0]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9016a60]]}
  lc.input_tensor.layernorm_3285.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x92ec660]]}
  lc.input_tensor.layernorm_3285.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4f075a0]]}
  dc.input_tensor.layernorm_3285.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x512bcc0], [1, 0x51e8b40]]}
  lc.input_tensor.layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x500de60]]}
  lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9099280]]}
  lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x516fdc0]]}
  input_1_multiply_3302_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5153740]]}
  lc.input_tensor.softmax_3304.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x947d1e0]]}
  lc.input_tensor.layernorm_3324.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8db4280]]}
  lc.input_tensor.layernorm_3324.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8dc3e60]]}
  dc.input_tensor.layernorm_3324.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x49bcd40], [0, 0x4af39e0]]}
  lc.input_tensor.layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4bc2ce0]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4acb800]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8dc46a0]]}
  lc.input_tensor.layernorm_3338.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4e04c80]]}
  lc.input_tensor.layernorm_3338.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ed16e0]]}
  dc.input_tensor.layernorm_3338.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4d9d300], [3, 0x90961a0]]}
  lc.input_tensor.layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9016220]]}
  lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4c21580]]}
  lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4ed1f20]]}
  input_1_multiply_3355_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7cc95e0]]}
  lc.input_tensor.softmax_3357.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3aef5e0]]}
  lc.input_tensor.layernorm_3377.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3bc6b80]]}
  lc.input_tensor.layernorm_3377.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a04be0]]}
  dc.input_tensor.layernorm_3377.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ad47e0], [3, 0x7d25d40]]}
  lc.input_tensor.layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d540a0]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3b7a0a0]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3b40280]]}
  lc.input_tensor.layernorm_3391.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3b3fa40]]}
  lc.input_tensor.layernorm_3391.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100]]}
  dc.input_tensor.layernorm_3391.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100], [3, 0x7ab4100]]}
  lc.input_tensor.layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100]]}
  lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da940]]}
  input_1_multiply_3408_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bbb260]]}
  lc.input_tensor.softmax_3410.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7bb89c0]]}
  lc.input_tensor.layernorm_3430.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a68c40]]}
  lc.input_tensor.layernorm_3430.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a79060]]}
  dc.input_tensor.layernorm_3430.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39ef680], [2, 0x3a01b00]]}
  lc.input_tensor.layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c45d20]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da100]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x417ae80]]}
  lc.input_tensor.layernorm_3444.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4450a80]]}
  lc.input_tensor.layernorm_3444.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x43d0b00]]}
  dc.input_tensor.layernorm_3444.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x86c8960], [4, 0x8662020]]}
  lc.input_tensor.layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42ebaa0]]}
  lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41a40a0]]}
  lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x43d1340]]}
  input_1_multiply_3461_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8769180]]}
  lc.input_tensor.softmax_3463.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x43f0360]]}
  lc.input_tensor.layernorm_3483.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80c9460]]}
  lc.input_tensor.layernorm_3483.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x80f4f20]]}
  dc.input_tensor.layernorm_3483.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e252a0], [0, 0x3e821a0]]}
  lc.input_tensor.layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e6b400]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e12620]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x80f5760]]}
  lc.input_tensor.layernorm_3497.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4193440]]}
  lc.input_tensor.layernorm_3497.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4179e00]]}
  dc.input_tensor.layernorm_3497.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40e4120], [3, 0x83ab380]]}
  lc.input_tensor.layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x83472e0]]}
  lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4089ae0]]}
  lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x417a640]]}
  input_1_multiply_3514_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa381620]]}
  lc.input_tensor.softmax_3516.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e67140]]}
  lc.input_tensor.layernorm_3536.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6218280]]}
  lc.input_tensor.layernorm_3536.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5de48c0]]}
  dc.input_tensor.layernorm_3536.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5f98500], [3, 0xa437ba0]]}
  lc.input_tensor.layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa40c0e0]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ef1c00]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6264560]]}
  lc.input_tensor.layernorm_3550.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa2d93c0]]}
  lc.input_tensor.layernorm_3550.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5fee800]]}
  dc.input_tensor.layernorm_3550.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5d9a540], [3, 0xa1d2260]]}
  lc.input_tensor.layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa16fa20]]}
  lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c55540]]}
  lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5fef040]]}
  input_1_multiply_3567_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5f2b800]]}
  lc.input_tensor.softmax_3569.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa2742e0]]}
  lc.input_tensor.layernorm_3589.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5de4080]]}
  lc.input_tensor.layernorm_3589.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x60ca760]]}
  dc.input_tensor.layernorm_3589.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6103d80], [2, 0x5ec1f40]]}
  lc.input_tensor.layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa363e80]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa2feda0]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa9ffb60]]}
  lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xacd5760]]}
  lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x66e7660]]}
  dc.input_tensor.layernorm_3603.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6b0bb80], [1, 0x6bb5540]]}
  lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x68189a0]]}
  lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaac02c0]]}
  lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6b4fc80]]}
  input_1_multiply_3620_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x695e280]]}
  lc.input_tensor.softmax_3622.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaea4220]]}
  lc.input_tensor.layernorm_3642.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa7db2c0]]}
  lc.input_tensor.layernorm_3642.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa7acf60]]}
  dc.input_tensor.layernorm_3642.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x619ce00], [0, 0x64d38a0]]}
  lc.input_tensor.layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x658f6e0]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x62d6340]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa7ad7a0]]}
  lc.input_tensor.layernorm_3656.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x67e4b40]]}
  lc.input_tensor.layernorm_3656.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x689e0e0]]}
  dc.input_tensor.layernorm_3656.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x65a7e40], [3, 0xaabd1e0]]}
  lc.input_tensor.layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa9ff320]]}
  lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6401640]]}
  lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x689e920]]}
  input_1_multiply_3673_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x968cda0]]}
  lc.input_tensor.softmax_3675.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x52770e0]]}
  lc.input_tensor.layernorm_3695.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5528320]]}
  lc.input_tensor.layernorm_3695.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x51f4860]]}
  dc.input_tensor.layernorm_3695.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5392f60], [3, 0x9724380]]}
  lc.input_tensor.layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9717860]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5301ba0]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x557e060]]}
  lc.input_tensor.layernorm_3709.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x95c5ba0]]}
  lc.input_tensor.layernorm_3709.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5308300]]}
  dc.input_tensor.layernorm_3709.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5194fa0], [3, 0x94bea40]]}
  lc.input_tensor.layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x947b1a0]]}
  lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x50654e0]]}
  lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5308b40]]}
  input_1_multiply_3726_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x523b8a0]]}
  lc.input_tensor.softmax_3728.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x957fa60]]}
  lc.input_tensor.layernorm_3748.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x51f4020]]}
  lc.input_tensor.layernorm_3748.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53da800]]}
  dc.input_tensor.layernorm_3748.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x541d880], [2, 0x52bc9a0]]}
  lc.input_tensor.layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9650660]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x960a520]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9d0b2e0]]}
  lc.input_tensor.layernorm_3762.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9fe0ee0]]}
  lc.input_tensor.layernorm_3762.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5af7600]]}
  dc.input_tensor.layernorm_3762.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e1bc20], [1, 0x5ecf040]]}
  lc.input_tensor.layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5c13400]]}
  lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9dacaa0]]}
  lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5e5fd20]]}
  input_1_multiply_3779_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5d58ce0]]}
  lc.input_tensor.softmax_3781.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa190a00]]}
  lc.input_tensor.layernorm_3801.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ac7aa0]]}
  lc.input_tensor.layernorm_3801.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ab86e0]]}
  dc.input_tensor.layernorm_3801.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x55acda0], [0, 0x57e3940]]}
  lc.input_tensor.layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x58a91e0]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56d0da0]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ab8f20]]}
  lc.input_tensor.layernorm_3815.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5af4be0]]}
  lc.input_tensor.layernorm_3815.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5bb7be0]]}
  dc.input_tensor.layernorm_3815.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x59a28a0], [3, 0x9da99c0]]}
  lc.input_tensor.layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9d0aaa0]]}
  lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x58115e0]]}
  lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5bb8420]]}

  # epoch_to_epoch
  e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0:     {input: buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                      {input: layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe0d14e0]]}
  e2e_layernorm_2596.dc.subtract.1_0:                                           {input: layernorm_2596.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0:     {input: buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_add_2634_0:                                                               {input: add_2634, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_gelu_2641_0:                                                              {input: gelu_2641, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420], [2, 0xa0bb440], [2, 0xa17e460], [2, 0xa241480], [2, 0xa3044a0], [2, 0xa3c74c0], [2, 0xa48a4e0]]}
  e2e_layernorm_2635.dc.add.10_0:                                               {input: layernorm_2635.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_softmax_2668.dc.exp.0_0:                                                  {input: softmax_2668.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9eeba00], [5, 0xa134a20], [5, 0xa37da40]]}
  e2e_softmax_2668.dc.reciprocal.2_0:                                           {input: softmax_2668.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe12cb60]]}
  e2e_layernorm_2649.dc.add.10_0:                                               {input: layernorm_2649.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_layernorm_2688.dc.add.10_0:                                               {input: layernorm_2688.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0:     {input: buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                      {input: layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9e30400]]}
  e2e_layernorm_2741.dc.subtract.1_0:                                           {input: layernorm_2741.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0:     {input: buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_gelu_2747_0:                                                              {input: gelu_2747, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60], [0, 0x9ff8c80], [0, 0xa0bbca0], [0, 0xa17ecc0], [0, 0xa241ce0], [0, 0xa304d00], [0, 0xa3c7d20]]}
  e2e_layernorm_2741.dc.add.10_0:                                               {input: layernorm_2741.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_softmax_2774.dc.exp.0_0:                                                  {input: softmax_2774.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0xa17e420], [2, 0xa3c7440], [2, 0xa610460]]}
  e2e_layernorm_2755.dc.add.10_0:                                               {input: layernorm_2755.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_layernorm_2794.dc.add.10_0:                                               {input: layernorm_2794.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_layernorm_2808.dc.reciprocal.7_0:                                         {input: layernorm_2808.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe264820]]}
  e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0:     {input: buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_layernorm_2847.dc.subtract.1_0:                                           {input: layernorm_2847.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_matmul_2856_0:                                                            {input: matmul_2856, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9e42880], [1, 0x9e5aea0], [1, 0x9e734c0], [1, 0x9e8bae0], [1, 0x9ea4100], [1, 0x9ebc720], [1, 0x9ed4d40], [1, 0x9eed360], [1, 0x9f05980], [1, 0x9f1dfa0], [1, 0x9f365c0], [1, 0x9f4ebe0], [1, 0x9f67200], [1, 0x9f7f820], [1, 0x9f97e40]]}
  e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0:                             {input: buffer_0_layernorm_2847.dc.add.10_add_2860, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_softmax_2880.dc.multiply.3_0:                                             {input: softmax_2880.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe6f06a0]]}
  e2e_matmul_2884_0:                                                            {input: matmul_2884, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9cd3600], [5, 0x9d04220], [5, 0x9d34e40], [5, 0x9d65a60], [5, 0x9d96680], [5, 0x9dc72a0], [5, 0x9df7ec0]]}
  e2e_layernorm_2861.dc.add.10_0:                                               {input: layernorm_2861.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_layernorm_2900.dc.add.10_0:                                               {input: layernorm_2900.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0:     {input: buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                      {input: layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9e30400]]}
  e2e_layernorm_2953.dc.subtract.1_0:                                           {input: layernorm_2953.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0:     {input: buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_gelu_2959_0:                                                              {input: gelu_2959, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60], [0, 0x9ff8c80], [0, 0xa0bbca0], [0, 0xa17ecc0], [0, 0xa241ce0], [0, 0xa304d00], [0, 0xa3c7d20]]}
  e2e_layernorm_2953.dc.add.10_0:                                               {input: layernorm_2953.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_softmax_2986.dc.exp.0_0:                                                  {input: softmax_2986.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0xa17e420], [2, 0xa3c7440], [2, 0xa610460]]}
  e2e_softmax_2986.dc.reciprocal.2_0:                                           {input: softmax_2986.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe12cb60]]}
  e2e_layernorm_2967.dc.add.10_0:                                               {input: layernorm_2967.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_layernorm_3006.dc.add.10_0:                                               {input: layernorm_3006.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_layernorm_3020.dc.reciprocal.7_0:                                         {input: layernorm_3020.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9ca8b80]]}
  e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0:     {input: buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_layernorm_3059.dc.subtract.1_0:                                           {input: layernorm_3059.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_gelu_3065_0:                                                              {input: gelu_3065, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360], [3, 0xe251380], [3, 0xe3143a0], [3, 0xe3d73c0], [3, 0xe49a3e0], [3, 0xe55d400], [3, 0xe620420]]}
  e2e_layernorm_3059.dc.add.10_0:                                               {input: layernorm_3059.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_softmax_3092.dc.exp.0_0:                                                  {input: softmax_3092.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe4a76a0], [4, 0xe6f06c0], [4, 0xe9396e0]]}
  e2e_layernorm_3073.dc.add.10_0:                                               {input: layernorm_3073.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_layernorm_3112.dc.add.10_0:                                               {input: layernorm_3112.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_layernorm_3126.dc.reciprocal.7_0:                                         {input: layernorm_3126.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9e30400]]}
  e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0:     {input: buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_layernorm_3165.dc.subtract.1_0:                                           {input: layernorm_3165.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_matmul_3174_0:                                                            {input: matmul_3174, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe276ca0], [4, 0xe28f2c0], [4, 0xe2a78e0], [4, 0xe2bff00], [4, 0xe2d8520], [4, 0xe2f0b40], [4, 0xe309160], [4, 0xe321780], [4, 0xe339da0], [4, 0xe3523c0], [4, 0xe36a9e0], [4, 0xe383000], [4, 0xe39b620], [4, 0xe3b3c40], [4, 0xe3cc260]]}
  e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0:                             {input: buffer_0_layernorm_3165.dc.add.10_add_3178, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_softmax_3198.dc.multiply.3_0:                                             {input: softmax_3198.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0xa2bc280]]}
  e2e_matmul_3202_0:                                                            {input: matmul_3202, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9f66020], [2, 0x9f96c40], [2, 0x9fc7860], [2, 0x9ff8480], [2, 0xa0290a0], [2, 0xa059cc0], [2, 0xa08a8e0]]}
  e2e_layernorm_3179.dc.add.10_0:                                               {input: layernorm_3179.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_layernorm_3218.dc.add.10_0:                                               {input: layernorm_3218.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0:     {input: buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                      {input: layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9ca8b80]]}
  e2e_layernorm_3271.dc.subtract.1_0:                                           {input: layernorm_3271.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0:     {input: buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_gelu_3277_0:                                                              {input: gelu_3277, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360], [3, 0xe251380], [3, 0xe3143a0], [3, 0xe3d73c0], [3, 0xe49a3e0], [3, 0xe55d400], [3, 0xe620420]]}
  e2e_layernorm_3271.dc.add.10_0:                                               {input: layernorm_3271.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_softmax_3304.dc.exp.0_0:                                                  {input: softmax_3304.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9eeba00], [5, 0xa134a20], [5, 0xa37da40]]}
  e2e_softmax_3304.dc.reciprocal.2_0:                                           {input: softmax_3304.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9ed4460]]}
  e2e_layernorm_3285.dc.add.10_0:                                               {input: layernorm_3285.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_layernorm_3324.dc.add.10_0:                                               {input: layernorm_3324.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_layernorm_3338.dc.reciprocal.7_0:                                         {input: layernorm_3338.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9f3b5a0]]}
  e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0:     {input: buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_layernorm_3377.dc.subtract.1_0:                                           {input: layernorm_3377.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_gelu_3383_0:                                                              {input: gelu_3383, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60], [0, 0x9ff8c80], [0, 0xa0bbca0], [0, 0xa17ecc0], [0, 0xa241ce0], [0, 0xa304d00], [0, 0xa3c7d20]]}
  e2e_layernorm_3377.dc.add.10_0:                                               {input: layernorm_3377.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_softmax_3410.dc.exp.0_0:                                                  {input: softmax_3410.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0xa17e420], [2, 0xa3c7440], [2, 0xa610460]]}
  e2e_layernorm_3391.dc.add.10_0:                                               {input: layernorm_3391.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_layernorm_3430.dc.add.10_0:                                               {input: layernorm_3430.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_layernorm_3444.dc.reciprocal.7_0:                                         {input: layernorm_3444.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe264820]]}
  e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0:     {input: buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_layernorm_3483.dc.subtract.1_0:                                           {input: layernorm_3483.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_matmul_3492_0:                                                            {input: matmul_3492, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9e42880], [1, 0x9e5aea0], [1, 0x9e734c0], [1, 0x9e8bae0], [1, 0x9ea4100], [1, 0x9ebc720], [1, 0x9ed4d40], [1, 0x9eed360], [1, 0x9f05980], [1, 0x9f1dfa0], [1, 0x9f365c0], [1, 0x9f4ebe0], [1, 0x9f67200], [1, 0x9f7f820], [1, 0x9f97e40]]}
  e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0:                             {input: buffer_0_layernorm_3483.dc.add.10_add_3496, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_softmax_3516.dc.multiply.3_0:                                             {input: softmax_3516.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe6f06a0]]}
  e2e_matmul_3520_0:                                                            {input: matmul_3520, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9cd3600], [5, 0x9d04220], [5, 0x9d34e40], [5, 0x9d65a60], [5, 0x9d96680], [5, 0x9dc72a0], [5, 0x9df7ec0]]}
  e2e_layernorm_3497.dc.add.10_0:                                               {input: layernorm_3497.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_layernorm_3536.dc.add.10_0:                                               {input: layernorm_3536.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0:     {input: buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                      {input: layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9f3b5a0]]}
  e2e_layernorm_3589.dc.subtract.1_0:                                           {input: layernorm_3589.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360]]}
  e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0:     {input: buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_gelu_3595_0:                                                              {input: gelu_3595, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60], [0, 0x9ff8c80], [0, 0xa0bbca0], [0, 0xa17ecc0], [0, 0xa241ce0], [0, 0xa304d00], [0, 0xa3c7d20]]}
  e2e_layernorm_3589.dc.add.10_0:                                               {input: layernorm_3589.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9d65a00]]}
  e2e_softmax_3622.dc.exp.0_0:                                                  {input: softmax_3622.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0xa17e420], [2, 0xa3c7440], [2, 0xa610460]]}
  e2e_softmax_3622.dc.reciprocal.2_0:                                           {input: softmax_3622.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe12cb60]]}
  e2e_layernorm_3603.dc.add.10_0:                                               {input: layernorm_3603.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_layernorm_3642.dc.add.10_0:                                               {input: layernorm_3642.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_layernorm_3656.dc.reciprocal.7_0:                                         {input: layernorm_3656.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9ca8b80]]}
  e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0:     {input: buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_layernorm_3695.dc.subtract.1_0:                                           {input: layernorm_3695.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9eed280]]}
  e2e_gelu_3701_0:                                                              {input: gelu_3701, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe18e360], [3, 0xe251380], [3, 0xe3143a0], [3, 0xe3d73c0], [3, 0xe49a3e0], [3, 0xe55d400], [3, 0xe620420]]}
  e2e_layernorm_3695.dc.add.10_0:                                               {input: layernorm_3695.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_softmax_3728.dc.exp.0_0:                                                  {input: softmax_3728.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9eeba00], [5, 0xa134a20], [5, 0xa37da40]]}
  e2e_layernorm_3709.dc.add.10_0:                                               {input: layernorm_3709.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_layernorm_3748.dc.add.10_0:                                               {input: layernorm_3748.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}
  e2e_layernorm_3762.dc.reciprocal.7_0:                                         {input: layernorm_3762.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x9e2a260], [1, 0x9e30400]]}
  e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0:     {input: buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x9f35400], [2, 0x9ff8420]]}
  e2e_matmul_3792_0:                                                            {input: matmul_3792, type: queue, entries: 2, grid_size: [3, 2], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xe0cb340], [3, 0xe10c360], [3, 0xe14d380], [3, 0xe18e3a0], [3, 0xe1cf3c0], [3, 0xe2103e0]]}
  e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0:                             {input: buffer_0_layernorm_3762.dc.add.10_add_3800, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xe25e680], [4, 0xe3216a0]]}
  e2e_matmul_3804_0:                                                            {input: matmul_3804, type: queue, entries: 2, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x9ca29e0], [5, 0x9cd3600], [5, 0x9d04220], [5, 0x9d34e40], [5, 0x9d65a60], [5, 0x9d96680], [5, 0x9dc72a0], [5, 0x9df7ec0], [5, 0x9e28ae0], [5, 0x9e59700], [5, 0x9e8a320], [5, 0x9ebaf40], [5, 0x9eebb60], [5, 0x9f1c780], [5, 0x9f4d3a0], [5, 0x9f7dfc0], [5, 0x9faebe0], [5, 0x9fdf800], [5, 0xa010420], [5, 0xa041040], [5, 0xa071c60], [5, 0xa0a2880], [5, 0xa0d34a0], [5, 0xa1040c0], [5, 0xa134ce0], [5, 0xa165900], [5, 0xa196520], [5, 0xa1c7140], [5, 0xa1f7d60], [5, 0xa228980], [5, 0xa2595a0], [5, 0xa28a1c0]]}
  e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0:                             {input: buffer_0_layernorm_3801.dc.add.10_add_3814, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x9e72c40], [0, 0x9f35c60]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 0
    input_count: 2
    matmul_2546: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2552: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2558: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_2546, matmul_2552],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2560: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_2558, input_1_multiply_2560_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2561: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [multiply_2560, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2562.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2561],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2562.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_2562.dc.exp.0, lc.input_tensor.softmax_2562.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2562.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2562.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2562.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2562.dc.exp.0, softmax_2562.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2566: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2573: {type: matmul, grid_loc: [4, 4], grid_size: [3, 2], inputs: [softmax_2562.dc.multiply.3, matmul_2566],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2577: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_2573, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_2581: {type: add, grid_loc: [4, 6], grid_size: [2, 1], inputs: [matmul_2577, hidden_states],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_2581, lc.input_tensor.layernorm_2582.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2582.dc.subtract.1: {type: subtract, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_2581, layernorm_2582.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2582.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2582.dc.subtract.1, layernorm_2582.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_2582.dc.multiply.2, lc.input_tensor.layernorm_2582.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2582.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2582.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2582.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2582.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2582.dc.reciprocal.7, lc.input_tensor.layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_2582.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8: {type: nop, grid_loc: [7, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_1_temporal_epoch_1:
    target_device: 0
    input_count: 2
    layernorm_2582.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0, e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2582.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2582.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2582.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2582.dc.multiply.9, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2585: {type: matmul, grid_loc: [2, 0], grid_size: [4, 8], inputs: [layernorm_2582.dc.add.10, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2588: {type: gelu, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_2585],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2591: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_2588, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2582.dc.add.10_add_2595: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2582.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [126], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2595: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2591, buffer_0_layernorm_2582.dc.add.10_add_2595],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 98], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_2595, lc.input_tensor.layernorm_2596.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2596.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_2595, layernorm_2596.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_2596.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_2_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2_temporal_epoch_2:
    target_device: 0
    input_count: 2
    layernorm_2596.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2596.dc.subtract.1_0, e2e_layernorm_2596.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_2596.dc.multiply.2, lc.input_tensor.layernorm_2596.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2596.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2596.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2596.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2596.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2596.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2596.dc.reciprocal.7, lc.input_tensor.layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2596.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0, layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2596.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_2596.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2596.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2596.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2599: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [layernorm_2596.dc.add.10, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2605: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_2596.dc.add.10, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2611: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_2599, matmul_2605],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2613: {type: multiply, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_2611, input_1_multiply_2613_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2614: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [multiply_2613, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2615.dc.exp.0: {type: exp, grid_loc: [4, 5], grid_size: [2, 2], inputs: [add_2614],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2615.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [softmax_2615.dc.exp.0, lc.input_tensor.softmax_2615.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2615.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_2615.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2615.dc.multiply.3: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [softmax_2615.dc.exp.0, softmax_2615.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2619: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [layernorm_2596.dc.add.10, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2626: {type: matmul, grid_loc: [7, 6], grid_size: [3, 2], inputs: [softmax_2615.dc.multiply.3, matmul_2619],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2630: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2626, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2596.dc.add.10_add_2634: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2596.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2634: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [matmul_2630, buffer_0_layernorm_2596.dc.add.10_add_2634],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3_temporal_epoch_3:
    target_device: 0
    input_count: 2
    layernorm_2635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_2634_0, lc.input_tensor.layernorm_2635.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2635.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_2634_0, layernorm_2635.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2635.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2635.dc.subtract.1, layernorm_2635.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2635.dc.multiply.2, lc.input_tensor.layernorm_2635.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2635.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2635.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_2635.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2635.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2635.dc.reciprocal.7, lc.input_tensor.layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2635.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_2_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8, layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2635.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_2635.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2635.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2635.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2638: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2635.dc.add.10, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2641: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2638],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_4_temporal_epoch_4:
    target_device: 0
    input_count: 2
    matmul_2644: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_2641_0, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2635.dc.add.10_add_2648: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2635.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2648: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_2644, buffer_0_layernorm_2635.dc.add.10_add_2648],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2648, lc.input_tensor.layernorm_2649.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2649.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_2648, layernorm_2649.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2649.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2649.dc.subtract.1, layernorm_2649.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2649.dc.multiply.2, lc.input_tensor.layernorm_2649.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2649.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2649.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2649.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2649.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2649.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2649.dc.reciprocal.7, lc.input_tensor.layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2649.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8, layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2649.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2649.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2649.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2649.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2652: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_2649.dc.add.10, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2658: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2649.dc.add.10, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2664: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_2652, matmul_2658],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2666: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_2664, input_1_multiply_2666_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2667: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_2666, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2668.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_2667],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2668.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_2668.dc.exp.0, lc.input_tensor.softmax_2668.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2668.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_2668.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_5_temporal_epoch_5:
    target_device: 0
    input_count: 2
    softmax_2668.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_2668.dc.exp.0_0, e2e_softmax_2668.dc.reciprocal.2_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2672: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_2649.dc.add.10_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2679: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_2668.dc.multiply.3, matmul_2672],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2683: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_2679, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2649.dc.add.10_add_2687: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_2649.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2687: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_2683, buffer_0_layernorm_2649.dc.add.10_add_2687],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_2687, lc.input_tensor.layernorm_2688.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2688.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_2687, layernorm_2688.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2688.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2688.dc.subtract.1, layernorm_2688.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2688.dc.multiply.2, lc.input_tensor.layernorm_2688.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2688.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2688.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2688.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2688.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_2688.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_2688.dc.reciprocal.7, lc.input_tensor.layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_2688.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8, layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2688.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_2688.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2688.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_2688.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_6_temporal_epoch_6:
    target_device: 0
    input_count: 2
    matmul_2691: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_2688.dc.add.10_0, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2694: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_2691],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2697: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2694, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_2701: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_2697, e2e_layernorm_2688.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_2701, lc.input_tensor.layernorm_2702.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2702.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_2701, layernorm_2702.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2702.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2702.dc.subtract.1, layernorm_2702.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2702.dc.multiply.2, lc.input_tensor.layernorm_2702.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2702.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2702.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2702.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2702.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2702.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2702.dc.reciprocal.7, lc.input_tensor.layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2702.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_7_temporal_epoch_7:
    target_device: 0
    input_count: 2
    layernorm_2702.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0, e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2702.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2702.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2702.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2702.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2705: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2702.dc.add.10, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2711: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_2702.dc.add.10, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2717: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2705, matmul_2711],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2719: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2717, input_1_multiply_2719_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2720: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_2719, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2721.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2720],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2721.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2721.dc.exp.0, lc.input_tensor.softmax_2721.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2721.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2721.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2721.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_2721.dc.exp.0, softmax_2721.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2725: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2702.dc.add.10, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2732: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_2721.dc.multiply.3, matmul_2725],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2736: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_2732, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2702.dc.add.10_add_2740: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2702.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2740: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_2736, buffer_0_layernorm_2702.dc.add.10_add_2740],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_2740, lc.input_tensor.layernorm_2741.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2741.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_2740, layernorm_2741.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2741.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_8_temporal_epoch_8:
    target_device: 0
    input_count: 2
    layernorm_2741.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_2741.dc.subtract.1_0, e2e_layernorm_2741.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2741.dc.multiply.2, lc.input_tensor.layernorm_2741.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2741.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2741.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2741.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2741.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2741.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2741.dc.reciprocal.7, lc.input_tensor.layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8, layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2741.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2741.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2741.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2741.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2744: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2741.dc.add.10, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2747: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2744],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_9_temporal_epoch_9:
    target_device: 0
    input_count: 2
    matmul_2750: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_2747_0, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_2754: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_2750, e2e_layernorm_2741.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_2754, lc.input_tensor.layernorm_2755.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2755.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2754, layernorm_2755.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2755.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2755.dc.subtract.1, layernorm_2755.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2755.dc.multiply.2, lc.input_tensor.layernorm_2755.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2755.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2755.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2755.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2755.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2755.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2755.dc.reciprocal.7, lc.input_tensor.layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_2755.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8, layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2755.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_2755.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2755.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_2755.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2758: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2755.dc.add.10, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2764: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_2755.dc.add.10, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2770: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_2758, matmul_2764],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2772: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_2770, input_1_multiply_2772_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2773: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_2772, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2774.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_2773],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_10_temporal_epoch_10:
    target_device: 0
    input_count: 2
    softmax_2774.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_2774.dc.exp.0_0, lc.input_tensor.softmax_2774.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2774.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_2774.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2774.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_2774.dc.exp.0_0, softmax_2774.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2778: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_2755.dc.add.10_0, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2785: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_2774.dc.multiply.3, matmul_2778],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2789: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_2785, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2755.dc.add.10_add_2793: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_2755.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2793: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_2789, buffer_0_layernorm_2755.dc.add.10_add_2793],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_2793, lc.input_tensor.layernorm_2794.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2794.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_2793, layernorm_2794.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2794.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_2794.dc.subtract.1, layernorm_2794.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2794.dc.multiply.2, lc.input_tensor.layernorm_2794.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2794.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_2794.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2794.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_2794.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_2794.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_2794.dc.reciprocal.7, lc.input_tensor.layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2794.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8, layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2794.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_2794.dc.multiply.8, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2794.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_2794.dc.multiply.9, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_11_temporal_epoch_11:
    target_device: 0
    input_count: 2
    matmul_2797: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_2794.dc.add.10_0, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2800: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_2797],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2803: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2800, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2794.dc.add.10_add_2807: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_2794.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2807: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_2803, buffer_0_layernorm_2794.dc.add.10_add_2807],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_2807, lc.input_tensor.layernorm_2808.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2808.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_2807, layernorm_2808.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2808.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2808.dc.subtract.1, layernorm_2808.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2808.dc.multiply.2, lc.input_tensor.layernorm_2808.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2808.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2808.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2808.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2808.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2808.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_2_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2808.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_12_temporal_epoch_12:
    target_device: 0
    input_count: 2
    layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2808.dc.reciprocal.7_0, lc.input_tensor.layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2808.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0, layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2808.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2808.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2808.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2808.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2811: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2808.dc.add.10, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2817: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_2808.dc.add.10, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2823: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2811, matmul_2817],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2825: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_2823, input_1_multiply_2825_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2826: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_2825, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2827.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_2826],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2827.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2827.dc.exp.0, lc.input_tensor.softmax_2827.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2827.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_2827.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2827.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_2827.dc.exp.0, softmax_2827.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2831: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2808.dc.add.10, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2838: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_2827.dc.multiply.3, matmul_2831],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2842: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_2838, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2808.dc.add.10_add_2846: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2808.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2846: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_2842, buffer_0_layernorm_2808.dc.add.10_add_2846],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_2846, lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2847.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_2846, layernorm_2847.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_13_temporal_epoch_13:
    target_device: 0
    input_count: 2
    layernorm_2847.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2847.dc.subtract.1_0, e2e_layernorm_2847.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.2, lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2847.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2847.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2847.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2847.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2847.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2847.dc.reciprocal.7, lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2847.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_2847.dc.subtract.1_0, layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2847.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2847.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.9, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2850: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2847.dc.add.10, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2853: {type: gelu, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_2850],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2856: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_2853, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2847.dc.add.10_add_2860: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2847.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_14_temporal_epoch_14:
    target_device: 0
    input_count: 2
    add_2860: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_2856_0, e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_2860, lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2861.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_2860, layernorm_2861.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2861.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2861.dc.subtract.1, layernorm_2861.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.2, lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2861.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_2861.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2861.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2861.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2861.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_2861.dc.reciprocal.7, lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2861.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8, layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2861.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2861.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2864: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2870: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2876: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_2864, matmul_2870],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2878: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_2876, input_1_multiply_2878_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2879: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_2878, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2880.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_2879],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2880.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_2880.dc.exp.0, lc.input_tensor.softmax_2880.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2880.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_2880.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2880.dc.multiply.3: {type: multiply, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_2880.dc.exp.0, softmax_2880.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2884: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}

  fwd_15_temporal_epoch_15:
    target_device: 0
    input_count: 2
    matmul_2891: {type: matmul, grid_loc: [0, 0], grid_size: [3, 2], inputs: [e2e_softmax_2880.dc.multiply.3_0, e2e_matmul_2884_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2895: {type: matmul, grid_loc: [0, 2], grid_size: [2, 4], inputs: [matmul_2891, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2861.dc.add.10_add_2899: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_2861.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2899: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_2895, buffer_0_layernorm_2861.dc.add.10_add_2899],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2899, lc.input_tensor.layernorm_2900.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2900.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_2899, layernorm_2900.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2900.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2900.dc.subtract.1, layernorm_2900.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [layernorm_2900.dc.multiply.2, lc.input_tensor.layernorm_2900.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2900.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [2, 1], inputs: [layernorm_2900.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2900.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2900.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2900.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2900.dc.reciprocal.7, lc.input_tensor.layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2900.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8, layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2900.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2900.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2900.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_2900.dc.multiply.9, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_16_temporal_epoch_16:
    target_device: 0
    input_count: 2
    matmul_2903: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_2900.dc.add.10_0, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2906: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_2903],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2909: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2906, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_2913: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_2909, e2e_layernorm_2900.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_2913, lc.input_tensor.layernorm_2914.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2914.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_2913, layernorm_2914.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2914.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2914.dc.subtract.1, layernorm_2914.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2914.dc.multiply.2, lc.input_tensor.layernorm_2914.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2914.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2914.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2914.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2914.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2914.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2914.dc.reciprocal.7, lc.input_tensor.layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2914.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_17_temporal_epoch_17:
    target_device: 0
    input_count: 2
    layernorm_2914.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0, e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2914.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2914.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2914.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2914.dc.multiply.9, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2917: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2914.dc.add.10, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2923: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_2914.dc.add.10, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2929: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2917, matmul_2923],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2931: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2929, input_1_multiply_2931_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2932: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_2931, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2933.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2932],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2933.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2933.dc.exp.0, lc.input_tensor.softmax_2933.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2933.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2933.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2933.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_2933.dc.exp.0, softmax_2933.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2937: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2914.dc.add.10, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2944: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_2933.dc.multiply.3, matmul_2937],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_2948: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_2944, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2914.dc.add.10_add_2952: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2914.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2952: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_2948, buffer_0_layernorm_2914.dc.add.10_add_2952],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_2952, lc.input_tensor.layernorm_2953.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2953.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_2952, layernorm_2953.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2953.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_18_temporal_epoch_18:
    target_device: 0
    input_count: 2
    layernorm_2953.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_2953.dc.subtract.1_0, e2e_layernorm_2953.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2953.dc.multiply.2, lc.input_tensor.layernorm_2953.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2953.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2953.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2953.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2953.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2953.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2953.dc.reciprocal.7, lc.input_tensor.layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8, layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2953.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2953.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2953.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2953.dc.multiply.9, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2956: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2953.dc.add.10, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2959: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2956],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_19_temporal_epoch_19:
    target_device: 0
    input_count: 2
    matmul_2962: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_2959_0, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2953.dc.add.10_add_2966: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2953.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2966: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_2962, buffer_0_layernorm_2953.dc.add.10_add_2966],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2966, lc.input_tensor.layernorm_2967.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2967.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_2966, layernorm_2967.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2967.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2967.dc.subtract.1, layernorm_2967.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2967.dc.multiply.2, lc.input_tensor.layernorm_2967.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2967.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2967.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2967.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2967.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2967.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2967.dc.reciprocal.7, lc.input_tensor.layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2967.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8, layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2967.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2967.dc.multiply.8, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2967.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2967.dc.multiply.9, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2970: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_2967.dc.add.10, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2976: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2967.dc.add.10, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2982: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_2970, matmul_2976],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2984: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_2982, input_1_multiply_2984_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2985: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_2984, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2986.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_2985],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2986.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_2986.dc.exp.0, lc.input_tensor.softmax_2986.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2986.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_2986.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_20_temporal_epoch_20:
    target_device: 0
    input_count: 2
    softmax_2986.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_2986.dc.exp.0_0, e2e_softmax_2986.dc.reciprocal.2_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2990: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_2967.dc.add.10_0, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2997: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_2986.dc.multiply.3, matmul_2990],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3001: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_2997, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2967.dc.add.10_add_3005: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_2967.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3005: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3001, buffer_0_layernorm_2967.dc.add.10_add_3005],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3005, lc.input_tensor.layernorm_3006.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3006.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_3005, layernorm_3006.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3006.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3006.dc.subtract.1, layernorm_3006.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3006.dc.multiply.2, lc.input_tensor.layernorm_3006.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3006.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3006.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3006.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3006.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3006.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_3006.dc.reciprocal.7, lc.input_tensor.layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_3006.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8, layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3006.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3006.dc.multiply.8, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3006.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3006.dc.multiply.9, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_21_temporal_epoch_21:
    target_device: 0
    input_count: 2
    matmul_3009: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3006.dc.add.10_0, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3012: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3009],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3015: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3012, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3006.dc.add.10_add_3019: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3006.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3019: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3015, buffer_0_layernorm_3006.dc.add.10_add_3019],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3019, lc.input_tensor.layernorm_3020.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3020.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3019, layernorm_3020.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3020.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3020.dc.subtract.1, layernorm_3020.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3020.dc.multiply.2, lc.input_tensor.layernorm_3020.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3020.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3020.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3020.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3020.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3020.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_2_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3020.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_22_temporal_epoch_22:
    target_device: 0
    input_count: 2
    layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3020.dc.reciprocal.7_0, lc.input_tensor.layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3020.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0, layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3020.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3020.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3020.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3020.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3023: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3020.dc.add.10, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3029: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3020.dc.add.10, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3035: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3023, matmul_3029],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3037: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3035, input_1_multiply_3037_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3038: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3037, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3039.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3038],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3039.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3039.dc.exp.0, lc.input_tensor.softmax_3039.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3039.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3039.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3039.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3039.dc.exp.0, softmax_3039.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3043: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3020.dc.add.10, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3050: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3039.dc.multiply.3, matmul_3043],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3054: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3050, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3020.dc.add.10_add_3058: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3020.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3058: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3054, buffer_0_layernorm_3020.dc.add.10_add_3058],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3058, lc.input_tensor.layernorm_3059.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3059.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3058, layernorm_3059.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_23_temporal_epoch_23:
    target_device: 0
    input_count: 2
    layernorm_3059.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_layernorm_3059.dc.subtract.1_0, e2e_layernorm_3059.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3059.dc.multiply.2, lc.input_tensor.layernorm_3059.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3059.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3059.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3059.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3059.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3059.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3059.dc.reciprocal.7, lc.input_tensor.layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3059.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8, layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3059.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3059.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3059.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3059.dc.multiply.9, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3062: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3059.dc.add.10, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3065: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3062],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_24_temporal_epoch_24:
    target_device: 0
    input_count: 2
    matmul_3068: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3065_0, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_3072: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3068, e2e_layernorm_3059.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3072, lc.input_tensor.layernorm_3073.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3073.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3072, layernorm_3073.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3073.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3073.dc.subtract.1, layernorm_3073.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3073.dc.multiply.2, lc.input_tensor.layernorm_3073.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3073.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3073.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3073.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3073.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3073.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3073.dc.reciprocal.7, lc.input_tensor.layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3073.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8, layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3073.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3073.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3073.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3073.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3076: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3073.dc.add.10, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3082: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_3073.dc.add.10, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3088: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_3076, matmul_3082],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3090: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3088, input_1_multiply_3090_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3091: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_3090, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3092.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_3091],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_25_temporal_epoch_25:
    target_device: 0
    input_count: 2
    softmax_3092.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3092.dc.exp.0_0, lc.input_tensor.softmax_3092.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3092.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_3092.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3092.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3092.dc.exp.0_0, softmax_3092.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3096: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_3073.dc.add.10_0, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3103: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_3092.dc.multiply.3, matmul_3096],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3107: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3103, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3073.dc.add.10_add_3111: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3073.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3111: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_3107, buffer_0_layernorm_3073.dc.add.10_add_3111],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3111, lc.input_tensor.layernorm_3112.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3112.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3111, layernorm_3112.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3112.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3112.dc.subtract.1, layernorm_3112.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3112.dc.multiply.2, lc.input_tensor.layernorm_3112.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3112.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_3112.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3112.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3112.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3112.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3112.dc.reciprocal.7, lc.input_tensor.layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3112.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8, layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3112.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_3112.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3112.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_3112.dc.multiply.9, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_26_temporal_epoch_26:
    target_device: 0
    input_count: 2
    matmul_3115: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3112.dc.add.10_0, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3118: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3115],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3121: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3118, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3112.dc.add.10_add_3125: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3112.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3125: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3121, buffer_0_layernorm_3112.dc.add.10_add_3125],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3125, lc.input_tensor.layernorm_3126.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3126.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3125, layernorm_3126.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3126.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3126.dc.subtract.1, layernorm_3126.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3126.dc.multiply.2, lc.input_tensor.layernorm_3126.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3126.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3126.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3126.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3126.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3126.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_2_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3126.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_27_temporal_epoch_27:
    target_device: 0
    input_count: 2
    layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3126.dc.reciprocal.7_0, lc.input_tensor.layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3126.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0, layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3126.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3126.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3126.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3126.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3129: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3126.dc.add.10, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3135: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3126.dc.add.10, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3141: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3129, matmul_3135],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3143: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3141, input_1_multiply_3143_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3144: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3143, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3145.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3144],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3145.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3145.dc.exp.0, lc.input_tensor.softmax_3145.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3145.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3145.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3145.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3145.dc.exp.0, softmax_3145.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3149: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3126.dc.add.10, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3156: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3145.dc.multiply.3, matmul_3149],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3160: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3156, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3126.dc.add.10_add_3164: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3126.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3164: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3160, buffer_0_layernorm_3126.dc.add.10_add_3164],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3164, lc.input_tensor.layernorm_3165.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3165.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3164, layernorm_3165.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_28_temporal_epoch_28:
    target_device: 0
    input_count: 2
    layernorm_3165.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3165.dc.subtract.1_0, e2e_layernorm_3165.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3165.dc.multiply.2, lc.input_tensor.layernorm_3165.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3165.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3165.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3165.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3165.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3165.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3165.dc.reciprocal.7, lc.input_tensor.layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3165.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3165.dc.subtract.1_0, layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3165.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3165.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3165.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3165.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3168: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3165.dc.add.10, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3171: {type: gelu, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3168],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3174: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_3171, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3165.dc.add.10_add_3178: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3165.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_29_temporal_epoch_29:
    target_device: 0
    input_count: 2
    add_3178: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3174_0, e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3178, lc.input_tensor.layernorm_3179.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3179.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3178, layernorm_3179.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3179.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3179.dc.subtract.1, layernorm_3179.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3179.dc.multiply.2, lc.input_tensor.layernorm_3179.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3179.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3179.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3179.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3179.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3179.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3179.dc.reciprocal.7, lc.input_tensor.layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3179.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8, layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3179.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3179.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3179.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3179.dc.multiply.9, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3182: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3179.dc.add.10, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3188: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3179.dc.add.10, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3194: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3182, matmul_3188],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3196: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3194, input_1_multiply_3196_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3197: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3196, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3198.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3197],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3198.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3198.dc.exp.0, lc.input_tensor.softmax_3198.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3198.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3198.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3198.dc.multiply.3: {type: multiply, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3198.dc.exp.0, softmax_3198.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3202: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3179.dc.add.10, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}

  fwd_30_temporal_epoch_30:
    target_device: 0
    input_count: 2
    matmul_3209: {type: matmul, grid_loc: [0, 0], grid_size: [3, 2], inputs: [e2e_softmax_3198.dc.multiply.3_0, e2e_matmul_3202_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3213: {type: matmul, grid_loc: [0, 2], grid_size: [2, 4], inputs: [matmul_3209, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3179.dc.add.10_add_3217: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3179.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3217: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3213, buffer_0_layernorm_3179.dc.add.10_add_3217],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3217, lc.input_tensor.layernorm_3218.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3218.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3217, layernorm_3218.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3218.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3218.dc.subtract.1, layernorm_3218.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [layernorm_3218.dc.multiply.2, lc.input_tensor.layernorm_3218.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3218.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [2, 1], inputs: [layernorm_3218.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3218.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3218.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3218.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3218.dc.reciprocal.7, lc.input_tensor.layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3218.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8, layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3218.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3218.dc.multiply.8, layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3218.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3218.dc.multiply.9, layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_31_temporal_epoch_31:
    target_device: 0
    input_count: 2
    matmul_3221: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3218.dc.add.10_0, layer.12.intermediate.dense.weight, layer.12.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3224: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3221],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3227: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3224, layer.12.output.dense.weight, layer.12.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_3231: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_3227, e2e_layernorm_3218.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_3231, lc.input_tensor.layernorm_3232.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3232.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3231, layernorm_3232.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3232.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3232.dc.subtract.1, layernorm_3232.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3232.dc.multiply.2, lc.input_tensor.layernorm_3232.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3232.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3232.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3232.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3232.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3232.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3232.dc.reciprocal.7, lc.input_tensor.layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3232.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_32_temporal_epoch_32:
    target_device: 0
    input_count: 2
    layernorm_3232.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0, e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3232.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3232.dc.multiply.8, layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3232.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3232.dc.multiply.9, layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3235: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3232.dc.add.10, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3241: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3232.dc.add.10, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3247: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3235, matmul_3241],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3249: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3247, input_1_multiply_3249_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3250: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_3249, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3251.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_3250],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3251.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3251.dc.exp.0, lc.input_tensor.softmax_3251.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3251.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3251.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3251.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3251.dc.exp.0, softmax_3251.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3255: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3232.dc.add.10, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3262: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_3251.dc.multiply.3, matmul_3255],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3266: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3262, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3232.dc.add.10_add_3270: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3232.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3270: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3266, buffer_0_layernorm_3232.dc.add.10_add_3270],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3270, lc.input_tensor.layernorm_3271.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3271.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3270, layernorm_3271.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3271.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_33_temporal_epoch_33:
    target_device: 0
    input_count: 2
    layernorm_3271.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_3271.dc.subtract.1_0, e2e_layernorm_3271.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3271.dc.multiply.2, lc.input_tensor.layernorm_3271.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3271.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3271.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3271.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3271.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3271.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3271.dc.reciprocal.7, lc.input_tensor.layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8, layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3271.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3271.dc.multiply.8, layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3271.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3271.dc.multiply.9, layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3274: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3271.dc.add.10, layer.13.intermediate.dense.weight, layer.13.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3277: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3274],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_34_temporal_epoch_34:
    target_device: 0
    input_count: 2
    matmul_3280: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3277_0, layer.13.output.dense.weight, layer.13.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3271.dc.add.10_add_3284: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3271.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3284: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_3280, buffer_0_layernorm_3271.dc.add.10_add_3284],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3284, lc.input_tensor.layernorm_3285.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3285.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3284, layernorm_3285.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3285.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3285.dc.subtract.1, layernorm_3285.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3285.dc.multiply.2, lc.input_tensor.layernorm_3285.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3285.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3285.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3285.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3285.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3285.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3285.dc.reciprocal.7, lc.input_tensor.layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3285.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8, layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3285.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3285.dc.multiply.8, layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3285.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3285.dc.multiply.9, layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3288: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_3285.dc.add.10, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3294: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3285.dc.add.10, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3300: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_3288, matmul_3294],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3302: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_3300, input_1_multiply_3302_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3303: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_3302, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3304.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_3303],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3304.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_3304.dc.exp.0, lc.input_tensor.softmax_3304.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3304.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_3304.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_35_temporal_epoch_35:
    target_device: 0
    input_count: 2
    softmax_3304.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3304.dc.exp.0_0, e2e_softmax_3304.dc.reciprocal.2_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3308: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_3285.dc.add.10_0, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3315: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_3304.dc.multiply.3, matmul_3308],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3319: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_3315, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3285.dc.add.10_add_3323: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3285.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3323: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3319, buffer_0_layernorm_3285.dc.add.10_add_3323],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3323, lc.input_tensor.layernorm_3324.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3324.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_3323, layernorm_3324.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3324.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3324.dc.subtract.1, layernorm_3324.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3324.dc.multiply.2, lc.input_tensor.layernorm_3324.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3324.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3324.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3324.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3324.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3324.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_3324.dc.reciprocal.7, lc.input_tensor.layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_3324.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8, layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3324.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3324.dc.multiply.8, layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3324.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3324.dc.multiply.9, layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_36_temporal_epoch_36:
    target_device: 0
    input_count: 2
    matmul_3327: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3324.dc.add.10_0, layer.14.intermediate.dense.weight, layer.14.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3330: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3327],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3333: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3330, layer.14.output.dense.weight, layer.14.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3324.dc.add.10_add_3337: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3324.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3337: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3333, buffer_0_layernorm_3324.dc.add.10_add_3337],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3337, lc.input_tensor.layernorm_3338.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3338.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3337, layernorm_3338.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3338.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3338.dc.subtract.1, layernorm_3338.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3338.dc.multiply.2, lc.input_tensor.layernorm_3338.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3338.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3338.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3338.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3338.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3338.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_2_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3338.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_37_temporal_epoch_37:
    target_device: 0
    input_count: 2
    layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3338.dc.reciprocal.7_0, lc.input_tensor.layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3338.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0, layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3338.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3338.dc.multiply.8, layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3338.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3338.dc.multiply.9, layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3341: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3338.dc.add.10, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3347: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3338.dc.add.10, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3353: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3341, matmul_3347],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3355: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3353, input_1_multiply_3355_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3356: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3355, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3357.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3356],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3357.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3357.dc.exp.0, lc.input_tensor.softmax_3357.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3357.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3357.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3357.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3357.dc.exp.0, softmax_3357.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3361: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3338.dc.add.10, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3368: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3357.dc.multiply.3, matmul_3361],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3372: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3368, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3338.dc.add.10_add_3376: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3338.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3376: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3372, buffer_0_layernorm_3338.dc.add.10_add_3376],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3376, lc.input_tensor.layernorm_3377.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3377.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3376, layernorm_3377.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_38_temporal_epoch_38:
    target_device: 0
    input_count: 2
    layernorm_3377.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_layernorm_3377.dc.subtract.1_0, e2e_layernorm_3377.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3377.dc.multiply.2, lc.input_tensor.layernorm_3377.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3377.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3377.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3377.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3377.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3377.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3377.dc.reciprocal.7, lc.input_tensor.layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3377.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8, layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3377.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3377.dc.multiply.8, layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3377.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3377.dc.multiply.9, layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3380: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3377.dc.add.10, layer.15.intermediate.dense.weight, layer.15.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3383: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3380],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_39_temporal_epoch_39:
    target_device: 0
    input_count: 2
    matmul_3386: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3383_0, layer.15.output.dense.weight, layer.15.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_3390: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3386, e2e_layernorm_3377.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3390, lc.input_tensor.layernorm_3391.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3391.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3390, layernorm_3391.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3391.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3391.dc.subtract.1, layernorm_3391.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3391.dc.multiply.2, lc.input_tensor.layernorm_3391.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3391.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3391.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3391.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3391.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3391.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3391.dc.reciprocal.7, lc.input_tensor.layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3391.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8, layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3391.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3391.dc.multiply.8, layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3391.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3391.dc.multiply.9, layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3394: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3391.dc.add.10, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3400: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_3391.dc.add.10, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3406: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_3394, matmul_3400],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3408: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3406, input_1_multiply_3408_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3409: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_3408, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3410.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_3409],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_40_temporal_epoch_40:
    target_device: 0
    input_count: 2
    softmax_3410.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3410.dc.exp.0_0, lc.input_tensor.softmax_3410.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3410.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_3410.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3410.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3410.dc.exp.0_0, softmax_3410.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3414: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_3391.dc.add.10_0, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3421: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_3410.dc.multiply.3, matmul_3414],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3425: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3421, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3391.dc.add.10_add_3429: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3391.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3429: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_3425, buffer_0_layernorm_3391.dc.add.10_add_3429],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3429, lc.input_tensor.layernorm_3430.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3430.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3429, layernorm_3430.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3430.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3430.dc.subtract.1, layernorm_3430.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3430.dc.multiply.2, lc.input_tensor.layernorm_3430.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3430.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_3430.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3430.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3430.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3430.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3430.dc.reciprocal.7, lc.input_tensor.layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3430.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8, layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3430.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_3430.dc.multiply.8, layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3430.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_3430.dc.multiply.9, layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_41_temporal_epoch_41:
    target_device: 0
    input_count: 2
    matmul_3433: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3430.dc.add.10_0, layer.16.intermediate.dense.weight, layer.16.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3436: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3433],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3439: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3436, layer.16.output.dense.weight, layer.16.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3430.dc.add.10_add_3443: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3430.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3443: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3439, buffer_0_layernorm_3430.dc.add.10_add_3443],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3443, lc.input_tensor.layernorm_3444.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3444.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3443, layernorm_3444.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3444.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3444.dc.subtract.1, layernorm_3444.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3444.dc.multiply.2, lc.input_tensor.layernorm_3444.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3444.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3444.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3444.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3444.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3444.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_2_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3444.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_42_temporal_epoch_42:
    target_device: 0
    input_count: 2
    layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3444.dc.reciprocal.7_0, lc.input_tensor.layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3444.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0, layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3444.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3444.dc.multiply.8, layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3444.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3444.dc.multiply.9, layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3447: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3444.dc.add.10, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3453: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3444.dc.add.10, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3459: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3447, matmul_3453],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3461: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3459, input_1_multiply_3461_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3462: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3461, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3463.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3462],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3463.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3463.dc.exp.0, lc.input_tensor.softmax_3463.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3463.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3463.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3463.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3463.dc.exp.0, softmax_3463.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3467: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3444.dc.add.10, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3474: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3463.dc.multiply.3, matmul_3467],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3478: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3474, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3444.dc.add.10_add_3482: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3444.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3482: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3478, buffer_0_layernorm_3444.dc.add.10_add_3482],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3482, lc.input_tensor.layernorm_3483.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3483.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3482, layernorm_3483.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_43_temporal_epoch_43:
    target_device: 0
    input_count: 2
    layernorm_3483.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3483.dc.subtract.1_0, e2e_layernorm_3483.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3483.dc.multiply.2, lc.input_tensor.layernorm_3483.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3483.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3483.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3483.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3483.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3483.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3483.dc.reciprocal.7, lc.input_tensor.layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3483.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3483.dc.subtract.1_0, layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3483.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3483.dc.multiply.8, layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3483.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3483.dc.multiply.9, layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3486: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3483.dc.add.10, layer.17.intermediate.dense.weight, layer.17.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3489: {type: gelu, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3486],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3492: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_3489, layer.17.output.dense.weight, layer.17.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3483.dc.add.10_add_3496: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3483.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_44_temporal_epoch_44:
    target_device: 0
    input_count: 2
    add_3496: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3492_0, e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3496, lc.input_tensor.layernorm_3497.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3497.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3496, layernorm_3497.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3497.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3497.dc.subtract.1, layernorm_3497.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3497.dc.multiply.2, lc.input_tensor.layernorm_3497.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3497.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3497.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3497.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3497.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3497.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3497.dc.reciprocal.7, lc.input_tensor.layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3497.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8, layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3497.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3497.dc.multiply.8, layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3497.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3497.dc.multiply.9, layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3500: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3497.dc.add.10, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3506: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3497.dc.add.10, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3512: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3500, matmul_3506],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3514: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3512, input_1_multiply_3514_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3515: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3514, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3516.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3515],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3516.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3516.dc.exp.0, lc.input_tensor.softmax_3516.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3516.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3516.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3516.dc.multiply.3: {type: multiply, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3516.dc.exp.0, softmax_3516.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3520: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3497.dc.add.10, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}

  fwd_45_temporal_epoch_45:
    target_device: 0
    input_count: 2
    matmul_3527: {type: matmul, grid_loc: [0, 0], grid_size: [3, 2], inputs: [e2e_softmax_3516.dc.multiply.3_0, e2e_matmul_3520_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3531: {type: matmul, grid_loc: [0, 2], grid_size: [2, 4], inputs: [matmul_3527, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3497.dc.add.10_add_3535: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3497.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3535: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3531, buffer_0_layernorm_3497.dc.add.10_add_3535],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3535, lc.input_tensor.layernorm_3536.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3536.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3535, layernorm_3536.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3536.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3536.dc.subtract.1, layernorm_3536.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [layernorm_3536.dc.multiply.2, lc.input_tensor.layernorm_3536.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3536.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [2, 1], inputs: [layernorm_3536.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3536.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3536.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3536.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3536.dc.reciprocal.7, lc.input_tensor.layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3536.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8, layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3536.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3536.dc.multiply.8, layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3536.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3536.dc.multiply.9, layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_46_temporal_epoch_46:
    target_device: 0
    input_count: 2
    matmul_3539: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3536.dc.add.10_0, layer.18.intermediate.dense.weight, layer.18.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3542: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3539],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3545: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3542, layer.18.output.dense.weight, layer.18.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_3549: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_3545, e2e_layernorm_3536.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_3549, lc.input_tensor.layernorm_3550.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3550.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3549, layernorm_3550.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3550.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3550.dc.subtract.1, layernorm_3550.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3550.dc.multiply.2, lc.input_tensor.layernorm_3550.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3550.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3550.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3550.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3550.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3550.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3550.dc.reciprocal.7, lc.input_tensor.layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3550.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_47_temporal_epoch_47:
    target_device: 0
    input_count: 2
    layernorm_3550.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0, e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3550.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3550.dc.multiply.8, layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3550.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3550.dc.multiply.9, layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3553: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3550.dc.add.10, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3559: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3550.dc.add.10, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3565: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3553, matmul_3559],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3567: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3565, input_1_multiply_3567_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3568: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_3567, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3569.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_3568],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3569.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3569.dc.exp.0, lc.input_tensor.softmax_3569.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3569.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3569.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3569.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3569.dc.exp.0, softmax_3569.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3573: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3550.dc.add.10, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3580: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_3569.dc.multiply.3, matmul_3573],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3584: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3580, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3550.dc.add.10_add_3588: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3550.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3588: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3584, buffer_0_layernorm_3550.dc.add.10_add_3588],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3588, lc.input_tensor.layernorm_3589.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3589.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3588, layernorm_3589.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3589.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_48_temporal_epoch_48:
    target_device: 0
    input_count: 2
    layernorm_3589.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_3589.dc.subtract.1_0, e2e_layernorm_3589.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3589.dc.multiply.2, lc.input_tensor.layernorm_3589.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3589.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3589.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3589.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3589.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3589.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3589.dc.reciprocal.7, lc.input_tensor.layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8, layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3589.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3589.dc.multiply.8, layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3589.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3589.dc.multiply.9, layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3592: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3589.dc.add.10, layer.19.intermediate.dense.weight, layer.19.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3595: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3592],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_49_temporal_epoch_49:
    target_device: 0
    input_count: 2
    matmul_3598: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3595_0, layer.19.output.dense.weight, layer.19.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3589.dc.add.10_add_3602: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3589.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3602: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_3598, buffer_0_layernorm_3589.dc.add.10_add_3602],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3602, lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3603.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3602, layernorm_3603.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3603.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3603.dc.subtract.1, layernorm_3603.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.2, lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3603.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3603.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3603.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3603.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3603.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3603.dc.reciprocal.7, lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3603.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8, layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3603.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.8, layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3603.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.9, layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3606: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_3603.dc.add.10, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3612: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3603.dc.add.10, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3618: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_3606, matmul_3612],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3620: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_3618, input_1_multiply_3620_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3621: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_3620, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3622.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_3621],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3622.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_3622.dc.exp.0, lc.input_tensor.softmax_3622.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3622.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_3622.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_50_temporal_epoch_50:
    target_device: 0
    input_count: 2
    softmax_3622.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3622.dc.exp.0_0, e2e_softmax_3622.dc.reciprocal.2_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3626: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_3603.dc.add.10_0, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3633: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_3622.dc.multiply.3, matmul_3626],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3637: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_3633, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3603.dc.add.10_add_3641: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3603.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3641: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3637, buffer_0_layernorm_3603.dc.add.10_add_3641],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3641, lc.input_tensor.layernorm_3642.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3642.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_3641, layernorm_3642.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3642.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3642.dc.subtract.1, layernorm_3642.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3642.dc.multiply.2, lc.input_tensor.layernorm_3642.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3642.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3642.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3642.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3642.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3642.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_3642.dc.reciprocal.7, lc.input_tensor.layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_3642.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8, layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3642.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3642.dc.multiply.8, layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3642.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3642.dc.multiply.9, layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_51_temporal_epoch_51:
    target_device: 0
    input_count: 2
    matmul_3645: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3642.dc.add.10_0, layer.20.intermediate.dense.weight, layer.20.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3648: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3645],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3651: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3648, layer.20.output.dense.weight, layer.20.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3642.dc.add.10_add_3655: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3642.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3655: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3651, buffer_0_layernorm_3642.dc.add.10_add_3655],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3655, lc.input_tensor.layernorm_3656.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3656.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3655, layernorm_3656.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3656.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3656.dc.subtract.1, layernorm_3656.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3656.dc.multiply.2, lc.input_tensor.layernorm_3656.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3656.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3656.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3656.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3656.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3656.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_2_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3656.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_52_temporal_epoch_52:
    target_device: 0
    input_count: 2
    layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3656.dc.reciprocal.7_0, lc.input_tensor.layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3656.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0, layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3656.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3656.dc.multiply.8, layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3656.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3656.dc.multiply.9, layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3659: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3656.dc.add.10, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3665: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3656.dc.add.10, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3671: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3659, matmul_3665],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3673: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3671, input_1_multiply_3673_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3674: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3673, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3675.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3674],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3675.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3675.dc.exp.0, lc.input_tensor.softmax_3675.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3675.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3675.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3675.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3675.dc.exp.0, softmax_3675.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3679: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3656.dc.add.10, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3686: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3675.dc.multiply.3, matmul_3679],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3690: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3686, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3656.dc.add.10_add_3694: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3656.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3694: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3690, buffer_0_layernorm_3656.dc.add.10_add_3694],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3694, lc.input_tensor.layernorm_3695.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3695.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3694, layernorm_3695.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_53_temporal_epoch_53:
    target_device: 0
    input_count: 2
    layernorm_3695.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_layernorm_3695.dc.subtract.1_0, e2e_layernorm_3695.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3695.dc.multiply.2, lc.input_tensor.layernorm_3695.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3695.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3695.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3695.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3695.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3695.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3695.dc.reciprocal.7, lc.input_tensor.layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3695.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8, layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3695.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3695.dc.multiply.8, layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3695.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3695.dc.multiply.9, layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3698: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3695.dc.add.10, layer.21.intermediate.dense.weight, layer.21.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3701: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3698],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_54_temporal_epoch_54:
    target_device: 0
    input_count: 2
    matmul_3704: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3701_0, layer.21.output.dense.weight, layer.21.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_3708: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3704, e2e_layernorm_3695.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3708, lc.input_tensor.layernorm_3709.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3709.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3708, layernorm_3709.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3709.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3709.dc.subtract.1, layernorm_3709.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3709.dc.multiply.2, lc.input_tensor.layernorm_3709.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3709.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3709.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3709.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3709.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3709.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3709.dc.reciprocal.7, lc.input_tensor.layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3709.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8, layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3709.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3709.dc.multiply.8, layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3709.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3709.dc.multiply.9, layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3712: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3709.dc.add.10, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3718: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_3709.dc.add.10, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3724: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_3712, matmul_3718],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3726: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3724, input_1_multiply_3726_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3727: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_3726, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3728.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_3727],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_55_temporal_epoch_55:
    target_device: 0
    input_count: 2
    softmax_3728.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3728.dc.exp.0_0, lc.input_tensor.softmax_3728.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3728.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_3728.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3728.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3728.dc.exp.0_0, softmax_3728.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3732: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_3709.dc.add.10_0, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3739: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_3728.dc.multiply.3, matmul_3732],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    matmul_3743: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3739, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3709.dc.add.10_add_3747: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3709.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3747: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_3743, buffer_0_layernorm_3709.dc.add.10_add_3747],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3747, lc.input_tensor.layernorm_3748.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3748.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3747, layernorm_3748.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3748.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3748.dc.subtract.1, layernorm_3748.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3748.dc.multiply.2, lc.input_tensor.layernorm_3748.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3748.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_3748.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3748.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3748.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3748.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3748.dc.reciprocal.7, lc.input_tensor.layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3748.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8, layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3748.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_3748.dc.multiply.8, layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3748.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_3748.dc.multiply.9, layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_56_temporal_epoch_56:
    target_device: 0
    input_count: 2
    matmul_3751: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3748.dc.add.10_0, layer.22.intermediate.dense.weight, layer.22.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3754: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3751],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3757: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3754, layer.22.output.dense.weight, layer.22.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3748.dc.add.10_add_3761: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3748.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3761: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3757, buffer_0_layernorm_3748.dc.add.10_add_3761],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3761, lc.input_tensor.layernorm_3762.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3762.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3761, layernorm_3762.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3762.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3762.dc.subtract.1, layernorm_3762.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3762.dc.multiply.2, lc.input_tensor.layernorm_3762.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3762.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3762.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3762.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3762.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3762.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_2_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3762.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_57_temporal_epoch_57:
    target_device: 0
    input_count: 2
    layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3762.dc.reciprocal.7_0, lc.input_tensor.layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3762.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0, layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3762.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3762.dc.multiply.8, layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3762.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3762.dc.multiply.9, layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3765: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3762.dc.add.10, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3771: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3762.dc.add.10, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3777: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3765, matmul_3771],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3779: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3777, input_1_multiply_3779_tile_bcast_tile_bcast],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3780: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3779, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3781.dc.exp.0: {type: exp, grid_loc: [4, 6], grid_size: [2, 2], inputs: [add_3780],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3781.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_3781.dc.exp.0, lc.input_tensor.softmax_3781.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3781.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 1], grid_size: [2, 1], inputs: [softmax_3781.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3781.dc.multiply.3: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_3781.dc.exp.0, softmax_3781.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3785: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [layernorm_3762.dc.add.10, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3792: {type: matmul, grid_loc: [6, 3], grid_size: [3, 2], inputs: [softmax_3781.dc.multiply.3, matmul_3785],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 12}}
    buffer_0_layernorm_3762.dc.add.10_add_3800: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3762.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_58_temporal_epoch_58:
    target_device: 0
    input_count: 2
    matmul_3796: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_3792_0, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_3800: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_3796, e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_3800, lc.input_tensor.layernorm_3801.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3801.dc.subtract.1: {type: subtract, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_3800, layernorm_3801.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3801.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3801.dc.subtract.1, layernorm_3801.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3801.dc.multiply.2, lc.input_tensor.layernorm_3801.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3801.dc.add.5: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3801.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3801.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.sqrt.6: {type: sqrt, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3801.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3801.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3801.dc.reciprocal.7, lc.input_tensor.layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3801.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [buffer_2_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.multiply.8: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8, layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3801.dc.multiply.9: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3801.dc.multiply.8, layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3801.dc.add.10: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3801.dc.multiply.9, layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3804: {type: matmul, grid_loc: [6, 0], grid_size: [4, 8], inputs: [layernorm_3801.dc.add.10, layer.23.intermediate.dense.weight, layer.23.intermediate.dense.bias],
         t: 1, mblock: [3, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0_layernorm_3801.dc.add.10_add_3814: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3801.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_59_temporal_epoch_59:
    target_device: 0
    input_count: 2
    gelu_3807: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_3804_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3810: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_3807, layer.23.output.dense.weight, layer.23.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_3814: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_3810, e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_3814, lc.input_tensor.layernorm_3815.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3815.dc.subtract.1: {type: subtract, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_3814, layernorm_3815.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3815.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3815.dc.subtract.1, layernorm_3815.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3815.dc.multiply.2, lc.input_tensor.layernorm_3815.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3815.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3815.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3815.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.sqrt.6: {type: sqrt, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3815.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3815.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3815.dc.reciprocal.7, lc.input_tensor.layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3815.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [buffer_2_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8, layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3815.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3815.dc.multiply.8, layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3815.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3815.dc.multiply.9, layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 2, $c_zero: 0, $lptr_q82: 0, $gptr_q81: 0, $lptr_q81: 0, $lptr_q79: 0, $gptr_q78: 0, $gptr_q75: 0, $gptr_q77: 0, $gptr_q70: 0, $lptr_q68: 0, $lptr_q67: 0, $gptr_q65: 0, $gptr_q67: 0, $gptr_q64: 0, $lptr_q64: 0, $lptr_q71: 0, $gptr_q63: 0, $gptr_q61: 0, $gptr_q60: 0, $lptr_q60: 0, $gptr_q58: 0, $gptr_q57: 0, $lptr_q57: 0, $gptr_q56: 0, $lptr_q54: 0, $lptr_q53: 0, $lptr_q50: 0, $gptr_q49: 0, $lptr_q49: 0, $gptr_q79: 0, $gptr_q54: 0, $gptr_q53: 0, $gptr_q47: 0, $lptr_q47: 0, $gptr_q44: 0, $gptr_q72: 0, $gptr_q71: 0, $gptr_q68: 0, $lptr_q30: 0, $gptr_q32: 0, $gptr_q35: 0, $lptr_q25: 0, $gptr_q30: 0, $gptr_q43: 0, $lptr_q32: 0, $gptr_q33: 0, $gptr_q82: 0, $lptr_q65: 0, $gptr_q29: 0, $lptr_q72: 0, $lptr_q51: 0, $lptr_q26: 0, $lptr_q16: 0, $gptr_q39: 0, $lptr_q58: 0, $gptr_q83: 0, $lptr_q23: 0, $gptr_q51: 0, $lptr_q39: 0, $gptr_q25: 0, $gptr_q26: 0, $lptr_q9: 0, $gptr_q40: 0, $lptr_q2: 0, $gptr_q37: 0, $gptr_q42: 0, $lptr_q14: 0, $gptr_q18: 0, $lptr_q74: 0, $lptr_q21: 0, $lptr_q29: 0, $lptr_q36: 0, $gptr_q5: 0, $gptr_q28: 0, $gptr_q2: 0, $gptr_q36: 0, $lptr_q83: 0, $lptr_q4: 0, $lptr_q56: 0, $lptr_q35: 0, $lptr_q70: 0, $lptr_q44: 0, $lptr_q43: 0, $gptr_q4: 0, $lptr_q33: 0, $lptr_q61: 0, $lptr_q7: 0, $gptr_q22: 0, $c_one: 1, $gptr_q9: 0, $lptr_q78: 0, $gptr_q50: 0, $lptr_q46: 0, $gptr_q21: 0, $lptr_q5: 0, $lptr_q19: 0, $gptr_q19: 0, $lptr_q18: 0, $lptr_q8: 0, $lptr_q40: 0, $gptr_q8: 0, $gptr_q14: 0, $lptr_q28: 0, $lptr_q11: 0, $gptr_q23: 0, $lptr_q15: 0, $gptr_q74: 0, $gptr_q11: 0, $gptr_q46: 0, $lptr_q22: 0, $lptr_q12: 0, $lptr_q75: 0, $lptr_q37: 0, $gptr_q15: 0, $lptr_q63: 0, $gptr_q12: 0, $lptr_q77: 0, $gptr_q16: 0, $gptr_q7: 0, $lptr_q42: 0}
    - staticvar: {$gptr_q80: 0, $lptr_q80: 0, $gptr_q76_shadow: 0, $gptr_q76: 0, $gptr_q69_shadow: 0, $gptr_q69: 0, $lptr_q69: 0, $gptr_q66_shadow: 0, $gptr_q73: 0, $gptr_q66: 0, $gptr_q34: 0, $lptr_q3: 0, $lptr_q0: 0, $lptr_q10: 0, $gptr_q48_shadow: 0, $gptr_q1: 0, $gptr_q31_shadow: 0, $lptr_q59: 0, $gptr_q20: 0, $gptr_q6: 0, $gptr_q17: 0, $gptr_q45: 0, $gptr_q73_shadow: 0, $gptr_q62_shadow: 0, $gptr_q17_shadow: 0, $gptr_q0: 0, $gptr_q20_shadow: 0, $lptr_q66: 0, $gptr_q10_shadow: 0, $gptr_q41: 0, $gptr_q24: 0, $lptr_q20: 0, $gptr_q27_shadow: 0, $gptr_q52_shadow: 0, $gptr_q27: 0, $lptr_q6: 0, $gptr_q38_shadow: 0, $lptr_q34: 0, $lptr_q45: 0, $gptr_q6_shadow: 0, $gptr_q45_shadow: 0, $gptr_q34_shadow: 0, $lptr_q31: 0, $lptr_q27: 0, $gptr_q3_shadow: 0, $gptr_q31: 0, $lptr_q24: 0, $gptr_q52: 0, $gptr_q55_shadow: 0, $gptr_q24_shadow: 0, $gptr_q10: 0, $lptr_q13: 0, $lptr_q1: 0, $gptr_q13_shadow: 0, $gptr_q13: 0, $lptr_q17: 0, $lptr_q41: 0, $gptr_q1_shadow: 0, $gptr_q41_shadow: 0, $lptr_q48: 0, $lptr_q73: 0, $lptr_q52: 0, $lptr_q38: 0, $gptr_q48: 0, $lptr_q55: 0, $gptr_q55: 0, $gptr_q3: 0, $gptr_q59: 0, $gptr_q38: 0, $gptr_q59_shadow: 0, $lptr_q76: 0, $lptr_q62: 0, $gptr_q62: 0}
    - varinst: [$gptr_q38, set, $gptr_q38_shadow]
    - varinst: [$gptr_q34, set, $gptr_q34_shadow]
    - varinst: [$gptr_q31, set, $gptr_q31_shadow]
    - varinst: [$gptr_q27, set, $gptr_q27_shadow]
    - varinst: [$gptr_q24, set, $gptr_q24_shadow]
    - varinst: [$gptr_q20, set, $gptr_q20_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - varinst: [$gptr_q3, set, $gptr_q3_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q10, set, $gptr_q10_shadow]
    - varinst: [$gptr_q13, set, $gptr_q13_shadow]
    - varinst: [$gptr_q17, set, $gptr_q17_shadow]
    - varinst: [$gptr_q41, set, $gptr_q41_shadow]
    - varinst: [$gptr_q45, set, $gptr_q45_shadow]
    - varinst: [$gptr_q48, set, $gptr_q48_shadow]
    - varinst: [$gptr_q52, set, $gptr_q52_shadow]
    - varinst: [$gptr_q55, set, $gptr_q55_shadow]
    - varinst: [$gptr_q59, set, $gptr_q59_shadow]
    - varinst: [$gptr_q62, set, $gptr_q62_shadow]
    - varinst: [$gptr_q66, set, $gptr_q66_shadow]
    - varinst: [$gptr_q69, set, $gptr_q69_shadow]
    - varinst: [$gptr_q73, set, $gptr_q73_shadow]
    - varinst: [$gptr_q76, set, $gptr_q76_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0]
    -   execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2560_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2562.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2582.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2582.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_2596.dc.subtract.1_0, e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0]
    -   execute: {graph_name: fwd_1_temporal_epoch_1, queue_settings: {
               e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2596.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_add_2634_0]
    -   execute: {graph_name: fwd_2_temporal_epoch_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_layernorm_2596.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.layernorm_2596.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2596.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2613_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2615.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2596.dc.subtract.1_0, e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0]
    -   varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2635.dc.add.10_0, e2e_gelu_2641_0]
    -   execute: {graph_name: fwd_3_temporal_epoch_3, queue_settings: {
               e2e_add_2634_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.layernorm_2635.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2635.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_2634_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2649.dc.add.10_0, e2e_softmax_2668.dc.exp.0_0, e2e_softmax_2668.dc.reciprocal.2_0]
    -   execute: {graph_name: fwd_4_temporal_epoch_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_2635.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_gelu_2641_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2649.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2649.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2649.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2666_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2668.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2635.dc.add.10_0, e2e_gelu_2641_0]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2688.dc.add.10_0]
    -   execute: {graph_name: fwd_5_temporal_epoch_5, queue_settings: {
               e2e_layernorm_2649.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_softmax_2668.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_softmax_2668.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2688.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2688.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2688.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2649.dc.add.10_0, e2e_softmax_2668.dc.exp.0_0, e2e_softmax_2668.dc.reciprocal.2_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0]
    -   execute: {graph_name: fwd_6_temporal_epoch_6, queue_settings: {
               e2e_layernorm_2688.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2702.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2702.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2702.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2688.dc.add.10_0]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2741.dc.subtract.1_0, e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0]
    -   execute: {graph_name: fwd_7_temporal_epoch_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2719_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2721.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2741.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0]
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2741.dc.add.10_0, e2e_gelu_2747_0]
    -   execute: {graph_name: fwd_8_temporal_epoch_8, queue_settings: {
               e2e_layernorm_2741.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.layernorm_2741.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2741.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2741.dc.subtract.1_0, e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2755.dc.add.10_0, e2e_softmax_2774.dc.exp.0_0]
    -   execute: {graph_name: fwd_9_temporal_epoch_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_layernorm_2741.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_gelu_2747_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2755.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2755.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2755.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2772_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2741.dc.add.10_0, e2e_gelu_2747_0]
    -   varinst: [$gptr_q13_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2794.dc.add.10_0]
    -   execute: {graph_name: fwd_10_temporal_epoch_10, queue_settings: {
               e2e_layernorm_2755.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_softmax_2774.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               lc.input_tensor.softmax_2774.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2794.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2794.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2794.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2755.dc.add.10_0, e2e_softmax_2774.dc.exp.0_0]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2808.dc.reciprocal.7_0, e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0]
    -   execute: {graph_name: fwd_11_temporal_epoch_11, queue_settings: {
               e2e_layernorm_2794.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2808.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2808.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2808.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2794.dc.add.10_0]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2847.dc.subtract.1_0]
    -   execute: {graph_name: fwd_12_temporal_epoch_12, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_layernorm_2808.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               lc.input_tensor.layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2825_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2827.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2808.dc.reciprocal.7_0, e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0]
    -   varinst: [$gptr_q17_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_2856_0, e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0]
    -   execute: {graph_name: fwd_13_temporal_epoch_13, queue_settings: {
               e2e_layernorm_2847.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2847.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2847.dc.subtract.1_0]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2861.dc.add.10_0, e2e_softmax_2880.dc.multiply.3_0, e2e_matmul_2884_0]
    -   execute: {graph_name: fwd_14_temporal_epoch_14, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_matmul_2856_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2861.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2878_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2880.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_2856_0, e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0]
    -   varinst: [$gptr_q20_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2900.dc.add.10_0]
    -   execute: {graph_name: fwd_15_temporal_epoch_15, queue_settings: {
               e2e_layernorm_2861.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_softmax_2880.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e_matmul_2884_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2900.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2900.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2900.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2861.dc.add.10_0, e2e_softmax_2880.dc.multiply.3_0, e2e_matmul_2884_0]
    -   varinst: [$gptr_q22, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0]
    -   execute: {graph_name: fwd_16_temporal_epoch_16, queue_settings: {
               e2e_layernorm_2900.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2914.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2914.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2914.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2900.dc.add.10_0]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2953.dc.subtract.1_0, e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0]
    -   execute: {graph_name: fwd_17_temporal_epoch_17, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2931_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2933.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2953.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0]
    -   varinst: [$gptr_q24_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2953.dc.add.10_0, e2e_gelu_2959_0]
    -   execute: {graph_name: fwd_18_temporal_epoch_18, queue_settings: {
               e2e_layernorm_2953.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               lc.input_tensor.layernorm_2953.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2953.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2953.dc.subtract.1_0, e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0]
    -   varinst: [$gptr_q26, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2967.dc.add.10_0, e2e_softmax_2986.dc.exp.0_0, e2e_softmax_2986.dc.reciprocal.2_0]
    -   execute: {graph_name: fwd_19_temporal_epoch_19, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_layernorm_2953.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_gelu_2959_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2967.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2967.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2967.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_2984_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2986.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2953.dc.add.10_0, e2e_gelu_2959_0]
    -   varinst: [$gptr_q27_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q28, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3006.dc.add.10_0]
    -   execute: {graph_name: fwd_20_temporal_epoch_20, queue_settings: {
               e2e_layernorm_2967.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e_softmax_2986.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e_softmax_2986.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3006.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3006.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3006.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2967.dc.add.10_0, e2e_softmax_2986.dc.exp.0_0, e2e_softmax_2986.dc.reciprocal.2_0]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3020.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0]
    -   execute: {graph_name: fwd_21_temporal_epoch_21, queue_settings: {
               e2e_layernorm_3006.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3020.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3020.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3020.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3006.dc.add.10_0]
    -   varinst: [$gptr_q30, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3059.dc.subtract.1_0]
    -   execute: {graph_name: fwd_22_temporal_epoch_22, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_layernorm_3020.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               lc.input_tensor.layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3037_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3039.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3059.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3020.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0]
    -   varinst: [$gptr_q31_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q32, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3059.dc.add.10_0, e2e_gelu_3065_0]
    -   execute: {graph_name: fwd_23_temporal_epoch_23, queue_settings: {
               e2e_layernorm_3059.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               lc.input_tensor.layernorm_3059.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3059.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3059.dc.subtract.1_0]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3073.dc.add.10_0, e2e_softmax_3092.dc.exp.0_0]
    -   execute: {graph_name: fwd_24_temporal_epoch_24, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_layernorm_3059.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               e2e_gelu_3065_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3073.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3073.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3073.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3090_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3059.dc.add.10_0, e2e_gelu_3065_0]
    -   varinst: [$gptr_q34_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q35, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3112.dc.add.10_0]
    -   execute: {graph_name: fwd_25_temporal_epoch_25, queue_settings: {
               e2e_layernorm_3073.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e_softmax_3092.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               lc.input_tensor.softmax_3092.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3112.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3112.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3112.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3073.dc.add.10_0, e2e_softmax_3092.dc.exp.0_0]
    -   varinst: [$gptr_q36, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3126.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0]
    -   execute: {graph_name: fwd_26_temporal_epoch_26, queue_settings: {
               e2e_layernorm_3112.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3126.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3126.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3126.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3112.dc.add.10_0]
    -   varinst: [$gptr_q37, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3165.dc.subtract.1_0]
    -   execute: {graph_name: fwd_27_temporal_epoch_27, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_layernorm_3126.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               lc.input_tensor.layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3143_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3145.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3165.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3126.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0]
    -   varinst: [$gptr_q38_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q39, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_3174_0, e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0]
    -   execute: {graph_name: fwd_28_temporal_epoch_28, queue_settings: {
               e2e_layernorm_3165.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               lc.input_tensor.layernorm_3165.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3165.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3165.dc.subtract.1_0]
    -   varinst: [$gptr_q40, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3179.dc.add.10_0, e2e_softmax_3198.dc.multiply.3_0, e2e_matmul_3202_0]
    -   execute: {graph_name: fwd_29_temporal_epoch_29, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               e2e_matmul_3174_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               lc.input_tensor.layernorm_3179.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3179.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3179.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3196_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3198.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_3174_0, e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0]
    -   varinst: [$gptr_q41_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q42, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3218.dc.add.10_0]
    -   execute: {graph_name: fwd_30_temporal_epoch_30, queue_settings: {
               e2e_layernorm_3179.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_softmax_3198.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_matmul_3202_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3218.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3218.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3218.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3179.dc.add.10_0, e2e_softmax_3198.dc.multiply.3_0, e2e_matmul_3202_0]
    -   varinst: [$gptr_q43, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0]
    -   execute: {graph_name: fwd_31_temporal_epoch_31, queue_settings: {
               e2e_layernorm_3218.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3232.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3232.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3232.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3218.dc.add.10_0]
    -   varinst: [$gptr_q44, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3271.dc.subtract.1_0, e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0]
    -   execute: {graph_name: fwd_32_temporal_epoch_32, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3249_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3251.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3271.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0]
    -   varinst: [$gptr_q45_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3271.dc.add.10_0, e2e_gelu_3277_0]
    -   execute: {graph_name: fwd_33_temporal_epoch_33, queue_settings: {
               e2e_layernorm_3271.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               lc.input_tensor.layernorm_3271.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3271.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3271.dc.subtract.1_0, e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0]
    -   varinst: [$gptr_q47, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3285.dc.add.10_0, e2e_softmax_3304.dc.exp.0_0, e2e_softmax_3304.dc.reciprocal.2_0]
    -   execute: {graph_name: fwd_34_temporal_epoch_34, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               e2e_layernorm_3271.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               e2e_gelu_3277_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3285.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3285.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3285.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3302_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3304.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3271.dc.add.10_0, e2e_gelu_3277_0]
    -   varinst: [$gptr_q48_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q49, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q49, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3324.dc.add.10_0]
    -   execute: {graph_name: fwd_35_temporal_epoch_35, queue_settings: {
               e2e_layernorm_3285.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               e2e_softmax_3304.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               e2e_softmax_3304.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3324.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3324.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3324.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3285.dc.add.10_0, e2e_softmax_3304.dc.exp.0_0, e2e_softmax_3304.dc.reciprocal.2_0]
    -   varinst: [$gptr_q50, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q50, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3338.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0]
    -   execute: {graph_name: fwd_36_temporal_epoch_36, queue_settings: {
               e2e_layernorm_3324.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3338.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3338.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3338.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3324.dc.add.10_0]
    -   varinst: [$gptr_q51, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q51, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3377.dc.subtract.1_0]
    -   execute: {graph_name: fwd_37_temporal_epoch_37, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52},
               e2e_layernorm_3338.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53},
               lc.input_tensor.layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3355_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3357.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3377.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3338.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0]
    -   varinst: [$gptr_q52_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q53, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q52, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q53, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3377.dc.add.10_0, e2e_gelu_3383_0]
    -   execute: {graph_name: fwd_38_temporal_epoch_38, queue_settings: {
               e2e_layernorm_3377.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
               lc.input_tensor.layernorm_3377.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3377.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3377.dc.subtract.1_0]
    -   varinst: [$gptr_q54, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q54, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3391.dc.add.10_0, e2e_softmax_3410.dc.exp.0_0]
    -   execute: {graph_name: fwd_39_temporal_epoch_39, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55},
               e2e_layernorm_3377.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               e2e_gelu_3383_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
               layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3391.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3391.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3391.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3408_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3377.dc.add.10_0, e2e_gelu_3383_0]
    -   varinst: [$gptr_q55_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q56, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q55, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q56, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3430.dc.add.10_0]
    -   execute: {graph_name: fwd_40_temporal_epoch_40, queue_settings: {
               e2e_layernorm_3391.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               e2e_softmax_3410.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57},
               lc.input_tensor.softmax_3410.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3430.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3430.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3430.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3391.dc.add.10_0, e2e_softmax_3410.dc.exp.0_0]
    -   varinst: [$gptr_q57, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q57, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3444.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0]
    -   execute: {graph_name: fwd_41_temporal_epoch_41, queue_settings: {
               e2e_layernorm_3430.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58},
               layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3444.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3444.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3444.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3430.dc.add.10_0]
    -   varinst: [$gptr_q58, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q58, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3483.dc.subtract.1_0]
    -   execute: {graph_name: fwd_42_temporal_epoch_42, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59},
               e2e_layernorm_3444.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60},
               e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60},
               lc.input_tensor.layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3461_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3463.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3483.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3444.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0]
    -   varinst: [$gptr_q59_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q60, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q59, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q60, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_3492_0, e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0]
    -   execute: {graph_name: fwd_43_temporal_epoch_43, queue_settings: {
               e2e_layernorm_3483.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
               lc.input_tensor.layernorm_3483.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3483.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3483.dc.subtract.1_0]
    -   varinst: [$gptr_q61, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q61, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3497.dc.add.10_0, e2e_softmax_3516.dc.multiply.3_0, e2e_matmul_3520_0]
    -   execute: {graph_name: fwd_44_temporal_epoch_44, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62},
               e2e_matmul_3492_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63},
               lc.input_tensor.layernorm_3497.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3497.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3497.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3514_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3516.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_3492_0, e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0]
    -   varinst: [$gptr_q62_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q63, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q62, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q63, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3536.dc.add.10_0]
    -   execute: {graph_name: fwd_45_temporal_epoch_45, queue_settings: {
               e2e_layernorm_3497.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               e2e_softmax_3516.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               e2e_matmul_3520_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
               layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3536.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3536.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3536.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3497.dc.add.10_0, e2e_softmax_3516.dc.multiply.3_0, e2e_matmul_3520_0]
    -   varinst: [$gptr_q64, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q64, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0]
    -   execute: {graph_name: fwd_46_temporal_epoch_46, queue_settings: {
               e2e_layernorm_3536.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q65, rd_ptr_global: $gptr_q65},
               layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3550.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3550.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3550.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3536.dc.add.10_0]
    -   varinst: [$gptr_q65, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q65, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3589.dc.subtract.1_0, e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0]
    -   execute: {graph_name: fwd_47_temporal_epoch_47, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66},
               e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67},
               lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3567_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3569.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3589.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0]
    -   varinst: [$gptr_q66_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q67, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q66, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q67, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3589.dc.add.10_0, e2e_gelu_3595_0]
    -   execute: {graph_name: fwd_48_temporal_epoch_48, queue_settings: {
               e2e_layernorm_3589.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68},
               e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68},
               lc.input_tensor.layernorm_3589.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3589.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3589.dc.subtract.1_0, e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0]
    -   varinst: [$gptr_q68, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q68, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3603.dc.add.10_0, e2e_softmax_3622.dc.exp.0_0, e2e_softmax_3622.dc.reciprocal.2_0]
    -   execute: {graph_name: fwd_49_temporal_epoch_49, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69},
               e2e_layernorm_3589.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               e2e_gelu_3595_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
               layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3603.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3620_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3622.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3589.dc.add.10_0, e2e_gelu_3595_0]
    -   varinst: [$gptr_q69_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q70, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q69, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q70, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3642.dc.add.10_0]
    -   execute: {graph_name: fwd_50_temporal_epoch_50, queue_settings: {
               e2e_layernorm_3603.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               e2e_softmax_3622.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               e2e_softmax_3622.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71},
               layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3642.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3642.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3642.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3603.dc.add.10_0, e2e_softmax_3622.dc.exp.0_0, e2e_softmax_3622.dc.reciprocal.2_0]
    -   varinst: [$gptr_q71, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q71, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3656.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0]
    -   execute: {graph_name: fwd_51_temporal_epoch_51, queue_settings: {
               e2e_layernorm_3642.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q72, rd_ptr_global: $gptr_q72},
               layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3656.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3656.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3656.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3642.dc.add.10_0]
    -   varinst: [$gptr_q72, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q72, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3695.dc.subtract.1_0]
    -   execute: {graph_name: fwd_52_temporal_epoch_52, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q73, rd_ptr_global: $gptr_q73},
               e2e_layernorm_3656.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q74, rd_ptr_global: $gptr_q74},
               e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q74, rd_ptr_global: $gptr_q74},
               lc.input_tensor.layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3673_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3675.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3695.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3656.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0]
    -   varinst: [$gptr_q73_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q74, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q73, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q74, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3695.dc.add.10_0, e2e_gelu_3701_0]
    -   execute: {graph_name: fwd_53_temporal_epoch_53, queue_settings: {
               e2e_layernorm_3695.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q75, rd_ptr_global: $gptr_q75},
               lc.input_tensor.layernorm_3695.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3695.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3695.dc.subtract.1_0]
    -   varinst: [$gptr_q75, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q75, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3709.dc.add.10_0, e2e_softmax_3728.dc.exp.0_0]
    -   execute: {graph_name: fwd_54_temporal_epoch_54, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q76, rd_ptr_global: $gptr_q76},
               e2e_layernorm_3695.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q77, rd_ptr_global: $gptr_q77},
               e2e_gelu_3701_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q77, rd_ptr_global: $gptr_q77},
               layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3709.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3709.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3709.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3726_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3695.dc.add.10_0, e2e_gelu_3701_0]
    -   varinst: [$gptr_q76_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q77, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q76, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q77, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3748.dc.add.10_0]
    -   execute: {graph_name: fwd_55_temporal_epoch_55, queue_settings: {
               e2e_layernorm_3709.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78},
               e2e_softmax_3728.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78},
               lc.input_tensor.softmax_3728.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3748.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3748.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3748.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3709.dc.add.10_0, e2e_softmax_3728.dc.exp.0_0]
    -   varinst: [$gptr_q78, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q78, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3762.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0]
    -   execute: {graph_name: fwd_56_temporal_epoch_56, queue_settings: {
               e2e_layernorm_3748.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q79, rd_ptr_global: $gptr_q79},
               layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3762.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3762.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3762.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3748.dc.add.10_0]
    -   varinst: [$gptr_q79, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q79, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_3792_0, e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0]
    -   execute: {graph_name: fwd_57_temporal_epoch_57, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q80, rd_ptr_global: $gptr_q80},
               e2e_layernorm_3762.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q81, rd_ptr_global: $gptr_q81},
               e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q81, rd_ptr_global: $gptr_q81},
               lc.input_tensor.layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_3779_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3781.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3762.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0]
    -   varinst: [$gptr_q80, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q81, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q80, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q81, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_3804_0, e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0]
    -   execute: {graph_name: fwd_58_temporal_epoch_58, queue_settings: {
               e2e_matmul_3792_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q82, rd_ptr_global: $gptr_q82},
               e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q82, rd_ptr_global: $gptr_q82},
               layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3801.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3801.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3801.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_3792_0, e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0]
    -   varinst: [$gptr_q82, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q82, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: fwd_59_temporal_epoch_59, queue_settings: {
               e2e_matmul_3804_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q83, rd_ptr_global: $gptr_q83},
               e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q83, rd_ptr_global: $gptr_q83},
               layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3815.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3815.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3815.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_3804_0, e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0]
    -   varinst: [$gptr_q83, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q83, incwrap, $c_microbatch_size, 4]
    - endloop


