# git checkout aba67f98c
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[training-Wormhole_B0-chip1-enc2-tiny]

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                                     {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30020820]]}
  attention_mask:                                                              {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}

  # output
  bert_encoder.output_layernorm_105:                                           {input: layernorm_105.dc.add.14, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                         {input: opt_in1_layer.0.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x396d800]]}
  layer.0.attention.self.query.bias:                                           {input: opt_in1_layer.0.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b24760]]}
  layer.0.attention.self.key.weight:                                           {input: opt_in1_layer.0.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b163e0]]}
  layer.0.attention.self.key.bias:                                             {input: opt_in1_layer.0.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39a04a0]]}
  layer.0.attention.self.value.weight:                                         {input: opt_in1_layer.0.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39a2540]]}
  layer.0.attention.self.value.bias:                                           {input: opt_in1_layer.0.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39625a0]]}
  layer.0.attention.output.dense.weight:                                       {input: opt_in0_layer.0.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39635a0]]}
  layer.0.attention.output.dense.bias:                                         {input: opt_in1_layer.0.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3961500]]}
  layer.0.attention.output.LayerNorm.weight:                                   {input: opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b27060]]}
  layer.0.attention.output.LayerNorm.bias:                                     {input: opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39666e0]]}
  layer.0.intermediate.dense.weight:                                           {input: opt_in0_layer.0.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x396d860]]}
  layer.0.intermediate.dense.bias:                                             {input: opt_in1_layer.0.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3978b40]]}
  layer.0.output.dense.weight:                                                 {input: opt_in0_layer.0.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3938ac0], [0, 0x3948ee0]]}
  layer.0.output.dense.bias:                                                   {input: opt_in1_layer.0.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39930c0]]}
  layer.0.output.LayerNorm.weight:                                             {input: opt_in1_layer.0.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x395b320]]}
  layer.0.output.LayerNorm.bias:                                               {input: opt_in2_layer.0.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b1fde0]]}
  layer.1.attention.self.query.weight:                                         {input: opt_in0_layer.1.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b09860]]}
  layer.1.attention.self.query.bias:                                           {input: opt_in1_layer.1.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39959a0]]}
  layer.1.attention.self.key.weight:                                           {input: opt_in0_layer.1.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7aff5a0]]}
  layer.1.attention.self.key.bias:                                             {input: opt_in1_layer.1.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x395d3c0]]}
  layer.1.attention.self.value.weight:                                         {input: opt_in0_layer.1.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3959300]]}
  layer.1.attention.self.value.bias:                                           {input: opt_in1_layer.1.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x395f460]]}
  layer.1.attention.output.dense.weight:                                       {input: opt_in0_layer.1.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39655e0]]}
  layer.1.attention.output.dense.bias:                                         {input: opt_in1_layer.1.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b226c0]]}
  layer.1.attention.output.LayerNorm.weight:                                   {input: opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3974300]]}
  layer.1.attention.output.LayerNorm.bias:                                     {input: opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b34440]]}
  layer.1.intermediate.dense.weight:                                           {input: opt_in0_layer.1.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39d52a0]]}
  layer.1.intermediate.dense.bias:                                             {input: opt_in1_layer.1.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x396c0e0]]}
  layer.1.output.dense.weight:                                                 {input: opt_in0_layer.1.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3993240], [1, 0x39a3660]]}
  layer.1.output.dense.bias:                                                   {input: opt_in1_layer.1.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39a2620]]}
  layer.1.output.LayerNorm.weight:                                             {input: opt_in1_layer.1.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b3e700]]}
  layer.1.output.LayerNorm.bias:                                               {input: opt_in2_layer.1.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b407a0]]}

  # constant
  input_1_multiply_16_fork_clone847_tile_bcast_tile_bcast:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3961d60]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3975a20]]}
  dc.input_tensor.softmax_18.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 2, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b26800]]}
  lc.input_tensor.softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b1e600]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2a920]]}
  dc.input_tensor.layernorm_38.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b1ee40]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39aa760]]}
  dc.input_tensor.layernorm_38.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3964640]]}
  dc.input_tensor.layernorm_38.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x396b7c0]]}
  lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3978300]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2b160]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39aafa0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3937a40]]}
  dc.input_tensor.layernorm_52.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3953100]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3963d20]]}
  dc.input_tensor.layernorm_52.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b1dd40]]}
  dc.input_tensor.layernorm_52.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b077c0]]}
  lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3995160]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3938280]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3964560]]}
  input_1_multiply_69_fork_clone864_tile_bcast_tile_bcast:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3964da0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b21e80]]}
  dc.input_tensor.softmax_71.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 2, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b11a80]]}
  lc.input_tensor.softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3997a40]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b15ba0]]}
  dc.input_tensor.layernorm_91.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3998280]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3961520]]}
  dc.input_tensor.layernorm_91.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3976260]]}
  dc.input_tensor.layernorm_91.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39763a0]]}
  lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39a6760]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39b5b20]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39f6300]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2fb40]]}
  dc.input_tensor.layernorm_105.1:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b364e0]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39f5ac0]]}
  dc.input_tensor.layernorm_105.6:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39b3a80]]}
  dc.input_tensor.layernorm_105.8:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39a46c0]]}
  lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2f300]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b30380]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b30bc0]]}
  lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39f6b40]]}
  lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2ca20]]}
  lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x398e080]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3980d60]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2c1e0]]}
  lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b2b1a0]]}
  dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39ac020]]}
  lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x396a820]]}
  lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2b9a0]]}
  lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x396b060]]}
  lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39a1de0]]}
  lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b33c00]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39d4a60]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x396b8a0]]}
  lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3992a00]]}
  dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39803a0]]}
  lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39ab7e0]]}
  lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7af5160]]}
  lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x394bd80]]}
  input_1_multiply_69_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x393fa80]]}
  lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x394bd60]]}
  lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afab60]]}
  lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x391ba20]]}
  lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x391b1e0]]}
  lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3955020]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afa320]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x395d220]]}
  lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b0b740]]}
  dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3944c40]]}
  lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3955860]]}
  lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x394ce60]]}
  lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7af59a0]]}
  lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39547c0]]}
  lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3942360]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afe520]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afd4a0]]}
  lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39547e0]]}
  dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3926580]]}
  lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x394f7a0]]}
  lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afdce0]]}
  lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x394ffe0]]}
  input_1_multiply_16_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x395aa80]]}
  lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b1c480]]}
  lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39369c0]]}

  # epoch_to_epoch
  e2e_matmul_94_0:                                                             {input: matmul_94, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100]]}
  e2e_layernorm_91.dc.add.14_0:                                                {input: layernorm_91.dc.add.14, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38ea520]]}
  e2e_layernorm_105.dc.multiply.12_0:                                          {input: layernorm_105.dc.multiply.12, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b31400]]}
  e2e_layernorm_105.dc.reciprocal.11_0:                                        {input: layernorm_105.dc.reciprocal.11, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39a6fa0]]}
  e2e_gelu_97_0:                                                               {input: gelu_97, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39b6360]]}
  e2e_layernorm_91.dc.multiply.12_0:                                           {input: layernorm_91.dc.multiply.12, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38ea520]]}
  e2e_layernorm_91.dc.reciprocal.11_0:                                         {input: layernorm_91.dc.reciprocal.11, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}
  e2e_matmul_82_0:                                                             {input: matmul_82, type: queue, entries: 2, grid_size: [1, 1], t: 2, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100]]}
  e2e_matmul_75_0:                                                             {input: matmul_75, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100]]}
  e2e_softmax_71.dc.multiply.5_0:                                              {input: softmax_71.dc.multiply.5, type: queue, entries: 2, grid_size: [1, 1], t: 2, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  e2e_layernorm_52.dc.add.14_0:                                                {input: layernorm_52.dc.add.14, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38de220]]}
  e2e_matmul_61_0:                                                             {input: matmul_61, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab8220]]}
  e2e_matmul_55_0:                                                             {input: matmul_55, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ad4920]]}
  e2e_bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9_0:                      {input: bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b42840]]}
  e2e_bw_in0_reshape_53.dc.squeeze.0_combine_add_1_0:                          {input: bw_in0_reshape_53.dc.squeeze.0_combine_add_1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39f7380]]}
  e2e_layernorm_52.dc.multiply.12_0:                                           {input: layernorm_52.dc.multiply.12, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38fa940]]}
  e2e_layernorm_52.dc.reciprocal.11_0:                                         {input: layernorm_52.dc.reciprocal.11, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da100]]}
  e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0:                      {input: layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3978440]]}
  e2e_gelu_44_0:                                                               {input: gelu_44, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38ee640]]}
  e2e_matmul_41_0:                                                             {input: matmul_41, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38fa940]]}
  e2e_layernorm_38.dc.add.14_0:                                                {input: layernorm_38.dc.add.14, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x393b940]]}
  e2e_layernorm_38.dc.multiply.12_0:                                           {input: layernorm_38.dc.multiply.12, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ad8a60]]}
  e2e_layernorm_38.dc.reciprocal.11_0:                                         {input: layernorm_38.dc.reciprocal.11, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x390ad60]]}
  e2e_matmul_29_0:                                                             {input: matmul_29, type: queue, entries: 2, grid_size: [1, 1], t: 2, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x393b960]]}
  e2e_matmul_22_0:                                                             {input: matmul_22, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x392f660]]}
  e2e_softmax_18.dc.multiply.5_0:                                              {input: softmax_18.dc.multiply.5, type: queue, entries: 2, grid_size: [1, 1], t: 2, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x391b120]]}
  e2e_matmul_8_0:                                                              {input: matmul_8, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ae4d40]]}
  e2e_matmul_2_0:                                                              {input: matmul_2, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ac8640]]}

  # optimizer_parameter
  input_opt_layer.0.attention.self.query.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b1ccc0]]}
  input_opt_layer.0.attention.self.query.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afed60]]}
  input_opt_layer.0.attention.self.key.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3992880]]}
  input_opt_layer.0.attention.self.key.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3937200]]}
  input_opt_layer.0.attention.self.value.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39528c0]]}
  input_opt_layer.0.attention.self.value.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39634e0]]}
  input_opt_layer.0.attention.output.dense.weight_0.lr:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b1d500]]}
  input_opt_layer.0.attention.output.dense.bias_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b11140]]}
  input_opt_layer.0.attention.output.LayerNorm.weight_0.lr:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b100c0]]}
  input_opt_layer.0.attention.output.LayerNorm.bias_0.lr:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afb3a0]]}
  input_opt_layer.0.intermediate.dense.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x397e2a0]]}
  input_opt_layer.0.intermediate.dense.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3924480]]}
  input_opt_layer.0.output.dense.weight_0.lr:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x394d6a0]]}
  input_opt_layer.0.output.dense.bias_0.lr:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39568e0]]}
  input_opt_layer.0.output.LayerNorm.weight_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b10900]]}
  input_opt_layer.0.output.LayerNorm.bias_0.lr:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afbbe0]]}
  input_opt_layer.1.attention.self.query.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x397eae0]]}
  input_opt_layer.1.attention.self.query.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3924cc0]]}
  input_opt_layer.1.attention.self.key.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x394dee0]]}
  input_opt_layer.1.attention.self.key.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3957120]]}
  input_opt_layer.1.attention.self.value.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39560a0]]}
  input_opt_layer.1.attention.self.value.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afc420]]}
  input_opt_layer.1.attention.output.dense.weight_0.lr:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x397f320]]}
  input_opt_layer.1.attention.output.dense.bias_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3925500]]}
  input_opt_layer.1.attention.output.LayerNorm.weight_0.lr:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x394e720]]}
  input_opt_layer.1.attention.output.LayerNorm.bias_0.lr:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3957960]]}
  input_opt_layer.1.intermediate.dense.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b11980]]}
  input_opt_layer.1.intermediate.dense.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7afcc60]]}
  input_opt_layer.1.output.dense.weight_0.lr:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x397fb60]]}
  input_opt_layer.1.output.dense.bias_0.lr:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3925d40]]}
  input_opt_layer.1.output.LayerNorm.weight_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x394ef60]]}
  input_opt_layer.1.output.LayerNorm.bias_0.lr:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39581a0]]}

  # loss
  loss_bert_encoder.output_layernorm_105:                                      {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30041040]]}

  # grad_accumulator
  grad_acc_layer.1.output.LayerNorm.bias:                                      {input: bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b29100]]}
  grad_acc_layer.1.output.LayerNorm.weight:                                    {input: bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3968780]]}
  grad_acc_layer.1.output.dense.bias:                                          {input: bw_in1_add_102_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x398e8c0]]}
  grad_acc_layer.1.output.dense.weight:                                        {input: bw_in1_matmul_100_matmul_1, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39815a0], [2, 0x39919c0]]}
  grad_acc_layer.1.intermediate.dense.bias:                                    {input: bw_in1_add_96_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b2b9e0]]}
  grad_acc_layer.1.intermediate.dense.weight:                                  {input: bw_in1_matmul_94_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39b4240]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                            {input: bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3990960]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                          {input: bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b2d260]]}
  grad_acc_layer.1.attention.output.dense.bias:                                {input: bw_in1_add_88_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7af61e0]]}
  grad_acc_layer.1.attention.output.dense.weight:                              {input: bw_in1_matmul_86_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x394c5c0]]}
  grad_acc_layer.1.attention.self.value.bias:                                  {input: bw_in1_add_77_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39170a0]]}
  grad_acc_layer.1.attention.self.value.weight:                                {input: bw_in1_matmul_75_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x394c5a0]]}
  grad_acc_layer.1.attention.self.key.bias:                                    {input: bw_in1_add_63_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39402c0]]}
  grad_acc_layer.1.attention.self.key.weight:                                  {input: bw_in1_matmul_61_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x391c260]]}
  grad_acc_layer.1.attention.self.query.bias:                                  {input: bw_in1_add_57_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b0bf80]]}
  grad_acc_layer.1.attention.self.query.weight:                                {input: bw_in1_matmul_55_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3955000]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                      {input: bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7af8280]]}
  grad_acc_layer.0.output.LayerNorm.weight:                                    {input: bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3942ba0]]}
  grad_acc_layer.0.output.dense.bias:                                          {input: bw_in1_add_49_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b0e020]]}
  grad_acc_layer.0.output.dense.weight:                                        {input: bw_in1_matmul_47_matmul_1, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [4, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x395da60], [5, 0x396de80]]}
  grad_acc_layer.0.intermediate.dense.bias:                                    {input: bw_in1_add_43_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x390ee80]]}
  grad_acc_layer.0.intermediate.dense.weight:                                  {input: bw_in1_matmul_41_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ae8e80]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                            {input: bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b096a0]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                          {input: bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3919140]]}
  grad_acc_layer.0.attention.output.dense.bias:                                {input: bw_in1_add_35_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39589e0]]}
  grad_acc_layer.0.attention.output.dense.weight:                              {input: bw_in1_matmul_33_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b14260]]}
  grad_acc_layer.0.attention.self.value.bias:                                  {input: bw_in1_add_24_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39885c0]]}
  grad_acc_layer.0.attention.self.value.weight:                                {input: bw_in1_matmul_22_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x392e7a0]]}
  grad_acc_layer.0.attention.self.key.bias:                                    {input: bw_in1_add_10_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b121c0]]}
  grad_acc_layer.0.attention.self.key.weight:                                  {input: bw_in1_matmul_8_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x398a660]]}
  grad_acc_layer.0.attention.self.query.bias:                                  {input: bw_in1_add_4_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3950820]]}
  grad_acc_layer.0.attention.self.query.weight:                                {input: bw_in1_matmul_2_matmul_1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x395b2c0]]}

graphs:
  fwd_0_0:
    target_device: 0
    input_count: 2
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_8: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_14: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 2], input_0_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_16: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_fork_clone847_tile_bcast_tile_bcast],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_17: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [multiply_16, attention_mask],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}]}
    softmax_18.dc.exp.0: {type: exp, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_17],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [softmax_18.dc.exp.0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 2}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_18.dc.add.3: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_18.2],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_18.dc.reciprocal.4: {type: reciprocal, grid_loc: [1, 1], grid_size: [1, 1], inputs: [softmax_18.dc.add.3],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_18.dc.reciprocal.4, lc.input_tensor.softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_18.dc.multiply.5: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_18.dc.exp.0, softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_22: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_29: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_18.dc.multiply.5, matmul_22],
         t: 2, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 2],
         attributes: {bias: true, m_k: 2, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [2, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layernorm_38.dc.subtract.3: {type: subtract, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_37, layernorm_38.dc.multiply.2],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.3, layernorm_38.dc.subtract.3],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_38.dc.multiply.7: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.add.9: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.7, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.sqrt.10: {type: sqrt, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_38.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reciprocal.11: {type: reciprocal, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_38.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_38.dc.reciprocal.11, lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_38.dc.multiply.12: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.3, layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [224, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_38.dc.multiply.13: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.12, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_38.dc.add.14: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.13, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_41: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_38.dc.add.14, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    gelu_44: {type: gelu, grid_loc: [3, 7], grid_size: [1, 1], inputs: [matmul_41],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 16}}
    add_51: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [matmul_47, layernorm_38.dc.add.14],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 96], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layernorm_52.dc.subtract.3: {type: subtract, grid_loc: [4, 4], grid_size: [1, 1], inputs: [add_51, layernorm_52.dc.multiply.2],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.3, layernorm_52.dc.subtract.3],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_52.dc.multiply.7: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.add.9: {type: add, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.7, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.sqrt.10: {type: sqrt, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_52.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reciprocal.11: {type: reciprocal, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_52.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_52.dc.reciprocal.11, lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.multiply.12: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.3, layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [224, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.multiply.13: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.12, layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.add.14: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.13, layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_55: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_61: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_67: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 2], input_0_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_69: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_fork_clone864_tile_bcast_tile_bcast],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_70: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [multiply_69, attention_mask],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}]}
    softmax_71.dc.exp.0: {type: exp, grid_loc: [6, 6], grid_size: [1, 1], inputs: [add_70],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_71.dc.exp.0, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 2}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_71.dc.add.3: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_71.2],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_71.dc.reciprocal.4: {type: reciprocal, grid_loc: [7, 1], grid_size: [1, 1], inputs: [softmax_71.dc.add.3],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_71.dc.reciprocal.4, lc.input_tensor.softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_71.dc.multiply.5: {type: multiply, grid_loc: [7, 3], grid_size: [1, 1], inputs: [softmax_71.dc.exp.0, softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_75: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0, 0], ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_82: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [softmax_71.dc.multiply.5, matmul_75],
         t: 2, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 2],
         attributes: {bias: true, m_k: 2, min_buffer_input: 0, u_kt: 2}}
    add_90: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_86, layernorm_52.dc.add.14],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 160], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layernorm_91.dc.subtract.3: {type: subtract, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_90, layernorm_91.dc.multiply.2],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.3, layernorm_91.dc.subtract.3],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_91.dc.multiply.7: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.add.9: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.7, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.sqrt.10: {type: sqrt, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_91.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reciprocal.11: {type: reciprocal, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_91.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_91.dc.reciprocal.11, lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_91.dc.multiply.12: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.3, layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [224, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_91.dc.multiply.13: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.12, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_91.dc.add.14: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.13, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_94: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_91.dc.add.14, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 4}}

  fwd_0_1:
    target_device: 0
    input_count: 2
    gelu_97: {type: gelu, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_94_0],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 1, min_buffer_input: 0, u_kt: 16}}
    add_104: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_100, e2e_layernorm_91.dc.add.14_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layernorm_105.dc.subtract.3: {type: subtract, grid_loc: [0, 5], grid_size: [1, 1], inputs: [add_104, layernorm_105.dc.multiply.2],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.3, layernorm_105.dc.subtract.3],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    layernorm_105.dc.multiply.7: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.add.9: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.7, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.sqrt.10: {type: sqrt, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_105.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reciprocal.11: {type: reciprocal, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_105.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_105.dc.reciprocal.11, lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_105.dc.multiply.12: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.3, layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [224, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_105.dc.multiply.13: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.12, layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_105.dc.add.14: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.13, layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], untilize_output: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  bwd_0_2:
    target_device: 0
    input_count: 2
    bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoder.output_layernorm_105], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.multiply.12_0, loss_bert_encoder.output_layernorm_105],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_105_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.reciprocal.11_0, lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [loss_bert_encoder.output_layernorm_105, layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [40, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0, e2e_layernorm_105.dc.multiply.12_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.multiply.12_0, bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_105_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6, bw_in0_layernorm_105_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_105_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [160, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1, bw_in0_layernorm_105_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in1_add_102_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_matmul_100_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9, layer.1.output.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_100_transpose_0: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [e2e_gelu_97_0],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_100_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [bw_in1_matmul_100_transpose_0, bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_gelu_97_gelu_derivative_0: {type: gelu_derivative, grid_loc: [4, 0], grid_size: [1, 8], inputs: [e2e_matmul_94_0],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    bw_in0_gelu_97_multiply_1: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [bw_in0_gelu_97_gelu_derivative_0, bw_in0_matmul_100_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_96_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0, bw_in0_gelu_97_multiply_1], gradient_op: true,
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_matmul_94_matmul_1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [bw_in0_gelu_97_multiply_1, layer.1.intermediate.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_94_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.add.14_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_94_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_94_transpose_0, bw_in0_gelu_97_multiply_1], gradient_op: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_layernorm_91_combine_add_0: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_105_layernorm_bw_0.dc.multiply.9, bw_in0_matmul_94_matmul_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_91_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.multiply.12_0, bw_in0_layernorm_91_combine_add_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_91_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.reciprocal.11_0, lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_combine_add_0, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [40, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0, e2e_layernorm_91.dc.multiply.12_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [e2e_layernorm_91.dc.multiply.12_0, bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_91_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6, bw_in0_layernorm_91_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_91_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [160, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.lc1, bw_in0_layernorm_91_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in1_add_88_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_matmul_86_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9, layer.1.attention.output.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_86_transpose_0: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [e2e_matmul_82_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 2]}
    bw_in1_matmul_86_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_86_transpose_0, bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_82_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_86_matmul_1, e2e_matmul_75_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 2], input_0_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_82_transpose_0: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [e2e_softmax_71.dc.multiply.5_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_82_matmul_1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_82_transpose_0, bw_in0_matmul_86_matmul_1],
         t: 2, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_77_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0, bw_in1_matmul_82_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_82_matmul_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 2]}
    bw_in0_matmul_75_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_75_transpose_0: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.add.14_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_75_matmul_1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_75_transpose_0, bw_in0_reshape_76.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, e2e_softmax_71.dc.multiply.5_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 2}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_softmax_71_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [7, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_82_matmul_1, bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.lc1],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_softmax_71_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.subtract.2, e2e_softmax_71.dc.multiply.5_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_multiply_69_multiply_0: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [bw_in0_softmax_71_softmax_bw_0.dc.multiply.3, input_1_multiply_69_tile_bcast_tile_bcast],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    bw_in0_matmul_67_matmul_1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [bw_in0_multiply_69_multiply_0, e2e_matmul_61_0],
         t: 2, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_67_transpose_0: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [e2e_matmul_55_0],
         t: 2, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 2]}
    bw_in1_matmul_67_matmul_1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_67_transpose_0, bw_in0_multiply_69_multiply_0],
         t: 2, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_63_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0, bw_in1_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 2], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [bw_in1_matmul_67_matmul_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 2]}
    bw_in0_matmul_61_matmul_1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_61_transpose_0: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.add.14_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_61_matmul_1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [bw_in1_matmul_61_transpose_0, bw_in0_reshape_62.dc.unsqueeze.0_squeeze_0], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_57_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_matmul_55_matmul_1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_67_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 2],
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_55_transpose_0: {type: nop, grid_loc: [9, 2], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.add.14_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_55_matmul_1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [bw_in1_matmul_55_transpose_0, bw_in0_matmul_67_matmul_1], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_reshape_53.dc.squeeze.0_combine_add_0: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_75_matmul_1, bw_in0_matmul_61_matmul_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_reshape_53.dc.squeeze.0_combine_add_1: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [bw_in0_reshape_53.dc.squeeze.0_combine_add_0, bw_in0_matmul_55_matmul_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.reciprocal.11_0, lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}

  bwd_0_3:
    target_device: 0
    input_count: 2
    bw_in0_layernorm_52_combine_add_0: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_reshape_53.dc.squeeze.0_combine_add_1_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_52_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.multiply.12_0, bw_in0_layernorm_52_combine_add_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_52_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_combine_add_0, layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [40, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0, e2e_layernorm_52.dc.multiply.12_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.multiply.12_0, bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_52_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6, bw_in0_layernorm_52_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_52_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [160, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0, bw_in0_layernorm_52_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in1_add_49_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_matmul_47_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9, layer.0.output.dense.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_47_transpose_0: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [e2e_gelu_44_0],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_47_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [bw_in1_matmul_47_transpose_0, bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_gelu_44_gelu_derivative_0: {type: gelu_derivative, grid_loc: [4, 0], grid_size: [1, 8], inputs: [e2e_matmul_41_0],
         t: 1, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    bw_in0_gelu_44_multiply_1: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [bw_in0_gelu_44_gelu_derivative_0, bw_in0_matmul_47_matmul_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_43_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0, bw_in0_gelu_44_multiply_1], gradient_op: true,
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_matmul_41_matmul_1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [bw_in0_gelu_44_multiply_1, layer.0.intermediate.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 16}}
    bw_in1_matmul_41_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.add.14_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_41_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [bw_in1_matmul_41_transpose_0, bw_in0_gelu_44_multiply_1], gradient_op: true,
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_layernorm_38_combine_add_0: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_52_layernorm_bw_0.dc.multiply.9, bw_in0_matmul_41_matmul_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_38_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.multiply.12_0, bw_in0_layernorm_38_combine_add_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_38_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.reciprocal.11_0, lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_combine_add_0, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, input_buf_min_size_tiles: [40, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0, e2e_layernorm_38.dc.multiply.12_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [e2e_layernorm_38.dc.multiply.12_0, bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_38_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6, bw_in0_layernorm_38_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_38_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [160, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.lc1, bw_in0_layernorm_38_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}]}
    bw_in1_add_35_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in0_matmul_33_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9, layer.0.attention.output.dense.weight],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_33_transpose_0: {type: nop, grid_loc: [6, 1], grid_size: [1, 1], inputs: [e2e_matmul_29_0],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 2]}
    bw_in1_matmul_33_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [bw_in1_matmul_33_transpose_0, bw_in0_layernorm_38_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_matmul_29_matmul_1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_33_matmul_1, e2e_matmul_22_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 2], input_0_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    bw_in1_matmul_29_transpose_0: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [e2e_softmax_18.dc.multiply.5_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_29_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_29_transpose_0, bw_in0_matmul_33_matmul_1],
         t: 2, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_24_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in1_matmul_22_transpose_0: {type: nop, grid_loc: [7, 7], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_22_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_22_transpose_0, bw_in1_matmul_29_matmul_1], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, e2e_softmax_18.dc.multiply.5_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0],
         t: 2, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 2}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    bw_in0_softmax_18_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [7, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_29_matmul_1, bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.lc1],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    bw_in0_softmax_18_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.subtract.2, e2e_softmax_18.dc.multiply.5_0],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_multiply_16_multiply_0: {type: multiply, grid_loc: [7, 3], grid_size: [1, 1], inputs: [bw_in0_softmax_18_softmax_bw_0.dc.multiply.3, input_1_multiply_16_tile_bcast_tile_bcast],
         t: 2, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 2}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    bw_in0_matmul_14_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [bw_in0_multiply_16_multiply_0, e2e_matmul_8_0],
         t: 2, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_matmul_14_transpose_0: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [e2e_matmul_2_0],
         t: 2, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vslice: 2]}
    bw_in1_matmul_14_matmul_1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_14_transpose_0, bw_in0_multiply_16_multiply_0],
         t: 2, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 4, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_10_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 2], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in1_matmul_8_transpose_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_8_matmul_1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [bw_in1_matmul_8_transpose_0, bw_in1_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    bw_in1_add_4_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0, bw_in0_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2], input_0_tms: [broadcast: {c: 4}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [0], u_kt: 4}}
    bw_in1_matmul_2_transpose_0: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_2_matmul_1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_2_transpose_0, bw_in0_matmul_14_matmul_1], gradient_op: true,
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}

  opt_0_4:
    target_device: 0
    input_count: 1
    opt_in1_layer.0.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.query.weight, input_opt_layer.0.attention.self.query.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layer.0.attention.self.query.weight, opt_in1_layer.0.attention.self.query.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.query.bias, input_opt_layer.0.attention.self.query.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layer.0.attention.self.query.bias, opt_in1_layer.0.attention.self.query.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.key.weight, input_opt_layer.0.attention.self.key.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layer.0.attention.self.key.weight, opt_in1_layer.0.attention.self.key.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.key.bias, input_opt_layer.0.attention.self.key.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layer.0.attention.self.key.bias, opt_in1_layer.0.attention.self.key.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.value.weight, input_opt_layer.0.attention.self.value.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layer.0.attention.self.value.weight, opt_in1_layer.0.attention.self.value.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.value.bias, input_opt_layer.0.attention.self.value.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layer.0.attention.self.value.bias, opt_in1_layer.0.attention.self.value.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.0.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.dense.weight, input_opt_layer.0.attention.output.dense.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.0.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layer.0.attention.output.dense.weight, opt_in0_layer.0.attention.output.dense.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.dense.bias, input_opt_layer.0.attention.output.dense.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layer.0.attention.output.dense.bias, opt_in1_layer.0.attention.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.weight, input_opt_layer.0.attention.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.weight, opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.bias, input_opt_layer.0.attention.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.bias, opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.0.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.intermediate.dense.weight, input_opt_layer.0.intermediate.dense.weight_0.lr],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 16}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.0.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layer.0.intermediate.dense.weight, opt_in0_layer.0.intermediate.dense.weight_multiply_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [grad_acc_layer.0.intermediate.dense.bias, input_opt_layer.0.intermediate.dense.bias_0.lr],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layer.0.intermediate.dense.bias, opt_in1_layer.0.intermediate.dense.bias_multiply_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.0.output.dense.weight_multiply_1: {type: multiply, grid_loc: [3, 0], grid_size: [2, 1], inputs: [grad_acc_layer.0.output.dense.weight, input_opt_layer.0.output.dense.weight_0.lr],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.0.output.dense.weight_subtract_2: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [layer.0.output.dense.weight, opt_in0_layer.0.output.dense.weight_multiply_1],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.dense.bias_multiply_1: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.dense.bias, input_opt_layer.0.output.dense.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.output.dense.bias_subtract_2: {type: subtract, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layer.0.output.dense.bias, opt_in1_layer.0.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [3, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.weight, input_opt_layer.0.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.0.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.weight, opt_in1_layer.0.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.bias, input_opt_layer.0.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in2_layer.0.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.bias, opt_in2_layer.0.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.1.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.query.weight, input_opt_layer.1.attention.self.query.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.1.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layer.1.attention.self.query.weight, opt_in0_layer.1.attention.self.query.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.query.bias, input_opt_layer.1.attention.self.query.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layer.1.attention.self.query.bias, opt_in1_layer.1.attention.self.query.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.1.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [4, 6], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.key.weight, input_opt_layer.1.attention.self.key.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.1.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layer.1.attention.self.key.weight, opt_in0_layer.1.attention.self.key.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.key.bias, input_opt_layer.1.attention.self.key.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layer.1.attention.self.key.bias, opt_in1_layer.1.attention.self.key.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.1.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.value.weight, input_opt_layer.1.attention.self.value.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.1.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layer.1.attention.self.value.weight, opt_in0_layer.1.attention.self.value.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.value.bias, input_opt_layer.1.attention.self.value.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layer.1.attention.self.value.bias, opt_in1_layer.1.attention.self.value.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.1.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.dense.weight, input_opt_layer.1.attention.output.dense.weight_0.lr],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.1.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layer.1.attention.output.dense.weight, opt_in0_layer.1.attention.output.dense.weight_multiply_1],
         t: 1, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.dense.bias, input_opt_layer.1.attention.output.dense.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layer.1.attention.output.dense.bias, opt_in1_layer.1.attention.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.weight, input_opt_layer.1.attention.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.weight, opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.bias, input_opt_layer.1.attention.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.bias, opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.1.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [grad_acc_layer.1.intermediate.dense.weight, input_opt_layer.1.intermediate.dense.weight_0.lr],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {c: 16}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.1.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layer.1.intermediate.dense.weight, opt_in0_layer.1.intermediate.dense.weight_multiply_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.intermediate.dense.bias, input_opt_layer.1.intermediate.dense.bias_0.lr],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 16}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layer.1.intermediate.dense.bias, opt_in1_layer.1.intermediate.dense.bias_multiply_1],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in0_layer.1.output.dense.weight_multiply_1: {type: multiply, grid_loc: [7, 2], grid_size: [2, 1], inputs: [grad_acc_layer.1.output.dense.weight, input_opt_layer.1.output.dense.weight_0.lr],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 16}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in0_layer.1.output.dense.weight_subtract_2: {type: subtract, grid_loc: [7, 3], grid_size: [2, 1], inputs: [layer.1.output.dense.weight, opt_in0_layer.1.output.dense.weight_multiply_1],
         t: 1, mblock: [4, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.dense.bias_multiply_1: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.dense.bias, input_opt_layer.1.output.dense.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.output.dense.bias_subtract_2: {type: subtract, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layer.1.output.dense.bias, opt_in1_layer.1.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.weight, input_opt_layer.1.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in1_layer.1.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.weight, opt_in1_layer.1.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.bias, input_opt_layer.1.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    opt_in2_layer.1.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.bias, opt_in2_layer.1.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0}
    - staticvar: {$lptr_q0: 0, $gptr_q0_shadow: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q2_shadow: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0}
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   execute: {graph_name: fwd_0_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_fork_clone847_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_fork_clone864_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_1, queue_settings: {
               e2e_layernorm_91.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
    - endloop

  - run_bwd_0:
    - param: [$p_zero_grad, $p_loop_count]
    - var: {$gptr_q4: 0, $lptr_q4: 0, $v_zero_grad: 0, $c_microbatch_size: 2, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q1: 0, $lptr_q1: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $lptr_q0: 0, $lptr_q3: 0}
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_reshape_53.dc.squeeze.0_combine_add_1_0, e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0]
    -   execute: {graph_name: bwd_0_2, queue_settings: {
               loss_bert_encoder.output_layernorm_105: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e_layernorm_52.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_52.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_55_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_61_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_softmax_71.dc.multiply.5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_75_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_82_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_91.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_91.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_91.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_94_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_gelu_97_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_105.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_105.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_105_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_105_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_102_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_96_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_91_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_91_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_88_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_77_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_71_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_63_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_57_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: bwd_0_3, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_matmul_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_softmax_18.dc.multiply.5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_22_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_29_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_layernorm_38.dc.reciprocal.11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_layernorm_38.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_layernorm_38.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_matmul_41_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_gelu_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_layernorm_52.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_bw_in0_reshape_53.dc.squeeze.0_combine_add_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_52_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_52_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_49_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_43_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_38_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_38_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_35_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_24_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_18_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_10_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_4_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_layernorm_91_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_reshape_53.dc.squeeze.0_combine_add_1_0, e2e_layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt_0:
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0}
    - execute: {graph_name: opt_0_4, queue_settings: {
             layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 1.0
  io-config:
    inputs: [input_1, attention_mask, loss_bert_encoder.output_layernorm_105]
    outputs: [bert_encoder.output_layernorm_105]
