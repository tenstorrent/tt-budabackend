# git checkout 435f4d73
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[training-Wormhole_B0-chip1-enc2-large]

devices:
  arch: wormhole_b0

queues:

  # input
  hidden_states:                                                               {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x48a00c0]]}
  attention_mask:                                                              {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46d6240]]}

  # output
  bert_encoder.output_layernorm_219:                                           {input: layernorm_219.dc.add.10, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                         {input: opt_in1_layer.0.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4416fe0], [0, 0x4458000], [0, 0x4499020], [0, 0x44da040], [0, 0x451b060], [0, 0x455c080], [0, 0x459d0a0], [0, 0x45de0c0]]}
  layer.0.attention.self.query.bias:                                           {input: opt_in1_layer.0.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ca09e0]]}
  layer.0.attention.self.key.weight:                                           {input: opt_in1_layer.0.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8b7b880], [3, 0x8bbc8a0], [3, 0x8bfd8c0], [3, 0x8c3e8e0], [3, 0x8c7f900], [3, 0x8cc0920], [3, 0x8d01940], [3, 0x8d42960]]}
  layer.0.attention.self.key.bias:                                             {input: opt_in1_layer.0.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46c7de0]]}
  layer.0.attention.self.value.weight:                                         {input: opt_in1_layer.0.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8d83980], [3, 0x8dc49a0], [3, 0x8e059c0], [3, 0x8e469e0], [3, 0x8e87a00], [3, 0x8ec8a20], [3, 0x8f09a40], [3, 0x8f4aa60]]}
  layer.0.attention.self.value.bias:                                           {input: opt_in1_layer.0.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46b79c0]]}
  layer.0.attention.output.dense.weight:                                       {input: opt_in0_layer.0.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4bac920], [0, 0x4bed940], [0, 0x4c2e960], [0, 0x4c6f980], [0, 0x4cb09a0], [0, 0x4cf19c0], [0, 0x4d329e0], [0, 0x4d73a00]]}
  layer.0.attention.output.dense.bias:                                         {input: opt_in1_layer.0.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cb1640]]}
  layer.0.attention.output.LayerNorm.weight:                                   {input: opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8f8c2c0]]}
  layer.0.attention.output.LayerNorm.bias:                                     {input: opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46dbb20]]}
  layer.0.intermediate.dense.weight:                                           {input: opt_in0_layer.0.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e96d80], [5, 0x3ed7da0], [5, 0x3f18dc0], [5, 0x3f59de0], [5, 0x3f9ae00], [5, 0x3fdbe20], [5, 0x401ce40], [5, 0x405de60], [5, 0x409ee80], [5, 0x40dfea0], [5, 0x4120ec0], [5, 0x4161ee0], [5, 0x41a2f00], [5, 0x41e3f20], [5, 0x4224f40], [5, 0x4265f60], [5, 0x42a6f80], [5, 0x42e7fa0], [5, 0x4328fc0], [5, 0x4369fe0], [5, 0x43ab000], [5, 0x43ec020], [5, 0x442d040], [5, 0x446e060], [5, 0x44af080], [5, 0x44f00a0], [5, 0x45310c0], [5, 0x45720e0], [5, 0x45b3100], [5, 0x45f4120], [5, 0x4635140], [5, 0x4676160]]}
  layer.0.intermediate.dense.bias:                                             {input: opt_in1_layer.0.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x483b780]]}
  layer.0.output.dense.weight:                                                 {input: opt_in0_layer.0.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ca9be0], [1, 0x3ceac00], [1, 0x3d2bc20], [1, 0x3d6cc40], [1, 0x3dadc60], [1, 0x3deec80], [1, 0x3e2fca0], [1, 0x3e70cc0], [1, 0x3eb1ce0], [1, 0x3ef2d00], [1, 0x3f33d20], [1, 0x3f74d40], [1, 0x3fb5d60], [1, 0x3ff6d80], [1, 0x4037da0], [1, 0x4078dc0], [1, 0x40b9de0], [1, 0x40fae00], [1, 0x413be20], [1, 0x417ce40], [1, 0x41bde60], [1, 0x41fee80], [1, 0x423fea0], [1, 0x4280ec0], [1, 0x42c1ee0], [1, 0x4302f00], [1, 0x4343f20], [1, 0x4384f40], [1, 0x43c5f60], [1, 0x4406f80], [1, 0x4447fa0], [1, 0x4488fc0]]}
  layer.0.output.dense.bias:                                                   {input: opt_in1_layer.0.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8952700]]}
  layer.0.output.LayerNorm.weight:                                             {input: opt_in1_layer.0.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8638140]]}
  layer.0.output.LayerNorm.bias:                                               {input: opt_in2_layer.0.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x487f880]]}
  layer.1.attention.self.query.weight:                                         {input: opt_in0_layer.1.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x44cd0c0], [1, 0x450e0e0], [1, 0x454f100], [1, 0x4590120], [1, 0x45d1140], [1, 0x4612160], [1, 0x4653180], [1, 0x46941a0]]}
  layer.1.attention.self.query.bias:                                           {input: opt_in1_layer.1.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8963360]]}
  layer.1.attention.self.key.weight:                                           {input: opt_in0_layer.1.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8648560], [4, 0x8689580], [4, 0x86ca5a0], [4, 0x870b5c0], [4, 0x874c5e0], [4, 0x878d600], [4, 0x87ce620], [4, 0x880f640]]}
  layer.1.attention.self.key.bias:                                             {input: opt_in1_layer.1.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x488fca0]]}
  layer.1.attention.self.value.weight:                                         {input: opt_in0_layer.1.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8973780], [3, 0x89b47a0], [3, 0x89f57c0], [3, 0x8a367e0], [3, 0x8a77800], [3, 0x8ab8820], [3, 0x8af9840], [3, 0x8b3a860]]}
  layer.1.attention.self.value.bias:                                           {input: opt_in1_layer.1.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44e3d60]]}
  layer.1.attention.output.dense.weight:                                       {input: opt_in0_layer.1.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4dc8760], [0, 0x4e09780], [0, 0x4e4a7a0], [0, 0x4e8b7c0], [0, 0x4ecc7e0], [0, 0x4f0d800], [0, 0x4f4e820], [0, 0x4f8f840]]}
  layer.1.attention.output.dense.bias:                                         {input: opt_in1_layer.1.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88a5460]]}
  layer.1.attention.output.LayerNorm.weight:                                   {input: opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4fd0a40]]}
  layer.1.attention.output.LayerNorm.bias:                                     {input: opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x48308a0]]}
  layer.1.intermediate.dense.weight:                                           {input: opt_in0_layer.1.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4841500], [1, 0x4882520], [1, 0x48c3540], [1, 0x4904560], [1, 0x4945580], [1, 0x49865a0], [1, 0x49c75c0], [1, 0x4a085e0], [1, 0x4a49600], [1, 0x4a8a620], [1, 0x4acb640], [1, 0x4b0c660], [1, 0x4b4d680], [1, 0x4b8e6a0], [1, 0x4bcf6c0], [1, 0x4c106e0], [1, 0x4c51700], [1, 0x4c92720], [1, 0x4cd3740], [1, 0x4d14760], [1, 0x4d55780], [1, 0x4d967a0], [1, 0x4dd77c0], [1, 0x4e187e0], [1, 0x4e59800], [1, 0x4e9a820], [1, 0x4edb840], [1, 0x4f1c860], [1, 0x4f5d880], [1, 0x4f9e8a0], [1, 0x4fdf8c0], [1, 0x50208e0]]}
  layer.1.intermediate.dense.bias:                                             {input: opt_in1_layer.1.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fe5620]]}
  layer.1.output.dense.weight:                                                 {input: opt_in0_layer.1.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4fe3f40], [5, 0x5024f60], [5, 0x5065f80], [5, 0x50a6fa0], [5, 0x50e7fc0], [5, 0x5128fe0], [5, 0x516a000], [5, 0x51ab020], [5, 0x51ec040], [5, 0x522d060], [5, 0x526e080], [5, 0x52af0a0], [5, 0x52f00c0], [5, 0x53310e0], [5, 0x5372100], [5, 0x53b3120], [5, 0x53f4140], [5, 0x5435160], [5, 0x5476180], [5, 0x54b71a0], [5, 0x54f81c0], [5, 0x55391e0], [5, 0x557a200], [5, 0x55bb220], [5, 0x55fc240], [5, 0x563d260], [5, 0x567e280], [5, 0x56bf2a0], [5, 0x57002c0], [5, 0x57412e0], [5, 0x5782300], [5, 0x57c3320]]}
  layer.1.output.dense.bias:                                                   {input: opt_in1_layer.1.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92bb460]]}
  layer.1.output.LayerNorm.weight:                                             {input: opt_in1_layer.1.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fd10a0]]}
  layer.1.output.LayerNorm.bias:                                               {input: opt_in2_layer.1.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47fbae0]]}

  # constant
  lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46d5a00]]}
  lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8850ea0]]}
  input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4bac0e0]]}
  lc.input_tensor.softmax_132.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cb0e00]]}
  lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88516e0]]}
  lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47faa60]]}
  lc.input_tensor.layernorm_152.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8f8ba80]]}
  lc.input_tensor.layernorm_152.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8851f20]]}
  dc.input_tensor.layernorm_152.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46d8a40], [0, 0x4db4a20]]}
  lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47fb2a0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cc1a60]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8852760]]}
  lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e95d00]]}
  lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c9e8e0]]}
  lc.input_tensor.layernorm_166.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8637900]]}
  lc.input_tensor.layernorm_166.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e96540]]}
  dc.input_tensor.layernorm_166.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x487c7a0], [1, 0x44c9fe0]]}
  lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c9f120]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8962b20]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x86370c0]]}
  lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c9f960]]}
  lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46b7180]]}
  input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46d51c0]]}
  lc.input_tensor.softmax_185.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ca01a0]]}
  lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8850660]]}
  lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92b9ba0]]}
  lc.input_tensor.layernorm_205.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4fd0200]]}
  lc.input_tensor.layernorm_205.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fe14c0]]}
  dc.input_tensor.layernorm_205.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x482d7c0], [2, 0x44f4180]]}
  lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92ba3e0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88b5880]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fe1d00]]}
  lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44f7260]]}
  lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88b6900]]}
  lc.input_tensor.layernorm_219.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44f7aa0]]}
  lc.input_tensor.layernorm_219.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4840cc0]]}
  dc.input_tensor.layernorm_219.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4fe0e60], [0, 0x4fe2540]]}
  lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88b60c0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92bac20]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x482cf80]]}
  lc.input_tensor.bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cc22a0]]}
  lc.input_tensor.bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46ebf40]]}
  lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x480bf00]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cc2ae0]]}
  lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92a8700]]}
  lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88633c0]]}
  dc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.6:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46ec780], [5, 0x474dfa0]]}
  lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4db7b00]]}
  lc.input_tensor.bw_in1_add_210_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92a8f40]]}
  lc.input_tensor.bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fd0860]]}
  lc.input_tensor.bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44e3520]]}
  lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88a4c20]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46d8200]]}
  lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4fcf9c0]]}
  lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x841a1e0]]}
  dc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.6:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39ba920], [1, 0x3a1c140]]}
  lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a71e20]]}
  lc.input_tensor.bw_in1_add_191_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c69a80]]}
  lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a72660]]}
  input_1_multiply_183_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x853e400]]}
  lc.input_tensor.bw_in1_add_177_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c69240]]}
  lc.input_tensor.bw_in1_add_171_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c859e0]]}
  lc.input_tensor.bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x86232e0]]}
  lc.input_tensor.bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4837580]]}
  lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a832c0]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8622aa0]]}
  lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a60140]]}
  lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8419160]]}
  dc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.6:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x845a340], [3, 0x84bbb60]]}
  lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a60980]]}
  lc.input_tensor.bw_in1_add_157_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a60140]]}
  lc.input_tensor.bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38e6440]]}
  lc.input_tensor.bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38f70a0]]}
  lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x851d380]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x84199a0]]}
  lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c68a00]]}
  lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x44167a0]]}
  dc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.6:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38f78e0], [1, 0x3959100]]}
  lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a715e0]]}
  lc.input_tensor.bw_in1_add_138_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x483a700]]}
  lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0:           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8625c20]]}
  input_1_multiply_130_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c8cc00]]}
  lc.input_tensor.bw_in1_add_124_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4839ec0]]}
  lc.input_tensor.bw_in1_add_118_brcst_reduce_sum_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8950e40]]}

  # epoch_to_epoch
  e2e_layernorm_152.dc.multiply.9_0:                                           {input: layernorm_152.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa081ac0], [3, 0xa144ae0]]}
  e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0:            {input: layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4e1c460]]}
  e2e_layernorm_166.dc.sqrt.6_0:                                               {input: layernorm_166.dc.sqrt.6, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8bcf500], [4, 0x8bd56a0]]}
  e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0:      {input: buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b1c700], [5, 0x5bdf720]]}
  e2e_layernorm_205.dc.add.5_0:                                                {input: layernorm_205.dc.add.5, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5adca00], [0, 0x5ae2ba0]]}
  e2e_layernorm_205.dc.subtract.1_0:                                           {input: layernorm_205.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5985a80], [1, 0x5a48aa0]]}
  e2e_add_218_0:                                                               {input: add_218, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4e1c460], [2, 0x4edf480]]}
  e2e_layernorm_219.dc.multiply.8_0:                                           {input: layernorm_219.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x57ffa40], [1, 0x58c2a60]]}
  e2e_layernorm_219.dc.reciprocal.7_0:                                         {input: layernorm_219.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5ad06c0], [0, 0x5ad6860]]}
  e2e_gelu_211_0:                                                              {input: gelu_211, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x51e7940], [1, 0x52aa960], [1, 0x536d980], [1, 0x54309a0], [1, 0x54f39c0], [1, 0x55b69e0], [1, 0x5679a00], [1, 0x573ca20]]}
  e2e_add_210_0:                                                               {input: add_210, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9a699c0], [3, 0x9b2c9e0], [3, 0x9befa00], [3, 0x9cb2a20], [3, 0x9d75a40], [3, 0x9e38a60], [3, 0x9efba80], [3, 0x9fbeaa0]]}
  e2e_bw_in0_matmul_214_matmul_1_0:                                            {input: bw_in0_matmul_214_matmul_1, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa081ac0], [3, 0xa0e32e0], [3, 0xa144b00], [3, 0xa1a6320], [3, 0xa207b40], [3, 0xa269360], [3, 0xa2cab80], [3, 0xa32c3a0], [3, 0xa38dbc0], [3, 0xa3ef3e0], [3, 0xa450c00], [3, 0xa4b2420], [3, 0xa513c40], [3, 0xa575460], [3, 0xa5d6c80], [3, 0xa6384a0]]}
  e2e_layernorm_205.dc.add.10_0:                                               {input: layernorm_205.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4c96420], [2, 0x4d59440]]}
  e2e_bw_in1_matmul_208_transpose_0_0:                                         {input: bw_in1_matmul_208_transpose_0, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5985a80], [1, 0x5a48aa0]]}
  e2e_bw_in0_gelu_211_multiply_1_0:                                            {input: bw_in0_gelu_211_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b1c700], [5, 0x5bdf720], [5, 0x5ca2740], [5, 0x5d65760], [5, 0x5e28780], [5, 0x5eeb7a0], [5, 0x5fae7c0], [5, 0x60717e0]]}
  e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0:                     {input: bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8bcf500], [4, 0x8c92520]]}
  e2e_bw_in0_matmul_208_matmul_1_0:                                            {input: bw_in0_matmul_208_matmul_1, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5adca00], [0, 0x5af5020], [0, 0x5b0d640], [0, 0x5b25c60], [0, 0x5b3e280], [0, 0x5b568a0], [0, 0x5b6eec0], [0, 0x5b874e0], [0, 0x5b9fb00], [0, 0x5bb8120], [0, 0x5bd0740], [0, 0x5be8d60], [0, 0x5c01380], [0, 0x5c199a0], [0, 0x5c31fc0], [0, 0x5c4a5e0]]}
  e2e_layernorm_205.dc.multiply.8_0:                                           {input: layernorm_205.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59966c0], [5, 0x5a596e0]]}
  e2e_layernorm_205.dc.reciprocal.7_0:                                         {input: layernorm_205.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8bc31c0], [4, 0x8bc9360]]}
  e2e_matmul_196_0:                                                            {input: matmul_196, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5804340], [5, 0x58c7360]]}
  e2e_add_191_0:                                                               {input: add_191, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5026640], [0, 0x50e9660]]}
  e2e_softmax_185.dc.multiply.3_0:                                             {input: softmax_185.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x51ac680], [0, 0x563e6a0]]}
  e2e_layernorm_166.dc.add.10_0:                                               {input: layernorm_166.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5061900], [1, 0x5124920]]}
  e2e_bw_in0_matmul_196_matmul_1_0:                                            {input: bw_in0_matmul_196_matmul_1, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa081ac0], [3, 0xa513ae0]]}
  e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0:                   {input: bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6134800], [5, 0x6196020]]}
  e2e_add_177_0:                                                               {input: add_177, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4b103e0], [2, 0x4bd3400]]}
  e2e_add_171_0:                                                               {input: add_171, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x98e3980], [3, 0x99a69a0]]}
  e2e_bw_in0_matmul_189_matmul_1_0:                                            {input: bw_in0_matmul_189_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8d55540], [4, 0x8db6d60], [4, 0x8e18580], [4, 0x8e79da0]]}
  e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0:                     {input: bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4e1c460], [2, 0x4edf480]]}
  e2e_layernorm_166.dc.multiply.8_0:                                           {input: layernorm_166.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8a3d180], [4, 0x8b001a0]]}
  e2e_layernorm_166.dc.reciprocal.7_0:                                         {input: layernorm_166.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x598a380], [5, 0x5990520]]}
  e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0:                     {input: bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5c62c00], [0, 0x5d25c20]]}
  e2e_gelu_158_0:                                                              {input: gelu_158, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92cb880], [3, 0x938e8a0], [3, 0x94518c0], [3, 0x95148e0], [3, 0x95d7900], [3, 0x969a920], [3, 0x975d940], [3, 0x9820960]]}
  e2e_add_157_0:                                                               {input: add_157, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44f82e0], [2, 0x45bb300], [2, 0x467e320], [2, 0x4741340], [2, 0x4804360], [2, 0x48c7380], [2, 0x498a3a0], [2, 0x4a4d3c0]]}
  e2e_bw_in0_gelu_158_multiply_1_0:                                            {input: bw_in0_gelu_158_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5985a80], [1, 0x5a48aa0], [1, 0x5b0bac0], [1, 0x5bceae0], [1, 0x5c91b00], [1, 0x5d54b20], [1, 0x5e17b40], [1, 0x5edab60]]}
  e2e_layernorm_152.dc.add.10_0:                                               {input: layernorm_152.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88b7140], [4, 0x897a160]]}
  e2e_layernorm_152.dc.multiply.8_0:                                           {input: layernorm_152.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100], [0, 0x399d120]]}
  e2e_layernorm_152.dc.reciprocal.7_0:                                         {input: layernorm_152.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da100], [1, 0x38e02a0]]}
  e2e_matmul_143_0:                                                            {input: matmul_143, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100], [2, 0x399d120]]}
  e2e_bw_in0_matmul_147_matmul_1_0:                                            {input: bw_in0_matmul_147_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4e1c460], [2, 0x4e7dc80], [2, 0x4edf4a0], [2, 0x4f40cc0]]}
  e2e_add_138_0:                                                               {input: add_138, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100], [3, 0x7b77120]]}
  e2e_softmax_132.dc.multiply.3_0:                                             {input: softmax_132.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100], [4, 0x7f46120]]}
  e2e_add_124_0:                                                               {input: add_124, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100], [5, 0x399d120]]}
  e2e_add_118_0:                                                               {input: add_118, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a60140], [0, 0x3b23160]]}
  e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0:             {input: input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa081ac0], [3, 0xa089ce0]]}
  e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0:                {input: opt_in0_layer.1.attention.self.query.weight_multiply_1, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8bcf500], [4, 0x8c10520], [4, 0x8c51540], [4, 0x8c92560], [4, 0x8cd3580], [4, 0x8d145a0], [4, 0x8d555c0], [4, 0x8d965e0]]}
  e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0:       {input: input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5b1c700], [5, 0x5b3cf20]]}
  e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0:             {input: input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5adca00], [0, 0x5ae4c20]]}

  # optimizer_parameter
  input_opt_layer.0.attention.self.query.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c97f00]]}
  input_opt_layer.0.attention.self.query.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ca8b60]]}
  input_opt_layer.0.attention.self.key.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c9d860]]}
  input_opt_layer.0.attention.self.key.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8951680]]}
  input_opt_layer.0.attention.self.value.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8636880]]}
  input_opt_layer.0.attention.self.value.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e954c0]]}
  input_opt_layer.0.attention.output.dense.weight_0.lr:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x483af40]]}
  input_opt_layer.0.attention.output.dense.bias_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ca93a0]]}
  input_opt_layer.0.attention.output.LayerNorm.weight_0.lr:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c9e0a0]]}
  input_opt_layer.0.attention.output.LayerNorm.bias_0.lr:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8951ec0]]}
  input_opt_layer.0.intermediate.dense.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a84340]]}
  input_opt_layer.0.intermediate.dense.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8623b20]]}
  input_opt_layer.0.output.dense.weight_0.lr:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c8ab00]]}
  input_opt_layer.0.output.dense.bias_0.lr:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4837dc0]]}
  input_opt_layer.0.output.LayerNorm.weight_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c96640]]}
  input_opt_layer.0.output.LayerNorm.bias_0.lr:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a83b00]]}
  input_opt_layer.1.attention.self.query.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8747500]]}
  input_opt_layer.1.attention.self.query.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8624360]]}
  input_opt_layer.1.attention.self.key.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c8b340]]}
  input_opt_layer.1.attention.self.key.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4838600]]}
  input_opt_layer.1.attention.self.value.weight_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c96e80]]}
  input_opt_layer.1.attention.self.value.bias_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8746cc0]]}
  input_opt_layer.1.attention.output.dense.weight_0.lr:                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8747d40]]}
  input_opt_layer.1.attention.output.dense.bias_0.lr:                          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8624ba0]]}
  input_opt_layer.1.attention.output.LayerNorm.weight_0.lr:                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c8bb80]]}
  input_opt_layer.1.attention.output.LayerNorm.bias_0.lr:                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4838e40]]}
  input_opt_layer.1.intermediate.dense.weight_0.lr:                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c976c0]]}
  input_opt_layer.1.intermediate.dense.bias_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a84b80]]}
  input_opt_layer.1.output.dense.weight_0.lr:                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8748580]]}
  input_opt_layer.1.output.dense.bias_0.lr:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x86253e0]]}
  input_opt_layer.1.output.LayerNorm.weight_0.lr:                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c8c3c0]]}
  input_opt_layer.1.output.LayerNorm.bias_0.lr:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4839680]]}

  # loss
  loss_bert_encoder.output_layernorm_219:                                      {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8f9c6e0]]}

  # grad_accumulator
  grad_acc_layer.1.output.LayerNorm.bias:                                      {input: bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8852fa0]]}
  grad_acc_layer.1.output.LayerNorm.weight:                                    {input: bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4db8340]]}
  grad_acc_layer.1.output.dense.bias:                                          {input: bw_in1_add_216_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x480c740]]}
  grad_acc_layer.1.output.dense.weight:                                        {input: bw_in1_matmul_214_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cc3320], [2, 0x3d45340], [2, 0x3dc7360], [2, 0x3e49380], [2, 0x3ecb3a0], [2, 0x3f4d3c0], [2, 0x3fcf3e0], [2, 0x4051400], [2, 0x40d3420], [2, 0x4155440], [2, 0x41d7460], [2, 0x4259480], [2, 0x42db4a0], [2, 0x435d4c0], [2, 0x43df4e0], [2, 0x4461500]]}
  grad_acc_layer.1.intermediate.dense.bias:                                    {input: bw_in1_add_210_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8863c00]]}
  grad_acc_layer.1.intermediate.dense.weight:                                  {input: bw_in1_matmul_208_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x47af7c0], [5, 0x48317e0], [5, 0x48b3800], [5, 0x4935820], [5, 0x49b7840], [5, 0x4a39860], [5, 0x4abb880], [5, 0x4b3d8a0], [5, 0x4bbf8c0], [5, 0x4c418e0], [5, 0x4cc3900], [5, 0x4d45920], [5, 0x4dc7940], [5, 0x4e49960], [5, 0x4ecb980], [5, 0x4f4d9a0]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias:                            {input: bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x481cb60]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight:                          {input: bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x92a9780]]}
  grad_acc_layer.1.attention.output.dense.bias:                                {input: bw_in1_add_202_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x852dfe0]]}
  grad_acc_layer.1.attention.output.dense.weight:                              {input: bw_in1_matmul_200_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x841aa20], [4, 0x849ca40], [4, 0x851ea60], [4, 0x85a0a80]]}
  grad_acc_layer.1.attention.self.value.bias:                                  {input: bw_in1_add_191_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x461f0e0]]}
  grad_acc_layer.1.attention.self.value.weight:                                {input: bw_in1_matmul_189_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a7d960], [1, 0x3aff980], [1, 0x3b819a0], [1, 0x3c039c0]]}
  grad_acc_layer.1.attention.self.key.bias:                                    {input: bw_in1_add_177_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c6a2c0]]}
  grad_acc_layer.1.attention.self.key.weight:                                  {input: bw_in1_matmul_175_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x462f500], [0, 0x46b1520], [0, 0x4733540], [0, 0x47b5560]]}
  grad_acc_layer.1.attention.self.query.bias:                                  {input: bw_in1_add_171_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a72ea0]]}
  grad_acc_layer.1.attention.self.query.weight:                                {input: bw_in1_matmul_169_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x853ec40], [3, 0x85c0c60], [3, 0x8642c80], [3, 0x86c4ca0]]}
  grad_acc_layer.0.output.LayerNorm.bias:                                      {input: bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c7a6e0]]}
  grad_acc_layer.0.output.LayerNorm.weight:                                    {input: bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c86220]]}
  grad_acc_layer.0.output.dense.bias:                                          {input: bw_in1_add_163_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38e6c80]]}
  grad_acc_layer.0.output.dense.weight:                                        {input: bw_in1_matmul_161_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3be6180], [0, 0x3c681a0], [0, 0x3cea1c0], [0, 0x3d6c1e0], [0, 0x3dee200], [0, 0x3e70220], [0, 0x3ef2240], [0, 0x3f74260], [0, 0x3ff6280], [0, 0x40782a0], [0, 0x40fa2c0], [0, 0x417c2e0], [0, 0x41fe300], [0, 0x4280320], [0, 0x4302340], [0, 0x4384360]]}
  grad_acc_layer.0.intermediate.dense.bias:                                    {input: bw_in1_add_157_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x83d8140]]}
  grad_acc_layer.0.intermediate.dense.weight:                                  {input: bw_in1_matmul_155_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c3a140], [3, 0x7cbc160], [3, 0x7d3e180], [3, 0x7dc01a0], [3, 0x7e421c0], [3, 0x7ec41e0], [3, 0x7f46200], [3, 0x7fc8220], [3, 0x804a240], [3, 0x80cc260], [3, 0x814e280], [3, 0x81d02a0], [3, 0x82522c0], [3, 0x82d42e0], [3, 0x8356300], [3, 0x83d8320]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias:                            {input: bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4406380]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight:                          {input: bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a611c0]]}
  grad_acc_layer.0.attention.output.dense.bias:                                {input: bw_in1_add_149_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x851dbc0]]}
  grad_acc_layer.0.attention.output.dense.weight:                              {input: bw_in1_matmul_147_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a60980], [5, 0x3ae29a0], [5, 0x3b649c0], [5, 0x3be69e0]]}
  grad_acc_layer.0.attention.self.value.bias:                                  {input: bw_in1_add_138_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a853c0]]}
  grad_acc_layer.0.attention.self.value.weight:                                {input: bw_in1_matmul_136_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8748dc0], [3, 0x87cade0], [3, 0x884ce00], [3, 0x88cee20]]}
  grad_acc_layer.0.attention.self.key.bias:                                    {input: bw_in1_add_124_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c98740]]}
  grad_acc_layer.0.attention.self.key.weight:                                  {input: bw_in1_matmul_122_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a957e0], [2, 0x3b17800], [2, 0x3b99820], [2, 0x3c1b840]]}
  grad_acc_layer.0.attention.self.query.bias:                                  {input: bw_in1_add_118_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8626460]]}
  grad_acc_layer.0.attention.self.query.weight:                                {input: bw_in1_matmul_116_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c8d440], [5, 0x3d0f460], [5, 0x3d91480], [5, 0x3e134a0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 2
    matmul_116: {type: matmul, grid_loc: [0, 0], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0, layer.0.attention.self.query.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_118: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_116, layer.0.attention.self.query.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_122: {type: matmul, grid_loc: [0, 2], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0, layer.0.attention.self.key.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_124: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_122, layer.0.attention.self.key.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_128: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_118, add_124],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0],
         t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}]}
    multiply_130: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_128, input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    add_131: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_130, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_132.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_131],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_132.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_132.dc.reduce_sum.1.0],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    softmax_132.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [2, 1], inputs: [softmax_132.dc.exp.0, lc.input_tensor.softmax_132.dc.reduce_sum.1.0_splt_brcst_1_0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_132.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_132.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_132.dc.multiply.3: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_132.dc.exp.0, softmax_132.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_136: {type: matmul, grid_loc: [0, 4], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0, layer.0.attention.self.value.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_138: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_136, layer.0.attention.self.value.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_143: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_132.dc.multiply.3, add_138],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_147: {type: matmul, grid_loc: [4, 6], grid_size: [2, 2], inputs: [matmul_143, layer.0.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.0.attention.output.dense.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_149: {type: add, grid_loc: [5, 3], grid_size: [2, 1], inputs: [matmul_147, layer.0.attention.output.dense.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    add_151: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [add_149, hidden_states],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [add_151, lc.input_tensor.layernorm_152.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_152.dc.subtract.1: {type: subtract, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_151, layernorm_152.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_152.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_152.dc.subtract.1, layernorm_152.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [2, 1], inputs: [layernorm_152.dc.multiply.2, lc.input_tensor.layernorm_152.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_152.dc.add.5: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_152.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_152.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.sqrt.6: {type: sqrt, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_152.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_152.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_152.dc.reciprocal.7, lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_layernorm_152.dc.subtract.1_buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_152.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_152.dc.subtract.1_buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8, layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_152.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_152.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_1:
    target_device: 0
    input_count: 2
    layernorm_152.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_155: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_152.dc.add.10, layer.0.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_157: {type: add, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_155, layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    gelu_158: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_157],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_161: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_158, layer.0.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 16, u_kt: 8}}
    layer.0.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0, layer.0.output.dense.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_163: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_161, layer.0.output.dense.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_152.dc.add.10_add_165: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_152.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_152.dc.add.10_add_165: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_152.dc.add.10_add_165],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_165: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_163, buffer_0_layernorm_152.dc.add.10_add_165],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_166.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [add_165, lc.input_tensor.layernorm_166.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_166.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_165, layernorm_166.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_166.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_166.dc.subtract.1, layernorm_166.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_166.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_166.dc.multiply.2, lc.input_tensor.layernorm_166.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_166.dc.add.5: {type: add, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_166.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_166.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_166.dc.sqrt.6: {type: sqrt, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_166.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_166.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2:
    target_device: 0
    input_count: 2
    layernorm_166.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.sqrt.6_0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_166.dc.reciprocal.7, lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_166.dc.multiply.8: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0, layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_166.dc.multiply.9: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_166.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_166.dc.add.10: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_166.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_169: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [layernorm_166.dc.add.10, layer.1.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0, layer.1.attention.self.query.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_171: {type: add, grid_loc: [1, 3], grid_size: [2, 1], inputs: [matmul_169, layer.1.attention.self.query.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_175: {type: matmul, grid_loc: [2, 4], grid_size: [2, 2], inputs: [layernorm_166.dc.add.10, layer.1.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0, layer.1.attention.self.key.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_177: {type: add, grid_loc: [1, 7], grid_size: [2, 1], inputs: [matmul_175, layer.1.attention.self.key.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_181: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_171, add_177],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0],
         t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}]}
    multiply_183: {type: multiply, grid_loc: [3, 6], grid_size: [2, 1], inputs: [matmul_181, input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    add_184: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [multiply_183, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_185.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_184],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_185.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_185.dc.reduce_sum.1.0],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    softmax_185.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_185.dc.exp.0, lc.input_tensor.softmax_185.dc.reduce_sum.1.0_splt_brcst_1_0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_185.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_185.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_185.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_185.dc.exp.0, softmax_185.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_189: {type: matmul, grid_loc: [5, 6], grid_size: [2, 2], inputs: [layernorm_166.dc.add.10, layer.1.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0, layer.1.attention.self.value.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_191: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_189, layer.1.attention.self.value.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_196: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [softmax_185.dc.multiply.3, add_191],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_200: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_196, layer.1.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 16],
         attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.1.attention.output.dense.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_202: {type: add, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_200, layer.1.attention.output.dense.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_166.dc.add.10_add_204: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [layernorm_166.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_166.dc.add.10_add_204: {type: nop, grid_loc: [7, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_166.dc.add.10_add_204],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_204: {type: add, grid_loc: [7, 7], grid_size: [2, 1], inputs: [add_202, buffer_0_layernorm_166.dc.add.10_add_204],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [add_204, lc.input_tensor.layernorm_205.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_205.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_204, layernorm_205.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_205.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_205.dc.subtract.1, layernorm_205.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_205.dc.multiply.2, lc.input_tensor.layernorm_205.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_205.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_205.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_205.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3:
    target_device: 0
    input_count: 2
    layernorm_205.dc.sqrt.6: {type: sqrt, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.add.5_0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_205.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_205.dc.reciprocal.7, lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.multiply.8: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8, layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_205.dc.multiply.9: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_205.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_205.dc.add.10: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_205.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_208: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [layernorm_205.dc.add.10, layer.1.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    layer.1.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 2], inputs: [lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_210: {type: add, grid_loc: [2, 3], grid_size: [2, 4], inputs: [matmul_208, layer.1.intermediate.dense.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    gelu_211: {type: gelu, grid_loc: [6, 0], grid_size: [2, 4], inputs: [add_210],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_214: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_211, layer.1.output.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 16, u_kt: 8}}
    layer.1.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0, layer.1.output.dense.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    add_216: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_214, layer.1.output.dense.bias_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_205.dc.add.10_add_218: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_205.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_205.dc.add.10_add_218: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_205.dc.add.10_add_218],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [72], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_218: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_216, buffer_0_layernorm_205.dc.add.10_add_218],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_4:
    target_device: 0
    input_count: 2
    layernorm_219.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_218_0, lc.input_tensor.layernorm_219.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_219.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_218_0, layernorm_219.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_219.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_219.dc.subtract.1, layernorm_219.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_219.dc.multiply.2, lc.input_tensor.layernorm_219.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_219.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_219.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_219.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_219.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_219.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_219.dc.reciprocal.7, lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_layernorm_219.dc.subtract.1_buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_219.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_219.dc.subtract.1_buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8, layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_219.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_219.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_219.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_219.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], untilize_output: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  bwd_5:
    target_device: 0
    input_count: 2
    bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoder.output_layernorm_219], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_219_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_219.dc.multiply.8_0, loss_bert_encoder.output_layernorm_219],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_219_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_219.dc.reciprocal.7_0, lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [loss_bert_encoder.output_layernorm_219, layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0, e2e_layernorm_219.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_219.dc.multiply.8_0, bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_219_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 1], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.6, bw_in0_layernorm_219_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_219_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [2, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_219_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_216_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_214_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9, layer.1.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_214_transpose_0: {type: nop, grid_loc: [6, 0], grid_size: [2, 4], inputs: [e2e_gelu_211_0],
         t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_214_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_214_transpose_0, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 12, u_kt: 1}}

  bwd_6:
    target_device: 0
    input_count: 2
    bw_in0_gelu_211_gelu_derivative_0: {type: gelu_derivative, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_add_210_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    bw_in0_gelu_211_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [2, 4], inputs: [bw_in0_gelu_211_gelu_derivative_0, e2e_bw_in0_matmul_214_matmul_1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_210_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_210_brcst_reduce_sum_0.0, bw_in0_gelu_211_multiply_1], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 12, u_kt: 1}}
    bw_in0_matmul_208_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in0_gelu_211_multiply_1, layer.1.intermediate.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_208_transpose_0: {type: nop, grid_loc: [2, 5], grid_size: [1, 2], inputs: [e2e_layernorm_205.dc.add.10_0],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}

  bwd_7:
    target_device: 0
    input_count: 2
    bw_in1_matmul_208_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 2], inputs: [e2e_bw_in1_matmul_208_transpose_0_0, e2e_bw_in0_gelu_211_multiply_1_0], gradient_op: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_205_combine_add_0: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_208_matmul_1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_205_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_205_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 4], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.multiply.8_0, bw_in0_layernorm_205_combine_add_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_205_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.reciprocal.7_0, lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_combine_add_0, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0, e2e_layernorm_205.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.multiply.8_0, bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_205_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.6, bw_in0_layernorm_205_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_205_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [2, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_205_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_202_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_200_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 2], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9, layer.1.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_200_transpose_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 2], inputs: [e2e_matmul_196_0],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_200_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_200_transpose_0, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 3}}
    bw_in0_matmul_196_matmul_1: {type: matmul, grid_loc: [7, 2], grid_size: [2, 1], inputs: [bw_in0_matmul_200_matmul_1, e2e_add_191_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_196_transpose_0: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [e2e_softmax_185.dc.multiply.3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_196_matmul_1: {type: matmul, grid_loc: [6, 7], grid_size: [2, 1], inputs: [bw_in1_matmul_196_transpose_0, bw_in0_matmul_200_matmul_1],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_191_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_191_brcst_reduce_sum_0.0, bw_in1_matmul_196_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_189_add_191_unsqueeze3_362_squeeze_0: {type: nop, grid_loc: [7, 3], grid_size: [2, 1], inputs: [bw_in1_matmul_196_matmul_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 16]}
    bw_in0_matmul_189_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 2], inputs: [bw_in0_matmul_189_add_191_unsqueeze3_362_squeeze_0, layer.1.attention.self.value.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_189_transpose_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 2], inputs: [e2e_layernorm_166.dc.add.10_0],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_189_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_189_transpose_0, bw_in0_matmul_189_add_191_unsqueeze3_362_squeeze_0], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 3}}
    bw_in0_softmax_185_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [bw_in0_matmul_196_matmul_1, e2e_softmax_185.dc.multiply.3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [bw_in0_softmax_185_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {m_k: 1, u_kt: 12}}

  bwd_8:
    target_device: 0
    input_count: 2
    bw_in0_softmax_185_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_196_matmul_1_0, e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    bw_in0_softmax_185_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [bw_in0_softmax_185_softmax_bw_0.dc.subtract.2, e2e_softmax_185.dc.multiply.3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_1_multiply_183_tile_bcast_tile_bcast],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0],
         t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}]}
    bw_in0_multiply_183_multiply_0: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_softmax_185_softmax_bw_0.dc.multiply.3, input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    bw_in0_matmul_181_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [2, 1], inputs: [bw_in0_multiply_183_multiply_0, e2e_add_177_0],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_181_transpose_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_171_0],
         t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_181_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [bw_in1_matmul_181_transpose_0, bw_in0_multiply_183_multiply_0],
         t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_177_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_177_brcst_reduce_sum_0.0, bw_in1_matmul_181_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_175_add_177_unsqueeze3_350_squeeze_0: {type: nop, grid_loc: [1, 5], grid_size: [2, 1], inputs: [bw_in1_matmul_181_matmul_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_175_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [2, 2], inputs: [bw_in0_matmul_175_add_177_unsqueeze3_350_squeeze_0, layer.1.attention.self.key.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_175_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_166.dc.add.10_0],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_175_matmul_1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_175_transpose_0, bw_in0_matmul_175_add_177_unsqueeze3_350_squeeze_0], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 3}}
    bw_in1_add_171_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_171_brcst_reduce_sum_0.0, bw_in0_matmul_181_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_169_matmul_1: {type: matmul, grid_loc: [3, 4], grid_size: [2, 2], inputs: [bw_in0_matmul_181_matmul_1, layer.1.attention.self.query.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 16],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_169_transpose_0: {type: nop, grid_loc: [3, 6], grid_size: [1, 2], inputs: [e2e_layernorm_166.dc.add.10_0],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_169_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_169_transpose_0, bw_in0_matmul_181_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 4, u_kt: 3}}
    bw_in0_reshape_167_combine_add_0: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_189_matmul_1_0, bw_in0_matmul_175_matmul_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_reshape_167_combine_add_1: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_reshape_167_combine_add_0, bw_in0_matmul_169_matmul_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_166_combine_add_0: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0, bw_in0_reshape_167_combine_add_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_166_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_166_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [7, 3], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.multiply.8_0, bw_in0_layernorm_166_combine_add_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_166_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.reciprocal.7_0, lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_combine_add_0, layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [5, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0, e2e_layernorm_166.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.multiply.8_0, bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_166_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [7, 0], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.6, bw_in0_layernorm_166_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_166_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [7, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [7, 2], grid_size: [2, 1], inputs: [layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_166_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_163_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0, bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}

  bwd_9:
    target_device: 0
    input_count: 2
    bw_in0_matmul_161_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0, layer.0.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_161_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [2, 4], inputs: [e2e_gelu_158_0],
         t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_161_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_161_transpose_0, e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0], gradient_op: true,
         t: 1, mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 12, u_kt: 1}}
    bw_in0_gelu_158_gelu_derivative_0: {type: gelu_derivative, grid_loc: [6, 0], grid_size: [2, 8], inputs: [e2e_add_157_0],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    bw_in0_gelu_158_multiply_1: {type: multiply, grid_loc: [2, 4], grid_size: [2, 4], inputs: [bw_in0_gelu_158_gelu_derivative_0, bw_in0_matmul_161_matmul_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_157_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_157_brcst_reduce_sum_0.0, bw_in0_gelu_158_multiply_1], gradient_op: true,
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 12, u_kt: 1}}

  bwd_10:
    target_device: 0
    input_count: 2
    bw_in0_matmul_155_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_gelu_158_multiply_1_0, layer.0.intermediate.dense.weight],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_155_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_152.dc.add.10_0],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_155_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [8, 2], inputs: [bw_in1_matmul_155_transpose_0, e2e_bw_in0_gelu_158_multiply_1_0], gradient_op: true,
         t: 1, mblock: [2, 16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_152_combine_add_0: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0, bw_in0_matmul_155_matmul_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_152_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_152_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.multiply.8_0, bw_in0_layernorm_152_combine_add_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_152_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.reciprocal.7_0, lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_combine_add_0, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0, e2e_layernorm_152.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.multiply.8_0, bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_152_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.6, bw_in0_layernorm_152_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_152_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_152_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in1_add_149_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_147_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9, layer.0.attention.output.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_147_transpose_0: {type: nop, grid_loc: [7, 4], grid_size: [1, 2], inputs: [e2e_matmul_143_0],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_147_matmul_1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_147_transpose_0, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 3}}

  bwd_11:
    target_device: 0
    input_count: 2
    bw_in0_matmul_143_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_147_matmul_1_0, e2e_add_138_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_143_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_softmax_132.dc.multiply.3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_143_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [bw_in1_matmul_143_transpose_0, e2e_bw_in0_matmul_147_matmul_1_0],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_138_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_138_brcst_reduce_sum_0.0, bw_in1_matmul_143_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_136_transpose_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 2], inputs: [hidden_states],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_136_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_136_transpose_0, bw_in1_matmul_143_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 4, u_kt: 3}}
    bw_in0_softmax_132_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_matmul_143_matmul_1, e2e_softmax_132.dc.multiply.3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [bw_in0_softmax_132_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}],
         attributes: {m_k: 1, u_kt: 12}}
    bw_in0_softmax_132_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_matmul_143_matmul_1, bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    bw_in0_softmax_132_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [1, 3], grid_size: [2, 1], inputs: [bw_in0_softmax_132_softmax_bw_0.dc.subtract.2, e2e_softmax_132.dc.multiply.3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_1_multiply_130_tile_bcast_tile_bcast],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0],
         t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 12}]}
    bw_in0_multiply_130_multiply_0: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [bw_in0_softmax_132_softmax_bw_0.dc.multiply.3, input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [bw_in0_multiply_130_multiply_0, e2e_add_124_0],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_add_118_0],
         t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 2], inputs: [bw_in1_matmul_128_transpose_0, bw_in0_multiply_130_multiply_0],
         t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_124_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_124_brcst_reduce_sum_0.0, bw_in1_matmul_128_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_122_transpose_0: {type: nop, grid_loc: [3, 5], grid_size: [1, 2], inputs: [hidden_states],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_122_matmul_1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_122_transpose_0, bw_in1_matmul_128_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 16],
         attributes: {m_k: 4, u_kt: 3}}
    bw_in1_add_118_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_118_brcst_reduce_sum_0.0, bw_in0_matmul_128_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 12}],
         attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_116_transpose_0: {type: nop, grid_loc: [5, 0], grid_size: [1, 2], inputs: [hidden_states],
         t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_116_matmul_1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_116_transpose_0, bw_in0_matmul_128_matmul_1], gradient_op: true,
         t: 1, mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 4, u_kt: 3}}

  opt_12:
    target_device: 0
    input_count: 1
    input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.query.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.query.weight, input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.query.weight, opt_in1_layer.0.attention.self.query.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.query.bias, input_opt_layer.0.attention.self.query.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layer.0.attention.self.query.bias, opt_in1_layer.0.attention.self.query.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.key.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [3, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.key.weight, input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.key.weight, opt_in1_layer.0.attention.self.key.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.key.bias, input_opt_layer.0.attention.self.key.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layer.0.attention.self.key.bias, opt_in1_layer.0.attention.self.key.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.value.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [5, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.value.weight, input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [6, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.value.weight, opt_in1_layer.0.attention.self.value.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.value.bias, input_opt_layer.0.attention.self.value.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layer.0.attention.self.value.bias, opt_in1_layer.0.attention.self.value.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.output.dense.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.0.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.output.dense.weight, input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in0_layer.0.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [9, 0], grid_size: [1, 8], inputs: [layer.0.attention.output.dense.weight, opt_in0_layer.0.attention.output.dense.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.dense.bias, input_opt_layer.0.attention.output.dense.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layer.0.attention.output.dense.bias, opt_in1_layer.0.attention.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.weight, input_opt_layer.0.attention.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.weight, opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.bias, input_opt_layer.0.attention.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.bias, opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  opt_13:
    target_device: 0
    input_count: 1
    input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 2], inputs: [input_opt_layer.0.intermediate.dense.weight_0.lr],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 128}]}
    opt_in0_layer.0.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [4, 8], inputs: [grad_acc_layer.0.intermediate.dense.weight, input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in0_layer.0.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [4, 8], inputs: [layer.0.intermediate.dense.weight, opt_in0_layer.0.intermediate.dense.weight_multiply_1],
         t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.intermediate.dense.bias, input_opt_layer.0.intermediate.dense.bias_0.lr],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 128}]}
    opt_in1_layer.0.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layer.0.intermediate.dense.bias, opt_in1_layer.0.intermediate.dense.bias_multiply_1],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 2], inputs: [input_opt_layer.0.output.dense.weight_0.lr],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}

  opt_14:
    target_device: 0
    input_count: 1
    opt_in0_layer.0.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.0.output.dense.weight, e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 128}]}
    opt_in0_layer.0.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.0.output.dense.weight, opt_in0_layer.0.output.dense.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.dense.bias, input_opt_layer.0.output.dense.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.0.output.dense.bias, opt_in1_layer.0.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.weight, input_opt_layer.0.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.weight, opt_in1_layer.0.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.bias, input_opt_layer.0.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in2_layer.0.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.bias, opt_in2_layer.0.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.query.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [9, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.query.weight, input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}

  opt_15:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.query.weight, e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.query.bias, input_opt_layer.1.attention.self.query.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layer.1.attention.self.query.bias, opt_in1_layer.1.attention.self.query.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.key.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.key.weight, input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in0_layer.1.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.key.weight, opt_in0_layer.1.attention.self.key.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.key.bias, input_opt_layer.1.attention.self.key.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layer.1.attention.self.key.bias, opt_in1_layer.1.attention.self.key.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.value.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [4, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.value.weight, input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in0_layer.1.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.value.weight, opt_in0_layer.1.attention.self.value.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.value.bias, input_opt_layer.1.attention.self.value.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layer.1.attention.self.value.bias, opt_in1_layer.1.attention.self.value.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.output.dense.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [7, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.output.dense.weight, input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in0_layer.1.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [8, 0], grid_size: [1, 8], inputs: [layer.1.attention.output.dense.weight, opt_in0_layer.1.attention.output.dense.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.dense.bias, input_opt_layer.1.attention.output.dense.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.1.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layer.1.attention.output.dense.bias, opt_in1_layer.1.attention.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.weight, input_opt_layer.1.attention.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.weight, opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.bias, input_opt_layer.1.attention.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.bias, opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [9, 0], grid_size: [1, 2], inputs: [input_opt_layer.1.intermediate.dense.weight_0.lr],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 128}]}

  opt_16:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.1.intermediate.dense.weight, e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0],
         t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    opt_in0_layer.1.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.1.intermediate.dense.weight, opt_in0_layer.1.intermediate.dense.weight_multiply_1],
         t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.intermediate.dense.bias, input_opt_layer.1.intermediate.dense.bias_0.lr],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 128}]}
    opt_in1_layer.1.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.1.intermediate.dense.bias, opt_in1_layer.1.intermediate.dense.bias_multiply_1],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 2], inputs: [input_opt_layer.1.output.dense.weight_0.lr],
         t: 1, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}

  opt_17:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.1.output.dense.weight, e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 128}]}
    opt_in0_layer.1.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.1.output.dense.weight, opt_in0_layer.1.output.dense.weight_multiply_1],
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.dense.bias, input_opt_layer.1.output.dense.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.1.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.1.output.dense.bias, opt_in1_layer.1.output.dense.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.weight, input_opt_layer.1.output.LayerNorm.weight_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in1_layer.1.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.weight, opt_in1_layer.1.output.LayerNorm.weight_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.bias, input_opt_layer.1.output.LayerNorm.bias_0.lr],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    opt_in2_layer.1.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.bias, opt_in2_layer.1.output.LayerNorm.bias_multiply_1],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q4: 0, $lptr_q3: 0, $lptr_q4: 0, $gptr_q3: 0, $gptr_q5: 0, $lptr_q5: 0}
    - staticvar: {$gptr_q0_shadow: 0, $lptr_q0: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0}
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_layernorm_152.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0]
    -   execute: {graph_name: fwd_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_132.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_152.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_152.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_152.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_166.dc.sqrt.6_0, e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0]
    -   execute: {graph_name: fwd_1, queue_settings: {
               e2e_layernorm_152.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_166.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_166.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_166.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_152.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_205.dc.subtract.1_0, e2e_layernorm_205.dc.add.5_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_166.dc.sqrt.6_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_185.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_205.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_205.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_205.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_166.dc.sqrt.6_0, e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_add_218_0]
    -   execute: {graph_name: fwd_3, queue_settings: {
               e2e_layernorm_205.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layernorm_205.dc.add.5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_205.dc.subtract.1_0, e2e_layernorm_205.dc.add.5_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: fwd_4, queue_settings: {
               e2e_add_218_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.layernorm_219.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_219.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_219.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_218_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
    - endloop

  - run_bwd:
    - param: [$p_zero_grad, $p_loop_count]
    - var: {$v_zero_grad: 0, $c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $lptr_q15: 0, $gptr_q10: 0, $gptr_q15: 0, $gptr_q10_shadow: 0, $lptr_q3: 0, $lptr_q8: 0, $gptr_q12: 0, $lptr_q10: 0, $gptr_q3: 0, $gptr_q8: 0, $lptr_q6: 0, $lptr_q12: 0, $gptr_q6: 0}
    - staticvar: {$gptr_q0: 0, $gptr_q1: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q4_shadow: 0, $lptr_q14: 0, $lptr_q4: 0, $gptr_q2: 0, $gptr_q14: 0, $gptr_q13: 0, $gptr_q9: 0, $lptr_q2: 0, $lptr_q13: 0, $lptr_q5: 0, $lptr_q11: 0, $lptr_q9: 0, $lptr_q7: 0, $gptr_q11: 0, $gptr_q4: 0, $gptr_q7: 0, $gptr_q5: 0}
    - varinst: [$gptr_q10, set, $gptr_q10_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_bw_in0_matmul_214_matmul_1_0, e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0]
    -   execute: {graph_name: bwd_5, queue_settings: {
               loss_bert_encoder.output_layernorm_219: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e_gelu_211_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_219.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_219.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_bw_in0_gelu_211_multiply_1_0, e2e_bw_in0_matmul_208_matmul_1_0, e2e_bw_in1_matmul_208_transpose_0_0]
    -   execute: {graph_name: bwd_6, queue_settings: {
               e2e_layernorm_205.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_add_210_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_bw_in0_matmul_214_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_210_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_matmul_214_matmul_1_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_196_matmul_1_0, e2e_bw_in0_matmul_189_matmul_1_0, e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0]
    -   execute: {graph_name: bwd_7, queue_settings: {
               e2e_layernorm_166.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_softmax_185.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_add_191_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_196_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_layernorm_205.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_layernorm_205.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_bw_in0_gelu_211_multiply_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_bw_in0_matmul_208_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_bw_in1_matmul_208_transpose_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_191_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_211_multiply_1_0, e2e_bw_in0_matmul_208_matmul_1_0, e2e_bw_in1_matmul_208_transpose_0_0]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0]
    -   execute: {graph_name: bwd_8, queue_settings: {
               e2e_layernorm_166.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_layernorm_166.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_layernorm_166.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_add_171_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_add_177_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_softmax_185.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_bw_in0_matmul_196_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_bw_in0_matmul_189_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_183_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_177_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_171_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.1.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_196_matmul_1_0, e2e_bw_in0_matmul_189_matmul_1_0, e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_bw_in0_gelu_158_multiply_1_0]
    -   execute: {graph_name: bwd_9, queue_settings: {
               e2e_add_157_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_gelu_158_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_157_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_bw_in0_matmul_147_matmul_1_0]
    -   execute: {graph_name: bwd_10, queue_settings: {
               e2e_matmul_143_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_layernorm_152.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_layernorm_152.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_layernorm_152.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_bw_in0_gelu_158_multiply_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_158_multiply_1_0]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
    -   execute: {graph_name: bwd_11, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_add_118_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_add_124_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_softmax_132.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_add_138_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_bw_in0_matmul_147_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               lc.input_tensor.bw_in1_add_138_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_130_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_124_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_118_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layer.0.attention.self.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_matmul_147_matmul_1_0]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 4]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt:
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $lptr_q0: 0, $lptr_q3: 0}
    - execute: {graph_name: opt_12, queue_settings: {
             layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - allocate_queue: [e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0]
    - execute: {graph_name: opt_13, queue_settings: {
             layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - allocate_queue: [e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0]
    - execute: {graph_name: opt_14, queue_settings: {
             e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
             layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - deallocate_queue: [e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0]
    - varinst: [$gptr_q0, incwrap, $c_one, 2]
    - varinst: [$lptr_q0, incwrap, $c_one, 2]
    - allocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
    - execute: {graph_name: opt_15, queue_settings: {
             e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
             layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - deallocate_queue: [e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0]
    - varinst: [$gptr_q1, incwrap, $c_one, 2]
    - varinst: [$lptr_q1, incwrap, $c_one, 2]
    - allocate_queue: [e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0]
    - execute: {graph_name: opt_16, queue_settings: {
             e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
             layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - deallocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
    - varinst: [$gptr_q2, incwrap, $c_one, 2]
    - varinst: [$lptr_q2, incwrap, $c_one, 2]
    - execute: {graph_name: opt_17, queue_settings: {
             e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
             layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
             grad_acc_layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    - deallocate_queue: [e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0]
    - varinst: [$gptr_q3, incwrap, $c_one, 2]
    - varinst: [$lptr_q3, incwrap, $c_one, 2]

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 2.0
  io-config:
    inputs: [hidden_states, attention_mask, loss_bert_encoder.output_layernorm_219]
    outputs: [bert_encoder.output_layernorm_219]