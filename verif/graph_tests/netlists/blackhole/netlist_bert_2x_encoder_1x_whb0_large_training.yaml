# git checkout 435f4d73
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[training-Wormhole_B0-chip1-enc2-large]

devices:
  arch: blackhole

queues:

  # input
  hidden_states: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10000000]]}
  attention_mask: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10000000]]}

  # output
  bert_encoder.output_layernorm_219: {input: layernorm_219.dc.add.10, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight: {input: opt_in1_layer.0.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x10330040], [0, 0x10374080], [0, 0x103b80c0], [0, 0x103fc100], [0, 0x10440140], [0, 0x10484180], [0, 0x104c81c0], [0, 0x1050c200]]}
  layer.0.attention.self.query.bias: {input: opt_in1_layer.0.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10000000]]}
  layer.0.attention.self.key.weight: {input: opt_in1_layer.0.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10000000], [3, 0x10044040], [3, 0x10088080], [3, 0x100cc0c0], [3, 0x10110100], [3, 0x10154140], [3, 0x10198180], [3, 0x101dc1c0]]}
  layer.0.attention.self.key.bias: {input: opt_in1_layer.0.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x10000000]]}
  layer.0.attention.self.value.weight: {input: opt_in1_layer.0.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10220200], [3, 0x10264240], [3, 0x102a8280], [3, 0x102ec2c0], [3, 0x10330300], [3, 0x10374340], [3, 0x103b8380], [3, 0x103fc3c0]]}
  layer.0.attention.self.value.bias: {input: opt_in1_layer.0.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x10011040]]}
  layer.0.attention.output.dense.weight: {input: opt_in0_layer.0.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10550240], [0, 0x10594280], [0, 0x105d82c0], [0, 0x1061c300], [0, 0x10660340], [0, 0x106a4380], [0, 0x106e83c0], [0, 0x1072c400]]}
  layer.0.attention.output.dense.bias: {input: opt_in1_layer.0.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10011040]]}
  layer.0.attention.output.LayerNorm.weight: {input: opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10440400]]}
  layer.0.attention.output.LayerNorm.bias: {input: opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10022080]]}
  layer.0.intermediate.dense.weight: {input: opt_in0_layer.0.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x100330c0], [5, 0x10077100], [5, 0x100bb140], [5, 0x100ff180], [5, 0x101431c0], [5, 0x10187200], [5, 0x101cb240], [5, 0x1020f280], [5, 0x102532c0], [5, 0x10297300],
      [5, 0x102db340], [5, 0x1031f380], [5, 0x103633c0], [5, 0x103a7400], [5, 0x103eb440], [5, 0x1042f480], [5, 0x104734c0], [5, 0x104b7500], [5, 0x104fb540], [5, 0x1053f580], [5, 0x105835c0], [5, 0x105c7600],
      [5, 0x1060b640], [5, 0x1064f680], [5, 0x106936c0], [5, 0x106d7700], [5, 0x1071b740], [5, 0x1075f780], [5, 0x107a37c0], [5, 0x107e7800], [5, 0x1082b840], [5, 0x1086f880]]}
  layer.0.intermediate.dense.bias: {input: opt_in1_layer.0.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x10770440]]}
  layer.0.output.dense.weight: {input: opt_in0_layer.0.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10132040], [1, 0x10176080], [1, 0x101ba0c0], [1, 0x101fe100], [1, 0x10242140], [1, 0x10286180], [1, 0x102ca1c0], [1, 0x1030e200], [1, 0x10352240], [1, 0x10396280], [1, 0x103da2c0],
      [1, 0x1041e300], [1, 0x10462340], [1, 0x104a6380], [1, 0x104ea3c0], [1, 0x1052e400], [1, 0x10572440], [1, 0x105b6480], [1, 0x105fa4c0], [1, 0x1063e500], [1, 0x10682540], [1, 0x106c6580], [1, 0x1070a5c0],
      [1, 0x1074e600], [1, 0x10792640], [1, 0x107d6680], [1, 0x1081a6c0], [1, 0x1085e700], [1, 0x108a2740], [1, 0x108e6780], [1, 0x1092a7c0], [1, 0x1096e800]]}
  layer.0.output.dense.bias: {input: opt_in1_layer.0.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x10451440]]}
  layer.0.output.LayerNorm.weight: {input: opt_in1_layer.0.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10000000]]}
  layer.0.output.LayerNorm.bias: {input: opt_in2_layer.0.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x107b4480]]}
  layer.1.attention.self.query.weight: {input: opt_in0_layer.1.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x109b2840], [1, 0x109f6880], [1, 0x10a3a8c0], [1, 0x10a7e900], [1, 0x10ac2940], [1, 0x10b06980], [1, 0x10b4a9c0], [1, 0x10b8ea00]]}
  layer.1.attention.self.query.bias: {input: opt_in1_layer.1.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10462480]]}
  layer.1.attention.self.key.weight: {input: opt_in0_layer.1.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10011040], [4, 0x10055080], [4, 0x100990c0], [4, 0x100dd100], [4, 0x10121140], [4, 0x10165180], [4, 0x101a91c0], [4, 0x101ed200]]}
  layer.1.attention.self.key.bias: {input: opt_in1_layer.1.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x107c54c0]]}
  layer.1.attention.self.value.weight: {input: opt_in0_layer.1.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x104734c0], [3, 0x104b7500], [3, 0x104fb540], [3, 0x1053f580], [3, 0x105835c0], [3, 0x105c7600], [3, 0x1060b640], [3, 0x1064f680]]}
  layer.1.attention.self.value.bias: {input: opt_in1_layer.1.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10022080]]}
  layer.1.attention.output.dense.weight: {input: opt_in0_layer.1.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x107d6500], [0, 0x1081a540], [0, 0x1085e580], [0, 0x108a25c0], [0, 0x108e6600], [0, 0x1092a640], [0, 0x1096e680], [0, 0x109b26c0]]}
  layer.1.attention.output.dense.bias: {input: opt_in1_layer.1.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10231240]]}
  layer.1.attention.output.LayerNorm.weight: {input: opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x108b38c0]]}
  layer.1.attention.output.LayerNorm.bias: {input: opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10bd2a40]]}
  layer.1.intermediate.dense.weight: {input: opt_in0_layer.1.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10be3a80], [1, 0x10c27ac0], [1, 0x10c6bb00], [1, 0x10cafb40], [1, 0x10cf3b80], [1, 0x10d37bc0], [1, 0x10d7bc00], [1, 0x10dbfc40], [1, 0x10e03c80], [1, 0x10e47cc0],
      [1, 0x10e8bd00], [1, 0x10ecfd40], [1, 0x10f13d80], [1, 0x10f57dc0], [1, 0x10f9be00], [1, 0x10fdfe40], [1, 0x11023e80], [1, 0x11067ec0], [1, 0x110abf00], [1, 0x110eff40], [1, 0x11133f80], [1, 0x11177fc0],
      [1, 0x111bc000], [1, 0x11200040], [1, 0x11244080], [1, 0x112880c0], [1, 0x112cc100], [1, 0x11310140], [1, 0x11354180], [1, 0x113981c0], [1, 0x113dc200], [1, 0x11420240]]}
  layer.1.intermediate.dense.bias: {input: opt_in1_layer.1.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x109f6700]]}
  layer.1.output.dense.weight: {input: opt_in0_layer.1.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x108c4900], [5, 0x10908940], [5, 0x1094c980], [5, 0x109909c0], [5, 0x109d4a00], [5, 0x10a18a40], [5, 0x10a5ca80], [5, 0x10aa0ac0], [5, 0x10ae4b00], [5, 0x10b28b40], [5, 0x10b6cb80],
      [5, 0x10bb0bc0], [5, 0x10bf4c00], [5, 0x10c38c40], [5, 0x10c7cc80], [5, 0x10cc0cc0], [5, 0x10d04d00], [5, 0x10d48d40], [5, 0x10d8cd80], [5, 0x10dd0dc0], [5, 0x10e14e00], [5, 0x10e58e40], [5, 0x10e9ce80],
      [5, 0x10ee0ec0], [5, 0x10f24f00], [5, 0x10f68f40], [5, 0x10facf80], [5, 0x10ff0fc0], [5, 0x11035000], [5, 0x11079040], [5, 0x110bd080], [5, 0x111010c0]]}
  layer.1.output.dense.bias: {input: opt_in1_layer.1.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x106936c0]]}
  layer.1.output.LayerNorm.weight: {input: opt_in1_layer.1.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x10a3a740]]}
  layer.1.output.LayerNorm.bias: {input: opt_in2_layer.1.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11464280]]}

  # constant
  lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x114752c0]]}
  lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10242280]]}
  input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10a4b780]]}
  lc.input_tensor.softmax_132.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x100330c0]]}
  lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10242b40]]}
  lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11475b80]]}
  lc.input_tensor.layernorm_152.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x106a4700]]}
  lc.input_tensor.layernorm_152.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x10243400]]}
  dc.input_tensor.layernorm_152.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x11145100], [5, 0x11148440]]}
  lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11476440]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10033980]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10243cc0]]}
  lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1114b780]]}
  lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10034240]]}
  lc.input_tensor.layernorm_166.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x10244580]]}
  lc.input_tensor.layernorm_166.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1114c040]]}
  dc.input_tensor.layernorm_166.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10a4c040], [0, 0x10a4f380]]}
  lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10034b00]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x106a4fc0]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10244e40]]}
  lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x100353c0]]}
  lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1114c900]]}
  input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11476d00]]}
  lc.input_tensor.softmax_185.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10035c80]]}
  lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10245700]]}
  lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x106a5880]]}
  lc.input_tensor.layernorm_205.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1114d1c0]]}
  lc.input_tensor.layernorm_205.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10a526c0]]}
  dc.input_tensor.layernorm_205.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x114775c0], [1, 0x1147a900]]}
  lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x106a6140]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10245fc0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x10a52f80]]}
  lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10036540]]}
  lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10246880]]}
  lc.input_tensor.layernorm_219.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10036e00]]}
  lc.input_tensor.layernorm_219.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1147dc40]]}
  dc.input_tensor.layernorm_219.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1114da80], [5, 0x11150dc0]]}
  lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10247140]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x106a6a00]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1147e500]]}
  lc.input_tensor.bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x100376c0]]}
  lc.input_tensor.bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11154100]]}
  lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1147edc0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10037f80]]}
  lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x106a72c0]]}
  lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10247a00]]}
  dc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x111549c0], [5, 0x111baa00]]}
  lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10a53840]]}
  lc.input_tensor.bw_in1_add_210_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x106a7b80]]}
  lc.input_tensor.bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10a54100]]}
  lc.input_tensor.bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10038840]]}
  lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x102482c0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x11220a40]]}
  lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11221300]]}
  lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10248b80]]}
  dc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1147f680], [1, 0x114e56c0]]}
  lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10039100]]}
  lc.input_tensor.bw_in1_add_191_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11221bc0]]}
  lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x100399c0]]}
  input_1_multiply_183_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x106a8440]]}
  lc.input_tensor.bw_in1_add_177_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11222480]]}
  lc.input_tensor.bw_in1_add_171_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1154b700]]}
  lc.input_tensor.bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10249440]]}
  lc.input_tensor.bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10a549c0]]}
  lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1003a280]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10249d00]]}
  lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1003ab40]]}
  lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x1024a5c0]]}
  dc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x106a8d00], [3, 0x1070ed40]]}
  lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1003b400]]}
  lc.input_tensor.bw_in1_add_157_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11222d40]]}
  lc.input_tensor.bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1154bfc0]]}
  lc.input_tensor.bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1154c880]]}
  lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x10774d80]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x1024ae80]]}
  lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11223600]]}
  lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10a55280]]}
  dc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1154d140], [1, 0x115b3180]]}
  lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1003bcc0]]}
  lc.input_tensor.bw_in1_add_138_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10a55b40]]}
  lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x1024b740]]}
  input_1_multiply_130_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11223ec0]]}
  lc.input_tensor.bw_in1_add_124_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10a56400]]}
  lc.input_tensor.bw_in1_add_118_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x10775640]]}

  # epoch_to_epoch
  e2e_layernorm_152.dc.multiply.9_0: {input: layernorm_152.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x10775f00], [3, 0x10841f40]]}
  e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0: {input: layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8],
    ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1003c580]]}
  e2e_layernorm_166.dc.sqrt.6_0: {input: layernorm_166.dc.sqrt.6, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1024c000], [4, 0x10252640]]}
  e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0: {input: buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11224780], [5, 0x112f07c0]]}
  e2e_layernorm_205.dc.add.5_0: {input: layernorm_205.dc.add.5, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10a56cc0], [0, 0x10a5d300]]}
  e2e_layernorm_205.dc.subtract.1_0: {input: layernorm_205.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x116191c0], [1, 0x116e5200]]}
  e2e_add_218_0: {input: add_218, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1005e5c0], [
        2, 0x1012a600]]}
  e2e_layernorm_219.dc.multiply.8_0: {input: layernorm_219.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x117b1240], [1, 0x1187d280]]}
  e2e_layernorm_219.dc.reciprocal.7_0: {input: layernorm_219.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10a63940], [0, 0x10a69f80]]}
  e2e_gelu_211_0: {input: gelu_211, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x119492c0],
      [1, 0x11a15300], [1, 0x11ae1340], [1, 0x11bad380], [1, 0x11c793c0], [1, 0x11d45400], [1, 0x11e11440], [1, 0x11edd480]]}
  e2e_add_210_0: {input: add_210, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1090df80], [
        3, 0x109d9fc0], [3, 0x10aa6000], [3, 0x10b72040], [3, 0x10c3e080], [3, 0x10d0a0c0], [3, 0x10dd6100], [3, 0x10ea2140]]}
  e2e_bw_in0_matmul_214_matmul_1_0: {input: bw_in0_matmul_214_matmul_1, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x10f6e180], [3, 0x10fd41c0], [3, 0x1103a200], [3, 0x110a0240], [3, 0x11106280], [3, 0x1116c2c0], [3, 0x111d2300], [3, 0x11238340], [3, 0x1129e380], [3, 0x113043c0], [3, 0x1136a400],
      [3, 0x113d0440], [3, 0x11436480], [3, 0x1149c4c0], [3, 0x11502500], [3, 0x11568540]]}
  e2e_layernorm_205.dc.add.10_0: {input: layernorm_205.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x101f6640], [2, 0x102c2680]]}
  e2e_bw_in1_matmul_208_transpose_0_0: {input: bw_in1_matmul_208_transpose_0, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11fa94c0], [1, 0x12075500]]}
  e2e_bw_in0_gelu_211_multiply_1_0: {input: bw_in0_gelu_211_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x113bc800], [5, 0x11488840], [5, 0x11554880], [5, 0x116208c0], [5, 0x116ec900], [5, 0x117b8940], [5, 0x11884980], [5, 0x119509c0]]}
  e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10258c80], [4, 0x10324cc0]]}
  e2e_bw_in0_matmul_208_matmul_1_0: {input: bw_in0_matmul_208_matmul_1, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10a705c0], [0, 0x10a89e00], [0, 0x10aa3640], [0, 0x10abce80], [0, 0x10ad66c0], [0, 0x10aeff00], [0, 0x10b09740], [0, 0x10b22f80], [0, 0x10b3c7c0], [0, 0x10b56000], [0, 0x10b6f840],
      [0, 0x10b89080], [0, 0x10ba28c0], [0, 0x10bbc100], [0, 0x10bd5940], [0, 0x10bef180]]}
  e2e_layernorm_205.dc.multiply.8_0: {input: layernorm_205.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11a1ca00], [5, 0x11ae8a40]]}
  e2e_layernorm_205.dc.reciprocal.7_0: {input: layernorm_205.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x103f0d00], [4, 0x103f7340]]}
  e2e_matmul_196_0: {input: matmul_196, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11bb4a80],
      [5, 0x11c80ac0]]}
  e2e_add_191_0: {input: add_191, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10c089c0], [
        0, 0x10cd4a00]]}
  e2e_softmax_185.dc.multiply.3_0: {input: softmax_185.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10da0a40], [0, 0x11268a80]]}
  e2e_layernorm_166.dc.add.10_0: {input: layernorm_166.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12141540], [1, 0x1220d580]]}
  e2e_bw_in0_matmul_196_matmul_1_0: {input: bw_in0_matmul_196_matmul_1, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x115ce580], [3, 0x11a965c0]]}
  e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0: {input: bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1],
    ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11d4cb00], [5, 0x11db2b40]]}
  e2e_add_177_0: {input: add_177, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1038e6c0], [
        2, 0x1045a700]]}
  e2e_add_171_0: {input: add_171, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11f5e600], [
        3, 0x1202a640]]}
  e2e_bw_in0_matmul_189_matmul_1_0: {input: bw_in0_matmul_189_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x103fd980], [4, 0x104639c0], [4, 0x104c9a00], [4, 0x1052fa40]]}
  e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10526740], [2, 0x105f2780]]}
  e2e_layernorm_166.dc.multiply.8_0: {input: layernorm_166.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10595a80], [4, 0x10661ac0]]}
  e2e_layernorm_166.dc.reciprocal.7_0: {input: layernorm_166.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11e18b80], [5, 0x11e1f1c0]]}
  e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11730ac0], [0, 0x117fcb00]]}
  e2e_gelu_158_0: {input: gelu_158, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x120f6680],
      [3, 0x121c26c0], [3, 0x1228e700], [3, 0x1235a740], [3, 0x12426780], [3, 0x124f27c0], [3, 0x125be800], [3, 0x1268a840]]}
  e2e_add_157_0: {input: add_157, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x106be7c0], [
        2, 0x1078a800], [2, 0x10856840], [2, 0x10922880], [2, 0x109ee8c0], [2, 0x10aba900], [2, 0x10b86940], [2, 0x10c52980]]}
  e2e_bw_in0_gelu_158_multiply_1_0: {input: bw_in0_gelu_158_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x122d95c0], [1, 0x123a5600], [1, 0x12471640], [1, 0x1253d680], [1, 0x126096c0], [1, 0x126d5700], [1, 0x127a1740], [1, 0x1286d780]]}
  e2e_layernorm_152.dc.add.10_0: {input: layernorm_152.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1072db00], [4, 0x107f9b40]]}
  e2e_layernorm_152.dc.multiply.8_0: {input: layernorm_152.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x118c8b40], [0, 0x11994b80]]}
  e2e_layernorm_152.dc.reciprocal.7_0: {input: layernorm_152.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x129397c0], [1, 0x1293fe00]]}
  e2e_matmul_143_0: {input: matmul_143, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10d1e9c0],
      [2, 0x10deaa00]]}
  e2e_bw_in0_matmul_147_matmul_1_0: {input: bw_in0_matmul_147_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10eb6a40], [2, 0x10f1ca80], [2, 0x10f82ac0], [2, 0x10fe8b00]]}
  e2e_add_138_0: {input: add_138, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x12756880], [
        3, 0x128228c0]]}
  e2e_softmax_132.dc.multiply.3_0: {input: softmax_132.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x108c5b80], [4, 0x10d8dbc0]]}
  e2e_add_124_0: {input: add_124, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11e25800], [
        5, 0x11ef1840]]}
  e2e_add_118_0: {input: add_118, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11a60bc0], [
        0, 0x11b2cc00]]}
  e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [
      1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x128ee900], [3, 0x128f7140]]}
  e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0: {input: opt_in0_layer.1.attention.self.query.weight_multiply_1, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [
      2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11255c00], [4, 0x11299c40], [4, 0x112ddc80], [4, 0x11321cc0], [4, 0x11365d00], [4, 0x113a9d40], [4, 0x113edd80], [
        4, 0x11431dc0]]}
  e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [
      1, 16], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11fbd880], [5, 0x11fdf8c0]]}
  e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [
      1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11bf8c40], [0, 0x11c01480]]}

  # optimizer_parameter
  input_opt_layer.0.attention.self.query.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12946440]]}
  input_opt_layer.0.attention.self.query.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12946d00]]}
  input_opt_layer.0.attention.self.key.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1104eb40]]}
  input_opt_layer.0.attention.self.key.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x128ff980]]}
  input_opt_layer.0.attention.self.value.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x11475e00]]}
  input_opt_layer.0.attention.self.value.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x12001900]]}
  input_opt_layer.0.attention.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11c09cc0]]}
  input_opt_layer.0.attention.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x129475c0]]}
  input_opt_layer.0.attention.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1104f400]]}
  input_opt_layer.0.attention.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x12900240]]}
  input_opt_layer.0.intermediate.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1104fcc0]]}
  input_opt_layer.0.intermediate.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x114766c0]]}
  input_opt_layer.0.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x120021c0]]}
  input_opt_layer.0.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x11c0a580]]}
  input_opt_layer.0.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12947e80]]}
  input_opt_layer.0.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11050580]]}
  input_opt_layer.1.attention.self.query.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x12900b00]]}
  input_opt_layer.1.attention.self.query.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x11476f80]]}
  input_opt_layer.1.attention.self.key.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x12002a80]]}
  input_opt_layer.1.attention.self.key.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11c0ae40]]}
  input_opt_layer.1.attention.self.value.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12948740]]}
  input_opt_layer.1.attention.self.value.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x129013c0]]}
  input_opt_layer.1.attention.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x12901c80]]}
  input_opt_layer.1.attention.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x11477840]]}
  input_opt_layer.1.attention.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x12003340]]}
  input_opt_layer.1.attention.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11c0b700]]}
  input_opt_layer.1.intermediate.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12949000]]}
  input_opt_layer.1.intermediate.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11050e40]]}
  input_opt_layer.1.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x12902540]]}
  input_opt_layer.1.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x11478100]]}
  input_opt_layer.1.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x12003c00]]}
  input_opt_layer.1.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11c0bfc0]]}

  # loss
  loss_bert_encoder.output_layernorm_219: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x12902e00]]}

  # grad_accumulator
  grad_acc_layer.1.output.LayerNorm.bias: {input: bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x114789c0]]}
  grad_acc_layer.1.output.LayerNorm.weight: {input: bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11c0c880]]}
  grad_acc_layer.1.output.dense.bias: {input: bw_in1_add_216_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x129498c0]]}
  grad_acc_layer.1.output.dense.weight: {input: bw_in1_matmul_214_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11051700], [2, 0x110d9740], [2, 0x11161780], [2, 0x111e97c0], [2, 0x11271800], [2, 0x112f9840], [2, 0x11381880], [2, 0x114098c0], [2, 0x11491900], [2, 0x11519940], [2, 0x115a1980],
      [2, 0x116299c0], [2, 0x116b1a00], [2, 0x11739a40], [2, 0x117c1a80], [2, 0x11849ac0]]}
  grad_acc_layer.1.intermediate.dense.bias: {input: bw_in1_add_210_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x11489a00]]}
  grad_acc_layer.1.intermediate.dense.weight: {input: bw_in1_matmul_208_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x120044c0], [5, 0x1208c500], [5, 0x12114540], [5, 0x1219c580], [5, 0x122245c0], [5, 0x122ac600], [5, 0x12334640], [5, 0x123bc680], [5, 0x124446c0], [5, 0x124cc700], [5, 0x12554740],
      [5, 0x125dc780], [5, 0x126647c0], [5, 0x126ec800], [5, 0x12774840], [5, 0x127fc880]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias: {input: bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1295a900]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight: {input: bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x12c32e40]]}
  grad_acc_layer.1.attention.output.dense.bias: {input: bw_in1_add_202_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x12c43e80]]}
  grad_acc_layer.1.attention.output.dense.weight: {input: bw_in1_matmul_200_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x114cda40], [4, 0x11555a80], [4, 0x115ddac0], [4, 0x11665b00]]}
  grad_acc_layer.1.attention.self.value.bias: {input: bw_in1_add_191_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x11c1d8c0]]}
  grad_acc_layer.1.attention.self.value.weight: {input: bw_in1_matmul_189_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1296b940], [1, 0x129f3980], [1, 0x12a7b9c0], [1, 0x12b03a00]]}
  grad_acc_layer.1.attention.self.key.bias: {input: bw_in1_add_177_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x128848c0]]}
  grad_acc_layer.1.attention.self.key.weight: {input: bw_in1_matmul_175_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11c2e900], [0, 0x11cb6940], [0, 0x11d3e980], [0, 0x11dc69c0]]}
  grad_acc_layer.1.attention.self.query.bias: {input: bw_in1_add_171_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x118d1b00]]}
  grad_acc_layer.1.attention.self.query.weight: {input: bw_in1_matmul_169_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x12c54ec0], [3, 0x12cdcf00], [3, 0x12d64f40], [3, 0x12decf80]]}
  grad_acc_layer.0.output.LayerNorm.bias: {input: bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x12895900]]}
  grad_acc_layer.0.output.LayerNorm.weight: {input: bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12b8ba40]]}
  grad_acc_layer.0.output.dense.bias: {input: bw_in1_add_163_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x12b9ca80]]}
  grad_acc_layer.0.output.dense.weight: {input: bw_in1_matmul_161_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11e4ea00], [0, 0x11ed6a40], [0, 0x11f5ea80], [0, 0x11fe6ac0], [0, 0x1206eb00], [0, 0x120f6b40], [0, 0x1217eb80], [0, 0x12206bc0], [0, 0x1228ec00], [0, 0x12316c40], [0, 0x1239ec80],
      [0, 0x12426cc0], [0, 0x124aed00], [0, 0x12536d40], [0, 0x125bed80], [0, 0x12646dc0]]}
  grad_acc_layer.0.intermediate.dense.bias: {input: bw_in1_add_157_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x116edb40]]}
  grad_acc_layer.0.intermediate.dense.weight: {input: bw_in1_matmul_155_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x12e74fc0], [3, 0x12efd000], [3, 0x12f85040], [3, 0x1300d080], [3, 0x130950c0], [3, 0x1311d100], [3, 0x131a5140], [3, 0x1322d180], [3, 0x132b51c0], [3, 0x1333d200], [3, 0x133c5240],
      [3, 0x1344d280], [3, 0x134d52c0], [3, 0x1355d300], [3, 0x135e5340], [3, 0x1366d380]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias: {input: bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x126cee00]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight: {input: bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x118e2b40]]}
  grad_acc_layer.0.attention.output.dense.bias: {input: bw_in1_add_149_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x136f53c0]]}
  grad_acc_layer.0.attention.output.dense.weight: {input: bw_in1_matmul_147_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x128a6940], [5, 0x1292e980], [5, 0x129b69c0], [5, 0x12a3ea00]]}
  grad_acc_layer.0.attention.self.value.bias: {input: bw_in1_add_138_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x118f3b80]]}
  grad_acc_layer.0.attention.self.value.weight: {input: bw_in1_matmul_136_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x13706400], [3, 0x1378e440], [3, 0x13816480], [3, 0x1389e4c0]]}
  grad_acc_layer.0.attention.self.key.bias: {input: bw_in1_add_124_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x12badac0]]}
  grad_acc_layer.0.attention.self.key.weight: {input: bw_in1_matmul_122_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11904bc0], [2, 0x1198cc00], [2, 0x11a14c40], [2, 0x11a9cc80]]}
  grad_acc_layer.0.attention.self.query.bias: {input: bw_in1_add_118_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x11731b80]]}
  grad_acc_layer.0.attention.self.query.weight: {input: bw_in1_matmul_116_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x12ac6a40], [5, 0x12b4ea80], [5, 0x12bd6ac0], [5, 0x12c5eb00]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 2
    matmul_116: {type: matmul, grid_loc: [0, 0], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0, layer.0.attention.self.query.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_118: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_116, layer.0.attention.self.query.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_122: {type: matmul, grid_loc: [0, 2], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.key.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0, layer.0.attention.self.key.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_124: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_122, layer.0.attention.self.key.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_128: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_118, add_124], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast], t: 16, mblock: [
        1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0],
      t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    multiply_130: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_128, input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0], t: 16, mblock: [3, 3],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    add_131: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_130, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_132.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_131], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_132.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_132.dc.reduce_sum.1.0], t: 16, mblock: [1, 1], ublock: [
        1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    softmax_132.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [2, 1], inputs: [softmax_132.dc.exp.0, lc.input_tensor.softmax_132.dc.reduce_sum.1.0_splt_brcst_1_0], t: 16, mblock: [3,
        1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}], attributes: {m_k: 1, u_kt: 12}}
    softmax_132.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_132.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_132.dc.multiply.3: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_132.dc.exp.0, softmax_132.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_136: {type: matmul, grid_loc: [0, 4], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.value.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0, layer.0.attention.self.value.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_138: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_136, layer.0.attention.self.value.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_143: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_132.dc.multiply.3, add_138], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_147: {type: matmul, grid_loc: [4, 6], grid_size: [2, 2], inputs: [matmul_143, layer.0.attention.output.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16], attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.0.attention.output.dense.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_149: {type: add, grid_loc: [5, 3], grid_size: [2, 1], inputs: [matmul_147, layer.0.attention.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    add_151: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [add_149, hidden_states], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [add_151, lc.input_tensor.layernorm_152.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_152.dc.subtract.1: {type: subtract, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_151, layernorm_152.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_152.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_152.dc.subtract.1, layernorm_152.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [2, 1], inputs: [layernorm_152.dc.multiply.2, lc.input_tensor.layernorm_152.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_152.dc.add.5: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_152.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_152.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.sqrt.6: {type: sqrt, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_152.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_152.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_152.dc.reciprocal.7, lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_152.dc.subtract.1_buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_152.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_152.dc.subtract.1_buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_152.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_152.dc.subtract.1_layernorm_152.dc.multiply.8, layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0,
        layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_152.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_152.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_1:
    target_device: 0
    input_count: 2
    layernorm_152.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    matmul_155: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_152.dc.add.10, layer.0.intermediate.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.0.intermediate.dense.bias],
      t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_157: {type: add, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_155, layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    gelu_158: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_157], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_161: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_158, layer.0.output.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 16, u_kt: 8}}
    layer.0.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0, layer.0.output.dense.bias], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1,
        u_kt: 1}}
    add_163: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_161, layer.0.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_152.dc.add.10_add_165: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_152.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_152.dc.add.10_add_165: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_152.dc.add.10_add_165], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        72], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_165: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_163, buffer_0_layernorm_152.dc.add.10_add_165], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_166.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [add_165, lc.input_tensor.layernorm_166.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_166.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_165, layernorm_166.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_166.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_166.dc.subtract.1, layernorm_166.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_166.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_166.dc.multiply.2, lc.input_tensor.layernorm_166.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_166.dc.add.5: {type: add, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_166.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_166.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_166.dc.sqrt.6: {type: sqrt, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_166.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_166.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2:
    target_device: 0
    input_count: 2
    layernorm_166.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.sqrt.6_0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_166.dc.reciprocal.7, lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_166.dc.multiply.8: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0, layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_166.dc.multiply.9: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_166.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_166.dc.add.10: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_166.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_169: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [layernorm_166.dc.add.10, layer.1.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0, layer.1.attention.self.query.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_171: {type: add, grid_loc: [1, 3], grid_size: [2, 1], inputs: [matmul_169, layer.1.attention.self.query.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_175: {type: matmul, grid_loc: [2, 4], grid_size: [2, 2], inputs: [layernorm_166.dc.add.10, layer.1.attention.self.key.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0, layer.1.attention.self.key.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_177: {type: add, grid_loc: [1, 7], grid_size: [2, 1], inputs: [matmul_175, layer.1.attention.self.key.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_181: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_171, add_177], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast], t: 16, mblock: [
        1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0],
      t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    multiply_183: {type: multiply, grid_loc: [3, 6], grid_size: [2, 1], inputs: [matmul_181, input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0], t: 16, mblock: [3, 3],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    add_184: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [multiply_183, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_185.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_184], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_185.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_185.dc.reduce_sum.1.0], t: 16, mblock: [1, 1], ublock: [
        1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    softmax_185.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_185.dc.exp.0, lc.input_tensor.softmax_185.dc.reduce_sum.1.0_splt_brcst_1_0], t: 16, mblock: [3,
        1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}], attributes: {m_k: 1, u_kt: 12}}
    softmax_185.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_185.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_185.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_185.dc.exp.0, softmax_185.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_189: {type: matmul, grid_loc: [5, 6], grid_size: [2, 2], inputs: [layernorm_166.dc.add.10, layer.1.attention.self.value.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0, layer.1.attention.self.value.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_191: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_189, layer.1.attention.self.value.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_196: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [softmax_185.dc.multiply.3, add_191], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_200: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_196, layer.1.attention.output.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16], attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.1.attention.output.dense.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_202: {type: add, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_200, layer.1.attention.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_166.dc.add.10_add_204: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [layernorm_166.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_166.dc.add.10_add_204: {type: nop, grid_loc: [7, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_166.dc.add.10_add_204], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_204: {type: add, grid_loc: [7, 7], grid_size: [2, 1], inputs: [add_202, buffer_0_layernorm_166.dc.add.10_add_204], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [add_204, lc.input_tensor.layernorm_205.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_205.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_204, layernorm_205.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_205.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_205.dc.subtract.1, layernorm_205.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_205.dc.multiply.2, lc.input_tensor.layernorm_205.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_205.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_205.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_205.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3:
    target_device: 0
    input_count: 2
    layernorm_205.dc.sqrt.6: {type: sqrt, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.add.5_0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_205.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_205.dc.reciprocal.7, lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_205.dc.multiply.8: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_205.dc.subtract.1_layernorm_205.dc.multiply.8, layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0,
        layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_205.dc.multiply.9: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_205.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_205.dc.add.10: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_205.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_208: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [layernorm_205.dc.add.10, layer.1.intermediate.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 2], inputs: [lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.1.intermediate.dense.bias],
      t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_210: {type: add, grid_loc: [2, 3], grid_size: [2, 4], inputs: [matmul_208, layer.1.intermediate.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    gelu_211: {type: gelu, grid_loc: [6, 0], grid_size: [2, 4], inputs: [add_210], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_214: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_211, layer.1.output.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 16, u_kt: 8}}
    layer.1.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0, layer.1.output.dense.bias], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1,
        u_kt: 1}}
    add_216: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_214, layer.1.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_205.dc.add.10_add_218: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_205.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_205.dc.add.10_add_218: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_205.dc.add.10_add_218], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        72], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_218: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_216, buffer_0_layernorm_205.dc.add.10_add_218], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_4:
    target_device: 0
    input_count: 2
    layernorm_219.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_218_0, lc.input_tensor.layernorm_219.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_219.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_218_0, layernorm_219.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_219.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_219.dc.subtract.1, layernorm_219.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_219.dc.multiply.2, lc.input_tensor.layernorm_219.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_219.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_219.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_219.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_219.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_219.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_219.dc.reciprocal.7, lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_219.dc.subtract.1_buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_219.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_219.dc.subtract.1_buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_219.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_219.dc.subtract.1_layernorm_219.dc.multiply.8, layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_219.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_219.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_219.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_219.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], untilize_output: true, t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}

  bwd_5:
    target_device: 0
    input_count: 2
    bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoder.output_layernorm_219],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_219_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_219.dc.multiply.8_0, loss_bert_encoder.output_layernorm_219], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_219_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_219.dc.reciprocal.7_0, lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.1.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [loss_bert_encoder.output_layernorm_219, layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0, e2e_layernorm_219.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_219.dc.multiply.8_0, bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_219_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 1], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.6, bw_in0_layernorm_219_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_219_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [2, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_219_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_216_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_214_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9, layer.1.output.dense.weight], t: 1, mblock: [3, 4], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_214_transpose_0: {type: nop, grid_loc: [6, 0], grid_size: [2, 4], inputs: [e2e_gelu_211_0], t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_214_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_214_transpose_0, bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12,
        u_kt: 1}}

  bwd_6:
    target_device: 0
    input_count: 2
    bw_in0_gelu_211_gelu_derivative_0: {type: gelu_derivative, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_add_210_0], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    bw_in0_gelu_211_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [2, 4], inputs: [bw_in0_gelu_211_gelu_derivative_0, e2e_bw_in0_matmul_214_matmul_1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_210_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_210_brcst_reduce_sum_0.0, bw_in0_gelu_211_multiply_1], gradient_op: true,
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 12}], attributes: {m_k: 12, u_kt: 1}}
    bw_in0_matmul_208_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in0_gelu_211_multiply_1, layer.1.intermediate.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_208_transpose_0: {type: nop, grid_loc: [2, 5], grid_size: [1, 2], inputs: [e2e_layernorm_205.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}

  bwd_7:
    target_device: 0
    input_count: 2
    bw_in1_matmul_208_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 2], inputs: [e2e_bw_in1_matmul_208_transpose_0_0, e2e_bw_in0_gelu_211_multiply_1_0], gradient_op: true, t: 1, mblock: [2,
        16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_205_combine_add_0: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_208_matmul_1_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_205_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_205_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 4], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.multiply.8_0, bw_in0_layernorm_205_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_205_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.reciprocal.7_0, lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0,
        layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_combine_add_0, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0, e2e_layernorm_205.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [e2e_layernorm_205.dc.multiply.8_0, bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_205_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.6, bw_in0_layernorm_205_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_205_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [2, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_205_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_202_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_200_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 2], inputs: [bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9, layer.1.attention.output.dense.weight], t: 1, mblock: [3,
        4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose],
      attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_200_transpose_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 2], inputs: [e2e_matmul_196_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_200_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_200_transpose_0, bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4, u_kt: 3}}
    bw_in0_matmul_196_matmul_1: {type: matmul, grid_loc: [7, 2], grid_size: [2, 1], inputs: [bw_in0_matmul_200_matmul_1, e2e_add_191_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1,
        u_kt: 2}}
    bw_in1_matmul_196_transpose_0: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [e2e_softmax_185.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_196_matmul_1: {type: matmul, grid_loc: [6, 7], grid_size: [2, 1], inputs: [bw_in1_matmul_196_transpose_0, bw_in0_matmul_200_matmul_1], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_191_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_191_brcst_reduce_sum_0.0, bw_in1_matmul_196_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_189_add_191_unsqueeze3_362_squeeze_0: {type: nop, grid_loc: [7, 3], grid_size: [2, 1], inputs: [bw_in1_matmul_196_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16]}
    bw_in0_matmul_189_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 2], inputs: [bw_in0_matmul_189_add_191_unsqueeze3_362_squeeze_0, layer.1.attention.self.value.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_189_transpose_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 2], inputs: [e2e_layernorm_166.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_189_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_189_transpose_0, bw_in0_matmul_189_add_191_unsqueeze3_362_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in0_softmax_185_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [bw_in0_matmul_196_matmul_1, e2e_softmax_185.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0],
      t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [bw_in0_softmax_185_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
      t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}], attributes: {m_k: 1, u_kt: 12}}

  bwd_8:
    target_device: 0
    input_count: 2
    bw_in0_softmax_185_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_196_matmul_1_0, e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 12}]}
    bw_in0_softmax_185_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [bw_in0_softmax_185_softmax_bw_0.dc.subtract.2, e2e_softmax_185.dc.multiply.3_0], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_1_multiply_183_tile_bcast_tile_bcast], t: 16, mblock: [1, 1], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0], t: 16,
      mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    bw_in0_multiply_183_multiply_0: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_softmax_185_softmax_bw_0.dc.multiply.3, input_1_multiply_183_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_matmul_181_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [2, 1], inputs: [bw_in0_multiply_183_multiply_0, e2e_add_177_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_181_transpose_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_171_0], t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_181_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [bw_in1_matmul_181_transpose_0, bw_in0_multiply_183_multiply_0], t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_177_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_177_brcst_reduce_sum_0.0, bw_in1_matmul_181_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_175_add_177_unsqueeze3_350_squeeze_0: {type: nop, grid_loc: [1, 5], grid_size: [2, 1], inputs: [bw_in1_matmul_181_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_175_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [2, 2], inputs: [bw_in0_matmul_175_add_177_unsqueeze3_350_squeeze_0, layer.1.attention.self.key.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_175_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_166.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_175_matmul_1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_175_transpose_0, bw_in0_matmul_175_add_177_unsqueeze3_350_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in1_add_171_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_171_brcst_reduce_sum_0.0, bw_in0_matmul_181_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_169_matmul_1: {type: matmul, grid_loc: [3, 4], grid_size: [2, 2], inputs: [bw_in0_matmul_181_matmul_1, layer.1.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], input_0_tms: [hstack: 16], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_169_transpose_0: {type: nop, grid_loc: [3, 6], grid_size: [1, 2], inputs: [e2e_layernorm_166.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_169_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_169_transpose_0, bw_in0_matmul_181_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in0_reshape_167_combine_add_0: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_189_matmul_1_0, bw_in0_matmul_175_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_reshape_167_combine_add_1: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_reshape_167_combine_add_0, bw_in0_matmul_169_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_166_combine_add_0: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0, bw_in0_reshape_167_combine_add_1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_166_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_166_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [7, 3], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.multiply.8_0, bw_in0_layernorm_166_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_166_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.reciprocal.7_0, lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.0.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_combine_add_0, layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [5, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0, e2e_layernorm_166.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [e2e_layernorm_166.dc.multiply.8_0, bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_166_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [7, 0], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.6, bw_in0_layernorm_166_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_166_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [7, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [7, 2], grid_size: [2, 1], inputs: [layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_166_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_163_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0, bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}

  bwd_9:
    target_device: 0
    input_count: 2
    bw_in0_matmul_161_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0, layer.0.output.dense.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_161_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [2, 4], inputs: [e2e_gelu_158_0], t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_161_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_161_transpose_0, e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0], gradient_op: true, t: 1,
      mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12,
        u_kt: 1}}
    bw_in0_gelu_158_gelu_derivative_0: {type: gelu_derivative, grid_loc: [6, 0], grid_size: [2, 8], inputs: [e2e_add_157_0], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    bw_in0_gelu_158_multiply_1: {type: multiply, grid_loc: [2, 4], grid_size: [2, 4], inputs: [bw_in0_gelu_158_gelu_derivative_0, bw_in0_matmul_161_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_157_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_157_brcst_reduce_sum_0.0, bw_in0_gelu_158_multiply_1], gradient_op: true,
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 12}], attributes: {m_k: 12, u_kt: 1}}

  bwd_10:
    target_device: 0
    input_count: 2
    bw_in0_matmul_155_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_gelu_158_multiply_1_0, layer.0.intermediate.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_155_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_152.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_155_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [8, 2], inputs: [bw_in1_matmul_155_transpose_0, e2e_bw_in0_gelu_158_multiply_1_0], gradient_op: true, t: 1, mblock: [2, 16], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_152_combine_add_0: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0, bw_in0_matmul_155_matmul_1], t: 1, mblock: [3,
        8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_152_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_152_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.multiply.8_0, bw_in0_layernorm_152_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_152_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.reciprocal.7_0, lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0,
        layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_combine_add_0, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0, e2e_layernorm_152.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_152.dc.multiply.8_0, bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_152_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.6, bw_in0_layernorm_152_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_152_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_152_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_149_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_147_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9, layer.0.attention.output.dense.weight], t: 1, mblock: [3,
        4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose],
      attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_147_transpose_0: {type: nop, grid_loc: [7, 4], grid_size: [1, 2], inputs: [e2e_matmul_143_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_147_matmul_1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_147_transpose_0, bw_in0_layernorm_152_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4, u_kt: 3}}

  bwd_11:
    target_device: 0
    input_count: 2
    bw_in0_matmul_143_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_147_matmul_1_0, e2e_add_138_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1,
        u_kt: 2}}
    bw_in1_matmul_143_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_softmax_132.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_143_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [bw_in1_matmul_143_transpose_0, e2e_bw_in0_matmul_147_matmul_1_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_138_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_138_brcst_reduce_sum_0.0, bw_in1_matmul_143_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_136_transpose_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 2], inputs: [hidden_states], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_136_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_136_transpose_0, bw_in1_matmul_143_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in0_softmax_132_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_matmul_143_matmul_1, e2e_softmax_132.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0],
      t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [bw_in0_softmax_132_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
      t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}], attributes: {m_k: 1, u_kt: 12}}
    bw_in0_softmax_132_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_matmul_143_matmul_1, bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.lc1], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_1_tms: [broadcast: {c: 12}]}
    bw_in0_softmax_132_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [1, 3], grid_size: [2, 1], inputs: [bw_in0_softmax_132_softmax_bw_0.dc.subtract.2, e2e_softmax_132.dc.multiply.3_0], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_1_multiply_130_tile_bcast_tile_bcast], t: 16, mblock: [1, 1], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0], t: 16,
      mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    bw_in0_multiply_130_multiply_0: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [bw_in0_softmax_132_softmax_bw_0.dc.multiply.3, input_1_multiply_130_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [bw_in0_multiply_130_multiply_0, e2e_add_124_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_add_118_0], t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 2], inputs: [bw_in1_matmul_128_transpose_0, bw_in0_multiply_130_multiply_0], t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_124_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_124_brcst_reduce_sum_0.0, bw_in1_matmul_128_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_122_transpose_0: {type: nop, grid_loc: [3, 5], grid_size: [1, 2], inputs: [hidden_states], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_122_matmul_1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_122_transpose_0, bw_in1_matmul_128_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose, hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in1_add_118_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_118_brcst_reduce_sum_0.0, bw_in0_matmul_128_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_116_transpose_0: {type: nop, grid_loc: [5, 0], grid_size: [1, 2], inputs: [hidden_states], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_116_matmul_1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_116_transpose_0, bw_in0_matmul_128_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}

  opt_12:
    target_device: 0
    input_count: 1
    input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.query.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.query.weight, input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.query.weight, opt_in1_layer.0.attention.self.query.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.query.bias, input_opt_layer.0.attention.self.query.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layer.0.attention.self.query.bias, opt_in1_layer.0.attention.self.query.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.key.weight_0.lr], t: 1, mblock: [1, 8], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [3, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.key.weight, input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.key.weight, opt_in1_layer.0.attention.self.key.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.key.bias, input_opt_layer.0.attention.self.key.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layer.0.attention.self.key.bias, opt_in1_layer.0.attention.self.key.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.value.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [5, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.value.weight, input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [6, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.value.weight, opt_in1_layer.0.attention.self.value.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.value.bias, input_opt_layer.0.attention.self.value.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layer.0.attention.self.value.bias, opt_in1_layer.0.attention.self.value.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.output.dense.weight_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.0.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.output.dense.weight, input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.0.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [9, 0], grid_size: [1, 8], inputs: [layer.0.attention.output.dense.weight, opt_in0_layer.0.attention.output.dense.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.dense.bias, input_opt_layer.0.attention.output.dense.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layer.0.attention.output.dense.bias, opt_in1_layer.0.attention.output.dense.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.weight, input_opt_layer.0.attention.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.weight, opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.bias, input_opt_layer.0.attention.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.bias, opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  opt_13:
    target_device: 0
    input_count: 1
    input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 2], inputs: [input_opt_layer.0.intermediate.dense.weight_0.lr], t: 1, mblock: [1, 16], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 128}]}
    opt_in0_layer.0.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [4, 8], inputs: [grad_acc_layer.0.intermediate.dense.weight, input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.0.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [4, 8], inputs: [layer.0.intermediate.dense.weight, opt_in0_layer.0.intermediate.dense.weight_multiply_1],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.intermediate.dense.bias, input_opt_layer.0.intermediate.dense.bias_0.lr],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 128}]}
    opt_in1_layer.0.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layer.0.intermediate.dense.bias, opt_in1_layer.0.intermediate.dense.bias_multiply_1],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 2], inputs: [input_opt_layer.0.output.dense.weight_0.lr], t: 1, mblock: [1, 4], ublock: [1, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}

  opt_14:
    target_device: 0
    input_count: 1
    opt_in0_layer.0.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.0.output.dense.weight, e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 128}]}
    opt_in0_layer.0.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.0.output.dense.weight, opt_in0_layer.0.output.dense.weight_multiply_1], t: 1, mblock: [
        16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.dense.bias, input_opt_layer.0.output.dense.bias_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    opt_in1_layer.0.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.0.output.dense.bias, opt_in1_layer.0.output.dense.bias_multiply_1], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.weight, input_opt_layer.0.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.weight, opt_in1_layer.0.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.bias, input_opt_layer.0.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.0.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.bias, opt_in2_layer.0.output.LayerNorm.bias_multiply_1], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.query.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [9, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.query.weight, input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}

  opt_15:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.query.weight, e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.query.bias, input_opt_layer.1.attention.self.query.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layer.1.attention.self.query.bias, opt_in1_layer.1.attention.self.query.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.key.weight_0.lr], t: 1, mblock: [1, 8], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.key.weight, input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.key.weight, opt_in0_layer.1.attention.self.key.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.key.bias, input_opt_layer.1.attention.self.key.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layer.1.attention.self.key.bias, opt_in1_layer.1.attention.self.key.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.value.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [4, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.value.weight, input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.value.weight, opt_in0_layer.1.attention.self.value.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.value.bias, input_opt_layer.1.attention.self.value.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layer.1.attention.self.value.bias, opt_in1_layer.1.attention.self.value.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.output.dense.weight_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [7, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.output.dense.weight, input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [8, 0], grid_size: [1, 8], inputs: [layer.1.attention.output.dense.weight, opt_in0_layer.1.attention.output.dense.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.dense.bias, input_opt_layer.1.attention.output.dense.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layer.1.attention.output.dense.bias, opt_in1_layer.1.attention.output.dense.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.weight, input_opt_layer.1.attention.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.weight, opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.bias, input_opt_layer.1.attention.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.bias, opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [9, 0], grid_size: [1, 2], inputs: [input_opt_layer.1.intermediate.dense.weight_0.lr], t: 1, mblock: [1, 16], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 128}]}

  opt_16:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.1.intermediate.dense.weight, e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.1.intermediate.dense.weight, opt_in0_layer.1.intermediate.dense.weight_multiply_1],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.intermediate.dense.bias, input_opt_layer.1.intermediate.dense.bias_0.lr],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 128}]}
    opt_in1_layer.1.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.1.intermediate.dense.bias, opt_in1_layer.1.intermediate.dense.bias_multiply_1],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 2], inputs: [input_opt_layer.1.output.dense.weight_0.lr], t: 1, mblock: [1, 4], ublock: [1, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}

  opt_17:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.1.output.dense.weight, e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 128}]}
    opt_in0_layer.1.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.1.output.dense.weight, opt_in0_layer.1.output.dense.weight_multiply_1], t: 1, mblock: [
        16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.dense.bias, input_opt_layer.1.output.dense.bias_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    opt_in1_layer.1.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.1.output.dense.bias, opt_in1_layer.1.output.dense.bias_multiply_1], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.weight, input_opt_layer.1.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.weight, opt_in1_layer.1.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.bias, input_opt_layer.1.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.1.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.bias, opt_in2_layer.1.output.LayerNorm.bias_multiply_1], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
- run_fwd:
  - param: [$p_loop_count]
  - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q4: 0, $lptr_q3: 0, $lptr_q4: 0, $gptr_q3: 0, $gptr_q5: 0, $lptr_q5: 0}
  - staticvar: {$gptr_q0_shadow: 0, $lptr_q0: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0}
  - varinst: [$gptr_q0, set, $gptr_q0_shadow]
  - loop: $p_loop_count
  - allocate_queue: [e2e_layernorm_152.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0]
  - execute: {graph_name: fwd_0, queue_settings: {hidden_states: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, attention_mask: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_130_fork_clone823_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_132.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_152.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_152.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_152.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
  - allocate_queue: [e2e_layernorm_166.dc.sqrt.6_0, e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0]
  - execute: {graph_name: fwd_1, queue_settings: {e2e_layernorm_152.dc.multiply.9_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_166.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_166.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_166.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_152.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0]
  - varinst: [$gptr_q1, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_205.dc.subtract.1_0, e2e_layernorm_205.dc.add.5_0]
  - execute: {graph_name: fwd_2, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_layernorm_166.dc.sqrt.6_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_183_fork_clone846_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_185.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_205.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_205.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_205.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_166.dc.sqrt.6_0, e2e_buffer_0_layernorm_166.dc.subtract.1_layernorm_166.dc.multiply.8_0]
  - varinst: [$gptr_q2, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_218_0]
  - execute: {graph_name: fwd_3, queue_settings: {e2e_layernorm_205.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_layernorm_205.dc.add.5_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_205.dc.subtract.1_0, e2e_layernorm_205.dc.add.5_0]
  - varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: fwd_4, queue_settings: {e2e_add_218_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, lc.input_tensor.layernorm_219.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_219.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_219.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_218_0]
  - varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
  - endloop

- run_bwd:
  - param: [$p_zero_grad, $p_loop_count]
  - var: {$v_zero_grad: 0, $c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $lptr_q15: 0, $gptr_q10: 0, $gptr_q15: 0, $gptr_q10_shadow: 0, $lptr_q3: 0, $lptr_q8: 0, $gptr_q12: 0, $lptr_q10: 0, $gptr_q3: 0,
      $gptr_q8: 0, $lptr_q6: 0, $lptr_q12: 0, $gptr_q6: 0}
  - staticvar: {$gptr_q0: 0, $gptr_q1: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q4_shadow: 0, $lptr_q14: 0, $lptr_q4: 0, $gptr_q2: 0, $gptr_q14: 0, $gptr_q13: 0, $gptr_q9: 0, $lptr_q2: 0, $lptr_q13: 0, $lptr_q5: 0,
      $lptr_q11: 0, $lptr_q9: 0, $lptr_q7: 0, $gptr_q11: 0, $gptr_q4: 0, $gptr_q7: 0, $gptr_q5: 0}
  - varinst: [$gptr_q10, set, $gptr_q10_shadow]
  - varinst: [$gptr_q4, set, $gptr_q4_shadow]
  - varinst: [$v_zero_grad, set, $p_zero_grad]
  - loop: $p_loop_count
  - allocate_queue: [e2e_bw_in0_matmul_214_matmul_1_0, e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0]
  - execute: {graph_name: bwd_5, queue_settings: {loss_bert_encoder.output_layernorm_219: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, e2e_gelu_211_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, e2e_layernorm_219.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1,
          rd_ptr_global: $gptr_q1}, e2e_layernorm_219.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, layer.1.output.dense.weight: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.bw_in2_layernorm_219_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_219.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.dc.reduce_sum.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_219_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false,
          rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_216_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.LayerNorm.weight: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q1, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_gelu_211_multiply_1_0, e2e_bw_in0_matmul_208_matmul_1_0, e2e_bw_in1_matmul_208_transpose_0_0]
  - execute: {graph_name: bwd_6, queue_settings: {e2e_layernorm_205.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_add_210_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_bw_in0_matmul_214_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
        layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_210_brcst_reduce_sum_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_matmul_214_matmul_1_0]
  - varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_196_matmul_1_0, e2e_bw_in0_matmul_189_matmul_1_0, e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0]
  - execute: {graph_name: bwd_7, queue_settings: {e2e_layernorm_166.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_softmax_185.dc.multiply.3_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_add_191_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
        e2e_matmul_196_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_layernorm_205.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_layernorm_205.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_bw_in0_gelu_211_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6,
          rd_ptr_global: $gptr_q6}, e2e_bw_in0_matmul_208_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_bw_in1_matmul_208_transpose_0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_205_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_205.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_205_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.bw_in1_add_202_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_191_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.value.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_219_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_211_multiply_1_0, e2e_bw_in0_matmul_208_matmul_1_0, e2e_bw_in1_matmul_208_transpose_0_0]
  - varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q6, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q6, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0]
  - execute: {graph_name: bwd_8, queue_settings: {e2e_layernorm_166.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_layernorm_166.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_layernorm_166.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7,
          rd_ptr_global: $gptr_q7}, e2e_add_171_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_add_177_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_softmax_185.dc.multiply.3_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_bw_in0_matmul_196_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8,
          rd_ptr_global: $gptr_q8}, e2e_bw_in0_matmul_189_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_183_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_177_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_171_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_166_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_166.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_166_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.bw_in1_add_163_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.bias: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.0.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_205_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_196_matmul_1_0, e2e_bw_in0_matmul_189_matmul_1_0, e2e_bw_in0_softmax_185_softmax_bw_0.dc.reduce_sum.1.lc1_0]
  - varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_gelu_158_multiply_1_0]
  - execute: {graph_name: bwd_9, queue_settings: {e2e_add_157_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, e2e_gelu_158_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q10,
          rd_ptr_global: $gptr_q10}, layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_157_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q10, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_matmul_147_matmul_1_0]
  - execute: {graph_name: bwd_10, queue_settings: {e2e_matmul_143_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_layernorm_152.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_layernorm_152.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11,
          rd_ptr_global: $gptr_q11}, e2e_layernorm_152.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}, e2e_bw_in0_gelu_158_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12,
          rd_ptr_global: $gptr_q12}, layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_152_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_152.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.dc.reduce_sum.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_152_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false,
          rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_149_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_166_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_158_multiply_1_0]
  - varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: bwd_11, queue_settings: {hidden_states: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_add_118_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_add_124_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_softmax_132.dc.multiply.3_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_add_138_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
        e2e_bw_in0_matmul_147_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15}, lc.input_tensor.bw_in1_add_138_brcst_reduce_sum_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_softmax_132_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, input_1_multiply_130_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_124_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_118_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.bias: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_matmul_147_matmul_1_0]
  - varinst: [$gptr_q13, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q14, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q15, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q13, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q15, incwrap, $c_microbatch_size, 4]
  - varinst: [$v_zero_grad, set, 0]
  - endloop

- run_opt:
  - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $lptr_q0: 0, $lptr_q3: 0}
  - execute: {graph_name: opt_12, queue_settings: {layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.query.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.value.bias: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - allocate_queue: [e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_13, queue_settings: {layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - allocate_queue: [e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0]
  - execute: {graph_name: opt_14, queue_settings: {e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
        layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q0, incwrap, $c_one, 2]
  - varinst: [$lptr_q0, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_15, queue_settings: {e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
        layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.value.weight: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.dense.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0]
  - varinst: [$gptr_q1, incwrap, $c_one, 2]
  - varinst: [$lptr_q1, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_16, queue_settings: {e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
        layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.intermediate.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q2, incwrap, $c_one, 2]
  - varinst: [$lptr_q2, incwrap, $c_one, 2]
  - execute: {graph_name: opt_17, queue_settings: {e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
        layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q3, incwrap, $c_one, 2]
  - varinst: [$lptr_q3, incwrap, $c_one, 2]

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 2.0
  io-config:
    inputs: [hidden_states, attention_mask, loss_bert_encoder.output_layernorm_219]
    outputs: [bert_encoder.output_layernorm_219]
