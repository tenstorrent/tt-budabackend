devices:
  arch: blackhole

queues:

  # input
  input_0_add_mha_0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10000000]]}
  input_0_mask_copy_0: {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10000000]]}

  # output
  encoder3.output_norm_ff_3_bias: {input: norm_ff_3_bias, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: host, host: [0x0]}
  encoder3.output_mask_copy_3: {input: mask_copy_3, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: host, host: [0x8840]}

  # parameter
  ff.bert.encoder.layer.3.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10004440]]}
  ff.bert.encoder.layer.3.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10000000]]}
  ff.bert.encoder.layer.3.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10000000]]}
  ff.bert.encoder.layer.3.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10006680]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10002240]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10002240]]}
  ff.bert.encoder.layer.3.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x1000eec0]]}
  ff.bert.encoder.layer.3.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10004480]]}
  ff.bert.encoder.layer.2.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10004480]]}
  ff.bert.encoder.layer.2.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10011100]]}
  ff.bert.encoder.layer.2.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x100066c0]]}
  ff.bert.encoder.layer.2.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x100066c0]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10013340]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10008900]]}
  ff.bert.encoder.layer.2.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1000ef00]]}
  ff.bert.encoder.layer.2.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10015580]]}
  ff.bert.encoder.layer.1.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1000ab40]]}
  ff.bert.encoder.layer.1.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10011140]]}
  ff.bert.encoder.layer.1.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x100177c0]]}
  ff.bert.encoder.layer.1.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1000cd80]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10013380]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10019a00]]}
  ff.bert.encoder.layer.1.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x100155c0]]}
  ff.bert.encoder.layer.1.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x100155c0]]}
  ff.bert.encoder.layer.0.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1001bc40]]}
  ff.bert.encoder.layer.0.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10017800]]}
  ff.bert.encoder.layer.0.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10017800]]}
  ff.bert.encoder.layer.0.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1001de80]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10019a40]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10019a40]]}
  ff.bert.encoder.layer.0.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x100266c0]]}
  ff.bert.encoder.layer.0.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x1001bc80]]}
  ff.bert.encoder.layer.0.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1001bc80]]}
  ff.reciprocal_of_sqrt_of_head_size_0: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1001dec0]]}
  ff.bert.encoder.layer.0.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10000000]]}
  ff.bert.encoder.layer.0.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10028900]]}
  ff.bert.encoder.layer.0.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x1001e780]]}
  ff.bert.encoder.layer.0.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x10008840]]}
  ff.bert.encoder.layer.0.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10002240]]}
  ff.bert.encoder.layer.0.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x100244c0]]}
  ff.bert.encoder.layer.0.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1000aa80]]}
  ff.bert.encoder.layer.1.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x10046500]]}
  ff.reciprocal_of_sqrt_of_head_size_1: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10000000]]}
  ff.bert.encoder.layer.1.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1004ed40]]}
  ff.bert.encoder.layer.1.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x1002cac0]]}
  ff.bert.encoder.layer.1.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x100008c0]]}
  ff.bert.encoder.layer.1.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x100209c0]]}
  ff.bert.encoder.layer.1.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10050f80]]}
  ff.bert.encoder.layer.1.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10011080]]}
  ff.bert.encoder.layer.1.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x100597c0]]}
  ff.bert.encoder.layer.2.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x100330c0]]}
  ff.reciprocal_of_sqrt_of_head_size_2: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10031140]]}
  ff.bert.encoder.layer.2.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1003b900]]}
  ff.bert.encoder.layer.2.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1007b800]]}
  ff.bert.encoder.layer.2.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10031a00]]}
  ff.bert.encoder.layer.2.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x10002b00]]}
  ff.bert.encoder.layer.2.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x1003db40]]}
  ff.bert.encoder.layer.2.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10029200]]}
  ff.bert.encoder.layer.2.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10046380]]}
  ff.bert.encoder.layer.3.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1004b240]]}
  ff.reciprocal_of_sqrt_of_head_size_3: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10035300]]}
  ff.bert.encoder.layer.3.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10053a80]]}
  ff.bert.encoder.layer.3.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x100683c0]]}
  ff.bert.encoder.layer.3.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10035bc0]]}
  ff.bert.encoder.layer.3.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x10033c40]]}
  ff.bert.encoder.layer.3.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10055cc0]]}
  ff.bert.encoder.layer.3.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 8], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x1000b340]]}
  ff.bert.encoder.layer.3.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [8, 2], ublock: [2, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1005e500]]}

  # constant
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10037e00]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1002d380]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10070c00]]}
  lc.input_tensor.ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x100386c0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x1002dc40]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x100714c0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10038f80]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1002e500]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10071d80]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10039840]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1002edc0]]}
  lc.input_tensor.ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10072640]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x1003a100]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x1002f680]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10072f00]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1003a9c0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1002ff40]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x100737c0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1003b280]]}
  lc.input_tensor.ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10030800]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10074080]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x1003bb40]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x100310c0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10074940]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1003c400]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10031980]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10075200]]}
  lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1003ccc0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10032240]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10075ac0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1003d580]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10032b00]]}
  lc.input_tensor.input_0_mask_copy_0_s_brcst_m2_1_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x1003de40]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x100333c0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10076380]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x10084040]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10033c80]]}
  lc.input_tensor.mha_0_as_softmax_sum.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 4, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10084900]]}
  lc.input_tensor.norm_mha_0_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1003c480]]}
  lc.input_tensor.norm_mha_0_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10034540]]}
  constant_1_norm_mha_0_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10080540]]}
  lc.input_tensor.norm_mha_0_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x10076c40]]}
  lc.input_tensor.norm_ff_0_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1003cd40]]}
  lc.input_tensor.norm_ff_0_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10034e00]]}
  constant_1_norm_ff_0_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10080e00]]}
  lc.input_tensor.norm_ff_0_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10077500]]}
  lc.input_tensor.mask_copy_0_s_brcst_m2_1_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1003e700]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1003d600]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x100816c0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10077dc0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1003dec0]]}
  lc.input_tensor.mha_1_as_softmax_sum.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 4, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10078680]]}
  lc.input_tensor.norm_mha_1_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1003efc0]]}
  lc.input_tensor.norm_mha_1_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1003e780]]}
  constant_1_norm_mha_1_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x100356c0]]}
  lc.input_tensor.norm_mha_1_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x10081f80]]}
  lc.input_tensor.norm_ff_1_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1003f880]]}
  lc.input_tensor.norm_ff_1_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1003f040]]}
  constant_1_norm_ff_1_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10035f80]]}
  lc.input_tensor.norm_ff_1_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10082840]]}
  lc.input_tensor.mask_copy_1_s_brcst_m2_1_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10086b40]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10040140]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10036840]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x10083100]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10040a00]]}
  lc.input_tensor.mha_2_as_softmax_sum.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 4, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x100839c0]]}
  lc.input_tensor.norm_mha_2_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10087400]]}
  lc.input_tensor.norm_mha_2_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x100412c0]]}
  constant_1_norm_mha_2_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1003f900]]}
  lc.input_tensor.norm_mha_2_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x10037100]]}
  lc.input_tensor.norm_ff_2_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10087cc0]]}
  lc.input_tensor.norm_ff_2_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10041b80]]}
  constant_1_norm_ff_2_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x100401c0]]}
  lc.input_tensor.norm_ff_2_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x100379c0]]}
  lc.input_tensor.mask_copy_2_s_brcst_m2_1_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1007a8c0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x10088580]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10040a80]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10038280]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x10088e40]]}
  lc.input_tensor.mha_3_as_softmax_sum.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 4, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10038b40]]}
  lc.input_tensor.norm_mha_3_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1007b180]]}
  lc.input_tensor.norm_mha_3_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10089700]]}
  constant_1_norm_mha_3_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10042440]]}
  lc.input_tensor.norm_mha_3_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x10041340]]}
  lc.input_tensor.norm_ff_3_mean.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1007ba40]]}
  lc.input_tensor.norm_ff_3_var.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10089fc0]]}
  constant_1_norm_ff_3_var_plus_eps: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10042d00]]}
  lc.input_tensor.norm_ff_3_recip_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10041c00]]}

  # epoch_to_epoch
  e2e_norm_ff_0_output_0: {input: norm_ff_0_output, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 2], ublock: [1, 2], df: Float16_b, target_device: 0, ublock_order: c, loc: dram, dram: [
      [4, 0x1007c300]]}
  e2e_ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1,
    mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10085c00]]}
  e2e_ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [
      1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1003ad80]]}
  e2e_ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1,
    mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x100424c0]]}
  e2e_ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1],
    t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x100435c0]]}
  e2e_ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [
      1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1008a880]]}
  e2e_ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [
      1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10084b40]]}
  e2e_ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1,
    mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10087e40]]}
  e2e_ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1,
      2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1003cfc0]]}
  e2e_ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1,
    mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10044700]]}
  e2e_ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [
      1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10045800]]}
  e2e_ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1,
    mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1008cac0]]}
  e2e_ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1],
    t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10086d80]]}
  e2e_norm_mha_2_sqrt_0: {input: norm_mha_2_sqrt, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], df: Float16_b, target_device: 0, ublock_order: c, loc: dram, dram: [[
        0, 0x10047a40]]}
  e2e_norm_mha_2_sub_0: {input: norm_mha_2_sub, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 4], ublock: [2, 1], df: Float16_b, target_device: 0, ublock_order: c, loc: dram, dram: [[5,
        0x1008ed00]]}
  e2e_ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [
      1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10090680]]}
  e2e_ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [
      1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1003f200]]}
  e2e_ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1,
    mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10046940]]}
  e2e_ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1,
      2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10049c80]]}
  e2e_ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1,
    mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10097540]]}
  e2e_ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [
      1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10088fc0]]}
  e2e_ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1,
    mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10094ac0]]}
  e2e_ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1],
    t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10043640]]}
  e2e_ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [
      1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10057980]]}
  e2e_ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [
      1, 1], t: 1, mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1004e0c0]]}
  e2e_ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1,
    mblock: [1, 8], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1009b980]]}
  e2e_ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1,
      2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1008d400]]}
  e2e_ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1,
    mblock: [1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10098f00]]}
  e2e_ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [
      1, 2], ublock: [1, 2], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10047a80]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 1
    ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.attention.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.attention.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.attention.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.attention.self.value.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.attention.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.attention.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.attention.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.attention.self.value.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.attention.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.attention.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.attention.self.value.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.attention.output.LayerNorm.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.attention.output.dense.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.attention.self.value.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_0_value: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [input_0_add_mha_0, ff.bert.encoder.layer.0.attention.self.value.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_0_value.bias: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [mha_0_value, ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 4], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    input_0_mask_copy_0_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.input_0_mask_copy_0_s_brcst_m2_1_1.0, input_0_mask_copy_0], t: 4, mblock: [1, 2],
      ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 4}],
      input_0_tms: [broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_0],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.attention.self.key.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_0_key: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [input_0_add_mha_0, ff.bert.encoder.layer.0.attention.self.key.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_0_key.bias: {type: add, grid_loc: [4, 7], grid_size: [1, 1], inputs: [mha_0_key, ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.0.attention.self.query.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_0_query: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [input_0_add_mha_0, ff.bert.encoder.layer.0.attention.self.query.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_0_query.bias: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [mha_0_query, ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_0_as: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [mha_0_query.bias, mha_0_key.bias], t: 4, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4, transpose], input_0_tms: [hslice: 4], attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_div: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [mha_0_as, ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1], t: 4, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}]}
    mha_0_as_mask: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [mha_0_as_div, input_0_mask_copy_0_s_brcst_m2_1_1.lc1], t: 4, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_0_as_softmax_exp: {type: exp, grid_loc: [5, 6], grid_size: [1, 1], inputs: [mha_0_as_mask], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax_sum.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [mha_0_as_softmax_exp, lc.input_tensor.mha_0_as_softmax_sum.0], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    mha_0_as_softmax_recip: {type: reciprocal, grid_loc: [6, 0], grid_size: [1, 1], inputs: [mha_0_as_softmax_sum.lc1], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax_mult: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [mha_0_as_softmax_exp, mha_0_as_softmax_recip], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    mha_0_ac: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [mha_0_as_softmax_mult, mha_0_value.bias], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_0_output: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [mha_0_ac, ff.bert.encoder.layer.0.attention.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_0_output.bias: {type: add, grid_loc: [6, 4], grid_size: [1, 1], inputs: [mha_0_output, ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 2], ublock: [2,
        2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_mha_0: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [input_0_add_mha_0, mha_0_output.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0_mean.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [add_mha_0, lc.input_tensor.norm_mha_0_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_mha_0_sub: {type: subtract, grid_loc: [6, 7], grid_size: [1, 1], inputs: [add_mha_0, norm_mha_0_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_0_sq: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [norm_mha_0_sub, norm_mha_0_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0_var.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [norm_mha_0_sq, lc.input_tensor.norm_mha_0_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_mha_0_var_plus_eps: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [norm_mha_0_var.lc1, constant_1_norm_mha_0_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_mha_0_sqrt: {type: sqrt, grid_loc: [7, 3], grid_size: [1, 1], inputs: [norm_mha_0_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0_recip: {type: reciprocal, grid_loc: [7, 4], grid_size: [1, 1], inputs: [norm_mha_0_sqrt], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [norm_mha_0_recip, lc.input_tensor.norm_mha_0_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0_output: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [norm_mha_0_sub, norm_mha_0_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_0_weights: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [norm_mha_0_output, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [4,
        2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    norm_mha_0_bias: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [norm_mha_0_weights, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 2], ublock: [
        2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff_0_ff1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [norm_mha_0_bias, ff.bert.encoder.layer.0.intermediate.dense.weight], t: 1, mblock: [4, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    ff_0_ff1.bias: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [ff_0_ff1, ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff0_gelu: {type: gelu, grid_loc: [8, 3], grid_size: [1, 1], inputs: [ff_0_ff1.bias], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3}
    ff_0_ff2: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [ff0_gelu, ff.bert.encoder.layer.0.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 8, u_kt: 2}}
    ff_0_ff2.bias: {type: add, grid_loc: [8, 5], grid_size: [1, 1], inputs: [ff_0_ff2, ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_ff_0: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [norm_mha_0_bias, ff_0_ff2.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0_mean.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [add_ff_0, lc.input_tensor.norm_ff_0_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_ff_0_sub: {type: subtract, grid_loc: [9, 0], grid_size: [1, 1], inputs: [add_ff_0, norm_ff_0_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_ff_0_sq: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [norm_ff_0_sub, norm_ff_0_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0_var.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [norm_ff_0_sq, lc.input_tensor.norm_ff_0_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_ff_0_var_plus_eps: {type: add, grid_loc: [9, 3], grid_size: [1, 1], inputs: [norm_ff_0_var.lc1, constant_1_norm_ff_0_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_ff_0_sqrt: {type: sqrt, grid_loc: [9, 4], grid_size: [1, 1], inputs: [norm_ff_0_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0_recip: {type: reciprocal, grid_loc: [9, 5], grid_size: [1, 1], inputs: [norm_ff_0_sqrt], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [norm_ff_0_recip, lc.input_tensor.norm_ff_0_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_ff_0_output: {type: multiply, grid_loc: [9, 7], grid_size: [1, 1], inputs: [norm_ff_0_sub, norm_ff_0_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}

  fwd_1:
    target_device: 0
    input_count: 1
    norm_ff_0_weights: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_norm_ff_0_output_0, e2e_ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [4,
        2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    norm_ff_0_bias: {type: add, grid_loc: [0, 1], grid_size: [1, 1], inputs: [norm_ff_0_weights, e2e_ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [
        2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_1_value: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [norm_ff_0_bias, ff.bert.encoder.layer.1.attention.self.value.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_1_value.bias: {type: add, grid_loc: [0, 3], grid_size: [1, 1], inputs: [mha_1_value, e2e_ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 4], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mask_copy_0_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.mask_copy_0_s_brcst_m2_1_1.0, input_0_mask_copy_0], t: 4, mblock: [1, 2], ublock: [1, 2],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 4}], input_0_tms: [
        broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_1],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.attention.self.key.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_1_key: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [norm_ff_0_bias, ff.bert.encoder.layer.1.attention.self.key.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_1_key.bias: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [mha_1_key, ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.1.attention.self.query.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_1_query: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [norm_ff_0_bias, ff.bert.encoder.layer.1.attention.self.query.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_1_query.bias: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [mha_1_query, ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_1_as: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [mha_1_query.bias, mha_1_key.bias], t: 4, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4, transpose], input_0_tms: [hslice: 4], attributes: {m_k: 1, u_kt: 1}}
    mha_1_as_div: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [mha_1_as, ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1], t: 4, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}]}
    mha_1_as_mask: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [mha_1_as_div, mask_copy_0_s_brcst_m2_1_1.lc1], t: 4, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_1_as_softmax_exp: {type: exp, grid_loc: [2, 0], grid_size: [1, 1], inputs: [mha_1_as_mask], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax_sum.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [mha_1_as_softmax_exp, lc.input_tensor.mha_1_as_softmax_sum.0], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    mha_1_as_softmax_recip: {type: reciprocal, grid_loc: [2, 2], grid_size: [1, 1], inputs: [mha_1_as_softmax_sum.lc1], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax_mult: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [mha_1_as_softmax_exp, mha_1_as_softmax_recip], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    mha_1_ac: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [mha_1_as_softmax_mult, mha_1_value.bias], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_1_output: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [mha_1_ac, ff.bert.encoder.layer.1.attention.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_1_output.bias: {type: add, grid_loc: [2, 6], grid_size: [1, 1], inputs: [mha_1_output, e2e_ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [
        2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_mha_1: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [norm_ff_0_bias, mha_1_output.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1_mean.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [add_mha_1, lc.input_tensor.norm_mha_1_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_mha_1_sub: {type: subtract, grid_loc: [3, 1], grid_size: [1, 1], inputs: [add_mha_1, norm_mha_1_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_1_sq: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [norm_mha_1_sub, norm_mha_1_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1_var.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [norm_mha_1_sq, lc.input_tensor.norm_mha_1_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_mha_1_var_plus_eps: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [norm_mha_1_var.lc1, constant_1_norm_mha_1_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_mha_1_sqrt: {type: sqrt, grid_loc: [3, 5], grid_size: [1, 1], inputs: [norm_mha_1_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1_recip: {type: reciprocal, grid_loc: [3, 6], grid_size: [1, 1], inputs: [norm_mha_1_sqrt], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [norm_mha_1_recip, lc.input_tensor.norm_mha_1_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1_output: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [norm_mha_1_sub, norm_mha_1_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_1_weights: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [norm_mha_1_output, e2e_ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [
        4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    norm_mha_1_bias: {type: add, grid_loc: [4, 2], grid_size: [1, 1], inputs: [norm_mha_1_weights, e2e_ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2],
      ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff_1_ff1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [norm_mha_1_bias, ff.bert.encoder.layer.1.intermediate.dense.weight], t: 1, mblock: [4, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    ff_1_ff1.bias: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [ff_1_ff1, e2e_ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff1_gelu: {type: gelu, grid_loc: [4, 5], grid_size: [1, 1], inputs: [ff_1_ff1.bias], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3}
    ff_1_ff2: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [ff1_gelu, ff.bert.encoder.layer.1.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 8, u_kt: 2}}
    ff_1_ff2.bias: {type: add, grid_loc: [4, 7], grid_size: [1, 1], inputs: [ff_1_ff2, e2e_ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_ff_1: {type: add, grid_loc: [5, 0], grid_size: [1, 1], inputs: [norm_mha_1_bias, ff_1_ff2.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1_mean.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [add_ff_1, lc.input_tensor.norm_ff_1_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_ff_1_sub: {type: subtract, grid_loc: [5, 2], grid_size: [1, 1], inputs: [add_ff_1, norm_ff_1_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_ff_1_sq: {type: multiply, grid_loc: [5, 3], grid_size: [1, 1], inputs: [norm_ff_1_sub, norm_ff_1_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1_var.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [norm_ff_1_sq, lc.input_tensor.norm_ff_1_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_ff_1_var_plus_eps: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [norm_ff_1_var.lc1, constant_1_norm_ff_1_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_ff_1_sqrt: {type: sqrt, grid_loc: [5, 6], grid_size: [1, 1], inputs: [norm_ff_1_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1_recip: {type: reciprocal, grid_loc: [5, 7], grid_size: [1, 1], inputs: [norm_ff_1_sqrt], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [norm_ff_1_recip, lc.input_tensor.norm_ff_1_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1_output: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [norm_ff_1_sub, norm_ff_1_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_ff_1_weights: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [norm_ff_1_output, e2e_ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [4, 2], ublock: [
        1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_ff_1_bias: {type: add, grid_loc: [6, 3], grid_size: [1, 1], inputs: [norm_ff_1_weights, e2e_ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [
        2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_2_value: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [norm_ff_1_bias, ff.bert.encoder.layer.2.attention.self.value.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_2_value.bias: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [mha_2_value, e2e_ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 4], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mask_copy_1_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.mask_copy_1_s_brcst_m2_1_1.0, input_0_mask_copy_0], t: 4, mblock: [1, 2], ublock: [1, 2],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 4}], input_0_tms: [
        broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_2],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.attention.self.key.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_2_key: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [norm_ff_1_bias, ff.bert.encoder.layer.2.attention.self.key.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_2_key.bias: {type: add, grid_loc: [7, 3], grid_size: [1, 1], inputs: [mha_2_key, ff.bert.encoder.layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff.bert.encoder.layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.2.attention.self.query.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_2_query: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [norm_ff_1_bias, ff.bert.encoder.layer.2.attention.self.query.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_2_query.bias: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [mha_2_query, ff.bert.encoder.layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_2_as: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [mha_2_query.bias, mha_2_key.bias], t: 4, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4, transpose], input_0_tms: [hslice: 4], attributes: {m_k: 1, u_kt: 1}}
    mha_2_as_div: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [mha_2_as, ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1], t: 4, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}]}
    mha_2_as_mask: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [mha_2_as_div, mask_copy_1_s_brcst_m2_1_1.lc1], t: 4, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_2_as_softmax_exp: {type: exp, grid_loc: [8, 2], grid_size: [1, 1], inputs: [mha_2_as_mask], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax_sum.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [mha_2_as_softmax_exp, lc.input_tensor.mha_2_as_softmax_sum.0], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    mha_2_as_softmax_recip: {type: reciprocal, grid_loc: [8, 4], grid_size: [1, 1], inputs: [mha_2_as_softmax_sum.lc1], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax_mult: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [mha_2_as_softmax_exp, mha_2_as_softmax_recip], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    mha_2_ac: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [mha_2_as_softmax_mult, mha_2_value.bias], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_2_output: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [mha_2_ac, ff.bert.encoder.layer.2.attention.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_2_output.bias: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [mha_2_output, e2e_ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [
        2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_mha_2: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [norm_ff_1_bias, mha_2_output.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2_mean.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [add_mha_2, lc.input_tensor.norm_mha_2_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_mha_2_sub: {type: subtract, grid_loc: [9, 3], grid_size: [1, 1], inputs: [add_mha_2, norm_mha_2_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_2_sq: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [norm_mha_2_sub, norm_mha_2_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2_var.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [norm_mha_2_sq, lc.input_tensor.norm_mha_2_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_mha_2_var_plus_eps: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [norm_mha_2_var.lc1, constant_1_norm_mha_2_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_mha_2_sqrt: {type: sqrt, grid_loc: [9, 7], grid_size: [1, 1], inputs: [norm_mha_2_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2:
    target_device: 0
    input_count: 1
    norm_mha_2_recip: {type: reciprocal, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_norm_mha_2_sqrt_0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [norm_mha_2_recip, lc.input_tensor.norm_mha_2_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2_output: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_norm_mha_2_sub_0, norm_mha_2_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_2_weights: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [norm_mha_2_output, e2e_ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [
        4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    norm_mha_2_bias: {type: add, grid_loc: [0, 4], grid_size: [1, 1], inputs: [norm_mha_2_weights, e2e_ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2],
      ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff_2_ff1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [norm_mha_2_bias, ff.bert.encoder.layer.2.intermediate.dense.weight], t: 1, mblock: [4, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    ff_2_ff1.bias: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [ff_2_ff1, e2e_ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff2_gelu: {type: gelu, grid_loc: [0, 7], grid_size: [1, 1], inputs: [ff_2_ff1.bias], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3}
    ff_2_ff2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [ff2_gelu, ff.bert.encoder.layer.2.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 8, u_kt: 2}}
    ff_2_ff2.bias: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [ff_2_ff2, e2e_ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_ff_2: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [norm_mha_2_bias, ff_2_ff2.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2_mean.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_ff_2, lc.input_tensor.norm_ff_2_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_ff_2_sub: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_ff_2, norm_ff_2_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_ff_2_sq: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [norm_ff_2_sub, norm_ff_2_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2_var.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [norm_ff_2_sq, lc.input_tensor.norm_ff_2_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_ff_2_var_plus_eps: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [norm_ff_2_var.lc1, constant_1_norm_ff_2_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_ff_2_sqrt: {type: sqrt, grid_loc: [2, 0], grid_size: [1, 1], inputs: [norm_ff_2_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2_recip: {type: reciprocal, grid_loc: [2, 1], grid_size: [1, 1], inputs: [norm_ff_2_sqrt], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [norm_ff_2_recip, lc.input_tensor.norm_ff_2_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_ff_2_output: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [norm_ff_2_sub, norm_ff_2_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_ff_2_weights: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [norm_ff_2_output, e2e_ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [4, 2], ublock: [
        1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_ff_2_bias: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [norm_ff_2_weights, e2e_ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [
        2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_3_value: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [norm_ff_2_bias, ff.bert.encoder.layer.3.attention.self.value.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_3_value.bias: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [mha_3_value, e2e_ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 4], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mask_copy_2_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.mask_copy_2_s_brcst_m2_1_1.0, input_0_mask_copy_0], t: 4, mblock: [1, 2], ublock: [1, 2],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 4}], input_0_tms: [
        broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_3],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], input_0_tms: [broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0],
      t: 4, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {z: 4}], attributes: {m_k: 1, u_kt: 1}}
    ff.bert.encoder.layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.attention.self.key.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_3_key: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [norm_ff_2_bias, ff.bert.encoder.layer.3.attention.self.key.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_3_key.bias: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [mha_3_key, ff.bert.encoder.layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff.bert.encoder.layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0,
        ff.bert.encoder.layer.3.attention.self.query.bias], t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    mha_3_query: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [norm_ff_2_bias, ff.bert.encoder.layer.3.attention.self.query.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    mha_3_query.bias: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [mha_3_query, ff.bert.encoder.layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [4, 4], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_3_as: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [mha_3_query.bias, mha_3_key.bias], t: 4, mblock: [4, 4], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4, transpose], input_0_tms: [hslice: 4], attributes: {m_k: 1, u_kt: 1}}
    mha_3_as_div: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [mha_3_as, ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1], t: 4, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}, broadcast: {r: 4}]}
    mha_3_as_mask: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [mha_3_as_div, mask_copy_2_s_brcst_m2_1_1.lc1], t: 4, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    mha_3_as_softmax_exp: {type: exp, grid_loc: [4, 4], grid_size: [1, 1], inputs: [mha_3_as_mask], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax_sum.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [mha_3_as_softmax_exp, lc.input_tensor.mha_3_as_softmax_sum.0], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    mha_3_as_softmax_recip: {type: reciprocal, grid_loc: [4, 6], grid_size: [1, 1], inputs: [mha_3_as_softmax_sum.lc1], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax_mult: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [mha_3_as_softmax_exp, mha_3_as_softmax_recip], t: 4, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    mha_3_ac: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [mha_3_as_softmax_mult, mha_3_value.bias], t: 4, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_3_output: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [mha_3_ac, ff.bert.encoder.layer.3.attention.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 4], attributes: {m_k: 4, u_kt: 1}}
    mha_3_output.bias: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [mha_3_output, e2e_ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [
        2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_mha_3: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [norm_ff_2_bias, mha_3_output.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3_mean.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_mha_3, lc.input_tensor.norm_mha_3_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_mha_3_sub: {type: subtract, grid_loc: [5, 5], grid_size: [1, 1], inputs: [add_mha_3, norm_mha_3_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_3_sq: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [norm_mha_3_sub, norm_mha_3_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3_var.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [norm_mha_3_sq, lc.input_tensor.norm_mha_3_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_mha_3_var_plus_eps: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [norm_mha_3_var.lc1, constant_1_norm_mha_3_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_mha_3_sqrt: {type: sqrt, grid_loc: [6, 1], grid_size: [1, 1], inputs: [norm_mha_3_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3_recip: {type: reciprocal, grid_loc: [6, 2], grid_size: [1, 1], inputs: [norm_mha_3_sqrt], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [norm_mha_3_recip, lc.input_tensor.norm_mha_3_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3_output: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [norm_mha_3_sub, norm_mha_3_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_mha_3_weights: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [norm_mha_3_output, e2e_ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [
        4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    norm_mha_3_bias: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [norm_mha_3_weights, e2e_ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2],
      ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff_3_ff1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [norm_mha_3_bias, ff.bert.encoder.layer.3.intermediate.dense.weight], t: 1, mblock: [4, 8], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 2}}
    ff_3_ff1.bias: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [ff_3_ff1, e2e_ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    ff3_gelu: {type: gelu, grid_loc: [7, 1], grid_size: [1, 1], inputs: [ff_3_ff1.bias], t: 1, mblock: [2, 8], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3}
    ff_3_ff2: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [ff3_gelu, ff.bert.encoder.layer.3.output.dense.weight], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 8, u_kt: 2}}
    ff_3_ff2.bias: {type: add, grid_loc: [7, 3], grid_size: [1, 1], inputs: [ff_3_ff2, e2e_ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    add_ff_3: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [norm_mha_3_bias, ff_3_ff2.bias], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3_mean.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [add_ff_3, lc.input_tensor.norm_ff_3_mean.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 4, u_kt: 1}}
    norm_ff_3_sub: {type: subtract, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_ff_3, norm_ff_3_mean.lc1], t: 1, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_ff_3_sq: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [norm_ff_3_sub, norm_ff_3_sub], t: 1, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3_var.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [norm_ff_3_sq, lc.input_tensor.norm_ff_3_var.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}], attributes: {m_k: 2, u_kt: 2}}
    norm_ff_3_var_plus_eps: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [norm_ff_3_var.lc1, constant_1_norm_ff_3_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_ff_3_sqrt: {type: sqrt, grid_loc: [8, 2], grid_size: [1, 1], inputs: [norm_ff_3_var_plus_eps], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3_recip: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [norm_ff_3_sqrt], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [norm_ff_3_recip, lc.input_tensor.norm_ff_3_recip_s_brcst_m1_0_0.0], t: 1, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    norm_ff_3_output: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [norm_ff_3_sub, norm_ff_3_recip_s_brcst_m1_0_0.lc1], t: 1, mblock: [4, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    norm_ff_3_weights: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [norm_ff_3_output, e2e_ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [4, 2], ublock: [
        1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    norm_ff_3_bias: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [norm_ff_3_weights, e2e_ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], untilize_output: true, t: 1, mblock: [
        2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    mask_copy_3: {type: nop, grid_loc: [9, 0], grid_size: [1, 1], inputs: [input_0_mask_copy_0], untilize_output: true, t: 1, mblock: [1, 2], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
- run_fwd:
  - param: [$p_microbatch_count]
  - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0}
  - staticvar: {$gptr_q0: 0, $lptr_q6: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q1: 0, $gptr_q6: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q1_shadow: 0, $gptr_q4: 0, $gptr_q3: 0,
      $gptr_q2: 0, $gptr_q2_shadow: 0}
  - varinst: [$gptr_q2, set, $gptr_q2_shadow]
  - varinst: [$gptr_q1, set, $gptr_q1_shadow]
  - loop: $p_microbatch_count
  - execute: {graph_name: fwd_0, queue_settings: {lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.input_0_mask_copy_0_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.reciprocal_of_sqrt_of_head_size_0: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.mha_0_as_softmax_sum.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.0.attention.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.norm_mha_0_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.norm_mha_0_var.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_mha_0_var_plus_eps: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.norm_mha_0_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.0.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.norm_ff_0_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.norm_ff_0_var.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_ff_0_var_plus_eps: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.norm_ff_0_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, input_0_add_mha_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, input_0_mask_copy_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}}}
  - varinst: [$gptr_q0, incwrap, $c_microbatch_size, 2]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 2]
  - varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: fwd_1, queue_settings: {ff.bert.encoder.layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.mask_copy_0_s_brcst_m2_1_1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.reciprocal_of_sqrt_of_head_size_1: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.mha_1_as_softmax_sum.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.1.attention.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.norm_mha_1_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.norm_mha_1_var.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_mha_1_var_plus_eps: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.norm_mha_1_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        ff.bert.encoder.layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.1.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.norm_ff_1_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.norm_ff_1_var.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_ff_1_var_plus_eps: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.norm_ff_1_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.mask_copy_1_s_brcst_m2_1_1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.reciprocal_of_sqrt_of_head_size_2: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.mha_2_as_softmax_sum.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.attention.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.norm_mha_2_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.norm_mha_2_var.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_mha_2_var_plus_eps: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, input_0_mask_copy_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_ff.bert.encoder.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3,
          rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
        e2e_ff.bert.encoder.layer.1.output.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3,
          rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
        e2e_ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_norm_ff_0_output_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3,
          rd_ptr_global: $gptr_q3}}}
  - varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
  - execute: {graph_name: fwd_2, queue_settings: {lc.input_tensor.norm_mha_2_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.2.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.norm_ff_2_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.norm_ff_2_var.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_ff_2_var_plus_eps: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.norm_ff_2_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.mask_copy_2_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.reciprocal_of_sqrt_of_head_size_3: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.self.key.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.bert.encoder.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.mha_3_as_softmax_sum.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.norm_mha_3_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.norm_mha_3_var.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_mha_3_var_plus_eps: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.norm_mha_3_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.bert.encoder.layer.3.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.bert.encoder.layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.norm_ff_3_mean.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.norm_ff_3_var.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, constant_1_norm_ff_3_var_plus_eps: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.norm_ff_3_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, input_0_mask_copy_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5,
          rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
        e2e_ff.bert.encoder.layer.3.output.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5,
          rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
        e2e_ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.2.output.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5,
          rd_ptr_global: $gptr_q5}, e2e_ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
        e2e_ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_norm_mha_2_sub_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_norm_mha_2_sqrt_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6,
          rd_ptr_global: $gptr_q6}}}
  - varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
  - varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
  - varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
  - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.50
    check_pcc: 0.92
    verbosity: Concise
  stimulus-config:
    type: Normal
    normal_mean: 0.0
    normal_stddev: 0.1
  io-config:
    inputs: [input_0_add_mha_0, input_0_mask_copy_0]
    outputs: [encoder3.output_norm_ff_3_bias, encoder3.output_mask_copy_3]
