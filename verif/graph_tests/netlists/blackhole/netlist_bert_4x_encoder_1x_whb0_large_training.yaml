# git checkout 435f4d73
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[training-Wormhole_B0-chip1-enc4-large]

devices:
  arch: blackhole

queues:

  # input
  hidden_states: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10000000]]}
  attention_mask: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10000000]]}

  # output
  bert_encoder.output_layernorm_439: {input: layernorm_439.dc.add.10, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight: {input: opt_in1_layer.0.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10000000], [1, 0x10044040], [1, 0x10088080], [1, 0x100cc0c0], [1, 0x10110100], [1, 0x10154140], [1, 0x10198180], [1, 0x101dc1c0]]}
  layer.0.attention.self.query.bias: {input: opt_in1_layer.0.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10000000]]}
  layer.0.attention.self.key.weight: {input: opt_in1_layer.0.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10000000], [2, 0x10044040], [2, 0x10088080], [2, 0x100cc0c0], [2, 0x10110100], [2, 0x10154140], [2, 0x10198180], [2, 0x101dc1c0]]}
  layer.0.attention.self.key.bias: {input: opt_in1_layer.0.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x10000000]]}
  layer.0.attention.self.value.weight: {input: opt_in1_layer.0.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10220200], [2, 0x10264240], [2, 0x102a8280], [2, 0x102ec2c0], [2, 0x10330300], [2, 0x10374340], [2, 0x103b8380], [2, 0x103fc3c0]]}
  layer.0.attention.self.value.bias: {input: opt_in1_layer.0.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x10011040]]}
  layer.0.attention.output.dense.weight: {input: opt_in0_layer.0.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10330040], [5, 0x10374080], [5, 0x103b80c0], [5, 0x103fc100], [5, 0x10440140], [5, 0x10484180], [5, 0x104c81c0], [5, 0x1050c200]]}
  layer.0.attention.output.dense.bias: {input: opt_in1_layer.0.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10011040]]}
  layer.0.attention.output.LayerNorm.weight: {input: opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10440400]]}
  layer.0.attention.output.LayerNorm.bias: {input: opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10220200]]}
  layer.0.intermediate.dense.weight: {input: opt_in0_layer.0.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x10550240], [5, 0x10594280], [5, 0x105d82c0], [5, 0x1061c300], [5, 0x10660340], [5, 0x106a4380], [5, 0x106e83c0], [5, 0x1072c400], [5, 0x10770440], [5, 0x107b4480],
      [5, 0x107f84c0], [5, 0x1083c500], [5, 0x10880540], [5, 0x108c4580], [5, 0x109085c0], [5, 0x1094c600], [5, 0x10990640], [5, 0x109d4680], [5, 0x10a186c0], [5, 0x10a5c700], [5, 0x10aa0740], [5, 0x10ae4780],
      [5, 0x10b287c0], [5, 0x10b6c800], [5, 0x10bb0840], [5, 0x10bf4880], [5, 0x10c388c0], [5, 0x10c7c900], [5, 0x10cc0940], [5, 0x10d04980], [5, 0x10d489c0], [5, 0x10d8ca00]]}
  layer.0.intermediate.dense.bias: {input: opt_in1_layer.0.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10022080]]}
  layer.0.output.dense.weight: {input: opt_in0_layer.0.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10451440], [2, 0x10495480], [2, 0x104d94c0], [2, 0x1051d500], [2, 0x10561540], [2, 0x105a5580], [2, 0x105e95c0], [2, 0x1062d600], [2, 0x10671640], [2, 0x106b5680], [2, 0x106f96c0],
      [2, 0x1073d700], [2, 0x10781740], [2, 0x107c5780], [2, 0x108097c0], [2, 0x1084d800], [2, 0x10891840], [2, 0x108d5880], [2, 0x109198c0], [2, 0x1095d900], [2, 0x109a1940], [2, 0x109e5980], [2, 0x10a299c0],
      [2, 0x10a6da00], [2, 0x10ab1a40], [2, 0x10af5a80], [2, 0x10b39ac0], [2, 0x10b7db00], [2, 0x10bc1b40], [2, 0x10c05b80], [2, 0x10c49bc0], [2, 0x10c8dc00]]}
  layer.0.output.dense.bias: {input: opt_in1_layer.0.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10022080]]}
  layer.0.output.LayerNorm.weight: {input: opt_in1_layer.0.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x10dd0a40]]}
  layer.0.output.LayerNorm.bias: {input: opt_in2_layer.0.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x100660c0]]}
  layer.1.attention.self.query.weight: {input: opt_in0_layer.1.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10cd1c40], [2, 0x10d15c80], [2, 0x10d59cc0], [2, 0x10d9dd00], [2, 0x10de1d40], [2, 0x10e25d80], [2, 0x10e69dc0], [2, 0x10eade00]]}
  layer.1.attention.self.query.bias: {input: opt_in1_layer.1.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x100330c0]]}
  layer.1.attention.self.key.weight: {input: opt_in0_layer.1.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x10de1a80], [5, 0x10e25ac0], [5, 0x10e69b00], [5, 0x10eadb40], [5, 0x10ef1b80], [5, 0x10f35bc0], [5, 0x10f79c00], [5, 0x10fbdc40]]}
  layer.1.attention.self.key.bias: {input: opt_in1_layer.1.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10077100]]}
  layer.1.attention.self.value.weight: {input: opt_in0_layer.1.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10231240], [1, 0x10275280], [1, 0x102b92c0], [1, 0x102fd300], [1, 0x10341340], [1, 0x10385380], [1, 0x103c93c0], [1, 0x1040d400]]}
  layer.1.attention.self.value.bias: {input: opt_in1_layer.1.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10132040]]}
  layer.1.attention.output.dense.weight: {input: opt_in0_layer.1.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10088140], [3, 0x100cc180], [3, 0x101101c0], [3, 0x10154200], [3, 0x10198240], [3, 0x101dc280], [3, 0x102202c0], [3, 0x10264300]]}
  layer.1.attention.output.dense.bias: {input: opt_in1_layer.1.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10451440]]}
  layer.1.attention.output.LayerNorm.weight: {input: opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10044100]]}
  layer.1.attention.output.LayerNorm.bias: {input: opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10143080]]}
  layer.1.intermediate.dense.weight: {input: opt_in0_layer.1.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x102a8340], [3, 0x102ec380], [3, 0x103303c0], [3, 0x10374400], [3, 0x103b8440], [3, 0x103fc480], [3, 0x104404c0], [3, 0x10484500], [3, 0x104c8540], [3, 0x1050c580],
      [3, 0x105505c0], [3, 0x10594600], [3, 0x105d8640], [3, 0x1061c680], [3, 0x106606c0], [3, 0x106a4700], [3, 0x106e8740], [3, 0x1072c780], [3, 0x107707c0], [3, 0x107b4800], [3, 0x107f8840], [3, 0x1083c880],
      [3, 0x108808c0], [3, 0x108c4900], [3, 0x10908940], [3, 0x1094c980], [3, 0x109909c0], [3, 0x109d4a00], [3, 0x10a18a40], [3, 0x10a5ca80], [3, 0x10aa0ac0], [3, 0x10ae4b00]]}
  layer.1.intermediate.dense.bias: {input: opt_in1_layer.1.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10462480]]}
  layer.1.output.dense.weight: {input: opt_in0_layer.1.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10055140], [0, 0x10099180], [0, 0x100dd1c0], [0, 0x10121200], [0, 0x10165240], [0, 0x101a9280], [0, 0x101ed2c0], [0, 0x10231300], [0, 0x10275340], [0, 0x102b9380], [0, 0x102fd3c0],
      [0, 0x10341400], [0, 0x10385440], [0, 0x103c9480], [0, 0x1040d4c0], [0, 0x10451500], [0, 0x10495540], [0, 0x104d9580], [0, 0x1051d5c0], [0, 0x10561600], [0, 0x105a5640], [0, 0x105e9680], [0, 0x1062d6c0],
      [0, 0x10671700], [0, 0x106b5740], [0, 0x106f9780], [0, 0x1073d7c0], [0, 0x10781800], [0, 0x107c5840], [0, 0x10809880], [0, 0x1084d8c0], [0, 0x10891900]]}
  layer.1.output.dense.bias: {input: opt_in1_layer.1.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101540c0]]}
  layer.1.output.LayerNorm.weight: {input: opt_in1_layer.1.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x11001c80]]}
  layer.1.output.LayerNorm.bias: {input: opt_in2_layer.1.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10b28b40]]}
  layer.2.attention.self.query.weight: {input: opt_in0_layer.2.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x10ef1e40], [2, 0x10f35e80], [2, 0x10f79ec0], [2, 0x10fbdf00], [2, 0x11001f40], [2, 0x11045f80], [2, 0x11089fc0], [2, 0x110ce000]]}
  layer.2.attention.self.query.bias: {input: opt_in1_layer.2.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x108d5940]]}
  layer.2.attention.self.key.weight: {input: opt_in0_layer.2.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x11012cc0], [5, 0x11056d00], [5, 0x1109ad40], [5, 0x110ded80], [5, 0x11122dc0], [5, 0x11166e00], [5, 0x111aae40], [5, 0x111eee80]]}
  layer.2.attention.self.key.bias: {input: opt_in1_layer.2.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10b39b80]]}
  layer.2.attention.self.value.weight: {input: opt_in0_layer.2.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x108e6980], [0, 0x1092a9c0], [0, 0x1096ea00], [0, 0x109b2a40], [0, 0x109f6a80], [0, 0x10a3aac0], [0, 0x10a7eb00], [0, 0x10ac2b40]]}
  layer.2.attention.self.value.bias: {input: opt_in1_layer.2.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x10165100]]}
  layer.2.attention.output.dense.weight: {input: opt_in0_layer.2.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10b4abc0], [3, 0x10b8ec00], [3, 0x10bd2c40], [3, 0x10c16c80], [3, 0x10c5acc0], [3, 0x10c9ed00], [3, 0x10ce2d40], [3, 0x10d26d80]]}
  layer.2.attention.output.dense.bias: {input: opt_in1_layer.2.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x104a64c0]]}
  layer.2.attention.output.LayerNorm.weight: {input: opt_in1_layer.2.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x104b7500]]}
  layer.2.attention.output.LayerNorm.bias: {input: opt_in2_layer.2.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10176140]]}
  layer.2.intermediate.dense.weight: {input: opt_in0_layer.2.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x10d6adc0], [3, 0x10daee00], [3, 0x10df2e40], [3, 0x10e36e80], [3, 0x10e7aec0], [3, 0x10ebef00], [3, 0x10f02f40], [3, 0x10f46f80], [3, 0x10f8afc0], [3, 0x10fcf000],
      [3, 0x11013040], [3, 0x11057080], [3, 0x1109b0c0], [3, 0x110df100], [3, 0x11123140], [3, 0x11167180], [3, 0x111ab1c0], [3, 0x111ef200], [3, 0x11233240], [3, 0x11277280], [3, 0x112bb2c0], [3, 0x112ff300],
      [3, 0x11343340], [3, 0x11387380], [3, 0x113cb3c0], [3, 0x1140f400], [3, 0x11453440], [3, 0x11497480], [3, 0x114db4c0], [3, 0x1151f500], [3, 0x11563540], [3, 0x115a7580]]}
  layer.2.intermediate.dense.bias: {input: opt_in1_layer.2.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x104c8540]]}
  layer.2.output.dense.weight: {input: opt_in0_layer.2.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10b06b80], [0, 0x10b4abc0], [0, 0x10b8ec00], [0, 0x10bd2c40], [0, 0x10c16c80], [0, 0x10c5acc0], [0, 0x10c9ed00], [0, 0x10ce2d40], [0, 0x10d26d80], [0, 0x10d6adc0], [0, 0x10daee00],
      [0, 0x10df2e40], [0, 0x10e36e80], [0, 0x10e7aec0], [0, 0x10ebef00], [0, 0x10f02f40], [0, 0x10f46f80], [0, 0x10f8afc0], [0, 0x10fcf000], [0, 0x11013040], [0, 0x11057080], [0, 0x1109b0c0], [0, 0x110df100],
      [0, 0x11123140], [0, 0x11167180], [0, 0x111ab1c0], [0, 0x111ef200], [0, 0x11233240], [0, 0x11277280], [0, 0x112bb2c0], [0, 0x112ff300], [0, 0x11343340]]}
  layer.2.output.dense.bias: {input: opt_in1_layer.2.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10187180]]}
  layer.2.output.LayerNorm.weight: {input: opt_in1_layer.2.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x115eb5c0]]}
  layer.2.output.LayerNorm.bias: {input: opt_in2_layer.2.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x1050c580]]}
  layer.3.attention.self.query.weight: {input: opt_in0_layer.3.attention.self.query.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x11387380], [0, 0x113cb3c0], [0, 0x1140f400], [0, 0x11453440], [0, 0x11497480], [0, 0x114db4c0], [0, 0x1151f500], [0, 0x11563540]]}
  layer.3.attention.self.query.bias: {input: opt_in1_layer.3.attention.self.query.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x101981c0]]}
  layer.3.attention.self.key.weight: {input: opt_in0_layer.3.attention.self.key.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x115fc600], [3, 0x11640640], [3, 0x11684680], [3, 0x116c86c0], [3, 0x1170c700], [3, 0x11750740], [3, 0x11794780], [3, 0x117d87c0]]}
  layer.3.attention.self.key.bias: {input: opt_in1_layer.3.attention.self.key.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x1051d5c0]]}
  layer.3.attention.self.value.weight: {input: opt_in0_layer.3.attention.self.value.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x11232ec0], [5, 0x11276f00], [5, 0x112baf40], [5, 0x112fef80], [5, 0x11342fc0], [5, 0x11387000], [5, 0x113cb040], [5, 0x1140f080]]}
  layer.3.attention.self.value.bias: {input: opt_in1_layer.3.attention.self.value.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x1181c800]]}
  layer.3.attention.output.dense.weight: {input: opt_in0_layer.3.attention.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11112040], [2, 0x11156080], [2, 0x1119a0c0], [2, 0x111de100], [2, 0x11222140], [2, 0x11266180], [2, 0x112aa1c0], [2, 0x112ee200]]}
  layer.3.attention.output.dense.bias: {input: opt_in1_layer.3.attention.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x115a7580]]}
  layer.3.attention.output.LayerNorm.weight: {input: opt_in1_layer.3.attention.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x114530c0]]}
  layer.3.attention.output.LayerNorm.bias: {input: opt_in2_layer.3.attention.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1182d840]]}
  layer.3.intermediate.dense.weight: {input: opt_in0_layer.3.intermediate.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x11332240], [2, 0x11376280], [2, 0x113ba2c0], [2, 0x113fe300], [2, 0x11442340], [2, 0x11486380], [2, 0x114ca3c0], [2, 0x1150e400], [2, 0x11552440], [2, 0x11596480],
      [2, 0x115da4c0], [2, 0x1161e500], [2, 0x11662540], [2, 0x116a6580], [2, 0x116ea5c0], [2, 0x1172e600], [2, 0x11772640], [2, 0x117b6680], [2, 0x117fa6c0], [2, 0x1183e700], [2, 0x11882740], [2, 0x118c6780],
      [2, 0x1190a7c0], [2, 0x1194e800], [2, 0x11992840], [2, 0x119d6880], [2, 0x11a1a8c0], [2, 0x11a5e900], [2, 0x11aa2940], [2, 0x11ae6980], [2, 0x11b2a9c0], [2, 0x11b6ea00]]}
  layer.3.intermediate.dense.bias: {input: opt_in1_layer.3.intermediate.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x115b85c0]]}
  layer.3.output.dense.weight: {input: opt_in0_layer.3.output.dense.weight_subtract_2, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11464100], [5, 0x114a8140], [5, 0x114ec180], [5, 0x115301c0], [5, 0x11574200], [5, 0x115b8240], [5, 0x115fc280], [5, 0x116402c0], [5, 0x11684300], [5, 0x116c8340], [5, 0x1170c380],
      [5, 0x117503c0], [5, 0x11794400], [5, 0x117d8440], [5, 0x1181c480], [5, 0x118604c0], [5, 0x118a4500], [5, 0x118e8540], [5, 0x1192c580], [5, 0x119705c0], [5, 0x119b4600], [5, 0x119f8640], [5, 0x11a3c680],
      [5, 0x11a806c0], [5, 0x11ac4700], [5, 0x11b08740], [5, 0x11b4c780], [5, 0x11b907c0], [5, 0x11bd4800], [5, 0x11c18840], [5, 0x11c5c880], [5, 0x11ca08c0]]}
  layer.3.output.dense.bias: {input: opt_in1_layer.3.output.dense.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1183e880]]}
  layer.3.output.LayerNorm.weight: {input: opt_in1_layer.3.output.LayerNorm.weight_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x1184f8c0]]}
  layer.3.output.LayerNorm.bias: {input: opt_in2_layer.3.output.LayerNorm.bias_subtract_2, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x1052e600]]}

  # constant
  lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101a9200]]}
  lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1053f640]]}
  input_1_multiply_244_fork_clone1647_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11ce4900]]}
  lc.input_tensor.softmax_246.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11860900]]}
  lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x115fc600]]}
  lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101a9ac0]]}
  lc.input_tensor.layernorm_266.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bb2a40]]}
  lc.input_tensor.layernorm_266.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1053ff00]]}
  dc.input_tensor.layernorm_266.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x11ce51c0], [5, 0x11ce8500]]}
  lc.input_tensor.layernorm_266.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101aa380]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x118611c0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x105407c0]]}
  lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101aac40]]}
  lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10541080]]}
  lc.input_tensor.layernorm_280.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11ceb840]]}
  lc.input_tensor.layernorm_280.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101ab500]]}
  dc.input_tensor.layernorm_280.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11bb3300], [2, 0x11bb6640]]}
  lc.input_tensor.layernorm_280.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10541940]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x115fcec0]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101abdc0]]}
  lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10542200]]}
  lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101ac680]]}
  input_1_multiply_297_fork_clone1670_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bb9980]]}
  lc.input_tensor.softmax_299.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x10542ac0]]}
  lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cec100]]}
  lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bba240]]}
  lc.input_tensor.layernorm_319.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x115fd780]]}
  lc.input_tensor.layernorm_319.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11cec9c0]]}
  dc.input_tensor.layernorm_319.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11861a80], [3, 0x11864dc0]]}
  lc.input_tensor.layernorm_319.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbab00]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10543380]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x101acf40]]}
  lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbb3c0]]}
  lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11ced280]]}
  lc.input_tensor.layernorm_333.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11868100]]}
  lc.input_tensor.layernorm_333.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bbbc80]]}
  dc.input_tensor.layernorm_333.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x115fe040], [0, 0x11601380]]}
  lc.input_tensor.layernorm_333.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cedb40]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cee400]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101ad800]]}
  lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10543c40]]}
  lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101ae0c0]]}
  input_1_multiply_350_fork_clone1693_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbc540]]}
  lc.input_tensor.softmax_352.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x116046c0]]}
  lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11ceecc0]]}
  lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbce00]]}
  lc.input_tensor.layernorm_372.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11604f80]]}
  lc.input_tensor.layernorm_372.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11cef580]]}
  dc.input_tensor.layernorm_372.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x118689c0], [3, 0x1186bd00]]}
  lc.input_tensor.layernorm_372.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbd6c0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10544500]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x11605840]]}
  lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbdf80]]}
  lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cefe40]]}
  lc.input_tensor.layernorm_386.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1186f040]]}
  lc.input_tensor.layernorm_386.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bbe840]]}
  dc.input_tensor.layernorm_386.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11606100], [0, 0x11609440]]}
  lc.input_tensor.layernorm_386.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cf0700]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101ae980]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbf100]]}
  lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cf0fc0]]}
  lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bbf9c0]]}
  input_1_multiply_403_fork_clone1716_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1160c780]]}
  lc.input_tensor.softmax_405.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11cf1880]]}
  lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101af240]]}
  lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10544dc0]]}
  lc.input_tensor.layernorm_425.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11cf2140]]}
  lc.input_tensor.layernorm_425.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101afb00]]}
  dc.input_tensor.layernorm_425.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11bc0280], [2, 0x11bc35c0]]}
  lc.input_tensor.layernorm_425.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10545680]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x1160d040]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x101b03c0]]}
  lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10545f40]]}
  lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b0c80]]}
  lc.input_tensor.layernorm_439.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bc6900]]}
  lc.input_tensor.layernorm_439.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11cf2a00]]}
  dc.input_tensor.layernorm_439.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x1160d900], [0, 0x11610c40]]}
  lc.input_tensor.layernorm_439.dc.reciprocal.7_s_brcst_m1_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cf32c0]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b1540]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_1_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bc71c0]]}
  lc.input_tensor.bw_in2_layernorm_439_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11613f80]]}
  lc.input_tensor.bw_in1_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1186f900]]}
  lc.input_tensor.layernorm_439.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10546800]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11614840]]}
  lc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cf3b80]]}
  lc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b1e00]]}
  dc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x118701c0], [3, 0x118d6200]]}
  lc.input_tensor.bw_in1_add_436_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bc7a80]]}
  lc.input_tensor.bw_in1_add_430_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11cf4440]]}
  lc.input_tensor.bw_in2_layernorm_425_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bc8340]]}
  lc.input_tensor.bw_in1_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11615100]]}
  lc.input_tensor.layernorm_425.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1193c240]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x11bc8c00]]}
  lc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x105470c0]]}
  lc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x116159c0]]}
  dc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11cf4d00], [5, 0x11d5ad40]]}
  lc.input_tensor.bw_in1_add_422_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101b26c0]]}
  lc.input_tensor.bw_in1_add_411_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x10547980]]}
  lc.input_tensor.bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b2f80]]}
  input_1_multiply_403_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1193cb00]]}
  lc.input_tensor.bw_in1_add_397_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bc94c0]]}
  lc.input_tensor.bw_in1_add_391_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11dc0d80]]}
  lc.input_tensor.bw_in2_layernorm_386_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11616280]]}
  lc.input_tensor.bw_in1_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10548240]]}
  lc.input_tensor.layernorm_386.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc1640]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b3840]]}
  lc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1193d3c0]]}
  lc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bc9d80]]}
  dc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10548b00], [1, 0x105aeb40]]}
  lc.input_tensor.bw_in1_add_383_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11616b40]]}
  lc.input_tensor.bw_in1_add_377_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101b4100]]}
  lc.input_tensor.bw_in2_layernorm_372_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11617400]]}
  lc.input_tensor.bw_in1_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b49c0]]}
  lc.input_tensor.layernorm_372.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bca640]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x10614b80]]}
  lc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11617cc0]]}
  lc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc1f00]]}
  dc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1193dc80], [3, 0x119a3cc0]]}
  lc.input_tensor.bw_in1_add_369_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11dc27c0]]}
  lc.input_tensor.bw_in1_add_358_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bcaf00]]}
  lc.input_tensor.bw_in0_softmax_352_softmax_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc3080]]}
  input_1_multiply_350_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101b5280]]}
  lc.input_tensor.bw_in1_add_344_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11a09d00]]}
  lc.input_tensor.bw_in1_add_338_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11618580]]}
  lc.input_tensor.bw_in2_layernorm_333_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11a0a5c0]]}
  lc.input_tensor.bw_in1_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10615440]]}
  lc.input_tensor.layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc3940]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b5b40]]}
  lc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11a0ae80]]}
  lc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10615d00]]}
  dc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11618e40], [0, 0x1167ee80]]}
  lc.input_tensor.bw_in1_add_330_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101b6400]]}
  lc.input_tensor.bw_in1_add_324_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x106165c0]]}
  lc.input_tensor.bw_in2_layernorm_319_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b6cc0]]}
  lc.input_tensor.bw_in1_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bcb7c0]]}
  lc.input_tensor.layernorm_319.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10616e80]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x11a0b740]]}
  lc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b7580]]}
  lc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc4200]]}
  dc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x116e4ec0], [0, 0x1174af00]]}
  lc.input_tensor.bw_in1_add_316_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x10617740]]}
  lc.input_tensor.bw_in1_add_305_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101b7e40]]}
  lc.input_tensor.bw_in0_softmax_299_softmax_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc4ac0]]}
  input_1_multiply_297_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11a0c000]]}
  lc.input_tensor.bw_in1_add_291_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bcc080]]}
  lc.input_tensor.bw_in1_add_285_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11dc5380]]}
  lc.input_tensor.bw_in2_layernorm_280_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bcc940]]}
  lc.input_tensor.bw_in1_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x117b0f40]]}
  lc.input_tensor.layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b8700]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11a0c8c0]]}
  lc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bcd200]]}
  lc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10618000]]}
  dc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x117b1800], [0, 0x11817840]]}
  lc.input_tensor.bw_in1_add_277_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11dc5c40]]}
  lc.input_tensor.bw_in1_add_271_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bcdac0]]}
  lc.input_tensor.bw_in2_layernorm_266_layernorm_bw_0.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc6500]]}
  lc.input_tensor.bw_in1_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11a0d180]]}
  lc.input_tensor.layernorm_266.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x106188c0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x1187d880]]}
  lc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11dc6dc0]]}
  lc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x101b8fc0]]}
  dc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11a0da40], [3, 0x11a73a80]]}
  lc.input_tensor.bw_in1_add_263_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11bce380]]}
  lc.input_tensor.bw_in1_add_252_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11dc7680]]}
  lc.input_tensor.bw_in0_softmax_246_softmax_bw_0.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11bcec40]]}
  input_1_multiply_244_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x10619180]]}
  lc.input_tensor.bw_in1_add_238_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1187e140]]}
  lc.input_tensor.bw_in1_add_232_brcst_reduce_sum_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11ad9ac0]]}

  # epoch_to_epoch
  e2e_layernorm_266.dc.multiply.9_0: {input: layernorm_266.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11ada380], [3, 0x11ba63c0]]}
  e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0: {input: layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8],
    ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11bcf500]]}
  e2e_layernorm_280.dc.sqrt.6_0: {input: layernorm_280.dc.sqrt.6, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101b9880], [4, 0x101bfec0]]}
  e2e_buffer_0_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8_0: {input: buffer_0_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11dc7f40], [5, 0x11e93f80]]}
  e2e_layernorm_319.dc.multiply.2_0: {input: layernorm_319.dc.multiply.2, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1187ea00], [0, 0x1194aa40]]}
  e2e_buffer_0_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8_0: {input: buffer_0_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10619a40], [1, 0x106e5a80]]}
  e2e_add_332_0: {input: add_332, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11bf1540], [
        2, 0x11cbd580]]}
  e2e_add_369_0: {input: add_369, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11c72400], [
        3, 0x11d3e440]]}
  e2e_layernorm_333.dc.add.10_0: {input: layernorm_333.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x101c6500], [4, 0x10292540]]}
  e2e_gelu_378_0: {input: gelu_378, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x107b1ac0],
      [1, 0x1087db00], [1, 0x10949b40], [1, 0x10a15b80], [1, 0x10ae1bc0], [1, 0x10badc00], [1, 0x10c79c40], [1, 0x10d45c80]]}
  e2e_layernorm_372.dc.add.10_0: {input: layernorm_372.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1035e580], [4, 0x1042a5c0]]}
  e2e_softmax_405.dc.exp.0_0: {input: softmax_405.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x104f6600], [4, 0x1075a640], [4, 0x109be680], [4, 0x10c226c0]]}
  e2e_lc.input_tensor.softmax_405.dc.reduce_sum.1.0_splt_brcst_1_0_0: {input: lc.input_tensor.softmax_405.dc.reduce_sum.1.0_splt_brcst_1_0, type: queue, entries: 2, grid_size: [1, 1], t: 16, mblock: [1,
      1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11f5ffc0]]}
  e2e_layernorm_386.dc.add.10_0: {input: layernorm_386.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x10e11cc0], [1, 0x10eddd00]]}
  e2e_matmul_428_0: {input: matmul_428, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11a16a80],
      [0, 0x11a7cac0], [0, 0x11ae2b00], [0, 0x11b48b40], [0, 0x11baeb80], [0, 0x11c14bc0], [0, 0x11c7ac00], [0, 0x11ce0c40], [0, 0x11d46c80], [0, 0x11daccc0], [0, 0x11e12d00], [0, 0x11e78d40], [0, 0x11eded80],
      [0, 0x11f44dc0], [0, 0x11faae00], [0, 0x12010e40]]}
  e2e_layer.3.intermediate.dense.bias_s_brcst_m2_1_0.lc1_0: {input: layer.3.intermediate.dense.bias_s_brcst_m2_1_0.lc1, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4],
    ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10fa9d40], [1, 0x10fedd80]]}
  e2e_layernorm_425.dc.add.10_0: {input: layernorm_425.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x10e86700], [4, 0x10f52740]]}
  e2e_layernorm_439.dc.multiply.8_0: {input: layernorm_439.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x12076e80], [0, 0x12142ec0]]}
  e2e_layernorm_439.dc.reciprocal.7_0: {input: layernorm_439.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11f71000], [5, 0x11f77640]]}
  e2e_gelu_431_0: {input: gelu_431, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1101e780],
      [4, 0x110ea7c0], [4, 0x111b6800], [4, 0x11282840], [4, 0x1134e880], [4, 0x1141a8c0], [4, 0x114e6900], [4, 0x115b2940]]}
  e2e_add_430_0: {input: add_430, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x11031dc0], [
        1, 0x110fde00], [1, 0x111c9e40], [1, 0x11295e80], [1, 0x11361ec0], [1, 0x1142df00], [1, 0x114f9f40], [1, 0x115c5f80]]}
  e2e_bw_in0_matmul_434_matmul_1_0: {input: bw_in0_matmul_434_matmul_1, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11d895c0], [2, 0x11def600], [2, 0x11e55640], [2, 0x11ebb680], [2, 0x11f216c0], [2, 0x11f87700], [2, 0x11fed740], [2, 0x12053780], [2, 0x120b97c0], [2, 0x1211f800], [2, 0x12185840],
      [2, 0x121eb880], [2, 0x122518c0], [2, 0x122b7900], [2, 0x1231d940], [2, 0x12383980]]}
  e2e_bw_in1_matmul_428_transpose_0_0: {input: bw_in1_matmul_428_transpose_0, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [16, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1220ef00], [0, 0x122daf40]]}
  e2e_bw_in0_gelu_431_multiply_1_0: {input: bw_in0_gelu_431_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x1167e980], [4, 0x1174a9c0], [4, 0x11816a00], [4, 0x118e2a40], [4, 0x119aea80], [4, 0x11a7aac0], [4, 0x11b46b00], [4, 0x11c12b40]]}
  e2e_bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11e0a480], [3, 0x11ed64c0]]}
  e2e_bw_in0_matmul_428_matmul_1_0: {input: bw_in0_matmul_428_matmul_1, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x11f7dc80], [5, 0x11f974c0], [5, 0x11fb0d00], [5, 0x11fca540], [5, 0x11fe3d80], [5, 0x11ffd5c0], [5, 0x12016e00], [5, 0x12030640], [5, 0x12049e80], [5, 0x120636c0], [5, 0x1207cf00],
      [5, 0x12096740], [5, 0x120aff80], [5, 0x120c97c0], [5, 0x120e3000], [5, 0x120fc840]]}
  e2e_layernorm_425.dc.multiply.8_0: {input: layernorm_425.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x123e99c0], [2, 0x124b5a00]]}
  e2e_layernorm_425.dc.reciprocal.7_0: {input: layernorm_425.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x12116080], [5, 0x1211c6c0]]}
  e2e_matmul_416_0: {input: matmul_416, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x123a6f80],
      [0, 0x12472fc0]]}
  e2e_add_411_0: {input: add_411, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x11691fc0], [
        1, 0x1175e000]]}
  e2e_softmax_405.dc.multiply.3_0: {input: softmax_405.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11fa2500], [3, 0x1246a540]]}
  e2e_bw_in0_matmul_416_matmul_1_0: {input: bw_in0_matmul_416_matmul_1, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x11cdeb80], [4, 0x121a6bc0]]}
  e2e_bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.lc1_0: {input: bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1],
    ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x12581a40], [2, 0x125e7a80]]}
  e2e_add_397_0: {input: add_397, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x12932580], [
        3, 0x129fe5c0]]}
  e2e_add_391_0: {input: add_391, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1264dac0], [
        2, 0x12719b00]]}
  e2e_bw_in0_matmul_409_matmul_1_0: {input: bw_in0_matmul_409_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x12aca600], [3, 0x12b30640], [3, 0x12b96680], [3, 0x12bfc6c0]]}
  e2e_bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1182a040], [1, 0x118f6080]]}
  e2e_layernorm_386.dc.multiply.8_0: {input: layernorm_386.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1253f000], [0, 0x1260b040]]}
  e2e_layernorm_386.dc.reciprocal.7_0: {input: layernorm_386.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x12122d00], [5, 0x12129340]]}
  e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1212f980], [5, 0x121fb9c0]]}
  e2e_add_377_0: {input: add_377, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x126d7080], [
        0, 0x127a30c0], [0, 0x1286f100], [0, 0x1293b140], [0, 0x12a07180], [0, 0x12ad31c0], [0, 0x12b9f200], [0, 0x12c6b240]]}
  e2e_bw_in0_gelu_378_multiply_1_0: {input: bw_in0_gelu_378_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x12d37280], [0, 0x12e032c0], [0, 0x12ecf300], [0, 0x12f9b340], [0, 0x13067380], [0, 0x131333c0], [0, 0x131ff400], [0, 0x132cb440]]}
  e2e_layernorm_372.dc.multiply.8_0: {input: layernorm_372.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x12c62700], [3, 0x12d2e740]]}
  e2e_layernorm_372.dc.reciprocal.7_0: {input: layernorm_372.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x127e5b40], [2, 0x127ec180]]}
  e2e_matmul_363_0: {input: matmul_363, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1266ec00],
      [4, 0x1273ac40]]}
  e2e_bw_in0_matmul_367_matmul_1_0: {input: bw_in0_matmul_367_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x127f27c0], [2, 0x12858800], [2, 0x128be840], [2, 0x12924880]]}
  e2e_add_358_0: {input: add_358, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x13397480], [
        0, 0x134634c0]]}
  e2e_softmax_352.dc.multiply.3_0: {input: softmax_352.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x119c20c0], [1, 0x11e8a100]]}
  e2e_add_344_0: {input: add_344, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1298a8c0], [
        2, 0x12a56900]]}
  e2e_add_338_0: {input: add_338, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x122c7a00], [
        5, 0x12393a40]]}
  e2e_bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12352140], [1, 0x1241e180]]}
  e2e_bw_in0_layernorm_333_combine_add_0_0: {input: bw_in0_layernorm_333_combine_add_0, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x1352f500], [0, 0x135fb540]]}
  e2e_layernorm_333.dc.multiply.8_0: {input: layernorm_333.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1245fa80], [5, 0x1252bac0]]}
  e2e_layernorm_333.dc.reciprocal.7_0: {input: layernorm_333.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x12dfa780], [3, 0x12e00dc0]]}
  e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.lc1_0: {input: bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [
      2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x12e07400], [3, 0x12e0da40]]}
  e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1_0: {input: bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [
      2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12806c80], [4, 0x1280d2c0]]}
  e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0_0: {input: bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x125f7b00], [5, 0x126c3b40]]}
  e2e_layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {input: layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x124ea1c0], [1, 0x124f0800]]}
  e2e_gelu_325_0: {input: gelu_325, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x136c7580],
      [0, 0x137935c0], [0, 0x1385f600], [0, 0x1392b640], [0, 0x139f7680], [0, 0x13ac36c0], [0, 0x13b8f700], [0, 0x13c5b740]]}
  e2e_add_324_0: {input: add_324, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1278fb80], [
        5, 0x1285bbc0], [5, 0x12927c00], [5, 0x129f3c40], [5, 0x12abfc80], [5, 0x12b8bcc0], [5, 0x12c57d00], [5, 0x12d23d40]]}
  e2e_bw_in0_gelu_325_multiply_1_0: {input: bw_in0_gelu_325_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x12e14080], [3, 0x12ee00c0], [3, 0x12fac100], [3, 0x13078140], [3, 0x13144180], [3, 0x132101c0], [3, 0x132dc200], [3, 0x133a8240]]}
  e2e_layernorm_319.dc.add.10_0: {input: layernorm_319.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13474280], [3, 0x135402c0]]}
  e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x12b22940], [2, 0x12bee980]]}
  e2e_layernorm_319.dc.multiply.8_0: {input: layernorm_319.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x12cba9c0], [2, 0x12d86a00]]}
  e2e_layernorm_319.dc.reciprocal.7_0: {input: layernorm_319.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x124f6e40], [1, 0x124fd480]]}
  e2e_matmul_310_0: {input: matmul_310, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1360c300],
      [3, 0x136d8340]]}
  e2e_bw_in0_matmul_314_matmul_1_0: {input: bw_in0_matmul_314_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x12defd80], [5, 0x12e55dc0], [5, 0x12ebbe00], [5, 0x12f21e40]]}
  e2e_add_305_0: {input: add_305, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x137a4380], [
        3, 0x138703c0]]}
  e2e_softmax_299.dc.multiply.3_0: {input: softmax_299.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x12e52a40], [2, 0x1331aa80]]}
  e2e_layernorm_280.dc.add.10_0: {input: layernorm_280.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x12f87e80], [5, 0x13053ec0]]}
  e2e_add_291_0: {input: add_291, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12503ac0], [
        1, 0x125cfb00]]}
  e2e_add_285_0: {input: add_285, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x13d27780], [
        0, 0x13df37c0]]}
  e2e_bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12813900], [4, 0x128df940]]}
  e2e_bw_in0_layernorm_280_combine_add_0_0: {input: bw_in0_layernorm_280_combine_add_0, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x13ebf800], [0, 0x13f8b840]]}
  e2e_layernorm_280.dc.multiply.8_0: {input: layernorm_280.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x129ab980], [4, 0x12a779c0]]}
  e2e_layernorm_280.dc.reciprocal.7_0: {input: layernorm_280.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12b43a00], [4, 0x12b4a040]]}
  e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.lc1_0: {input: bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [
      2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12b50680], [4, 0x12b56cc0]]}
  e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1_0: {input: bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [
      2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1393c400], [3, 0x13942a40]]}
  e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0_0: {input: bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x137e2ac0], [2, 0x138aeb00]]}
  e2e_layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {input: layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1269bb40], [1, 0x126a2180]]}
  e2e_gelu_272_0: {input: gelu_272, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x126a87c0],
      [1, 0x12774800], [1, 0x12840840], [1, 0x1290c880], [1, 0x129d88c0], [1, 0x12aa4900], [1, 0x12b70940], [1, 0x12c3c980]]}
  e2e_add_271_0: {input: add_271, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x14057880], [
        0, 0x141238c0], [0, 0x141ef900], [0, 0x142bb940], [0, 0x14387980], [0, 0x144539c0], [0, 0x1451fa00], [0, 0x145eba40]]}
  e2e_bw_in0_gelu_272_multiply_1_0: {input: bw_in0_gelu_272_multiply_1, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1311ff00], [5, 0x131ebf40], [5, 0x132b7f80], [5, 0x13383fc0], [5, 0x13450000], [5, 0x1351c040], [5, 0x135e8080], [5, 0x136b40c0]]}
  e2e_layernorm_266.dc.add.10_0: {input: layernorm_266.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1397ab40], [2, 0x13a46b80]]}
  e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9_0: {input: bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x146b7a80], [0, 0x14783ac0]]}
  e2e_layernorm_266.dc.multiply.8_0: {input: layernorm_266.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1484fb00], [0, 0x1491bb40]]}
  e2e_layernorm_266.dc.reciprocal.7_0: {input: layernorm_266.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x12d089c0], [1, 0x12d0f000]]}
  e2e_matmul_257_0: {input: matmul_257, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13b12bc0],
      [2, 0x13bdec00]]}
  e2e_bw_in0_matmul_261_matmul_1_0: {input: bw_in0_matmul_261_matmul_1, type: queue, entries: 2, grid_size: [2, 2], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x12d15640], [1, 0x12d7b680], [1, 0x12de16c0], [1, 0x12e47700]]}
  e2e_add_252_0: {input: add_252, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x13949080], [
        3, 0x13a150c0]]}
  e2e_softmax_246.dc.multiply.3_0: {input: softmax_246.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12b5d300], [4, 0x13025340]]}
  e2e_add_238_0: {input: add_238, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13780100], [
        5, 0x1384c140]]}
  e2e_add_232_0: {input: add_232, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x149e7b80], [
        0, 0x14ab3bc0]]}
  e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [
      1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13caac40], [2, 0x13cb3480]]}
  e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0: {input: opt_in0_layer.1.attention.self.query.weight_multiply_1, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [
      2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x13ae1100], [3, 0x13b25140], [3, 0x13b69180], [3, 0x13bad1c0], [3, 0x13bf1200], [3, 0x13c35240], [3, 0x13c79280], [
        3, 0x13cbd2c0]]}
  e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [
      1, 16], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x134ed380], [4, 0x1350f3c0]]}
  e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [
      1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13918180], [5, 0x139209c0]]}
  e2e_opt_in0_layer.2.attention.self.query.weight_multiply_1_0: {input: opt_in0_layer.2.attention.self.query.weight_multiply_1, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [
      2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x14b7fc00], [0, 0x14bc3c40], [0, 0x14c07c80], [0, 0x14c4bcc0], [0, 0x14c8fd00], [0, 0x14cd3d40], [0, 0x14d17d80], [
        0, 0x14d5bdc0]]}
  e2e_input_opt_layer.2.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.2.intermediate.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [
      1, 16], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12ead740], [1, 0x12ecf780]]}
  e2e_input_opt_layer.2.output.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.2.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [
      1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13cbbcc0], [2, 0x13cc4500]]}
  e2e_opt_in0_layer.3.attention.self.query.weight_multiply_1_0: {input: opt_in0_layer.3.attention.self.query.weight_multiply_1, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [16, 1], ublock: [
      2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x13d01300], [3, 0x13d45340], [3, 0x13d89380], [3, 0x13dcd3c0], [3, 0x13e11400], [3, 0x13e55440], [3, 0x13e99480], [
        3, 0x13edd4c0]]}
  e2e_input_opt_layer.3.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.3.intermediate.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [
      1, 16], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x13531400], [4, 0x13553440]]}
  e2e_input_opt_layer.3.output.dense.weight_0.lr_splt_brcst_3_0_0: {input: input_opt_layer.3.output.dense.weight_0.lr_splt_brcst_3_0, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [
      1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13929200], [5, 0x13931a40]]}

  # optimizer_parameter
  input_opt_layer.0.attention.self.query.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x13cccd40]]}
  input_opt_layer.0.attention.self.query.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef17c0]]}
  input_opt_layer.0.attention.self.key.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x14d9fe00]]}
  input_opt_layer.0.attention.self.key.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1393a280]]}
  input_opt_layer.0.attention.self.value.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13575480]]}
  input_opt_layer.0.attention.self.value.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f21500]]}
  input_opt_layer.0.attention.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x13ccd600]]}
  input_opt_layer.0.attention.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef2080]]}
  input_opt_layer.0.attention.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x14da06c0]]}
  input_opt_layer.0.attention.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x14da0f80]]}
  input_opt_layer.0.intermediate.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13575d40]]}
  input_opt_layer.0.intermediate.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f21dc0]]}
  input_opt_layer.0.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x13ccdec0]]}
  input_opt_layer.0.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12ef2940]]}
  input_opt_layer.0.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x14da1840]]}
  input_opt_layer.0.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1393ab40]]}
  input_opt_layer.1.attention.self.query.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13576600]]}
  input_opt_layer.1.attention.self.query.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f22680]]}
  input_opt_layer.1.attention.self.key.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x13cce780]]}
  input_opt_layer.1.attention.self.key.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef3200]]}
  input_opt_layer.1.attention.self.value.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f22f40]]}
  input_opt_layer.1.attention.self.value.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f23800]]}
  input_opt_layer.1.attention.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x13ccf040]]}
  input_opt_layer.1.attention.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef3ac0]]}
  input_opt_layer.1.attention.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x14da2100]]}
  input_opt_layer.1.attention.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1393b400]]}
  input_opt_layer.1.intermediate.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13576ec0]]}
  input_opt_layer.1.intermediate.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f240c0]]}
  input_opt_layer.1.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x13ccf900]]}
  input_opt_layer.1.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12ef4380]]}
  input_opt_layer.1.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x14da29c0]]}
  input_opt_layer.1.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x14da3280]]}
  input_opt_layer.2.attention.self.query.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13577780]]}
  input_opt_layer.2.attention.self.query.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f24980]]}
  input_opt_layer.2.attention.self.key.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x13cd01c0]]}
  input_opt_layer.2.attention.self.key.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef4c40]]}
  input_opt_layer.2.attention.self.value.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x14da3b40]]}
  input_opt_layer.2.attention.self.value.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1393bcc0]]}
  input_opt_layer.2.attention.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13578040]]}
  input_opt_layer.2.attention.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f25240]]}
  input_opt_layer.2.attention.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x13cd0a80]]}
  input_opt_layer.2.attention.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x12ef5500]]}
  input_opt_layer.2.intermediate.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1393c580]]}
  input_opt_layer.2.intermediate.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef5dc0]]}
  input_opt_layer.2.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14da4400]]}
  input_opt_layer.2.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1393ce40]]}
  input_opt_layer.2.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13578900]]}
  input_opt_layer.2.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f25b00]]}
  input_opt_layer.3.attention.self.query.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x13cd1340]]}
  input_opt_layer.3.attention.self.query.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef6680]]}
  input_opt_layer.3.attention.self.key.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x14da4cc0]]}
  input_opt_layer.3.attention.self.key.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1393d700]]}
  input_opt_layer.3.attention.self.value.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x135791c0]]}
  input_opt_layer.3.attention.self.value.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x13579a80]]}
  input_opt_layer.3.attention.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x13cd1c00]]}
  input_opt_layer.3.attention.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x12ef6f40]]}
  input_opt_layer.3.attention.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x14da5580]]}
  input_opt_layer.3.attention.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1393dfc0]]}
  input_opt_layer.3.intermediate.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1357a340]]}
  input_opt_layer.3.intermediate.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x13f263c0]]}
  input_opt_layer.3.output.dense.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x13cd24c0]]}
  input_opt_layer.3.output.dense.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12ef7800]]}
  input_opt_layer.3.output.LayerNorm.weight_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x14da5e40]]}
  input_opt_layer.3.output.LayerNorm.bias_0.lr: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1393e880]]}

  # loss
  loss_bert_encoder.output_layernorm_439: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1393f140]]}

  # grad_accumulator
  grad_acc_layer.3.output.LayerNorm.bias: {input: bw_in2_layernorm_439_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1357ac00]]}
  grad_acc_layer.3.output.LayerNorm.weight: {input: bw_in1_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13cd2d80]]}
  grad_acc_layer.3.output.dense.bias: {input: bw_in1_add_436_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x12ef80c0]]}
  grad_acc_layer.3.output.dense.weight: {input: bw_in1_matmul_434_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x14da6700], [0, 0x14e2e740], [0, 0x14eb6780], [0, 0x14f3e7c0], [0, 0x14fc6800], [0, 0x1504e840], [0, 0x150d6880], [0, 0x1515e8c0], [0, 0x151e6900], [0, 0x1526e940], [0, 0x152f6980],
      [0, 0x1537e9c0], [0, 0x15406a00], [0, 0x1548ea40], [0, 0x15516a80], [0, 0x1559eac0]]}
  grad_acc_layer.3.intermediate.dense.bias: {input: bw_in1_add_430_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x1358bc40]]}
  grad_acc_layer.3.intermediate.dense.weight: {input: bw_in1_matmul_428_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x13f26c80], [3, 0x13faecc0], [3, 0x14036d00], [3, 0x140bed40], [3, 0x14146d80], [3, 0x141cedc0], [3, 0x14256e00], [3, 0x142dee40], [3, 0x14366e80], [3, 0x143eeec0], [3, 0x14476f00],
      [3, 0x144fef40], [3, 0x14586f80], [3, 0x1460efc0], [3, 0x14697000], [3, 0x1471f040]]}
  grad_acc_layer.3.attention.output.LayerNorm.bias: {input: bw_in2_layernorm_425_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12f09100]]}
  grad_acc_layer.3.attention.output.LayerNorm.weight: {input: bw_in1_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x135cfc80]]}
  grad_acc_layer.3.attention.output.dense.bias: {input: bw_in1_add_422_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x147a7080]]}
  grad_acc_layer.3.attention.output.dense.weight: {input: bw_in1_matmul_420_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x13ce3dc0], [2, 0x13d6be00], [2, 0x13df3e40], [2, 0x13e7be80]]}
  grad_acc_layer.3.attention.self.value.bias: {input: bw_in1_add_411_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x15626b00]]}
  grad_acc_layer.3.attention.self.value.weight: {input: bw_in1_matmul_409_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x13c6f180], [5, 0x13cf71c0], [5, 0x13d7f200], [5, 0x13e07240]]}
  grad_acc_layer.3.attention.self.key.bias: {input: bw_in1_add_397_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x12f1a140]]}
  grad_acc_layer.3.attention.self.key.weight: {input: bw_in1_matmul_395_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x15637b40], [0, 0x156bfb80], [0, 0x15747bc0], [0, 0x157cfc00]]}
  grad_acc_layer.3.attention.self.query.bias: {input: bw_in1_add_391_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x135e0cc0]]}
  grad_acc_layer.3.attention.self.query.weight: {input: bw_in1_matmul_389_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x147b80c0], [3, 0x14840100], [3, 0x148c8140], [3, 0x14950180]]}
  grad_acc_layer.2.output.LayerNorm.bias: {input: bw_in2_layernorm_386_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13f03ec0]]}
  grad_acc_layer.2.output.LayerNorm.weight: {input: bw_in1_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x15857c40]]}
  grad_acc_layer.2.output.dense.bias: {input: bw_in1_add_383_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x13e8f280]]}
  grad_acc_layer.2.output.dense.weight: {input: bw_in1_matmul_381_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x135f1d00], [4, 0x13679d40], [4, 0x13701d80], [4, 0x13789dc0], [4, 0x13811e00], [4, 0x13899e40], [4, 0x13921e80], [4, 0x139a9ec0], [4, 0x13a31f00], [4, 0x13ab9f40], [4, 0x13b41f80],
      [4, 0x13bc9fc0], [4, 0x13c52000], [4, 0x13cda040], [4, 0x13d62080], [4, 0x13dea0c0]]}
  grad_acc_layer.2.intermediate.dense.bias: {input: bw_in1_add_377_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x13f14f00]]}
  grad_acc_layer.2.intermediate.dense.weight: {input: bw_in1_matmul_375_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x12f2b180], [1, 0x12fb31c0], [1, 0x1303b200], [1, 0x130c3240], [1, 0x1314b280], [1, 0x131d32c0], [1, 0x1325b300], [1, 0x132e3340], [1, 0x1336b380], [1, 0x133f33c0], [1, 0x1347b400],
      [1, 0x13503440], [1, 0x1358b480], [1, 0x136134c0], [1, 0x1369b500], [1, 0x13723540]]}
  grad_acc_layer.2.attention.output.LayerNorm.bias: {input: bw_in2_layernorm_372_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13ea02c0]]}
  grad_acc_layer.2.attention.output.LayerNorm.weight: {input: bw_in1_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x149d81c0]]}
  grad_acc_layer.2.attention.output.dense.bias: {input: bw_in1_add_369_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x13e72100]]}
  grad_acc_layer.2.attention.output.dense.weight: {input: bw_in1_matmul_367_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x149e9200], [3, 0x14a71240], [3, 0x14af9280], [3, 0x14b812c0]]}
  grad_acc_layer.2.attention.self.value.bias: {input: bw_in1_add_358_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x137ab580]]}
  grad_acc_layer.2.attention.self.value.weight: {input: bw_in1_matmul_356_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x15868c80], [0, 0x158f0cc0], [0, 0x15978d00], [0, 0x15a00d40]]}
  grad_acc_layer.2.attention.self.key.bias: {input: bw_in1_add_344_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x13f58f40]]}
  grad_acc_layer.2.attention.self.key.weight: {input: bw_in1_matmul_342_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x13f69f80], [2, 0x13ff1fc0], [2, 0x1407a000], [2, 0x14102040]]}
  grad_acc_layer.2.attention.self.query.bias: {input: bw_in1_add_338_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x13eb1300]]}
  grad_acc_layer.2.attention.self.query.weight: {input: bw_in1_matmul_336_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x13e83140], [4, 0x13f0b180], [4, 0x13f931c0], [4, 0x1401b200]]}
  grad_acc_layer.1.output.LayerNorm.bias: {input: bw_in2_layernorm_333_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1418a080]]}
  grad_acc_layer.1.output.LayerNorm.weight: {input: bw_in1_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x15a88d80]]}
  grad_acc_layer.1.output.dense.bias: {input: bw_in1_add_330_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x14c09300]]}
  grad_acc_layer.1.output.dense.weight: {input: bw_in1_matmul_328_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1419b0c0], [2, 0x14223100], [2, 0x142ab140], [2, 0x14333180], [2, 0x143bb1c0], [2, 0x14443200], [2, 0x144cb240], [2, 0x14553280], [2, 0x145db2c0], [2, 0x14663300], [2, 0x146eb340],
      [2, 0x14773380], [2, 0x147fb3c0], [2, 0x14883400], [2, 0x1490b440], [2, 0x14993480]]}
  grad_acc_layer.1.intermediate.dense.bias: {input: bw_in1_add_324_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x15a99dc0]]}
  grad_acc_layer.1.intermediate.dense.weight: {input: bw_in1_matmul_322_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x13ec2340], [5, 0x13f4a380], [5, 0x13fd23c0], [5, 0x1405a400], [5, 0x140e2440], [5, 0x1416a480], [5, 0x141f24c0], [5, 0x1427a500], [5, 0x14302540], [5, 0x1438a580], [5, 0x144125c0],
      [5, 0x1449a600], [5, 0x14522640], [5, 0x145aa680], [5, 0x146326c0], [5, 0x146ba700]]}
  grad_acc_layer.1.attention.output.LayerNorm.bias: {input: bw_in2_layernorm_319_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x14c1a340]]}
  grad_acc_layer.1.attention.output.LayerNorm.weight: {input: bw_in1_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x137bc5c0]]}
  grad_acc_layer.1.attention.output.dense.bias: {input: bw_in1_add_316_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x14a1b4c0]]}
  grad_acc_layer.1.attention.output.dense.weight: {input: bw_in1_matmul_314_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x14c2b380], [3, 0x14cb33c0], [3, 0x14d3b400], [3, 0x14dc3440]]}
  grad_acc_layer.1.attention.self.value.bias: {input: bw_in1_add_305_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x14742740]]}
  grad_acc_layer.1.attention.self.value.weight: {input: bw_in1_matmul_303_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x14a2c500], [2, 0x14ab4540], [2, 0x14b3c580], [2, 0x14bc45c0]]}
  grad_acc_layer.1.attention.self.key.bias: {input: bw_in1_add_291_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x137cd600]]}
  grad_acc_layer.1.attention.self.key.weight: {input: bw_in1_matmul_289_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x15adde00], [0, 0x15b65e40], [0, 0x15bede80], [0, 0x15c75ec0]]}
  grad_acc_layer.1.attention.self.query.bias: {input: bw_in1_add_285_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x140a3240]]}
  grad_acc_layer.1.attention.self.query.weight: {input: bw_in1_matmul_283_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x14e4b480], [3, 0x14ed34c0], [3, 0x14f5b500], [3, 0x14fe3540]]}
  grad_acc_layer.0.output.LayerNorm.bias: {input: bw_in2_layernorm_280_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x137de640]]}
  grad_acc_layer.0.output.LayerNorm.weight: {input: bw_in1_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x15cfdf00]]}
  grad_acc_layer.0.output.dense.bias: {input: bw_in1_add_277_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x140b4280]]}
  grad_acc_layer.0.output.dense.weight: {input: bw_in1_matmul_275_matmul_1, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [32, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1506b580], [3, 0x150f35c0], [3, 0x1517b600], [3, 0x15203640], [3, 0x1528b680], [3, 0x153136c0], [3, 0x1539b700], [3, 0x15423740], [3, 0x154ab780], [3, 0x155337c0], [3, 0x155bb800],
      [3, 0x15643840], [3, 0x156cb880], [3, 0x157538c0], [3, 0x157db900], [3, 0x15863940]]}
  grad_acc_layer.0.intermediate.dense.bias: {input: bw_in1_add_271_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x14753780]]}
  grad_acc_layer.0.intermediate.dense.weight: {input: bw_in1_matmul_269_matmul_1, type: ram, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 16], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x147977c0], [5, 0x1481f800], [5, 0x148a7840], [5, 0x1492f880], [5, 0x149b78c0], [5, 0x14a3f900], [5, 0x14ac7940], [5, 0x14b4f980], [5, 0x14bd79c0], [5, 0x14c5fa00], [5, 0x14ce7a40],
      [5, 0x14d6fa80], [5, 0x14df7ac0], [5, 0x14e7fb00], [5, 0x14f07b40], [5, 0x14f8fb80]]}
  grad_acc_layer.0.attention.output.LayerNorm.bias: {input: bw_in2_layernorm_266_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x140c52c0]]}
  grad_acc_layer.0.attention.output.LayerNorm.weight: {input: bw_in1_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14c4c600]]}
  grad_acc_layer.0.attention.output.dense.bias: {input: bw_in1_add_263_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x14c5d640]]}
  grad_acc_layer.0.attention.output.dense.weight: {input: bw_in1_matmul_261_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x15d0ef40], [0, 0x15d96f80], [0, 0x15e1efc0], [0, 0x15ea7000]]}
  grad_acc_layer.0.attention.self.value.bias: {input: bw_in1_add_252_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x140d6300]]}
  grad_acc_layer.0.attention.self.value.weight: {input: bw_in1_matmul_250_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x158eb980], [3, 0x159739c0], [3, 0x159fba00], [3, 0x15a83a40]]}
  grad_acc_layer.0.attention.self.key.bias: {input: bw_in1_add_238_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x15017bc0]]}
  grad_acc_layer.0.attention.self.key.weight: {input: bw_in1_matmul_236_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x140e7340], [4, 0x1416f380], [4, 0x141f73c0], [4, 0x1427f400]]}
  grad_acc_layer.0.attention.self.query.bias: {input: bw_in1_add_232_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x137ef680]]}
  grad_acc_layer.0.attention.self.query.weight: {input: bw_in1_matmul_230_matmul_1, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15b0ba80], [3, 0x15b93ac0], [3, 0x15c1bb00], [3, 0x15ca3b40]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 2
    matmul_230: {type: matmul, grid_loc: [0, 0], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0, layer.0.attention.self.query.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_232: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_230, layer.0.attention.self.query.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_236: {type: matmul, grid_loc: [0, 2], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.key.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0, layer.0.attention.self.key.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_238: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_236, layer.0.attention.self.key.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_242: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_232, add_238], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_244_fork_clone1647_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [input_1_multiply_244_fork_clone1647_tile_bcast_tile_bcast], t: 16,
      mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    input_1_multiply_244_fork_clone1647_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [input_1_multiply_244_fork_clone1647_tile_bcast_tile_bcast_splt_brcst_1_0],
      t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    multiply_244: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_242, input_1_multiply_244_fork_clone1647_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0], t: 16, mblock: [3,
        3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    add_245: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_244, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_246.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_245], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_246.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_246.dc.reduce_sum.1.0], t: 16, mblock: [1, 1], ublock: [
        1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    softmax_246.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [2, 1], inputs: [softmax_246.dc.exp.0, lc.input_tensor.softmax_246.dc.reduce_sum.1.0_splt_brcst_1_0], t: 16, mblock: [3,
        1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}], attributes: {m_k: 1, u_kt: 12}}
    softmax_246.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_246.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_246.dc.multiply.3: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_246.dc.exp.0, softmax_246.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_250: {type: matmul, grid_loc: [0, 4], grid_size: [2, 2], inputs: [hidden_states, layer.0.attention.self.value.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0, layer.0.attention.self.value.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_252: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_250, layer.0.attention.self.value.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_257: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_246.dc.multiply.3, add_252], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_261: {type: matmul, grid_loc: [4, 6], grid_size: [2, 2], inputs: [matmul_257, layer.0.attention.output.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16], attributes: {m_k: 32, u_kt: 1}}
    layer.0.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.0.attention.output.dense.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_263: {type: add, grid_loc: [5, 3], grid_size: [2, 1], inputs: [matmul_261, layer.0.attention.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    add_265: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [add_263, hidden_states], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_266.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [add_265, lc.input_tensor.layernorm_266.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_266.dc.subtract.1: {type: subtract, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_265, layernorm_266.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_266.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_266.dc.subtract.1, layernorm_266.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_266.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [2, 1], inputs: [layernorm_266.dc.multiply.2, lc.input_tensor.layernorm_266.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_266.dc.add.5: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_266.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_266.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_266.dc.sqrt.6: {type: sqrt, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_266.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_266.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_266.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_266.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_266.dc.reciprocal.7, lc.input_tensor.layernorm_266.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_266.dc.subtract.1_buffer_1_layernorm_266.dc.subtract.1_layernorm_266.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_266.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_266.dc.subtract.1_layernorm_266.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_266.dc.subtract.1_buffer_1_layernorm_266.dc.subtract.1_layernorm_266.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_266.dc.subtract.1_layernorm_266.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_266.dc.subtract.1_layernorm_266.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_266.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_266.dc.subtract.1_layernorm_266.dc.multiply.8, layernorm_266.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0,
        layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_266.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_266.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_1:
    target_device: 0
    input_count: 2
    layernorm_266.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_266.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    matmul_269: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_266.dc.add.10, layer.0.intermediate.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.0.intermediate.dense.bias],
      t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_271: {type: add, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_269, layer.0.intermediate.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    gelu_272: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_271], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_275: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_272, layer.0.output.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 16, u_kt: 8}}
    layer.0.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0, layer.0.output.dense.bias], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1,
        u_kt: 1}}
    add_277: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_275, layer.0.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_266.dc.add.10_add_279: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_266.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_266.dc.add.10_add_279: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_266.dc.add.10_add_279], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        72], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_279: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_277, buffer_0_layernorm_266.dc.add.10_add_279], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_280.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [add_279, lc.input_tensor.layernorm_280.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_280.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_279, layernorm_280.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_280.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_280.dc.subtract.1, layernorm_280.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_280.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_280.dc.multiply.2, lc.input_tensor.layernorm_280.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_280.dc.add.5: {type: add, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_280.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_280.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_280.dc.sqrt.6: {type: sqrt, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_280.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_280.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2:
    target_device: 0
    input_count: 2
    layernorm_280.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_280.dc.sqrt.6_0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_280.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_280.dc.reciprocal.7, lc.input_tensor.layernorm_280.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_280.dc.multiply.8: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8_0, layernorm_280.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_280.dc.multiply.9: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_280.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.0.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_280.dc.add.10: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_280.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_283: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [layernorm_280.dc.add.10, layer.1.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0, layer.1.attention.self.query.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_285: {type: add, grid_loc: [1, 3], grid_size: [2, 1], inputs: [matmul_283, layer.1.attention.self.query.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_289: {type: matmul, grid_loc: [2, 4], grid_size: [2, 2], inputs: [layernorm_280.dc.add.10, layer.1.attention.self.key.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0, layer.1.attention.self.key.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_291: {type: add, grid_loc: [1, 7], grid_size: [2, 1], inputs: [matmul_289, layer.1.attention.self.key.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_295: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_285, add_291], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_297_fork_clone1670_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [input_1_multiply_297_fork_clone1670_tile_bcast_tile_bcast], t: 16,
      mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    input_1_multiply_297_fork_clone1670_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [input_1_multiply_297_fork_clone1670_tile_bcast_tile_bcast_splt_brcst_1_0],
      t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    multiply_297: {type: multiply, grid_loc: [3, 6], grid_size: [2, 1], inputs: [matmul_295, input_1_multiply_297_fork_clone1670_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0], t: 16, mblock: [3,
        3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    add_298: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [multiply_297, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_299.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_298], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_299.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_299.dc.reduce_sum.1.0], t: 16, mblock: [1, 1], ublock: [
        1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    softmax_299.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_299.dc.exp.0, lc.input_tensor.softmax_299.dc.reduce_sum.1.0_splt_brcst_1_0], t: 16, mblock: [3,
        1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}], attributes: {m_k: 1, u_kt: 12}}
    softmax_299.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_299.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_299.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_299.dc.exp.0, softmax_299.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_303: {type: matmul, grid_loc: [5, 6], grid_size: [2, 2], inputs: [layernorm_280.dc.add.10, layer.1.attention.self.value.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0, layer.1.attention.self.value.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_305: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_303, layer.1.attention.self.value.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_310: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [softmax_299.dc.multiply.3, add_305], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_314: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [matmul_310, layer.1.attention.output.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16], attributes: {m_k: 32, u_kt: 1}}
    layer.1.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.1.attention.output.dense.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_316: {type: add, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_314, layer.1.attention.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_280.dc.add.10_add_318: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [layernorm_280.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_280.dc.add.10_add_318: {type: nop, grid_loc: [7, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_280.dc.add.10_add_318], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_318: {type: add, grid_loc: [7, 7], grid_size: [2, 1], inputs: [add_316, buffer_0_layernorm_280.dc.add.10_add_318], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_319.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [add_318, lc.input_tensor.layernorm_319.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_319.dc.subtract.1: {type: subtract, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_318, layernorm_319.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_319.dc.multiply.2: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_319.dc.subtract.1, layernorm_319.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_319.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3:
    target_device: 0
    input_count: 2
    layernorm_319.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_319.dc.multiply.2_0, lc.input_tensor.layernorm_319.dc.reduce_avg.3.0], t: 1, mblock: [3,
        1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_319.dc.add.5: {type: add, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_319.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_319.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_319.dc.sqrt.6: {type: sqrt, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_319.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_319.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_319.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_319.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_319.dc.reciprocal.7, lc.input_tensor.layernorm_319.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_319.dc.multiply.8: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8_0, layernorm_319.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0,
        layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_319.dc.multiply.9: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_319.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_319.dc.add.10: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_319.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_322: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [layernorm_319.dc.add.10, layer.1.intermediate.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.1.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 2], inputs: [lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.1.intermediate.dense.bias],
      t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_324: {type: add, grid_loc: [2, 3], grid_size: [2, 4], inputs: [matmul_322, layer.1.intermediate.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    gelu_325: {type: gelu, grid_loc: [6, 0], grid_size: [2, 4], inputs: [add_324], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_328: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_325, layer.1.output.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 16, u_kt: 8}}
    layer.1.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0, layer.1.output.dense.bias], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1,
        u_kt: 1}}
    add_330: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_328, layer.1.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    buffer_1_layernorm_319.dc.add.10_add_332: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_319.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_319.dc.add.10_add_332: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_319.dc.add.10_add_332], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        72], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_332: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_330, buffer_0_layernorm_319.dc.add.10_add_332], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_4:
    target_device: 0
    input_count: 2
    layernorm_333.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_332_0, lc.input_tensor.layernorm_333.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_333.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_332_0, layernorm_333.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_333.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_333.dc.subtract.1, layernorm_333.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_333.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_333.dc.multiply.2, lc.input_tensor.layernorm_333.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_333.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_333.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_333.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_333.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_333.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_333.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_333.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_333.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_333.dc.reciprocal.7, lc.input_tensor.layernorm_333.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_333.dc.subtract.1_buffer_1_layernorm_333.dc.subtract.1_layernorm_333.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_333.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_333.dc.subtract.1_layernorm_333.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_333.dc.subtract.1_buffer_1_layernorm_333.dc.subtract.1_layernorm_333.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_333.dc.subtract.1_layernorm_333.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_333.dc.subtract.1_layernorm_333.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_333.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_333.dc.subtract.1_layernorm_333.dc.multiply.8, layernorm_333.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_333.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_333.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.1.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_333.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_333.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_336: {type: matmul, grid_loc: [4, 0], grid_size: [2, 2], inputs: [layernorm_333.dc.add.10, layer.2.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.2.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_1_0.0, layer.2.attention.self.query.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_338: {type: add, grid_loc: [3, 6], grid_size: [2, 1], inputs: [matmul_336, layer.2.attention.self.query.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_342: {type: matmul, grid_loc: [4, 2], grid_size: [2, 2], inputs: [layernorm_333.dc.add.10, layer.2.attention.self.key.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.2.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_1_0.0, layer.2.attention.self.key.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_344: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_342, layer.2.attention.self.key.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_348: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_338, add_344], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_350_fork_clone1693_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [input_1_multiply_350_fork_clone1693_tile_bcast_tile_bcast], t: 16,
      mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    input_1_multiply_350_fork_clone1693_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [input_1_multiply_350_fork_clone1693_tile_bcast_tile_bcast_splt_brcst_1_0],
      t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    multiply_350: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_348, input_1_multiply_350_fork_clone1693_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0], t: 16, mblock: [3,
        3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    add_351: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_350, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_352.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_351], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_352.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_352.dc.reduce_sum.1.0], t: 16, mblock: [1, 1], ublock: [
        1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    softmax_352.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [softmax_352.dc.exp.0, lc.input_tensor.softmax_352.dc.reduce_sum.1.0_splt_brcst_1_0], t: 16, mblock: [3,
        1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}], attributes: {m_k: 1, u_kt: 12}}
    softmax_352.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 6], grid_size: [2, 1], inputs: [softmax_352.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_352.dc.multiply.3: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [softmax_352.dc.exp.0, softmax_352.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_356: {type: matmul, grid_loc: [8, 0], grid_size: [2, 2], inputs: [layernorm_333.dc.add.10, layer.2.attention.self.value.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.2.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_1_0.0, layer.2.attention.self.value.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_358: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [matmul_356, layer.2.attention.self.value.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_363: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [softmax_352.dc.multiply.3, add_358], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_367: {type: matmul, grid_loc: [8, 4], grid_size: [2, 2], inputs: [matmul_363, layer.2.attention.output.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16], attributes: {m_k: 32, u_kt: 1}}
    layer.2.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.2.attention.output.dense.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_369: {type: add, grid_loc: [8, 7], grid_size: [2, 1], inputs: [matmul_367, layer.2.attention.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_5:
    target_device: 0
    input_count: 2
    buffer_1_layernorm_333.dc.add.10_add_371: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_333.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_333.dc.add.10_add_371: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_333.dc.add.10_add_371], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_371: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_add_369_0, buffer_0_layernorm_333.dc.add.10_add_371], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_372.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [add_371, lc.input_tensor.layernorm_372.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_372.dc.subtract.1: {type: subtract, grid_loc: [0, 4], grid_size: [2, 1], inputs: [add_371, layernorm_372.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_372.dc.multiply.2: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_372.dc.subtract.1, layernorm_372.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_372.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_372.dc.multiply.2, lc.input_tensor.layernorm_372.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_372.dc.add.5: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_372.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_372.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_372.dc.sqrt.6: {type: sqrt, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_372.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_372.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_372.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_372.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_372.dc.reciprocal.7, lc.input_tensor.layernorm_372.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_372.dc.subtract.1_buffer_1_layernorm_372.dc.subtract.1_layernorm_372.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_372.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_372.dc.subtract.1_layernorm_372.dc.multiply.8: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_372.dc.subtract.1_buffer_1_layernorm_372.dc.subtract.1_layernorm_372.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_372.dc.subtract.1_layernorm_372.dc.multiply.8: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_372.dc.subtract.1_layernorm_372.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_372.dc.multiply.8: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_372.dc.subtract.1_layernorm_372.dc.multiply.8, layernorm_372.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0,
        layer.2.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_372.dc.multiply.9: {type: multiply, grid_loc: [3, 7], grid_size: [2, 1], inputs: [layernorm_372.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.2.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_372.dc.add.10: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_372.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_375: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [layernorm_372.dc.add.10, layer.2.intermediate.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.2.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 2], inputs: [lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.2.intermediate.dense.bias],
      t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_377: {type: add, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_375, layer.2.intermediate.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    gelu_378: {type: gelu, grid_loc: [8, 4], grid_size: [2, 4], inputs: [add_377], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_6:
    target_device: 0
    input_count: 2
    matmul_381: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_378_0, layer.2.output.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 16, u_kt: 8}}
    layer.2.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_1_0.0, layer.2.output.dense.bias], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1,
        u_kt: 1}}
    add_383: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_381, layer.2.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    add_385: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_383, e2e_layernorm_372.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_386.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_385, lc.input_tensor.layernorm_386.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_386.dc.subtract.1: {type: subtract, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_385, layernorm_386.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_386.dc.multiply.2: {type: multiply, grid_loc: [3, 0], grid_size: [2, 1], inputs: [layernorm_386.dc.subtract.1, layernorm_386.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_386.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_386.dc.multiply.2, lc.input_tensor.layernorm_386.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_386.dc.add.5: {type: add, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_386.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_386.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_386.dc.sqrt.6: {type: sqrt, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_386.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_386.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_386.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_386.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_386.dc.reciprocal.7, lc.input_tensor.layernorm_386.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_386.dc.subtract.1_buffer_1_layernorm_386.dc.subtract.1_layernorm_386.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_386.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_386.dc.subtract.1_layernorm_386.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_386.dc.subtract.1_buffer_1_layernorm_386.dc.subtract.1_layernorm_386.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_386.dc.subtract.1_layernorm_386.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_386.dc.subtract.1_layernorm_386.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_386.dc.multiply.8: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_386.dc.subtract.1_layernorm_386.dc.multiply.8, layernorm_386.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.2.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_386.dc.multiply.9: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_386.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.2.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_386.dc.add.10: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_386.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_389: {type: matmul, grid_loc: [6, 2], grid_size: [2, 2], inputs: [layernorm_386.dc.add.10, layer.3.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.3.attention.self.query.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_1_0.0, layer.3.attention.self.query.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_391: {type: add, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_389, layer.3.attention.self.query.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_395: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [layernorm_386.dc.add.10, layer.3.attention.self.key.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.3.attention.self.key.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_1_0.0, layer.3.attention.self.key.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_397: {type: add, grid_loc: [7, 4], grid_size: [2, 1], inputs: [matmul_395, layer.3.attention.self.key.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_401: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [add_391, add_397], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_403_fork_clone1716_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [input_1_multiply_403_fork_clone1716_tile_bcast_tile_bcast], t: 16,
      mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    input_1_multiply_403_fork_clone1716_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [input_1_multiply_403_fork_clone1716_tile_bcast_tile_bcast_splt_brcst_1_0],
      t: 16, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    multiply_403: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [matmul_401, input_1_multiply_403_fork_clone1716_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0], t: 16, mblock: [3,
        3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    add_404: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [multiply_403, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_405.dc.exp.0: {type: exp, grid_loc: [8, 6], grid_size: [2, 2], inputs: [add_404], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_405.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_405.dc.reduce_sum.1.0], t: 16, mblock: [1, 1], ublock: [
        1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}

  fwd_7:
    target_device: 0
    input_count: 2
    softmax_405.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_405.dc.exp.0_0, e2e_lc.input_tensor.softmax_405.dc.reduce_sum.1.0_splt_brcst_1_0_0], t: 16,
      mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}], attributes: {m_k: 1, u_kt: 12}}
    softmax_405.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_405.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_405.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_405.dc.exp.0_0, softmax_405.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_409: {type: matmul, grid_loc: [0, 3], grid_size: [2, 2], inputs: [e2e_layernorm_386.dc.add.10_0, layer.3.attention.self.value.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.3.attention.self.value.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_1_0.0, layer.3.attention.self.value.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_411: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_409, layer.3.attention.self.value.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_416: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_405.dc.multiply.3, add_411], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_420: {type: matmul, grid_loc: [2, 0], grid_size: [2, 2], inputs: [matmul_416, layer.3.attention.output.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16], attributes: {m_k: 32, u_kt: 1}}
    layer.3.attention.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_1_0.0, layer.3.attention.output.dense.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    add_422: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_420, layer.3.attention.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    add_424: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_422, e2e_layernorm_386.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_425.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_424, lc.input_tensor.layernorm_425.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_425.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_424, layernorm_425.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_425.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_425.dc.subtract.1, layernorm_425.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_425.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_425.dc.multiply.2, lc.input_tensor.layernorm_425.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_425.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_425.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_425.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_425.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_425.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_425.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_425.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_425.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_425.dc.reciprocal.7, lc.input_tensor.layernorm_425.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_425.dc.subtract.1_buffer_1_layernorm_425.dc.subtract.1_layernorm_425.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_425.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_425.dc.subtract.1_layernorm_425.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_425.dc.subtract.1_buffer_1_layernorm_425.dc.subtract.1_layernorm_425.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_425.dc.subtract.1_layernorm_425.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_425.dc.subtract.1_layernorm_425.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_425.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_425.dc.subtract.1_layernorm_425.dc.multiply.8, layernorm_425.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0,
        layer.3.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_425.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_425.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.3.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_425.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_425.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_428: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [layernorm_425.dc.add.10, layer.3.intermediate.dense.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 32, u_kt: 1}}
    layer.3.intermediate.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 2], inputs: [lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_1_0.0, layer.3.intermediate.dense.bias],
      t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_8:
    target_device: 0
    input_count: 2
    add_430: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_428_0, e2e_layer.3.intermediate.dense.bias_s_brcst_m2_1_0.lc1_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    gelu_431: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_430], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_434: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_431, layer.3.output.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 16, u_kt: 8}}
    layer.3.output.dense.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_1_0.0, layer.3.output.dense.bias], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 1,
        u_kt: 1}}
    add_436: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_434, layer.3.output.dense.bias_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    add_438: {type: add, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_436, e2e_layernorm_425.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_439.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_438, lc.input_tensor.layernorm_439.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_439.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_438, layernorm_439.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_439.dc.multiply.2: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_439.dc.subtract.1, layernorm_439.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_439.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_439.dc.multiply.2, lc.input_tensor.layernorm_439.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_439.dc.add.5: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_439.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_439.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_439.dc.sqrt.6: {type: sqrt, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_439.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_439.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_439.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_439.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_439.dc.reciprocal.7, lc.input_tensor.layernorm_439.dc.reciprocal.7_s_brcst_m1_1_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_439.dc.subtract.1_buffer_1_layernorm_439.dc.subtract.1_layernorm_439.dc.multiply.8: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_439.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_439.dc.subtract.1_layernorm_439.dc.multiply.8: {type: nop, grid_loc: [6, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_439.dc.subtract.1_buffer_1_layernorm_439.dc.subtract.1_layernorm_439.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_439.dc.subtract.1_layernorm_439.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_439.dc.subtract.1_layernorm_439.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_439.dc.multiply.8: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_439.dc.subtract.1_layernorm_439.dc.multiply.8, layernorm_439.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_1_0.0, layer.3.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_439.dc.multiply.9: {type: multiply, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_439.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_1_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_1_0.0, layer.3.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_439.dc.add.10: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_439.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_1_0.lc1], untilize_output: true, t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}

  bwd_9:
    target_device: 0
    input_count: 2
    bw_in2_layernorm_439_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_439_layernorm_bw_0.dc.reduce_sum.0.0, loss_bert_encoder.output_layernorm_439],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_439_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_439.dc.multiply.8_0, loss_bert_encoder.output_layernorm_439], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_439_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_439.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_439.dc.reciprocal.7_0, lc.input_tensor.layernorm_439.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.3.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.3.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [loss_bert_encoder.output_layernorm_439, layer.3.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.0, e2e_layernorm_439.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_439.dc.multiply.8_0, bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_439_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [1, 1], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.6, bw_in0_layernorm_439_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_439_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [2, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_439.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_439_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_436_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_436_brcst_reduce_sum_0.0, bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_434_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9, layer.3.output.dense.weight], t: 1, mblock: [3, 4], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_434_transpose_0: {type: nop, grid_loc: [6, 0], grid_size: [2, 4], inputs: [e2e_gelu_431_0], t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_434_matmul_1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_434_transpose_0, bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12,
        u_kt: 1}}

  bwd_10:
    target_device: 0
    input_count: 2
    bw_in0_gelu_431_gelu_derivative_0: {type: gelu_derivative, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_add_430_0], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    bw_in0_gelu_431_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [2, 4], inputs: [bw_in0_gelu_431_gelu_derivative_0, e2e_bw_in0_matmul_434_matmul_1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_430_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_430_brcst_reduce_sum_0.0, bw_in0_gelu_431_multiply_1], gradient_op: true,
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 12}], attributes: {m_k: 12, u_kt: 1}}
    bw_in0_matmul_428_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in0_gelu_431_multiply_1, layer.3.intermediate.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_428_transpose_0: {type: nop, grid_loc: [2, 5], grid_size: [1, 2], inputs: [e2e_layernorm_425.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}

  bwd_11:
    target_device: 0
    input_count: 2
    bw_in1_matmul_428_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 2], inputs: [e2e_bw_in1_matmul_428_transpose_0_0, e2e_bw_in0_gelu_431_multiply_1_0], gradient_op: true, t: 1, mblock: [2,
        16], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_425_combine_add_0: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_428_matmul_1_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_425_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_425_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_425_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_425_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [3, 4], grid_size: [2, 1], inputs: [e2e_layernorm_425.dc.multiply.8_0, bw_in0_layernorm_425_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_425_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_425.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_425.dc.reciprocal.7_0, lc.input_tensor.layernorm_425.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0,
        layer.3.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_425_combine_add_0, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.0, e2e_layernorm_425.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [e2e_layernorm_425.dc.multiply.8_0, bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_425_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.6, bw_in0_layernorm_425_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_425_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [2, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_425.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_425_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_422_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_422_brcst_reduce_sum_0.0, bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_420_matmul_1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 2], inputs: [bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9, layer.3.attention.output.dense.weight], t: 1, mblock: [3,
        4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose],
      attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_420_transpose_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 2], inputs: [e2e_matmul_416_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_420_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_420_transpose_0, bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4, u_kt: 3}}
    bw_in0_matmul_416_matmul_1: {type: matmul, grid_loc: [7, 2], grid_size: [2, 1], inputs: [bw_in0_matmul_420_matmul_1, e2e_add_411_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1,
        u_kt: 2}}
    bw_in1_matmul_416_transpose_0: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [e2e_softmax_405.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_416_matmul_1: {type: matmul, grid_loc: [6, 7], grid_size: [2, 1], inputs: [bw_in1_matmul_416_transpose_0, bw_in0_matmul_420_matmul_1], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_411_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_411_brcst_reduce_sum_0.0, bw_in1_matmul_416_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_409_add_411_unsqueeze3_746_squeeze_0: {type: nop, grid_loc: [7, 3], grid_size: [2, 1], inputs: [bw_in1_matmul_416_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16]}
    bw_in0_matmul_409_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 2], inputs: [bw_in0_matmul_409_add_411_unsqueeze3_746_squeeze_0, layer.3.attention.self.value.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_409_transpose_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 2], inputs: [e2e_layernorm_386.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_409_matmul_1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_409_transpose_0, bw_in0_matmul_409_add_411_unsqueeze3_746_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in0_softmax_405_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [bw_in0_matmul_416_matmul_1, e2e_softmax_405.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.0],
      t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [bw_in0_softmax_405_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
      t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}], attributes: {m_k: 1, u_kt: 12}}

  bwd_12:
    target_device: 0
    input_count: 2
    bw_in0_softmax_405_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_416_matmul_1_0, e2e_bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.lc1_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 12}]}
    bw_in0_softmax_405_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [bw_in0_softmax_405_softmax_bw_0.dc.subtract.2, e2e_softmax_405.dc.multiply.3_0], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_403_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_1_multiply_403_tile_bcast_tile_bcast], t: 16, mblock: [1, 1], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_403_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [input_1_multiply_403_tile_bcast_tile_bcast_splt_brcst_1_0], t: 16,
      mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    bw_in0_multiply_403_multiply_0: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_softmax_405_softmax_bw_0.dc.multiply.3, input_1_multiply_403_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_matmul_401_matmul_1: {type: matmul, grid_loc: [1, 2], grid_size: [2, 1], inputs: [bw_in0_multiply_403_multiply_0, e2e_add_397_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_401_transpose_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_add_391_0], t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_401_matmul_1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [bw_in1_matmul_401_transpose_0, bw_in0_multiply_403_multiply_0], t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_397_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_397_brcst_reduce_sum_0.0, bw_in1_matmul_401_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_395_add_397_unsqueeze3_734_squeeze_0: {type: nop, grid_loc: [1, 5], grid_size: [2, 1], inputs: [bw_in1_matmul_401_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_395_matmul_1: {type: matmul, grid_loc: [1, 6], grid_size: [2, 2], inputs: [bw_in0_matmul_395_add_397_unsqueeze3_734_squeeze_0, layer.3.attention.self.key.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_395_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_386.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_395_matmul_1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_395_transpose_0, bw_in0_matmul_395_add_397_unsqueeze3_734_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in1_add_391_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_391_brcst_reduce_sum_0.0, bw_in0_matmul_401_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_389_matmul_1: {type: matmul, grid_loc: [3, 4], grid_size: [2, 2], inputs: [bw_in0_matmul_401_matmul_1, layer.3.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], input_0_tms: [hstack: 16], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_389_transpose_0: {type: nop, grid_loc: [3, 6], grid_size: [1, 2], inputs: [e2e_layernorm_386.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_389_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_389_transpose_0, bw_in0_matmul_401_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in0_reshape_387_combine_add_0: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_409_matmul_1_0, bw_in0_matmul_395_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_reshape_387_combine_add_1: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_reshape_387_combine_add_0, bw_in0_matmul_389_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_386_combine_add_0: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9_0, bw_in0_reshape_387_combine_add_1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_386_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_386_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_386_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_386_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [7, 3], grid_size: [2, 1], inputs: [e2e_layernorm_386.dc.multiply.8_0, bw_in0_layernorm_386_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_386_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_386.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [e2e_layernorm_386.dc.reciprocal.7_0, lc.input_tensor.layernorm_386.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.2.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.2.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 2], grid_size: [2, 1], inputs: [bw_in0_layernorm_386_combine_add_0, layer.2.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [5, 4], grid_size: [2, 1], inputs: [bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.0, e2e_layernorm_386.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [e2e_layernorm_386.dc.multiply.8_0, bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_386_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [7, 0], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.6, bw_in0_layernorm_386_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_386_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [7, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [7, 2], grid_size: [2, 1], inputs: [layernorm_386.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_386_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_383_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_383_brcst_reduce_sum_0.0, bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}

  bwd_13:
    target_device: 0
    input_count: 2
    bw_in0_matmul_381_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0, layer.2.output.dense.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_381_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [2, 4], inputs: [e2e_gelu_378_0], t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_381_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_381_transpose_0, e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0], gradient_op: true, t: 1,
      mblock: [32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12,
        u_kt: 1}}
    bw_in0_gelu_378_gelu_derivative_0: {type: gelu_derivative, grid_loc: [6, 0], grid_size: [2, 8], inputs: [e2e_add_377_0], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    bw_in0_gelu_378_multiply_1: {type: multiply, grid_loc: [2, 4], grid_size: [2, 4], inputs: [bw_in0_gelu_378_gelu_derivative_0, bw_in0_matmul_381_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_377_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_377_brcst_reduce_sum_0.0, bw_in0_gelu_378_multiply_1], gradient_op: true,
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 12}], attributes: {m_k: 12, u_kt: 1}}

  bwd_14:
    target_device: 0
    input_count: 2
    bw_in0_matmul_375_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_gelu_378_multiply_1_0, layer.2.intermediate.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_375_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_372.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_375_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [8, 2], inputs: [bw_in1_matmul_375_transpose_0, e2e_bw_in0_gelu_378_multiply_1_0], gradient_op: true, t: 1, mblock: [2, 16], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_372_combine_add_0: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0, bw_in0_matmul_375_matmul_1], t: 1, mblock: [3,
        8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_372_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_372_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_372_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_372_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [e2e_layernorm_372.dc.multiply.8_0, bw_in0_layernorm_372_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_372_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_372.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_372.dc.reciprocal.7_0, lc.input_tensor.layernorm_372.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0,
        layer.2.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_372_combine_add_0, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.0, e2e_layernorm_372.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_372.dc.multiply.8_0, bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_372_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.6, bw_in0_layernorm_372_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_372_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_372.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_372_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_369_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_369_brcst_reduce_sum_0.0, bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_367_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9, layer.2.attention.output.dense.weight], t: 1, mblock: [3,
        4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose],
      attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_367_transpose_0: {type: nop, grid_loc: [7, 4], grid_size: [1, 2], inputs: [e2e_matmul_363_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_367_matmul_1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_367_transpose_0, bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4, u_kt: 3}}

  bwd_15:
    target_device: 0
    input_count: 2
    bw_in0_matmul_363_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_367_matmul_1_0, e2e_add_358_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1,
        u_kt: 2}}
    bw_in1_matmul_363_transpose_0: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_352.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_363_matmul_1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [bw_in1_matmul_363_transpose_0, e2e_bw_in0_matmul_367_matmul_1_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_358_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_358_brcst_reduce_sum_0.0, bw_in1_matmul_363_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_356_add_358_unsqueeze3_704_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in1_matmul_363_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16]}
    bw_in0_matmul_356_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 2], inputs: [bw_in0_matmul_356_add_358_unsqueeze3_704_squeeze_0, layer.2.attention.self.value.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_356_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_333.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_356_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_356_transpose_0, bw_in0_matmul_356_add_358_unsqueeze3_704_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in0_softmax_352_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_matmul_363_matmul_1, e2e_softmax_352.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_352_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_352_softmax_bw_0.dc.reduce_sum.1.0],
      t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    bw_in0_softmax_352_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [bw_in0_softmax_352_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_352_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
      t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}], attributes: {m_k: 1, u_kt: 12}}
    bw_in0_softmax_352_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [bw_in0_matmul_363_matmul_1, bw_in0_softmax_352_softmax_bw_0.dc.reduce_sum.1.lc1], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_1_tms: [broadcast: {c: 12}]}
    bw_in0_softmax_352_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [3, 0], grid_size: [2, 1], inputs: [bw_in0_softmax_352_softmax_bw_0.dc.subtract.2, e2e_softmax_352.dc.multiply.3_0], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_350_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [input_1_multiply_350_tile_bcast_tile_bcast], t: 16, mblock: [1, 1], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_350_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [input_1_multiply_350_tile_bcast_tile_bcast_splt_brcst_1_0], t: 16,
      mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    bw_in0_multiply_350_multiply_0: {type: multiply, grid_loc: [3, 3], grid_size: [2, 1], inputs: [bw_in0_softmax_352_softmax_bw_0.dc.multiply.3, input_1_multiply_350_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_matmul_348_matmul_1: {type: matmul, grid_loc: [3, 5], grid_size: [2, 1], inputs: [bw_in0_multiply_350_multiply_0, e2e_add_344_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_348_transpose_0: {type: nop, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e_add_338_0], t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_348_matmul_1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 2], inputs: [bw_in1_matmul_348_transpose_0, bw_in0_multiply_350_multiply_0], t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_344_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_344_brcst_reduce_sum_0.0, bw_in1_matmul_348_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_342_add_344_unsqueeze3_692_squeeze_0: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [bw_in1_matmul_348_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_342_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [bw_in0_matmul_342_add_344_unsqueeze3_692_squeeze_0, layer.2.attention.self.key.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_342_transpose_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 2], inputs: [e2e_layernorm_333.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_342_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_342_transpose_0, bw_in0_matmul_342_add_344_unsqueeze3_692_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in1_add_338_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_338_brcst_reduce_sum_0.0, bw_in0_matmul_348_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_336_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [bw_in0_matmul_348_matmul_1, layer.2.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], input_0_tms: [hstack: 16], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_336_transpose_0: {type: nop, grid_loc: [5, 4], grid_size: [1, 2], inputs: [e2e_layernorm_333.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_336_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_336_transpose_0, bw_in0_matmul_348_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in0_reshape_334_combine_add_0: {type: add, grid_loc: [7, 5], grid_size: [2, 1], inputs: [bw_in0_matmul_356_matmul_1, bw_in0_matmul_342_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_reshape_334_combine_add_1: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [bw_in0_reshape_334_combine_add_0, bw_in0_matmul_336_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_333_combine_add_0: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9_0, bw_in0_reshape_334_combine_add_1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [e2e_layernorm_333.dc.reciprocal.7_0, lc.input_tensor.layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.1.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [bw_in0_layernorm_333_combine_add_0, layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0, e2e_layernorm_333.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}

  bwd_16:
    target_device: 0
    input_count: 2
    bw_in2_layernorm_333_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_333_layernorm_bw_0.dc.reduce_sum.0.0, e2e_bw_in0_layernorm_333_combine_add_0_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_333_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [e2e_layernorm_333.dc.multiply.8_0, e2e_bw_in0_layernorm_333_combine_add_0_0], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_333_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_333.dc.multiply.8_0, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.lc1_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_333_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1_0, bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.6, bw_in0_layernorm_333_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_333_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0_0, bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, bw_in0_layernorm_333_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_330_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_330_brcst_reduce_sum_0.0, bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_328_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9, layer.1.output.dense.weight], t: 1, mblock: [3, 4], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_328_transpose_0: {type: nop, grid_loc: [4, 0], grid_size: [2, 4], inputs: [e2e_gelu_325_0], t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_328_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_328_transpose_0, bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12,
        u_kt: 1}}
    bw_in0_gelu_325_gelu_derivative_0: {type: gelu_derivative, grid_loc: [8, 0], grid_size: [2, 8], inputs: [e2e_add_324_0], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    bw_in0_gelu_325_multiply_1: {type: multiply, grid_loc: [4, 4], grid_size: [2, 4], inputs: [bw_in0_gelu_325_gelu_derivative_0, bw_in0_matmul_328_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_324_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_324_brcst_reduce_sum_0.0, bw_in0_gelu_325_multiply_1], gradient_op: true,
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 12}], attributes: {m_k: 12, u_kt: 1}}

  bwd_17:
    target_device: 0
    input_count: 2
    bw_in0_matmul_322_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_gelu_325_multiply_1_0, layer.1.intermediate.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_322_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_319.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_322_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [8, 2], inputs: [bw_in1_matmul_322_transpose_0, e2e_bw_in0_gelu_325_multiply_1_0], gradient_op: true, t: 1, mblock: [2, 16], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_319_combine_add_0: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9_0, bw_in0_matmul_322_matmul_1], t: 1, mblock: [3,
        8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_319_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_319_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_319_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_319_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [e2e_layernorm_319.dc.multiply.8_0, bw_in0_layernorm_319_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_319_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_319.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_319.dc.reciprocal.7_0, lc.input_tensor.layernorm_319.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0,
        layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_319_combine_add_0, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.0, e2e_layernorm_319.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_319.dc.multiply.8_0, bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_319_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.6, bw_in0_layernorm_319_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_319_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_319.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_319_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_316_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_316_brcst_reduce_sum_0.0, bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_314_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9, layer.1.attention.output.dense.weight], t: 1, mblock: [3,
        4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose],
      attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_314_transpose_0: {type: nop, grid_loc: [7, 4], grid_size: [1, 2], inputs: [e2e_matmul_310_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_314_matmul_1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_314_transpose_0, bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4, u_kt: 3}}

  bwd_18:
    target_device: 0
    input_count: 2
    bw_in0_matmul_310_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_314_matmul_1_0, e2e_add_305_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1,
        u_kt: 2}}
    bw_in1_matmul_310_transpose_0: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_299.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_310_matmul_1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [bw_in1_matmul_310_transpose_0, e2e_bw_in0_matmul_314_matmul_1_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_305_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_305_brcst_reduce_sum_0.0, bw_in1_matmul_310_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_303_add_305_unsqueeze3_662_squeeze_0: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in1_matmul_310_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hstack: 16]}
    bw_in0_matmul_303_matmul_1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 2], inputs: [bw_in0_matmul_303_add_305_unsqueeze3_662_squeeze_0, layer.1.attention.self.value.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_303_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_280.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_303_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_303_transpose_0, bw_in0_matmul_303_add_305_unsqueeze3_662_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in0_softmax_299_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_matmul_310_matmul_1, e2e_softmax_299.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_299_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_299_softmax_bw_0.dc.reduce_sum.1.0],
      t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    bw_in0_softmax_299_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [bw_in0_softmax_299_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_299_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
      t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}], attributes: {m_k: 1, u_kt: 12}}
    bw_in0_softmax_299_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [bw_in0_matmul_310_matmul_1, bw_in0_softmax_299_softmax_bw_0.dc.reduce_sum.1.lc1], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_1_tms: [broadcast: {c: 12}]}
    bw_in0_softmax_299_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [3, 0], grid_size: [2, 1], inputs: [bw_in0_softmax_299_softmax_bw_0.dc.subtract.2, e2e_softmax_299.dc.multiply.3_0], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_297_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [input_1_multiply_297_tile_bcast_tile_bcast], t: 16, mblock: [1, 1], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_297_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [input_1_multiply_297_tile_bcast_tile_bcast_splt_brcst_1_0], t: 16,
      mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    bw_in0_multiply_297_multiply_0: {type: multiply, grid_loc: [3, 3], grid_size: [2, 1], inputs: [bw_in0_softmax_299_softmax_bw_0.dc.multiply.3, input_1_multiply_297_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_matmul_295_matmul_1: {type: matmul, grid_loc: [3, 5], grid_size: [2, 1], inputs: [bw_in0_multiply_297_multiply_0, e2e_add_291_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_295_transpose_0: {type: nop, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e_add_285_0], t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_295_matmul_1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 2], inputs: [bw_in1_matmul_295_transpose_0, bw_in0_multiply_297_multiply_0], t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_291_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_291_brcst_reduce_sum_0.0, bw_in1_matmul_295_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_289_add_291_unsqueeze3_650_squeeze_0: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [bw_in1_matmul_295_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, hstack: 16]}
    bw_in0_matmul_289_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 2], inputs: [bw_in0_matmul_289_add_291_unsqueeze3_650_squeeze_0, layer.1.attention.self.key.weight], t: 1, mblock: [3, 4],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_289_transpose_0: {type: nop, grid_loc: [5, 2], grid_size: [1, 2], inputs: [e2e_layernorm_280.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_289_matmul_1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_289_transpose_0, bw_in0_matmul_289_add_291_unsqueeze3_650_squeeze_0], gradient_op: true, t: 1,
      mblock: [16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4,
        u_kt: 3}}
    bw_in1_add_285_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_285_brcst_reduce_sum_0.0, bw_in0_matmul_295_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_283_matmul_1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 2], inputs: [bw_in0_matmul_295_matmul_1, layer.1.attention.self.query.weight], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], input_0_tms: [hstack: 16], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_283_transpose_0: {type: nop, grid_loc: [5, 4], grid_size: [1, 2], inputs: [e2e_layernorm_280.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_283_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_283_transpose_0, bw_in0_matmul_295_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in0_reshape_281_combine_add_0: {type: add, grid_loc: [7, 5], grid_size: [2, 1], inputs: [bw_in0_matmul_303_matmul_1, bw_in0_matmul_289_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_reshape_281_combine_add_1: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [bw_in0_reshape_281_combine_add_0, bw_in0_matmul_283_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_280_combine_add_0: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9_0, bw_in0_reshape_281_combine_add_1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [e2e_layernorm_280.dc.reciprocal.7_0, lc.input_tensor.layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0, layer.0.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [bw_in0_layernorm_280_combine_add_0, layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0, e2e_layernorm_280.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}

  bwd_19:
    target_device: 0
    input_count: 2
    bw_in2_layernorm_280_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_280_layernorm_bw_0.dc.reduce_sum.0.0, e2e_bw_in0_layernorm_280_combine_add_0_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_280_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [e2e_layernorm_280.dc.multiply.8_0, e2e_bw_in0_layernorm_280_combine_add_0_0], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_280_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_280.dc.multiply.8_0, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.lc1_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_280_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1_0, bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.6, bw_in0_layernorm_280_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_280_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0_0, bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, bw_in0_layernorm_280_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_277_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_277_brcst_reduce_sum_0.0, bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_275_matmul_1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9, layer.0.output.dense.weight], t: 1, mblock: [3, 4], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {
        m_k: 32, u_kt: 1}}
    bw_in1_matmul_275_transpose_0: {type: nop, grid_loc: [4, 0], grid_size: [2, 4], inputs: [e2e_gelu_272_0], t: 1, mblock: [32, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_275_matmul_1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_275_transpose_0, bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        32, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12,
        u_kt: 1}}
    bw_in0_gelu_272_gelu_derivative_0: {type: gelu_derivative, grid_loc: [8, 0], grid_size: [2, 8], inputs: [e2e_add_271_0], t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    bw_in0_gelu_272_multiply_1: {type: multiply, grid_loc: [4, 4], grid_size: [2, 4], inputs: [bw_in0_gelu_272_gelu_derivative_0, bw_in0_matmul_275_matmul_1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_add_271_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_271_brcst_reduce_sum_0.0, bw_in0_gelu_272_multiply_1], gradient_op: true,
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 12}], attributes: {m_k: 12, u_kt: 1}}

  bwd_20:
    target_device: 0
    input_count: 2
    bw_in0_matmul_269_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_bw_in0_gelu_272_multiply_1_0, layer.0.intermediate.dense.weight], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose], attributes: {m_k: 16, u_kt: 8}}
    bw_in1_matmul_269_transpose_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 2], inputs: [e2e_layernorm_266.dc.add.10_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_269_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [8, 2], inputs: [bw_in1_matmul_269_transpose_0, e2e_bw_in0_gelu_272_multiply_1_0], gradient_op: true, t: 1, mblock: [2, 16], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 12, u_kt: 1}}
    bw_in0_layernorm_266_combine_add_0: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9_0, bw_in0_matmul_269_matmul_1], t: 1, mblock: [3,
        8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in2_layernorm_266_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_266_layernorm_bw_0.dc.reduce_sum.0.0, bw_in0_layernorm_266_combine_add_0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_layernorm_266_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 6], grid_size: [2, 1], inputs: [e2e_layernorm_266.dc.multiply.8_0, bw_in0_layernorm_266_combine_add_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in1_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_266_layernorm_bw_0.dc.multiply.0],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    layernorm_266.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_266.dc.reciprocal.7_0, lc.input_tensor.layernorm_266.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0,
        layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [bw_in0_layernorm_266_combine_add_0, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [3, 1], grid_size: [2, 1], inputs: [bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.0, e2e_layernorm_266.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [2, 1], inputs: [bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.3.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_266.dc.multiply.8_0, bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.3.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    bw_in0_layernorm_266_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.4],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [dc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.6, bw_in0_layernorm_266_layernorm_bw_0.dc.add.5],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    bw_in0_layernorm_266_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [5, 0], grid_size: [2, 1], inputs: [bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.0, bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.7],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [104, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3}
    bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_266.dc.reciprocal.7_s_brcst_m1_0_0.lc1, bw_in0_layernorm_266_layernorm_bw_0.dc.subtract.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [
        broadcast: {c: 32}]}
    bw_in1_add_263_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_263_brcst_reduce_sum_0.0, bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.9],
      gradient_op: true, t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in0_matmul_261_matmul_1: {type: matmul, grid_loc: [7, 0], grid_size: [2, 2], inputs: [bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.9, layer.0.attention.output.dense.weight], t: 1, mblock: [3,
        4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose],
      attributes: {m_k: 32, u_kt: 1}}
    bw_in1_matmul_261_transpose_0: {type: nop, grid_loc: [7, 4], grid_size: [1, 2], inputs: [e2e_matmul_257_0], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_261_matmul_1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_261_transpose_0, bw_in0_layernorm_266_layernorm_bw_0.dc.multiply.9], gradient_op: true, t: 1, mblock: [
        16, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 4, u_kt: 3}}

  bwd_21:
    target_device: 0
    input_count: 2
    bw_in0_matmul_257_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_bw_in0_matmul_261_matmul_1_0, e2e_add_252_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1,
        u_kt: 2}}
    bw_in1_matmul_257_transpose_0: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_softmax_246.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_257_matmul_1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [bw_in1_matmul_257_transpose_0, e2e_bw_in0_matmul_261_matmul_1_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_252_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_252_brcst_reduce_sum_0.0, bw_in1_matmul_257_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_250_transpose_0: {type: nop, grid_loc: [3, 3], grid_size: [1, 2], inputs: [hidden_states], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_250_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 4], inputs: [bw_in1_matmul_250_transpose_0, bw_in1_matmul_257_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in0_softmax_246_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [bw_in0_matmul_257_matmul_1, e2e_softmax_246.dc.multiply.3_0], t: 16, mblock: [3, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    lc.input_tensor.bw_in0_softmax_246_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_246_softmax_bw_0.dc.reduce_sum.1.0],
      t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            z: 16}]}
    bw_in0_softmax_246_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [bw_in0_softmax_246_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_246_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
      t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}], attributes: {m_k: 1, u_kt: 12}}
    bw_in0_softmax_246_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [0, 7], grid_size: [2, 1], inputs: [bw_in0_matmul_257_matmul_1, bw_in0_softmax_246_softmax_bw_0.dc.reduce_sum.1.lc1], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [232, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
      input_1_tms: [broadcast: {c: 12}]}
    bw_in0_softmax_246_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [1, 3], grid_size: [2, 1], inputs: [bw_in0_softmax_246_softmax_bw_0.dc.subtract.2, e2e_softmax_246.dc.multiply.3_0], t: 16,
      mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_244_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_1_multiply_244_tile_bcast_tile_bcast], t: 16, mblock: [1, 1], ublock: [1, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_244_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [input_1_multiply_244_tile_bcast_tile_bcast_splt_brcst_1_0], t: 16,
      mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {
            c: 12}]}
    bw_in0_multiply_244_multiply_0: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [bw_in0_softmax_246_softmax_bw_0.dc.multiply.3, input_1_multiply_244_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
      t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}
    bw_in0_matmul_242_matmul_1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [bw_in0_multiply_244_multiply_0, e2e_add_238_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_242_transpose_0: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [e2e_add_232_0], t: 16, mblock: [1, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_242_matmul_1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 2], inputs: [bw_in1_matmul_242_transpose_0, bw_in0_multiply_244_multiply_0], t: 16, mblock: [1, 3], ublock: [2, 2], buf_size_mb: 32,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 6}}
    bw_in1_add_238_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_238_brcst_reduce_sum_0.0, bw_in1_matmul_242_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        transpose, hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_236_transpose_0: {type: nop, grid_loc: [3, 5], grid_size: [1, 2], inputs: [hidden_states], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_236_matmul_1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 4], inputs: [bw_in1_matmul_236_transpose_0, bw_in1_matmul_242_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [transpose, hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}
    bw_in1_add_232_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_232_brcst_reduce_sum_0.0, bw_in0_matmul_242_matmul_1], gradient_op: true,
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        hstack: 16], input_0_tms: [broadcast: {c: 12}], attributes: {m_k: 2, u_kt: 6}}
    bw_in1_matmul_230_transpose_0: {type: nop, grid_loc: [5, 0], grid_size: [1, 2], inputs: [hidden_states], t: 1, mblock: [16, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [transpose]}
    bw_in1_matmul_230_matmul_1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 4], inputs: [bw_in1_matmul_230_transpose_0, bw_in0_matmul_242_matmul_1], gradient_op: true, t: 1, mblock: [16, 2], ublock: [
        2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hstack: 16], attributes: {
        m_k: 4, u_kt: 3}}

  opt_22:
    target_device: 0
    input_count: 1
    input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.query.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.query.weight, input_opt_layer.0.attention.self.query.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.query.weight, opt_in1_layer.0.attention.self.query.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.query.bias, input_opt_layer.0.attention.self.query.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layer.0.attention.self.query.bias, opt_in1_layer.0.attention.self.query.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.key.weight_0.lr], t: 1, mblock: [1, 8], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [3, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.key.weight, input_opt_layer.0.attention.self.key.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.key.weight, opt_in1_layer.0.attention.self.key.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.key.bias, input_opt_layer.0.attention.self.key.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layer.0.attention.self.key.bias, opt_in1_layer.0.attention.self.key.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.self.value.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [5, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.self.value.weight, input_opt_layer.0.attention.self.value.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in1_layer.0.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [6, 0], grid_size: [1, 8], inputs: [layer.0.attention.self.value.weight, opt_in1_layer.0.attention.self.value.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.self.value.bias, input_opt_layer.0.attention.self.value.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layer.0.attention.self.value.bias, opt_in1_layer.0.attention.self.value.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [input_opt_layer.0.attention.output.dense.weight_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.0.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 8], inputs: [grad_acc_layer.0.attention.output.dense.weight, input_opt_layer.0.attention.output.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.0.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [9, 0], grid_size: [1, 8], inputs: [layer.0.attention.output.dense.weight, opt_in0_layer.0.attention.output.dense.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.dense.bias, input_opt_layer.0.attention.output.dense.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layer.0.attention.output.dense.bias, opt_in1_layer.0.attention.output.dense.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.weight, input_opt_layer.0.attention.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.weight, opt_in1_layer.0.attention.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [grad_acc_layer.0.attention.output.LayerNorm.bias, input_opt_layer.0.attention.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.0.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layer.0.attention.output.LayerNorm.bias, opt_in2_layer.0.attention.output.LayerNorm.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  opt_23:
    target_device: 0
    input_count: 1
    input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 2], inputs: [input_opt_layer.0.intermediate.dense.weight_0.lr], t: 1, mblock: [1, 16], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 128}]}
    opt_in0_layer.0.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [4, 8], inputs: [grad_acc_layer.0.intermediate.dense.weight, input_opt_layer.0.intermediate.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.0.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [4, 8], inputs: [layer.0.intermediate.dense.weight, opt_in0_layer.0.intermediate.dense.weight_multiply_1],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.intermediate.dense.bias, input_opt_layer.0.intermediate.dense.bias_0.lr],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 128}]}
    opt_in1_layer.0.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layer.0.intermediate.dense.bias, opt_in1_layer.0.intermediate.dense.bias_multiply_1],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 2], inputs: [input_opt_layer.0.output.dense.weight_0.lr], t: 1, mblock: [1, 4], ublock: [1, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}

  opt_24:
    target_device: 0
    input_count: 1
    opt_in0_layer.0.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.0.output.dense.weight, e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 128}]}
    opt_in0_layer.0.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.0.output.dense.weight, opt_in0_layer.0.output.dense.weight_multiply_1], t: 1, mblock: [
        16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.dense.bias, input_opt_layer.0.output.dense.bias_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    opt_in1_layer.0.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.0.output.dense.bias, opt_in1_layer.0.output.dense.bias_multiply_1], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.0.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.weight, input_opt_layer.0.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.0.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.weight, opt_in1_layer.0.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.0.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.0.output.LayerNorm.bias, input_opt_layer.0.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.0.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.0.output.LayerNorm.bias, opt_in2_layer.0.output.LayerNorm.bias_multiply_1], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.query.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [9, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.query.weight, input_opt_layer.1.attention.self.query.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}

  opt_25:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.query.weight, e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.query.bias, input_opt_layer.1.attention.self.query.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layer.1.attention.self.query.bias, opt_in1_layer.1.attention.self.query.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.key.weight_0.lr], t: 1, mblock: [1, 8], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.key.weight, input_opt_layer.1.attention.self.key.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.key.weight, opt_in0_layer.1.attention.self.key.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.key.bias, input_opt_layer.1.attention.self.key.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layer.1.attention.self.key.bias, opt_in1_layer.1.attention.self.key.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.self.value.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [4, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.self.value.weight, input_opt_layer.1.attention.self.value.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layer.1.attention.self.value.weight, opt_in0_layer.1.attention.self.value.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.self.value.bias, input_opt_layer.1.attention.self.value.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layer.1.attention.self.value.bias, opt_in1_layer.1.attention.self.value.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [input_opt_layer.1.attention.output.dense.weight_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.1.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [7, 0], grid_size: [1, 8], inputs: [grad_acc_layer.1.attention.output.dense.weight, input_opt_layer.1.attention.output.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [8, 0], grid_size: [1, 8], inputs: [layer.1.attention.output.dense.weight, opt_in0_layer.1.attention.output.dense.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.dense.bias, input_opt_layer.1.attention.output.dense.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layer.1.attention.output.dense.bias, opt_in1_layer.1.attention.output.dense.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.weight, input_opt_layer.1.attention.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.weight, opt_in1_layer.1.attention.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [grad_acc_layer.1.attention.output.LayerNorm.bias, input_opt_layer.1.attention.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.1.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layer.1.attention.output.LayerNorm.bias, opt_in2_layer.1.attention.output.LayerNorm.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [9, 0], grid_size: [1, 2], inputs: [input_opt_layer.1.intermediate.dense.weight_0.lr], t: 1, mblock: [1, 16], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 128}]}

  opt_26:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.1.intermediate.dense.weight, e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.1.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.1.intermediate.dense.weight, opt_in0_layer.1.intermediate.dense.weight_multiply_1],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.intermediate.dense.bias, input_opt_layer.1.intermediate.dense.bias_0.lr],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 128}]}
    opt_in1_layer.1.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.1.intermediate.dense.bias, opt_in1_layer.1.intermediate.dense.bias_multiply_1],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 2], inputs: [input_opt_layer.1.output.dense.weight_0.lr], t: 1, mblock: [1, 4], ublock: [1, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}

  opt_27:
    target_device: 0
    input_count: 1
    opt_in0_layer.1.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.1.output.dense.weight, e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 128}]}
    opt_in0_layer.1.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.1.output.dense.weight, opt_in0_layer.1.output.dense.weight_multiply_1], t: 1, mblock: [
        16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.dense.bias, input_opt_layer.1.output.dense.bias_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    opt_in1_layer.1.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.1.output.dense.bias, opt_in1_layer.1.output.dense.bias_multiply_1], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.1.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.weight, input_opt_layer.1.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.1.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.weight, opt_in1_layer.1.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.1.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.1.output.LayerNorm.bias, input_opt_layer.1.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.1.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.1.output.LayerNorm.bias, opt_in2_layer.1.output.LayerNorm.bias_multiply_1], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.2.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [input_opt_layer.2.attention.self.query.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.2.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [9, 0], grid_size: [1, 8], inputs: [grad_acc_layer.2.attention.self.query.weight, input_opt_layer.2.attention.self.query.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}

  opt_28:
    target_device: 0
    input_count: 1
    opt_in0_layer.2.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layer.2.attention.self.query.weight, e2e_opt_in0_layer.2.attention.self.query.weight_multiply_1_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [grad_acc_layer.2.attention.self.query.bias, input_opt_layer.2.attention.self.query.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.2.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layer.2.attention.self.query.bias, opt_in1_layer.2.attention.self.query.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.2.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [input_opt_layer.2.attention.self.key.weight_0.lr], t: 1, mblock: [1, 8], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.2.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [1, 8], inputs: [grad_acc_layer.2.attention.self.key.weight, input_opt_layer.2.attention.self.key.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.2.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layer.2.attention.self.key.weight, opt_in0_layer.2.attention.self.key.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [grad_acc_layer.2.attention.self.key.bias, input_opt_layer.2.attention.self.key.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.2.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layer.2.attention.self.key.bias, opt_in1_layer.2.attention.self.key.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.2.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_opt_layer.2.attention.self.value.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.2.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [4, 0], grid_size: [1, 8], inputs: [grad_acc_layer.2.attention.self.value.weight, input_opt_layer.2.attention.self.value.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.2.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layer.2.attention.self.value.weight, opt_in0_layer.2.attention.self.value.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [grad_acc_layer.2.attention.self.value.bias, input_opt_layer.2.attention.self.value.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.2.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layer.2.attention.self.value.bias, opt_in1_layer.2.attention.self.value.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.2.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [input_opt_layer.2.attention.output.dense.weight_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.2.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [7, 0], grid_size: [1, 8], inputs: [grad_acc_layer.2.attention.output.dense.weight, input_opt_layer.2.attention.output.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.2.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [8, 0], grid_size: [1, 8], inputs: [layer.2.attention.output.dense.weight, opt_in0_layer.2.attention.output.dense.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [grad_acc_layer.2.attention.output.dense.bias, input_opt_layer.2.attention.output.dense.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.2.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layer.2.attention.output.dense.bias, opt_in1_layer.2.attention.output.dense.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [grad_acc_layer.2.attention.output.LayerNorm.weight, input_opt_layer.2.attention.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.2.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layer.2.attention.output.LayerNorm.weight, opt_in1_layer.2.attention.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.2.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [grad_acc_layer.2.attention.output.LayerNorm.bias, input_opt_layer.2.attention.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.2.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layer.2.attention.output.LayerNorm.bias, opt_in2_layer.2.attention.output.LayerNorm.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.2.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [9, 0], grid_size: [1, 2], inputs: [input_opt_layer.2.intermediate.dense.weight_0.lr], t: 1, mblock: [1, 16], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 128}]}

  opt_29:
    target_device: 0
    input_count: 1
    opt_in0_layer.2.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.2.intermediate.dense.weight, e2e_input_opt_layer.2.intermediate.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.2.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.2.intermediate.dense.weight, opt_in0_layer.2.intermediate.dense.weight_multiply_1],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.2.intermediate.dense.bias, input_opt_layer.2.intermediate.dense.bias_0.lr],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 128}]}
    opt_in1_layer.2.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.2.intermediate.dense.bias, opt_in1_layer.2.intermediate.dense.bias_multiply_1],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.2.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 2], inputs: [input_opt_layer.2.output.dense.weight_0.lr], t: 1, mblock: [1, 4], ublock: [1, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}

  opt_30:
    target_device: 0
    input_count: 1
    opt_in0_layer.2.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.2.output.dense.weight, e2e_input_opt_layer.2.output.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 128}]}
    opt_in0_layer.2.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.2.output.dense.weight, opt_in0_layer.2.output.dense.weight_multiply_1], t: 1, mblock: [
        16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.2.output.dense.bias, input_opt_layer.2.output.dense.bias_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    opt_in1_layer.2.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.2.output.dense.bias, opt_in1_layer.2.output.dense.bias_multiply_1], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.2.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.2.output.LayerNorm.weight, input_opt_layer.2.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.2.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.2.output.LayerNorm.weight, opt_in1_layer.2.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.2.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.2.output.LayerNorm.bias, input_opt_layer.2.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.2.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.2.output.LayerNorm.bias, opt_in2_layer.2.output.LayerNorm.bias_multiply_1], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.3.attention.self.query.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [input_opt_layer.3.attention.self.query.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.3.attention.self.query.weight_multiply_1: {type: multiply, grid_loc: [9, 0], grid_size: [1, 8], inputs: [grad_acc_layer.3.attention.self.query.weight, input_opt_layer.3.attention.self.query.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}

  opt_31:
    target_device: 0
    input_count: 1
    opt_in0_layer.3.attention.self.query.weight_subtract_2: {type: subtract, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layer.3.attention.self.query.weight, e2e_opt_in0_layer.3.attention.self.query.weight_multiply_1_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.attention.self.query.bias_multiply_1: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [grad_acc_layer.3.attention.self.query.bias, input_opt_layer.3.attention.self.query.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.3.attention.self.query.bias_subtract_2: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layer.3.attention.self.query.bias, opt_in1_layer.3.attention.self.query.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.3.attention.self.key.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [input_opt_layer.3.attention.self.key.weight_0.lr], t: 1, mblock: [1, 8], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.3.attention.self.key.weight_multiply_1: {type: multiply, grid_loc: [2, 0], grid_size: [1, 8], inputs: [grad_acc_layer.3.attention.self.key.weight, input_opt_layer.3.attention.self.key.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.3.attention.self.key.weight_subtract_2: {type: subtract, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layer.3.attention.self.key.weight, opt_in0_layer.3.attention.self.key.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.attention.self.key.bias_multiply_1: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [grad_acc_layer.3.attention.self.key.bias, input_opt_layer.3.attention.self.key.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.3.attention.self.key.bias_subtract_2: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layer.3.attention.self.key.bias, opt_in1_layer.3.attention.self.key.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.3.attention.self.value.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [input_opt_layer.3.attention.self.value.weight_0.lr], t: 1, mblock: [1, 8],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.3.attention.self.value.weight_multiply_1: {type: multiply, grid_loc: [4, 0], grid_size: [1, 8], inputs: [grad_acc_layer.3.attention.self.value.weight, input_opt_layer.3.attention.self.value.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.3.attention.self.value.weight_subtract_2: {type: subtract, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layer.3.attention.self.value.weight, opt_in0_layer.3.attention.self.value.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.attention.self.value.bias_multiply_1: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [grad_acc_layer.3.attention.self.value.bias, input_opt_layer.3.attention.self.value.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.3.attention.self.value.bias_subtract_2: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layer.3.attention.self.value.bias, opt_in1_layer.3.attention.self.value.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.3.attention.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [input_opt_layer.3.attention.output.dense.weight_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}
    opt_in0_layer.3.attention.output.dense.weight_multiply_1: {type: multiply, grid_loc: [7, 0], grid_size: [1, 8], inputs: [grad_acc_layer.3.attention.output.dense.weight, input_opt_layer.3.attention.output.dense.weight_0.lr_splt_brcst_3_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.3.attention.output.dense.weight_subtract_2: {type: subtract, grid_loc: [8, 0], grid_size: [1, 8], inputs: [layer.3.attention.output.dense.weight, opt_in0_layer.3.attention.output.dense.weight_multiply_1],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.attention.output.dense.bias_multiply_1: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [grad_acc_layer.3.attention.output.dense.bias, input_opt_layer.3.attention.output.dense.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.3.attention.output.dense.bias_subtract_2: {type: subtract, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layer.3.attention.output.dense.bias, opt_in1_layer.3.attention.output.dense.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.attention.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [grad_acc_layer.3.attention.output.LayerNorm.weight, input_opt_layer.3.attention.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.3.attention.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layer.3.attention.output.LayerNorm.weight, opt_in1_layer.3.attention.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.3.attention.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [grad_acc_layer.3.attention.output.LayerNorm.bias, input_opt_layer.3.attention.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.3.attention.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layer.3.attention.output.LayerNorm.bias, opt_in2_layer.3.attention.output.LayerNorm.bias_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.3.intermediate.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [9, 0], grid_size: [1, 2], inputs: [input_opt_layer.3.intermediate.dense.weight_0.lr], t: 1, mblock: [1, 16], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 128}]}

  opt_32:
    target_device: 0
    input_count: 1
    opt_in0_layer.3.intermediate.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.3.intermediate.dense.weight, e2e_input_opt_layer.3.intermediate.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 32}]}
    opt_in0_layer.3.intermediate.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.3.intermediate.dense.weight, opt_in0_layer.3.intermediate.dense.weight_multiply_1],
      t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.intermediate.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.3.intermediate.dense.bias, input_opt_layer.3.intermediate.dense.bias_0.lr],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 128}]}
    opt_in1_layer.3.intermediate.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.3.intermediate.dense.bias, opt_in1_layer.3.intermediate.dense.bias_multiply_1],
      t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_opt_layer.3.output.dense.weight_0.lr_splt_brcst_3_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 2], inputs: [input_opt_layer.3.output.dense.weight_0.lr], t: 1, mblock: [1, 4], ublock: [1, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_0_tms: [broadcast: {c: 32}]}

  opt_33:
    target_device: 0
    input_count: 1
    opt_in0_layer.3.output.dense.weight_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [4, 8], inputs: [grad_acc_layer.3.output.dense.weight, e2e_input_opt_layer.3.output.dense.weight_0.lr_splt_brcst_3_0_0],
      t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 128}]}
    opt_in0_layer.3.output.dense.weight_subtract_2: {type: subtract, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layer.3.output.dense.weight, opt_in0_layer.3.output.dense.weight_multiply_1], t: 1, mblock: [
        16, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.output.dense.bias_multiply_1: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [grad_acc_layer.3.output.dense.bias, input_opt_layer.3.output.dense.bias_0.lr], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    opt_in1_layer.3.output.dense.bias_subtract_2: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layer.3.output.dense.bias, opt_in1_layer.3.output.dense.bias_multiply_1], t: 1, mblock: [
        1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in1_layer.3.output.LayerNorm.weight_multiply_1: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [grad_acc_layer.3.output.LayerNorm.weight, input_opt_layer.3.output.LayerNorm.weight_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in1_layer.3.output.LayerNorm.weight_subtract_2: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layer.3.output.LayerNorm.weight, opt_in1_layer.3.output.LayerNorm.weight_multiply_1],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    opt_in2_layer.3.output.LayerNorm.bias_multiply_1: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [grad_acc_layer.3.output.LayerNorm.bias, input_opt_layer.3.output.LayerNorm.bias_0.lr],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    opt_in2_layer.3.output.LayerNorm.bias_subtract_2: {type: subtract, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layer.3.output.LayerNorm.bias, opt_in2_layer.3.output.LayerNorm.bias_multiply_1], t: 1,
      mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
- run_fwd:
  - param: [$p_loop_count]
  - var: {$c_microbatch_size: 2, $lptr_q14: 0, $gptr_q12: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q1: 0, $lptr_q8: 0, $gptr_q8: 0, $lptr_q6: 0, $c_one: 1, $c_zero: 0, $lptr_q1: 0, $lptr_q12: 0, $gptr_q6: 0,
      $gptr_q4: 0, $gptr_q14: 0, $gptr_q3: 0}
  - staticvar: {$gptr_q0_shadow: 0, $gptr_q0: 0, $lptr_q0: 0, $gptr_q2_shadow: 0, $gptr_q2: 0, $gptr_q5_shadow: 0, $gptr_q5: 0, $gptr_q7_shadow: 0, $lptr_q2: 0, $lptr_q13: 0, $gptr_q13: 0, $gptr_q9: 0,
      $gptr_q13_shadow: 0, $lptr_q11: 0, $lptr_q9: 0, $lptr_q7: 0, $gptr_q11: 0, $gptr_q10: 0, $gptr_q11_shadow: 0, $lptr_q10: 0, $lptr_q5: 0, $gptr_q10_shadow: 0, $gptr_q7: 0}
  - varinst: [$gptr_q13, set, $gptr_q13_shadow]
  - varinst: [$gptr_q11, set, $gptr_q11_shadow]
  - varinst: [$gptr_q10, set, $gptr_q10_shadow]
  - varinst: [$gptr_q7, set, $gptr_q7_shadow]
  - varinst: [$gptr_q5, set, $gptr_q5_shadow]
  - varinst: [$gptr_q2, set, $gptr_q2_shadow]
  - varinst: [$gptr_q0, set, $gptr_q0_shadow]
  - loop: $p_loop_count
  - allocate_queue: [e2e_layernorm_266.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0]
  - execute: {graph_name: fwd_0, queue_settings: {hidden_states: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, attention_mask: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_244_fork_clone1647_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_246.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_266.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_266.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_266.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_266.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
  - allocate_queue: [e2e_layernorm_280.dc.sqrt.6_0, e2e_buffer_0_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8_0]
  - execute: {graph_name: fwd_1, queue_settings: {e2e_layernorm_266.dc.multiply.9_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_280.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_280.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_280.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_266.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_1_0.lc1_0]
  - varinst: [$gptr_q1, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_319.dc.multiply.2_0, e2e_buffer_0_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8_0]
  - execute: {graph_name: fwd_2, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_layernorm_280.dc.sqrt.6_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_buffer_0_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, lc.input_tensor.layernorm_280.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_297_fork_clone1670_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_299.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_319.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_280.dc.sqrt.6_0, e2e_buffer_0_layernorm_280.dc.subtract.1_layernorm_280.dc.multiply.8_0]
  - varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_332_0]
  - execute: {graph_name: fwd_3, queue_settings: {e2e_layernorm_319.dc.multiply.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_buffer_0_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, lc.input_tensor.layernorm_319.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_319.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_319.dc.reciprocal.7_s_brcst_m1_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.intermediate.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_319.dc.multiply.2_0, e2e_buffer_0_layernorm_319.dc.subtract.1_layernorm_319.dc.multiply.8_0]
  - varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_369_0]
  - execute: {graph_name: fwd_4, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_add_332_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, lc.input_tensor.layernorm_333.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_333.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_333.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_333.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.2.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_350_fork_clone1693_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_352.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_332_0]
  - varinst: [$gptr_q5_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q6, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q6, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: fwd_5, queue_settings: {e2e_layernorm_333.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_add_369_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, lc.input_tensor.layernorm_372.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_372.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_372.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_372.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_369_0]
  - varinst: [$gptr_q7_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_softmax_405.dc.exp.0_0, e2e_lc.input_tensor.softmax_405.dc.reduce_sum.1.0_splt_brcst_1_0_0]
  - execute: {graph_name: fwd_6, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, e2e_layernorm_372.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10}, e2e_gelu_378_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
        layer.2.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_386.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_386.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_386.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_386.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_403_fork_clone1716_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_405.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q9, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q10, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q9, incwrap, $c_microbatch_size, 8]
  - allocate_queue: [e2e_matmul_428_0, e2e_layer.3.intermediate.dense.bias_s_brcst_m2_1_0.lc1_0]
  - execute: {graph_name: fwd_7, queue_settings: {e2e_layernorm_386.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_softmax_405.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}, e2e_lc.input_tensor.softmax_405.dc.reduce_sum.1.0_splt_brcst_1_0_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}, layer.3.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_425.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_425.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_425.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_425.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_softmax_405.dc.exp.0_0, e2e_lc.input_tensor.softmax_405.dc.reduce_sum.1.0_splt_brcst_1_0_0]
  - varinst: [$gptr_q11_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: fwd_8, queue_settings: {e2e_layernorm_425.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_matmul_428_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_layer.3.intermediate.dense.bias_s_brcst_m2_1_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14,
          rd_ptr_global: $gptr_q14}, layer.3.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_439.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_439.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_439.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_439.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_1_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_matmul_428_0, e2e_layer.3.intermediate.dense.bias_s_brcst_m2_1_0.lc1_0]
  - varinst: [$gptr_q13_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q14, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q13, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
  - endloop

- run_bwd:
  - param: [$p_zero_grad, $p_loop_count]
  - var: {$c_microbatch_size: 2, $lptr_q3: 0, $gptr_q8: 0, $lptr_q8: 0, $gptr_q10_shadow: 0, $gptr_q10: 0, $lptr_q10: 0, $gptr_q12: 0, $gptr_q6: 0, $lptr_q12: 0, $gptr_q15: 0, $lptr_q6: 0, $gptr_q29: 0,
      $c_one: 1, $c_zero: 0, $gptr_q26: 0, $gptr_q24: 0, $lptr_q24: 0, $lptr_q29: 0, $lptr_q22: 0, $v_zero_grad: 0, $gptr_q22: 0, $lptr_q17: 0, $lptr_q19: 0, $lptr_q26: 0, $gptr_q19: 0, $gptr_q3: 0, $gptr_q17: 0,
      $lptr_q15: 0}
  - staticvar: {$gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q4: 0, $lptr_q4: 0, $gptr_q4_shadow: 0, $gptr_q5: 0, $gptr_q20: 0, $gptr_q21: 0, $lptr_q5: 0, $gptr_q23: 0, $gptr_q21_shadow: 0,
      $lptr_q23: 0, $gptr_q9: 0, $lptr_q20: 0, $gptr_q11: 0, $gptr_q1: 0, $gptr_q25: 0, $lptr_q9: 0, $lptr_q21: 0, $gptr_q27: 0, $lptr_q16: 0, $lptr_q27: 0, $lptr_q11: 0, $gptr_q28: 0, $lptr_q25: 0, $lptr_q28: 0,
      $gptr_q14_shadow: 0, $lptr_q18: 0, $gptr_q18: 0, $lptr_q14: 0, $gptr_q14: 0, $gptr_q16: 0, $lptr_q13: 0, $gptr_q13: 0, $lptr_q7: 0, $gptr_q7: 0}
  - varinst: [$gptr_q21, set, $gptr_q21_shadow]
  - varinst: [$gptr_q14, set, $gptr_q14_shadow]
  - varinst: [$gptr_q10, set, $gptr_q10_shadow]
  - varinst: [$gptr_q4, set, $gptr_q4_shadow]
  - varinst: [$v_zero_grad, set, $p_zero_grad]
  - loop: $p_loop_count
  - allocate_queue: [e2e_bw_in0_matmul_434_matmul_1_0, e2e_bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9_0]
  - execute: {graph_name: bwd_9, queue_settings: {loss_bert_encoder.output_layernorm_439: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, e2e_gelu_431_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, e2e_layernorm_439.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1,
          rd_ptr_global: $gptr_q1}, e2e_layernorm_439.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, layer.3.output.dense.weight: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.bw_in2_layernorm_439_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_439.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.dc.reduce_sum.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_439_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false,
          rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_436_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        grad_acc_layer.3.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.output.LayerNorm.weight: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.3.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q1, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_gelu_431_multiply_1_0, e2e_bw_in0_matmul_428_matmul_1_0, e2e_bw_in1_matmul_428_transpose_0_0]
  - execute: {graph_name: bwd_10, queue_settings: {e2e_layernorm_425.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_add_430_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_bw_in0_matmul_434_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
        layer.3.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_430_brcst_reduce_sum_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.3.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_matmul_434_matmul_1_0]
  - varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_416_matmul_1_0, e2e_bw_in0_matmul_409_matmul_1_0, e2e_bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.lc1_0]
  - execute: {graph_name: bwd_11, queue_settings: {e2e_layernorm_386.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_softmax_405.dc.multiply.3_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_add_411_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
        e2e_matmul_416_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_layernorm_425.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_layernorm_425.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_bw_in0_gelu_431_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6,
          rd_ptr_global: $gptr_q6}, e2e_bw_in0_matmul_428_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_bw_in1_matmul_428_transpose_0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, layer.3.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_425_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_425.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_425_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.bw_in1_add_422_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_411_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.3.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.3.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.value.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_439_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_431_multiply_1_0, e2e_bw_in0_matmul_428_matmul_1_0, e2e_bw_in1_matmul_428_transpose_0_0]
  - varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q6, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q6, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0]
  - execute: {graph_name: bwd_12, queue_settings: {e2e_layernorm_386.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_layernorm_386.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_layernorm_386.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7,
          rd_ptr_global: $gptr_q7}, e2e_add_391_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_add_397_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_softmax_405.dc.multiply.3_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_bw_in0_matmul_416_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8,
          rd_ptr_global: $gptr_q8}, e2e_bw_in0_matmul_409_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.key.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_403_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_397_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_391_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_386_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_386.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_2_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_386_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.bw_in1_add_383_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.key.bias: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.query.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.2.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_425_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_416_matmul_1_0, e2e_bw_in0_matmul_409_matmul_1_0, e2e_bw_in0_softmax_405_softmax_bw_0.dc.reduce_sum.1.lc1_0]
  - varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_gelu_378_multiply_1_0]
  - execute: {graph_name: bwd_13, queue_settings: {e2e_add_377_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, e2e_gelu_378_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q10,
          rd_ptr_global: $gptr_q10}, layer.2.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_377_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.2.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.2.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q10, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_367_matmul_1_0]
  - execute: {graph_name: bwd_14, queue_settings: {e2e_matmul_363_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_layernorm_372.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_layernorm_372.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11,
          rd_ptr_global: $gptr_q11}, e2e_layernorm_372.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}, e2e_bw_in0_gelu_378_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12,
          rd_ptr_global: $gptr_q12}, layer.2.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_372_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_372.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.dc.reduce_sum.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_372_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false,
          rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_369_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        grad_acc_layer.2.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.2.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_386_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_378_multiply_1_0]
  - varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_333_combine_add_0_0, e2e_layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1_0,
      e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.lc1_0]
  - execute: {graph_name: bwd_15, queue_settings: {e2e_layernorm_333.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_layernorm_333.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_layernorm_333.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13,
          rd_ptr_global: $gptr_q13}, e2e_add_338_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_add_344_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_softmax_352.dc.multiply.3_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
        e2e_add_358_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15}, e2e_bw_in0_matmul_367_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
        layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.weight: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.2.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_358_brcst_reduce_sum_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_softmax_352_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, input_1_multiply_350_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_344_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_338_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.2.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.key.bias: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.query.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_372_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_367_matmul_1_0]
  - varinst: [$gptr_q13, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q14_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q15, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q13, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q15, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_325_multiply_1_0]
  - execute: {graph_name: bwd_16, queue_settings: {e2e_add_324_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16}, e2e_gelu_325_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16}, e2e_layernorm_333.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
        e2e_bw_in0_layernorm_333_combine_add_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, e2e_layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
        e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, layer.1.output.dense.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_333_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_333_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.bw_in1_add_330_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_324_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.1.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.bias: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_333_combine_add_0_0, e2e_layernorm_333.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.1.lc1_0,
      e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.reduce_sum.3.lc1_0]
  - varinst: [$gptr_q16, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q17, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q16, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q17, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_314_matmul_1_0]
  - execute: {graph_name: bwd_17, queue_settings: {e2e_matmul_310_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, e2e_layernorm_319.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, e2e_layernorm_319.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18,
          rd_ptr_global: $gptr_q18}, e2e_layernorm_319.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19}, e2e_bw_in0_gelu_325_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q19,
          rd_ptr_global: $gptr_q19}, layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_319_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_319.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.dc.reduce_sum.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_319_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false,
          rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_316_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        grad_acc_layer.1.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.1.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_333_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_325_multiply_1_0]
  - varinst: [$gptr_q18, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q19, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q18, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q19, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_280_combine_add_0_0, e2e_layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1_0,
      e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.lc1_0]
  - execute: {graph_name: bwd_18, queue_settings: {e2e_layernorm_280.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, e2e_layernorm_280.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21}, e2e_layernorm_280.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20,
          rd_ptr_global: $gptr_q20}, e2e_add_285_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, e2e_add_291_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, e2e_softmax_299.dc.multiply.3_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
        e2e_add_305_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, e2e_bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22}, e2e_bw_in0_matmul_314_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
        layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.weight: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_305_brcst_reduce_sum_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_softmax_299_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, input_1_multiply_297_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_291_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_285_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.1.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.bias: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_319_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_matmul_314_matmul_1_0]
  - varinst: [$gptr_q20, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q21_shadow, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q22, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q20, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q21, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q22, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_272_multiply_1_0]
  - execute: {graph_name: bwd_19, queue_settings: {e2e_add_271_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23}, e2e_gelu_272_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23}, e2e_layernorm_280.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
        e2e_bw_in0_layernorm_280_combine_add_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24}, e2e_layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24}, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24}, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
        e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24}, layer.0.output.dense.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_280_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_280_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.bw_in1_add_277_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_271_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.dense.bias: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.intermediate.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_280_combine_add_0_0, e2e_layernorm_280.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.1.lc1_0,
      e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.reduce_sum.3.lc1_0]
  - varinst: [$gptr_q23, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q24, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q23, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q24, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_bw_in0_matmul_261_matmul_1_0]
  - execute: {graph_name: bwd_20, queue_settings: {e2e_matmul_257_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25}, e2e_layernorm_266.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25}, e2e_layernorm_266.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q25,
          rd_ptr_global: $gptr_q25}, e2e_layernorm_266.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25}, e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26}, e2e_bw_in0_gelu_272_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q26,
          rd_ptr_global: $gptr_q26}, layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.bw_in2_layernorm_266_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_266.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_2_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.dc.reduce_sum.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.bw_in0_layernorm_266_layernorm_bw_0.6: {prologue: false, epilogue: false, zero: false,
          rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_263_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        grad_acc_layer.0.intermediate.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.0.attention.output.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_layernorm_280_layernorm_bw_0.dc.multiply.9_0, e2e_bw_in0_gelu_272_multiply_1_0]
  - varinst: [$gptr_q25, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q26, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q25, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q26, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: bwd_21, queue_settings: {hidden_states: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27}, e2e_add_232_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28}, e2e_add_238_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28}, e2e_softmax_246.dc.multiply.3_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28}, e2e_add_252_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
        e2e_bw_in0_matmul_261_matmul_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29}, lc.input_tensor.bw_in1_add_252_brcst_reduce_sum_0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in0_softmax_246_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, input_1_multiply_244_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_238_brcst_reduce_sum_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.bw_in1_add_232_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.0.attention.self.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.bias: {prologue: true,
          epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.weight: {
          prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_bw_in0_matmul_261_matmul_1_0]
  - varinst: [$gptr_q27, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q28, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q29, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q27, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q28, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q29, incwrap, $c_microbatch_size, 4]
  - varinst: [$v_zero_grad, set, 0]
  - endloop

- run_opt:
  - var: {$c_microbatch_size: 2, $gptr_q0: 0, $c_one: 1, $c_zero: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $gptr_q8: 0, $lptr_q5: 0, $gptr_q9: 0, $gptr_q1: 0, $lptr_q8: 0, $lptr_q9: 0,
      $lptr_q7: 0, $lptr_q6: 0, $gptr_q6: 0, $gptr_q5: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q7: 0, $gptr_q4: 0}
  - execute: {graph_name: opt_22, queue_settings: {layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.query.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.0.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.value.bias: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.key.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - allocate_queue: [e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_23, queue_settings: {layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - allocate_queue: [e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0]
  - execute: {graph_name: opt_24, queue_settings: {e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
        layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.LayerNorm.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.0.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.0.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.0.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q0, incwrap, $c_one, 2]
  - varinst: [$lptr_q0, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_25, queue_settings: {e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
        layer.1.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.value.weight: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.LayerNorm.bias: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.output.dense.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.attention.self.query.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_opt_in0_layer.1.attention.self.query.weight_multiply_1_0]
  - varinst: [$gptr_q1, incwrap, $c_one, 2]
  - varinst: [$lptr_q1, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_26, queue_settings: {e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
        layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.intermediate.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.1.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.1.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q2, incwrap, $c_one, 2]
  - varinst: [$lptr_q2, incwrap, $c_one, 2]
  - allocate_queue: [e2e_opt_in0_layer.2.attention.self.query.weight_multiply_1_0]
  - execute: {graph_name: opt_27, queue_settings: {e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
        layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.LayerNorm.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.1.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.1.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q3, incwrap, $c_one, 2]
  - varinst: [$lptr_q3, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.2.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_28, queue_settings: {e2e_opt_in0_layer.2.attention.self.query.weight_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
        layer.2.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.2.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.value.weight: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.2.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.2.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.output.LayerNorm.bias: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.output.dense.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.key.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.2.attention.self.query.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_opt_in0_layer.2.attention.self.query.weight_multiply_1_0]
  - varinst: [$gptr_q4, incwrap, $c_one, 2]
  - varinst: [$lptr_q4, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.2.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_29, queue_settings: {e2e_input_opt_layer.2.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
        layer.2.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.intermediate.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.intermediate.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.2.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.2.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q5, incwrap, $c_one, 2]
  - varinst: [$lptr_q5, incwrap, $c_one, 2]
  - allocate_queue: [e2e_opt_in0_layer.3.attention.self.query.weight_multiply_1_0]
  - execute: {graph_name: opt_30, queue_settings: {e2e_input_opt_layer.2.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
        layer.2.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.output.dense.bias: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.2.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.output.LayerNorm.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.2.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.2.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.2.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q6, incwrap, $c_one, 2]
  - varinst: [$lptr_q6, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.3.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_31, queue_settings: {e2e_opt_in0_layer.3.attention.self.query.weight_multiply_1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
        layer.3.attention.self.query.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.query.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.3.attention.self.key.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.value.weight: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.3.attention.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.output.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.3.attention.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.output.LayerNorm.bias: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.output.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.output.dense.weight: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.value.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.value.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.key.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.key.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.3.attention.self.query.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_opt_in0_layer.3.attention.self.query.weight_multiply_1_0]
  - varinst: [$gptr_q7, incwrap, $c_one, 2]
  - varinst: [$lptr_q7, incwrap, $c_one, 2]
  - allocate_queue: [e2e_input_opt_layer.3.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - execute: {graph_name: opt_32, queue_settings: {e2e_input_opt_layer.3.intermediate.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
        layer.3.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.intermediate.dense.bias: {prologue: false, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.intermediate.dense.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        grad_acc_layer.3.intermediate.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.3.intermediate.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q8, incwrap, $c_one, 2]
  - varinst: [$lptr_q8, incwrap, $c_one, 2]
  - execute: {graph_name: opt_33, queue_settings: {e2e_input_opt_layer.3.output.dense.weight_0.lr_splt_brcst_3_0_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
        layer.3.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.dense.bias: {prologue: false, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.output.LayerNorm.bias: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, grad_acc_layer.3.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.output.dense.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, grad_acc_layer.3.output.dense.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_input_opt_layer.3.output.dense.weight_0.lr_splt_brcst_3_0_0]
  - varinst: [$gptr_q9, incwrap, $c_one, 2]
  - varinst: [$lptr_q9, incwrap, $c_one, 2]

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 2.0
  io-config:
    inputs: [hidden_states, attention_mask, loss_bert_encoder.output_layernorm_439]
    outputs: [bert_encoder.output_layernorm_439]

