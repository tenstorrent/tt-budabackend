# git checkout 435f4d73
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole_B0-chip1-enc12-base]

devices:
  arch: blackhole

queues:

  # input
  hidden_states: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10000000]]}
  attention_mask: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10000000]]}

  # output
  bert_encoder.output_layernorm_683: {input: layernorm_683.dc.add.10, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10000000], [4, 0x10099040]]}
  layer.0.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10022040], [3, 0x10028680]]}
  layer.0.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10000000], [1, 0x10099040]]}
  layer.0.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x100cc040],
      [5, 0x100d2680]]}
  layer.0.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10000000], [0, 0x10099040]]}
  layer.0.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10132080], [4, 0x101386c0]]}
  layer.0.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10000000], [2, 0x10099040]]}
  layer.0.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10132080], [2, 0x101386c0]]}
  layer.0.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10132080]]}
  layer.0.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x1013ed00]]}
  layer.0.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x1013ed00], [2, 0x101d7d40], [2, 0x10270d80], [2, 0x10309dc0], [2, 0x103a2e00], [2, 0x1043be40], [2, 0x104d4e80], [2, 0x1056dec0]]}
  layer.0.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1013ecc0],
      [0, 0x10145300], [0, 0x1014b940], [0, 0x10151f80], [0, 0x101585c0], [0, 0x1015ec00], [0, 0x10165240], [0, 0x1016b880]]}
  layer.0.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1014b940],
      [4, 0x101e4980], [4, 0x1027d9c0], [4, 0x10316a00], [4, 0x103afa40], [4, 0x10448a80], [4, 0x104e1ac0], [4, 0x1057ab00]]}
  layer.0.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10171ec0],
      [0, 0x10173880], [0, 0x10175240], [0, 0x10176c00], [0, 0x101785c0], [0, 0x10179f80], [0, 0x1017b940], [0, 0x1017d300]]}
  layer.0.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1017ecc0]]}
  layer.0.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10613b40]]}
  layer.1.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10606f00], [2, 0x1069ff40]]}
  layer.1.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x1018b900], [0, 0x10191f40]]}
  layer.1.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10198580], [0, 0x102315c0]]}
  layer.1.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1002ecc0],
      [3, 0x10035300]]}
  layer.1.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x100d8cc0], [5, 0x10171d00]]}
  layer.1.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x1003b940], [3, 0x10041f80]]}
  layer.1.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10132080], [1, 0x101cb0c0]]}
  layer.1.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1020ad40], [5, 0x10211380]]}
  layer.1.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x102ca600]]}
  layer.1.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10620780]]}
  layer.1.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x10738f80], [2, 0x107d1fc0], [2, 0x1086b000], [2, 0x10904040], [2, 0x1099d080], [2, 0x10a360c0], [2, 0x10acf100], [2, 0x10b68140]]}
  layer.1.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x102d7240],
      [0, 0x102dd880], [0, 0x102e3ec0], [0, 0x102ea500], [0, 0x102f0b40], [0, 0x102f7180], [0, 0x102fd7c0], [0, 0x10303e00]]}
  layer.1.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1062d3c0],
      [4, 0x106c6400], [4, 0x1075f440], [4, 0x107f8480], [4, 0x108914c0], [4, 0x1092a500], [4, 0x109c3540], [4, 0x10a5c580]]}
  layer.1.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10c01180],
      [2, 0x10c02b40], [2, 0x10c04500], [2, 0x10c05ec0], [2, 0x10c07880], [2, 0x10c09240], [2, 0x10c0ac00], [2, 0x10c0c5c0]]}
  layer.1.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10264100]]}
  layer.1.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x102179c0]]}
  layer.2.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x100485c0], [3, 0x100e1600]]}
  layer.2.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10270d40], [1, 0x10277380]]}
  layer.2.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10224600], [5, 0x102bd640]]}
  layer.2.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1017a640],
      [3, 0x10180c80]]}
  layer.2.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10af55c0], [4, 0x10b8e600]]}
  layer.2.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x1030a440], [0, 0x10310a80]]}
  layer.2.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10c27640], [4, 0x10cc0680]]}
  layer.2.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10c0df80], [2, 0x10c145c0]]}
  layer.2.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10c1ac00]]}
  layer.2.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10c27840]]}
  layer.2.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10d596c0], [4, 0x10df2700], [4, 0x10e8b740], [4, 0x10f24780], [4, 0x10fbd7c0], [4, 0x11056800], [4, 0x110ef840], [4, 0x11188880]]}
  layer.2.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10c34480],
      [2, 0x10c3aac0], [2, 0x10c41100], [2, 0x10c47740], [2, 0x10c4dd80], [2, 0x10c543c0], [2, 0x10c5aa00], [2, 0x10c61040]]}
  layer.2.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x103170c0],
      [0, 0x103b0100], [0, 0x10449140], [0, 0x104e2180], [0, 0x1057b1c0], [0, 0x10614200], [0, 0x106ad240], [0, 0x10746280]]}
  layer.2.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x112218c0],
      [4, 0x11223280], [4, 0x11224c40], [4, 0x11226600], [4, 0x11227fc0], [4, 0x11229980], [4, 0x1122b340], [4, 0x1122cd00]]}
  layer.2.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1122e6c0]]}
  layer.2.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1027d9c0]]}
  layer.3.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10c67680], [2, 0x10d006c0]]}
  layer.3.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10356680], [5, 0x1035ccc0]]}
  layer.3.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x101872c0], [3, 0x10220300]]}
  layer.3.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1028a600],
      [1, 0x10290c40]]}
  layer.3.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x102b9340], [3, 0x10352380]]}
  layer.3.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10297280], [1, 0x1029d8c0]]}
  layer.3.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10363300], [5, 0x103fc340]]}
  layer.3.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x103eb3c0], [3, 0x103f1a00]]}
  layer.3.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x103f8040]]}
  layer.3.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x102a3f00]]}
  layer.3.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10495380], [5, 0x1052e3c0], [5, 0x105c7400], [5, 0x10660440], [5, 0x106f9480], [5, 0x107924c0], [5, 0x1082b500], [5, 0x108c4540]]}
  layer.3.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10404c80],
      [3, 0x1040b2c0], [3, 0x10411900], [3, 0x10417f40], [3, 0x1041e580], [3, 0x10424bc0], [3, 0x1042b200], [3, 0x10431840]]}
  layer.3.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x102b0b40],
      [1, 0x10349b80], [1, 0x103e2bc0], [1, 0x1047bc00], [1, 0x10514c40], [1, 0x105adc80], [1, 0x10646cc0], [1, 0x106dfd00]]}
  layer.3.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1095d580],
      [5, 0x1095ef40], [5, 0x10960900], [5, 0x109622c0], [5, 0x10963c80], [5, 0x10965640], [5, 0x10967000], [5, 0x109689c0]]}
  layer.3.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10778d40]]}
  layer.3.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1096a380]]}
  layer.4.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10437e80], [3, 0x104d0ec0]]}
  layer.4.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10785980], [1, 0x1078bfc0]]}
  layer.4.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10976fc0], [5, 0x10a10000]]}
  layer.4.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x107df2c0],
      [0, 0x107e5900]]}
  layer.4.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10792600], [1, 0x1082b640]]}
  layer.4.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10aa9040], [5, 0x10aaf680]]}
  layer.4.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10569f00], [3, 0x10602f40]]}
  layer.4.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x108c4680], [1, 0x108cacc0]]}
  layer.4.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10d99700]]}
  layer.4.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x107ebf40]]}
  layer.4.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x1123b300], [4, 0x112d4340], [4, 0x1136d380], [4, 0x114063c0], [4, 0x1149f400], [4, 0x11538440], [4, 0x115d1480], [4, 0x1166a4c0]]}
  layer.4.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10da6340],
      [2, 0x10dac980], [2, 0x10db2fc0], [2, 0x10db9600], [2, 0x10dbfc40], [2, 0x10dc6280], [2, 0x10dcc8c0], [2, 0x10dd2f00]]}
  layer.4.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x107f8b80],
      [0, 0x10891bc0], [0, 0x1092ac00], [0, 0x109c3c40], [0, 0x10a5cc80], [0, 0x10af5cc0], [0, 0x10b8ed00], [0, 0x10c27d40]]}
  layer.4.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11703500],
      [4, 0x11704ec0], [4, 0x11706880], [4, 0x11708240], [4, 0x11709c00], [4, 0x1170b5c0], [4, 0x1170cf80], [4, 0x1170e940]]}
  layer.4.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1069bf80]]}
  layer.4.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x108d1300]]}
  layer.5.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10ab5cc0], [5, 0x10b4ed00]]}
  layer.5.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x106a8bc0], [3, 0x106af200]]}
  layer.5.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x108ddf40], [1, 0x10976f80]]}
  layer.5.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10be7d40],
      [5, 0x10bee380]]}
  layer.5.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10a0ffc0], [1, 0x10aa9000]]}
  layer.5.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10bf49c0], [5, 0x10bfb000]]}
  layer.5.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10dd9540], [2, 0x10e72580]]}
  layer.5.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10cc0d80], [0, 0x10cc73c0]]}
  layer.5.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10ccda00]]}
  layer.5.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x11710300]]}
  layer.5.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x106b5840], [3, 0x1074e880], [3, 0x107e78c0], [3, 0x10880900], [3, 0x10919940], [3, 0x109b2980], [3, 0x10a4b9c0], [3, 0x10ae4a00]]}
  layer.5.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10cda640],
      [0, 0x10ce0c80], [0, 0x10ce72c0], [0, 0x10ced900], [0, 0x10cf3f40], [0, 0x10cfa580], [0, 0x10d00bc0], [0, 0x10d07200]]}
  layer.5.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1171cf40],
      [4, 0x117b5f80], [4, 0x1184efc0], [4, 0x118e8000], [4, 0x11981040], [4, 0x11a1a080], [4, 0x11ab30c0], [4, 0x11b4c100]]}
  layer.5.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10f0b5c0],
      [2, 0x10f0cf80], [2, 0x10f0e940], [2, 0x10f10300], [2, 0x10f11cc0], [2, 0x10f13680], [2, 0x10f15040], [2, 0x10f16a00]]}
  layer.5.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10f183c0]]}
  layer.5.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10c01640]]}
  layer.6.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10c0e280], [5, 0x10ca72c0]]}
  layer.6.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10d40300], [5, 0x10d46940]]}
  layer.6.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x10f25000], [2, 0x10fbe040]]}
  layer.6.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d0d840],
      [0, 0x10d13e80]]}
  layer.6.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11057080], [2, 0x110f00c0]]}
  layer.6.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10d1a4c0], [0, 0x10d20b00]]}
  layer.6.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x11be5140], [4, 0x11c7e180]]}
  layer.6.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11189100], [2, 0x1118f740]]}
  layer.6.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11195d80]]}
  layer.6.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10d27140]]}
  layer.6.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x11d171c0], [4, 0x11db0200], [4, 0x11e49240], [4, 0x11ee2280], [4, 0x11f7b2c0], [4, 0x12014300], [4, 0x120ad340], [4, 0x12146380]]}
  layer.6.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x111a29c0],
      [2, 0x111a9000], [2, 0x111af640], [2, 0x111b5c80], [2, 0x111bc2c0], [2, 0x111c2900], [2, 0x111c8f40], [2, 0x111cf580]]}
  layer.6.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d33d80],
      [0, 0x10dccdc0], [0, 0x10e65e00], [0, 0x10efee40], [0, 0x10f97e80], [0, 0x11030ec0], [0, 0x110c9f00], [0, 0x11162f40]]}
  layer.6.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x121df3c0],
      [4, 0x121e0d80], [4, 0x121e2740], [4, 0x121e4100], [4, 0x121e5ac0], [4, 0x121e7480], [4, 0x121e8e40], [4, 0x121ea800]]}
  layer.6.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x111d5bc0]]}
  layer.6.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x111fbf80]]}
  layer.7.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x121ec1c0], [4, 0x12285200]]}
  layer.7.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x111e2800], [2, 0x111e8e40]]}
  layer.7.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11208bc0], [0, 0x112a1c00]]}
  layer.7.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1231e240],
      [4, 0x12324880]]}
  layer.7.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x1133ac40], [0, 0x113d3c80]]}
  layer.7.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x1232aec0], [4, 0x12331500]]}
  layer.7.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x111ef480], [2, 0x112884c0]]}
  layer.7.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x1146ccc0], [0, 0x11473300]]}
  layer.7.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10b7da40]]}
  layer.7.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10b42040]]}
  layer.7.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10d4cf80], [5, 0x10de5fc0], [5, 0x10e7f000], [5, 0x10f18040], [5, 0x10fb1080], [5, 0x1104a0c0], [5, 0x110e3100], [5, 0x1117c140]]}
  layer.7.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10b8a680],
      [3, 0x10b90cc0], [3, 0x10b97300], [3, 0x10b9d940], [3, 0x10ba3f80], [3, 0x10baa5c0], [3, 0x10bb0c00], [3, 0x10bb7240]]}
  layer.7.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10b4ec80],
      [1, 0x10be7cc0], [1, 0x10c80d00], [1, 0x10d19d40], [1, 0x10db2d80], [1, 0x10e4bdc0], [1, 0x10ee4e00], [1, 0x10f7de40]]}
  layer.7.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11215180],
      [5, 0x11216b40], [5, 0x11218500], [5, 0x11219ec0], [5, 0x1121b880], [5, 0x1121d240], [5, 0x1121ec00], [5, 0x112205c0]]}
  layer.7.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11221f80]]}
  layer.7.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10bbd880]]}
  layer.8.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x11016e80], [1, 0x110afec0]]}
  layer.8.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1122ebc0], [5, 0x11235200]]}
  layer.8.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10bca4c0], [3, 0x10c63500]]}
  layer.8.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x11148f00],
      [1, 0x1114f540]]}
  layer.8.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10cfc540], [3, 0x10d95580]]}
  layer.8.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11479940], [0, 0x1147ff80]]}
  layer.8.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x114865c0], [0, 0x1151f600]]}
  layer.8.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x115b8640], [0, 0x115bec80]]}
  layer.8.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x115c52c0]]}
  layer.8.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x12337b40]]}
  layer.8.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11321500], [2, 0x113ba540], [2, 0x11453580], [2, 0x114ec5c0], [2, 0x11585600], [2, 0x1161e640], [2, 0x116b7680], [2, 0x117506c0]]}
  layer.8.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10e2e5c0],
      [3, 0x10e34c00], [3, 0x10e3b240], [3, 0x10e41880], [3, 0x10e47ec0], [3, 0x10e4e500], [3, 0x10e54b40], [3, 0x10e5b180]]}
  layer.8.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12344780],
      [4, 0x123dd7c0], [4, 0x12476800], [4, 0x1250f840], [4, 0x125a8880], [4, 0x126418c0], [4, 0x126da900], [4, 0x12773940]]}
  layer.8.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x117e9700],
      [2, 0x117eb0c0], [2, 0x117eca80], [2, 0x117ee440], [2, 0x117efe00], [2, 0x117f17c0], [2, 0x117f3180], [2, 0x117f4b40]]}
  layer.8.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x117f6500]]}
  layer.8.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11803140]]}
  layer.9.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1180fd80], [2, 0x118a8dc0]]}
  layer.9.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10e617c0], [3, 0x10e67e00]]}
  layer.9.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x11155b80], [1, 0x111eebc0]]}
  layer.9.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1123b840],
      [5, 0x11241e80]]}
  layer.9.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x11287c00], [1, 0x11320c40]]}
  layer.9.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x112484c0], [5, 0x1124eb00]]}
  layer.9.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10e6e440], [3, 0x10f07480]]}
  layer.9.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x113b9c80], [1, 0x113c02c0]]}
  layer.9.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x113c6900]]}
  layer.9.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x11255140]]}
  layer.9.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10fa04c0], [3, 0x11039500], [3, 0x110d2540], [3, 0x1116b580], [3, 0x112045c0], [3, 0x1129d600], [3, 0x11336640], [3, 0x113cf680]]}
  layer.9.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x113d3540],
      [1, 0x113d9b80], [1, 0x113e01c0], [1, 0x113e6800], [1, 0x113ece40], [1, 0x113f3480], [1, 0x113f9ac0], [1, 0x11400100]]}
  layer.9.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11261d80],
      [5, 0x112fadc0], [5, 0x11393e00], [5, 0x1142ce40], [5, 0x114c5e80], [5, 0x1155eec0], [5, 0x115f7f00], [5, 0x11690f40]]}
  layer.9.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x114686c0],
      [3, 0x1146a080], [3, 0x1146ba40], [3, 0x1146d400], [3, 0x1146edc0], [3, 0x11470780], [3, 0x11472140], [3, 0x11473b00]]}
  layer.9.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11941e00]]}
  layer.9.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x115d1f00]]}
  layer.10.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x1280c980], [4, 0x128a59c0]]}
  layer.10.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x1194ea40], [2, 0x11955080]]}
  layer.10.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x115deb40], [0, 0x11677b80]]}
  layer.10.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x1293ea00], [4, 0x12945040]]}
  layer.10.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x11729f80], [5, 0x117c2fc0]]}
  layer.10.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x114754c0], [3, 0x1147bb00]]}
  layer.10.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x11406740], [1, 0x1149f780]]}
  layer.10.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1185c000], [5, 0x11862640]]}
  layer.10.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x11710bc0]]}
  layer.10.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1195b6c0]]}
  layer.10.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x1171d800], [0, 0x117b6840], [0, 0x1184f880], [0, 0x118e88c0], [0, 0x11981900], [0, 0x11a1a940], [0, 0x11ab3980], [0, 0x11b4c9c0]]}
  layer.10.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x1294b680], [4, 0x12951cc0], [4, 0x12958300], [4, 0x1295e940], [4, 0x12964f80], [4, 0x1296b5c0], [4, 0x12971c00], [4, 0x12978240]]}
  layer.10.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11968300],
      [2, 0x11a01340], [2, 0x11a9a380], [2, 0x11b333c0], [2, 0x11bcc400], [2, 0x11c65440], [2, 0x11cfe480], [2, 0x11d974c0]]}
  layer.10.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11be5a00],
      [0, 0x11be73c0], [0, 0x11be8d80], [0, 0x11bea740], [0, 0x11bec100], [0, 0x11bedac0], [0, 0x11bef480], [0, 0x11bf0e40]]}
  layer.10.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x11868c80]]}
  layer.10.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11482140]]}
  layer.11.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x115387c0], [1, 0x115d1800]]}
  layer.11.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x118758c0], [5, 0x1187bf00]]}
  layer.11.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1148ed80], [3, 0x11527dc0]]}
  layer.11.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1166a840], [1, 0x11670e80]]}
  layer.11.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x115c0e00], [3, 0x11659e40]]}
  layer.11.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x116774c0], [1, 0x1167db00]]}
  layer.11.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x11bf2800], [0, 0x11c8b840]]}
  layer.11.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x11882540], [5, 0x11888b80]]}
  layer.11.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1188f1c0]]}
  layer.11.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x116f2e80]]}
  layer.11.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x11684140], [1, 0x1171d180], [1, 0x117b61c0], [1, 0x1184f200], [1, 0x118e8240], [1, 0x11981280], [1, 0x11a1a2c0], [1, 0x11ab3300]]}
  layer.11.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x1297e880], [4, 0x12984ec0], [4, 0x1298b500], [4, 0x12991b40], [4, 0x12998180], [4, 0x1299e7c0], [4, 0x129a4e00], [4, 0x129ab440]]}
  layer.11.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x129b1a80],
      [4, 0x12a4aac0], [4, 0x12ae3b00], [4, 0x12b7cb40], [4, 0x12c15b80], [4, 0x12caebc0], [4, 0x12d47c00], [4, 0x12de0c40]]}
  layer.11.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11e30500],
      [2, 0x11e31ec0], [2, 0x11e33880], [2, 0x11e35240], [2, 0x11e36c00], [2, 0x11e385c0], [2, 0x11e39f80], [2, 0x11e3b940]]}
  layer.11.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11e3d300]]}
  layer.11.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11d24880]]}

  # constant
  input_1_multiply_64_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e79c80]]}
  lc.input_tensor.softmax_66.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e49f40]]}
  lc.input_tensor.layernorm_86.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1189be00]]}
  lc.input_tensor.layernorm_86.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e7a540]]}
  dc.input_tensor.layernorm_86.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x116ffac0]]}
  lc.input_tensor.layernorm_86.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e4a800]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b4c340]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x1189c6c0]]}
  lc.input_tensor.layernorm_100.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1189cf80]]}
  lc.input_tensor.layernorm_100.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e7ae00]]}
  dc.input_tensor.layernorm_100.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11701d00]]}
  lc.input_tensor.layernorm_100.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e4b0c0]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b4cc00]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1189d840]]}
  input_1_multiply_117_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e4b980]]}
  lc.input_tensor.softmax_119.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b4d4c0]]}
  lc.input_tensor.layernorm_139.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e7b6c0]]}
  lc.input_tensor.layernorm_139.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11703f40]]}
  dc.input_tensor.layernorm_139.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11e4c240]]}
  lc.input_tensor.layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11d314c0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b4dd80]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x1189e100]]}
  lc.input_tensor.layernorm_153.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b4e640]]}
  lc.input_tensor.layernorm_153.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d31d80]]}
  dc.input_tensor.layernorm_153.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11d32640]]}
  lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11704800]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e4e480]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11d34880]]}
  input_1_multiply_170_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e4ed40]]}
  lc.input_tensor.softmax_172.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b4ef00]]}
  lc.input_tensor.layernorm_192.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b4f7c0]]}
  lc.input_tensor.layernorm_192.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d35140]]}
  dc.input_tensor.layernorm_192.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1189e9c0]]}
  lc.input_tensor.layernorm_192.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e7bf80]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x117050c0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b50080]]}
  lc.input_tensor.layernorm_206.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11705980]]}
  lc.input_tensor.layernorm_206.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e4f600]]}
  dc.input_tensor.layernorm_206.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x11b50940]]}
  lc.input_tensor.layernorm_206.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11d35a00]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x118a0c00]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11706240]]}
  input_1_multiply_223_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d362c0]]}
  lc.input_tensor.softmax_225.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118a14c0]]}
  lc.input_tensor.layernorm_245.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e7c840]]}
  lc.input_tensor.layernorm_245.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b52b80]]}
  dc.input_tensor.layernorm_245.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11d36b80]]}
  lc.input_tensor.layernorm_245.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x118a1d80]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x12e7d100]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x11e4fec0]]}
  lc.input_tensor.layernorm_259.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118a2640]]}
  lc.input_tensor.layernorm_259.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e50780]]}
  dc.input_tensor.layernorm_259.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x12e7d9c0]]}
  lc.input_tensor.layernorm_259.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11706b00]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e51040]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11d38dc0]]}
  input_1_multiply_276_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e7fc00]]}
  lc.input_tensor.softmax_278.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x117073c0]]}
  lc.input_tensor.layernorm_298.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d39680]]}
  lc.input_tensor.layernorm_298.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118a2f00]]}
  dc.input_tensor.layernorm_298.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x12e804c0]]}
  lc.input_tensor.layernorm_298.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e51900]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x11707c80]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b53440]]}
  lc.input_tensor.layernorm_312.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11708540]]}
  lc.input_tensor.layernorm_312.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e521c0]]}
  dc.input_tensor.layernorm_312.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x11b53d00]]}
  lc.input_tensor.layernorm_312.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11d39f40]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e82700]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e52a80]]}
  input_1_multiply_329_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e82fc0]]}
  lc.input_tensor.softmax_331.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11708e00]]}
  lc.input_tensor.layernorm_351.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118a37c0]]}
  lc.input_tensor.layernorm_351.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e83880]]}
  dc.input_tensor.layernorm_351.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x117096c0]]}
  lc.input_tensor.layernorm_351.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e53340]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b55f40]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x118a4080]]}
  lc.input_tensor.layernorm_365.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b56800]]}
  lc.input_tensor.layernorm_365.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d3a800]]}
  dc.input_tensor.layernorm_365.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x118a4940]]}
  lc.input_tensor.layernorm_365.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e84140]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1170b900]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b570c0]]}
  input_1_multiply_382_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118a6b80]]}
  lc.input_tensor.softmax_384.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e84a00]]}
  lc.input_tensor.layernorm_404.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b57980]]}
  lc.input_tensor.layernorm_404.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e53c00]]}
  dc.input_tensor.layernorm_404.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x118a7440]]}
  lc.input_tensor.layernorm_404.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e852c0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x1170c1c0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b58240]]}
  lc.input_tensor.layernorm_418.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1170ca80]]}
  lc.input_tensor.layernorm_418.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d3b0c0]]}
  dc.input_tensor.layernorm_418.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x118a9680]]}
  lc.input_tensor.layernorm_418.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e85b80]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1170d340]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b58b00]]}
  input_1_multiply_435_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d3b980]]}
  lc.input_tensor.softmax_437.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e544c0]]}
  lc.input_tensor.layernorm_457.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118ab8c0]]}
  lc.input_tensor.layernorm_457.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e86440]]}
  dc.input_tensor.layernorm_457.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x1170dc00]]}
  lc.input_tensor.layernorm_457.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e54d80]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b593c0]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x11e55640]]}
  lc.input_tensor.layernorm_471.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e86d00]]}
  lc.input_tensor.layernorm_471.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1170fe40]]}
  dc.input_tensor.layernorm_471.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11e55f00]]}
  lc.input_tensor.layernorm_471.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b59c80]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11d3c240]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e875c0]]}
  input_1_multiply_488_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d3cb00]]}
  lc.input_tensor.softmax_490.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118ac180]]}
  lc.input_tensor.layernorm_510.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118aca40]]}
  lc.input_tensor.layernorm_510.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e87e80]]}
  dc.input_tensor.layernorm_510.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11710700]]}
  lc.input_tensor.layernorm_510.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e58140]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11b5a540]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x118ad300]]}
  lc.input_tensor.layernorm_524.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b5ae00]]}
  lc.input_tensor.layernorm_524.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d3d3c0]]}
  dc.input_tensor.layernorm_524.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x118adbc0]]}
  lc.input_tensor.layernorm_524.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e88740]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11712940]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b5b6c0]]}
  input_1_multiply_541_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e89000]]}
  lc.input_tensor.softmax_543.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11713200]]}
  lc.input_tensor.layernorm_563.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d3dc80]]}
  lc.input_tensor.layernorm_563.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b5bf80]]}
  dc.input_tensor.layernorm_563.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x12e898c0]]}
  lc.input_tensor.layernorm_563.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11713ac0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x11e58a00]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x11d3e540]]}
  lc.input_tensor.layernorm_577.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11e592c0]]}
  lc.input_tensor.layernorm_577.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118afe00]]}
  dc.input_tensor.layernorm_577.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x118b06c0]]}
  lc.input_tensor.layernorm_577.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e8bb00]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11714380]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b5c840]]}
  input_1_multiply_594_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118b2900]]}
  lc.input_tensor.softmax_596.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b5d100]]}
  lc.input_tensor.layernorm_616.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e8c3c0]]}
  lc.input_tensor.layernorm_616.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11714c40]]}
  dc.input_tensor.layernorm_616.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11e59b80]]}
  lc.input_tensor.layernorm_616.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b5d9c0]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x11d3ee00]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x11715500]]}
  lc.input_tensor.layernorm_630.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118b31c0]]}
  lc.input_tensor.layernorm_630.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e8cc80]]}
  dc.input_tensor.layernorm_630.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11715dc0]]}
  lc.input_tensor.layernorm_630.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11e5bdc0]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x11d3f6c0]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e8d540]]}
  input_1_multiply_647_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d3ff80]]}
  lc.input_tensor.softmax_649.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118b3a80]]}
  lc.input_tensor.layernorm_669.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e8de00]]}
  lc.input_tensor.layernorm_669.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x11718000]]}
  dc.input_tensor.layernorm_669.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11e5c680]]}
  lc.input_tensor.layernorm_669.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b5e280]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x11d40840]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x12e8e6c0]]}
  lc.input_tensor.layernorm_683.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b5eb40]]}
  lc.input_tensor.layernorm_683.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x11d41100]]}
  dc.input_tensor.layernorm_683.4: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x118b4340]]}
  lc.input_tensor.layernorm_683.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12e8ef80]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x117188c0]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x11b5f400]]}

  # epoch_to_epoch
  e2e_softmax_119.dc.multiply.3_0: {input: softmax_119.dc.multiply.3, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x11719180]]}
  e2e_matmul_123_0: {input: matmul_123, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11e5e8c0],
      [2, 0x11e91900]]}
  e2e_layernorm_100.dc.add.10_0: {input: layernorm_100.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x12e8f840]]}
  e2e_layernorm_192.dc.add.5_0: {input: layernorm_192.dc.add.5, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x118b6580]]}
  e2e_buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8_0: {input: buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [
      2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11d419c0]]}
  e2e_layernorm_245.dc.add.10_0: {input: layernorm_245.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11b5fcc0]]}
  e2e_gelu_304_0: {input: gelu_304, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 12], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x117e51c0],
      [3, 0x118b1200]]}
  e2e_layernorm_298.dc.add.10_0: {input: layernorm_298.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x11ec4940]]}
  e2e_layernorm_365.dc.subtract.1_0: {input: layernorm_365.dc.subtract.1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x12ef5880]]}
  e2e_layernorm_418.dc.multiply.9_0: {input: layernorm_418.dc.multiply.9, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x118ba9c0]]}
  e2e_layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11da7a00]]}
  e2e_layernorm_471.dc.add.10_0: {input: layernorm_471.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x11bc5d00]]}
  e2e_softmax_490.dc.multiply.3_0: {input: softmax_490.dc.multiply.3, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x11f2a980]]}
  e2e_layernorm_563.dc.multiply.2_0: {input: layernorm_563.dc.multiply.2, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1197d240]]}
  e2e_buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8_0: {input: buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [
      2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12f5b8c0]]}
  e2e_layernorm_616.dc.add.10_0: {input: layernorm_616.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x11920a00]]}
  e2e_gelu_675_0: {input: gelu_675, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 12], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11dc1240],
      [0, 0x11e8d280]]}
  e2e_buffer_0_layernorm_669.dc.add.10_add_682_0: {input: buffer_0_layernorm_669.dc.add.10_add_682, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x11c2bd40]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 2
    matmul_50: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {
        bias: true, m_k: 8, u_kt: 3}}
    matmul_56: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true,
        m_k: 8, u_kt: 3}}
    matmul_62: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_50, matmul_56], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_64: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [matmul_62, input_1_multiply_64_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_65: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_64, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_66.dc.exp.0: {type: exp, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_65], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_66.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_66.dc.exp.0, lc.input_tensor.softmax_66.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
      attributes: {m_k: 1, u_kt: 4}}
    softmax_66.dc.reciprocal.2: {type: reciprocal, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_66.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_66.dc.multiply.3: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_66.dc.exp.0, softmax_66.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_70: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {
        bias: true, m_k: 8, u_kt: 3}}
    matmul_77: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_66.dc.multiply.3, matmul_70], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_81: {type: matmul, grid_loc: [1, 6], grid_size: [1, 2], inputs: [matmul_77, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_85: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_81, hidden_states], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_86.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_85, lc.input_tensor.layernorm_86.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_86.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_85, layernorm_86.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_86.dc.multiply.2: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_86.dc.subtract.1, layernorm_86.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_86.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_86.dc.multiply.2, lc.input_tensor.layernorm_86.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_86.dc.add.5: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_86.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_86.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_86.dc.sqrt.6: {type: sqrt, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_86.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_86.dc.reciprocal.7: {type: reciprocal, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_86.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_86.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_86.dc.reciprocal.7, lc.input_tensor.layernorm_86.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_86.dc.subtract.1_buffer_0_layernorm_86.dc.subtract.1_layernorm_86.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_86.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_86.dc.subtract.1_layernorm_86.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_86.dc.subtract.1_buffer_0_layernorm_86.dc.subtract.1_layernorm_86.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_86.dc.multiply.8: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [buffer_0_layernorm_86.dc.subtract.1_layernorm_86.dc.multiply.8, layernorm_86.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_86.dc.multiply.9: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_86.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_86.dc.add.10: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_86.dc.multiply.9, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_89: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [layernorm_86.dc.add.10, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_92: {type: gelu, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_89], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_95: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [gelu_92, layer.0.output.dense.weight, layer.0.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_86.dc.add.10_add_99: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_86.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_99: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [matmul_95, buffer_0_layernorm_86.dc.add.10_add_99], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_100.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_99, lc.input_tensor.layernorm_100.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_100.dc.subtract.1: {type: subtract, grid_loc: [5, 5], grid_size: [1, 1], inputs: [add_99, layernorm_100.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_100.dc.multiply.2: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_100.dc.subtract.1, layernorm_100.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_100.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_100.dc.multiply.2, lc.input_tensor.layernorm_100.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_100.dc.add.5: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_100.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_100.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_100.dc.sqrt.6: {type: sqrt, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_100.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_100.dc.reciprocal.7: {type: reciprocal, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_100.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_100.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_100.dc.reciprocal.7, lc.input_tensor.layernorm_100.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_100.dc.subtract.1_buffer_0_layernorm_100.dc.subtract.1_layernorm_100.dc.multiply.8: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_100.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_100.dc.subtract.1_layernorm_100.dc.multiply.8: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_100.dc.subtract.1_buffer_0_layernorm_100.dc.subtract.1_layernorm_100.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_100.dc.multiply.8: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_100.dc.subtract.1_layernorm_100.dc.multiply.8, layernorm_100.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_100.dc.multiply.9: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_100.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_100.dc.add.10: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_100.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_103: {type: matmul, grid_loc: [8, 3], grid_size: [1, 2], inputs: [layernorm_100.dc.add.10, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_109: {type: matmul, grid_loc: [8, 5], grid_size: [1, 2], inputs: [layernorm_100.dc.add.10, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_115: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [matmul_103, matmul_109], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_117: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [matmul_115, input_1_multiply_117_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_118: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [multiply_117, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_119.dc.exp.0: {type: exp, grid_loc: [9, 2], grid_size: [1, 1], inputs: [add_118], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_119.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [softmax_119.dc.exp.0, lc.input_tensor.softmax_119.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_119.dc.reciprocal.2: {type: reciprocal, grid_loc: [9, 4], grid_size: [1, 1], inputs: [softmax_119.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_119.dc.multiply.3: {type: multiply, grid_loc: [9, 5], grid_size: [1, 1], inputs: [softmax_119.dc.exp.0, softmax_119.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_123: {type: matmul, grid_loc: [9, 6], grid_size: [1, 2], inputs: [layernorm_100.dc.add.10, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}

  fwd_1:
    target_device: 0
    input_count: 2
    matmul_130: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_softmax_119.dc.multiply.3_0, e2e_matmul_123_0], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_134: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [matmul_130, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_100.dc.add.10_add_138: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_layernorm_100.dc.add.10_0], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_138: {type: add, grid_loc: [0, 4], grid_size: [1, 1], inputs: [matmul_134, buffer_0_layernorm_100.dc.add.10_add_138], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_139.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [add_138, lc.input_tensor.layernorm_139.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_139.dc.subtract.1: {type: subtract, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_138, layernorm_139.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_139.dc.multiply.2: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_139.dc.subtract.1, layernorm_139.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_139.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_139.dc.multiply.2, lc.input_tensor.layernorm_139.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_139.dc.add.5: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_139.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_139.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_139.dc.sqrt.6: {type: sqrt, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_139.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_139.dc.reciprocal.7: {type: reciprocal, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_139.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_139.dc.reciprocal.7, lc.input_tensor.layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_139.dc.subtract.1_buffer_0_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_139.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_139.dc.subtract.1_buffer_0_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_139.dc.multiply.8: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_139.dc.subtract.1_layernorm_139.dc.multiply.8, layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_139.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_139.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_139.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_139.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_142: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layernorm_139.dc.add.10, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_145: {type: gelu, grid_loc: [2, 4], grid_size: [1, 2], inputs: [matmul_142], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_148: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [gelu_145, layer.1.output.dense.weight, layer.1.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_139.dc.add.10_add_152: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_139.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_152: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [matmul_148, buffer_0_layernorm_139.dc.add.10_add_152], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_153.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [add_152, lc.input_tensor.layernorm_153.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_153.dc.subtract.1: {type: subtract, grid_loc: [5, 1], grid_size: [1, 1], inputs: [add_152, layernorm_153.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_153.dc.multiply.2: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_153.dc.subtract.1, layernorm_153.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_153.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_153.dc.multiply.2, lc.input_tensor.layernorm_153.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_153.dc.add.5: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_153.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_153.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_153.dc.sqrt.6: {type: sqrt, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_153.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_153.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_153.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_153.dc.reciprocal.7, lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_153.dc.subtract.1_buffer_0_layernorm_153.dc.subtract.1_layernorm_153.dc.multiply.8: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_153.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_153.dc.subtract.1_layernorm_153.dc.multiply.8: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [buffer_0_layernorm_153.dc.subtract.1_buffer_0_layernorm_153.dc.subtract.1_layernorm_153.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_153.dc.multiply.8: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_153.dc.subtract.1_layernorm_153.dc.multiply.8, layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_153.dc.multiply.9: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_153.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_153.dc.add.10: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_153.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_156: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [layernorm_153.dc.add.10, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_162: {type: matmul, grid_loc: [7, 2], grid_size: [1, 2], inputs: [layernorm_153.dc.add.10, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_168: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [matmul_156, matmul_162], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_170: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_168, input_1_multiply_170_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_171: {type: add, grid_loc: [7, 5], grid_size: [1, 1], inputs: [multiply_170, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_172.dc.exp.0: {type: exp, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_171], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_172.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [softmax_172.dc.exp.0, lc.input_tensor.softmax_172.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_172.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 0], grid_size: [1, 1], inputs: [softmax_172.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_172.dc.multiply.3: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [softmax_172.dc.exp.0, softmax_172.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_176: {type: matmul, grid_loc: [8, 2], grid_size: [1, 2], inputs: [layernorm_153.dc.add.10, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_183: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [softmax_172.dc.multiply.3, matmul_176], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_187: {type: matmul, grid_loc: [8, 5], grid_size: [1, 2], inputs: [matmul_183, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_153.dc.add.10_add_191: {type: nop, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_153.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_191: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [matmul_187, buffer_0_layernorm_153.dc.add.10_add_191], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_192.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [add_191, lc.input_tensor.layernorm_192.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_192.dc.subtract.1: {type: subtract, grid_loc: [9, 2], grid_size: [1, 1], inputs: [add_191, layernorm_192.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_192.dc.multiply.2: {type: multiply, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_192.dc.subtract.1, layernorm_192.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_192.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_192.dc.multiply.2, lc.input_tensor.layernorm_192.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_192.dc.add.5: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_192.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_192.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_192.dc.subtract.1_buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8: {type: nop, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_192.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8: {type: nop, grid_loc: [9, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_192.dc.subtract.1_buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2:
    target_device: 0
    input_count: 2
    layernorm_192.dc.sqrt.6: {type: sqrt, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_192.dc.add.5_0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_192.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_192.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_192.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_192.dc.reciprocal.7, lc.input_tensor.layernorm_192.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_192.dc.multiply.8: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8_0, layernorm_192.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.2.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_192.dc.multiply.9: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_192.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_192.dc.add.10: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_192.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_195: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [layernorm_192.dc.add.10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_198: {type: gelu, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_195], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_201: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_198, layer.2.output.dense.weight, layer.2.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_192.dc.add.10_add_205: {type: nop, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_192.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_205: {type: add, grid_loc: [2, 3], grid_size: [1, 1], inputs: [matmul_201, buffer_0_layernorm_192.dc.add.10_add_205], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_206.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [add_205, lc.input_tensor.layernorm_206.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_206.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [1, 1], inputs: [add_205, layernorm_206.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_206.dc.multiply.2: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_206.dc.subtract.1, layernorm_206.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_206.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_206.dc.multiply.2, lc.input_tensor.layernorm_206.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_206.dc.add.5: {type: add, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_206.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_206.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_206.dc.sqrt.6: {type: sqrt, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_206.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_206.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_206.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_206.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_206.dc.reciprocal.7, lc.input_tensor.layernorm_206.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_206.dc.subtract.1_buffer_0_layernorm_206.dc.subtract.1_layernorm_206.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_206.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_206.dc.subtract.1_layernorm_206.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_206.dc.subtract.1_buffer_0_layernorm_206.dc.subtract.1_layernorm_206.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_206.dc.multiply.8: {type: multiply, grid_loc: [4, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_206.dc.subtract.1_layernorm_206.dc.multiply.8, layernorm_206.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_206.dc.multiply.9: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_206.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_206.dc.add.10: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_206.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_209: {type: matmul, grid_loc: [5, 3], grid_size: [1, 2], inputs: [layernorm_206.dc.add.10, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_215: {type: matmul, grid_loc: [5, 5], grid_size: [1, 2], inputs: [layernorm_206.dc.add.10, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_221: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [matmul_209, matmul_215], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_223: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_221, input_1_multiply_223_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_224: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [multiply_223, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_225.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_224], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_225.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [softmax_225.dc.exp.0, lc.input_tensor.softmax_225.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_225.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_225.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_225.dc.multiply.3: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [softmax_225.dc.exp.0, softmax_225.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_229: {type: matmul, grid_loc: [6, 6], grid_size: [1, 2], inputs: [layernorm_206.dc.add.10, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_236: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [softmax_225.dc.multiply.3, matmul_229], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_240: {type: matmul, grid_loc: [7, 1], grid_size: [1, 2], inputs: [matmul_236, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_206.dc.add.10_add_244: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_206.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_244: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_240, buffer_0_layernorm_206.dc.add.10_add_244], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_245.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [add_244, lc.input_tensor.layernorm_245.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_245.dc.subtract.1: {type: subtract, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_244, layernorm_245.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_245.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_245.dc.subtract.1, layernorm_245.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_245.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_245.dc.multiply.2, lc.input_tensor.layernorm_245.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_245.dc.add.5: {type: add, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_245.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_245.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_245.dc.sqrt.6: {type: sqrt, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_245.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_245.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_245.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_245.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_245.dc.reciprocal.7, lc.input_tensor.layernorm_245.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_245.dc.subtract.1_buffer_0_layernorm_245.dc.subtract.1_layernorm_245.dc.multiply.8: {type: nop, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_245.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_245.dc.subtract.1_layernorm_245.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_245.dc.subtract.1_buffer_0_layernorm_245.dc.subtract.1_layernorm_245.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_245.dc.multiply.8: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_245.dc.subtract.1_layernorm_245.dc.multiply.8, layernorm_245.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.3.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_245.dc.multiply.9: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_245.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_245.dc.add.10: {type: add, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_245.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}

  fwd_3:
    target_device: 0
    input_count: 2
    matmul_248: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_layernorm_245.dc.add.10_0, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_251: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_248], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_254: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_251, layer.3.output.dense.weight, layer.3.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_258: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [matmul_254, e2e_layernorm_245.dc.add.10_0], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_259.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_258, lc.input_tensor.layernorm_259.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_259.dc.subtract.1: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_258, layernorm_259.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_259.dc.multiply.2: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_259.dc.subtract.1, layernorm_259.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_259.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_259.dc.multiply.2, lc.input_tensor.layernorm_259.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_259.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_259.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_259.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_259.dc.sqrt.6: {type: sqrt, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_259.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_259.dc.reciprocal.7: {type: reciprocal, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_259.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_259.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_259.dc.reciprocal.7, lc.input_tensor.layernorm_259.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_259.dc.subtract.1_buffer_0_layernorm_259.dc.subtract.1_layernorm_259.dc.multiply.8: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_259.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_259.dc.subtract.1_layernorm_259.dc.multiply.8: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_259.dc.subtract.1_buffer_0_layernorm_259.dc.subtract.1_layernorm_259.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_259.dc.multiply.8: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_259.dc.subtract.1_layernorm_259.dc.multiply.8, layernorm_259.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_259.dc.multiply.9: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_259.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_259.dc.add.10: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_259.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_262: {type: matmul, grid_loc: [4, 2], grid_size: [1, 2], inputs: [layernorm_259.dc.add.10, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_268: {type: matmul, grid_loc: [4, 4], grid_size: [1, 2], inputs: [layernorm_259.dc.add.10, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_274: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [matmul_262, matmul_268], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_276: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_274, input_1_multiply_276_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_277: {type: add, grid_loc: [5, 0], grid_size: [1, 1], inputs: [multiply_276, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_278.dc.exp.0: {type: exp, grid_loc: [5, 1], grid_size: [1, 1], inputs: [add_277], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_278.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [softmax_278.dc.exp.0, lc.input_tensor.softmax_278.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_278.dc.reciprocal.2: {type: reciprocal, grid_loc: [5, 3], grid_size: [1, 1], inputs: [softmax_278.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_278.dc.multiply.3: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [softmax_278.dc.exp.0, softmax_278.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_282: {type: matmul, grid_loc: [5, 5], grid_size: [1, 2], inputs: [layernorm_259.dc.add.10, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_289: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [softmax_278.dc.multiply.3, matmul_282], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_293: {type: matmul, grid_loc: [6, 0], grid_size: [1, 2], inputs: [matmul_289, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_259.dc.add.10_add_297: {type: nop, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_259.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_297: {type: add, grid_loc: [6, 3], grid_size: [1, 1], inputs: [matmul_293, buffer_0_layernorm_259.dc.add.10_add_297], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_298.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_297, lc.input_tensor.layernorm_298.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_298.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [1, 1], inputs: [add_297, layernorm_298.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_298.dc.multiply.2: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_298.dc.subtract.1, layernorm_298.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_298.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_298.dc.multiply.2, lc.input_tensor.layernorm_298.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_298.dc.add.5: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_298.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_298.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_298.dc.sqrt.6: {type: sqrt, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_298.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_298.dc.reciprocal.7: {type: reciprocal, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_298.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_298.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_298.dc.reciprocal.7, lc.input_tensor.layernorm_298.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_298.dc.subtract.1_buffer_0_layernorm_298.dc.subtract.1_layernorm_298.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_298.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_298.dc.subtract.1_layernorm_298.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_298.dc.subtract.1_buffer_0_layernorm_298.dc.subtract.1_layernorm_298.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_298.dc.multiply.8: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_298.dc.subtract.1_layernorm_298.dc.multiply.8, layernorm_298.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.4.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_298.dc.multiply.9: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_298.dc.multiply.8, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_298.dc.add.10: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_298.dc.multiply.9, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_301: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [layernorm_298.dc.add.10, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_304: {type: gelu, grid_loc: [8, 3], grid_size: [1, 2], inputs: [matmul_301], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_4:
    target_device: 0
    input_count: 2
    matmul_307: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_gelu_304_0, layer.4.output.dense.weight, layer.4.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true,
        m_k: 8, u_kt: 12}}
    buffer_0_layernorm_298.dc.add.10_add_311: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [e2e_layernorm_298.dc.add.10_0], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_311: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_307, buffer_0_layernorm_298.dc.add.10_add_311], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_312.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_311, lc.input_tensor.layernorm_312.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_312.dc.subtract.1: {type: subtract, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_311, layernorm_312.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_312.dc.multiply.2: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_312.dc.subtract.1, layernorm_312.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_312.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_312.dc.multiply.2, lc.input_tensor.layernorm_312.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_312.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_312.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_312.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_312.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_312.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_312.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_312.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_312.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_312.dc.reciprocal.7, lc.input_tensor.layernorm_312.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_312.dc.subtract.1_buffer_0_layernorm_312.dc.subtract.1_layernorm_312.dc.multiply.8: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_312.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_312.dc.subtract.1_layernorm_312.dc.multiply.8: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_312.dc.subtract.1_buffer_0_layernorm_312.dc.subtract.1_layernorm_312.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_312.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_312.dc.subtract.1_layernorm_312.dc.multiply.8, layernorm_312.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_312.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_312.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_312.dc.add.10: {type: add, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_312.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_315: {type: matmul, grid_loc: [3, 1], grid_size: [1, 2], inputs: [layernorm_312.dc.add.10, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_321: {type: matmul, grid_loc: [3, 3], grid_size: [1, 2], inputs: [layernorm_312.dc.add.10, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_327: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [matmul_315, matmul_321], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_329: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_327, input_1_multiply_329_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_330: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [multiply_329, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_331.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_330], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_331.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [softmax_331.dc.exp.0, lc.input_tensor.softmax_331.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_331.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [1, 1], inputs: [softmax_331.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_331.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [softmax_331.dc.exp.0, softmax_331.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_335: {type: matmul, grid_loc: [4, 4], grid_size: [1, 2], inputs: [layernorm_312.dc.add.10, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_342: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [softmax_331.dc.multiply.3, matmul_335], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_346: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [matmul_342, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_312.dc.add.10_add_350: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_312.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_350: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_346, buffer_0_layernorm_312.dc.add.10_add_350], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_351.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [add_350, lc.input_tensor.layernorm_351.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_351.dc.subtract.1: {type: subtract, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_350, layernorm_351.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_351.dc.multiply.2: {type: multiply, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_351.dc.subtract.1, layernorm_351.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_351.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_351.dc.multiply.2, lc.input_tensor.layernorm_351.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_351.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_351.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_351.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_351.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_351.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_351.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_351.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_351.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_351.dc.reciprocal.7, lc.input_tensor.layernorm_351.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_351.dc.subtract.1_buffer_0_layernorm_351.dc.subtract.1_layernorm_351.dc.multiply.8: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_351.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_351.dc.subtract.1_layernorm_351.dc.multiply.8: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_351.dc.subtract.1_buffer_0_layernorm_351.dc.subtract.1_layernorm_351.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_351.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_351.dc.subtract.1_layernorm_351.dc.multiply.8, layernorm_351.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.5.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_351.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_351.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_351.dc.add.10: {type: add, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_351.dc.multiply.9, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_354: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [layernorm_351.dc.add.10, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_357: {type: gelu, grid_loc: [7, 2], grid_size: [1, 2], inputs: [matmul_354], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_360: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [gelu_357, layer.5.output.dense.weight, layer.5.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_351.dc.add.10_add_364: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_351.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_364: {type: add, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_360, buffer_0_layernorm_351.dc.add.10_add_364], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_365.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_364, lc.input_tensor.layernorm_365.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_365.dc.subtract.1: {type: subtract, grid_loc: [7, 7], grid_size: [1, 1], inputs: [add_364, layernorm_365.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}

  fwd_5:
    target_device: 0
    input_count: 2
    layernorm_365.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_layernorm_365.dc.subtract.1_0, e2e_layernorm_365.dc.subtract.1_0], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_365.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_365.dc.multiply.2, lc.input_tensor.layernorm_365.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_365.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_365.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_365.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_365.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_365.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_365.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_365.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_365.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_365.dc.reciprocal.7, lc.input_tensor.layernorm_365.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_365.dc.subtract.1_buffer_0_layernorm_365.dc.subtract.1_layernorm_365.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_365.dc.subtract.1_0], t: 1,
      mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_365.dc.subtract.1_layernorm_365.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_365.dc.subtract.1_buffer_0_layernorm_365.dc.subtract.1_layernorm_365.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_365.dc.multiply.8: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_365.dc.subtract.1_layernorm_365.dc.multiply.8, layernorm_365.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_365.dc.multiply.9: {type: multiply, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_365.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_365.dc.add.10: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_365.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_368: {type: matmul, grid_loc: [1, 5], grid_size: [1, 2], inputs: [layernorm_365.dc.add.10, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_374: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [layernorm_365.dc.add.10, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_380: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_368, matmul_374], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_382: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_380, input_1_multiply_382_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_383: {type: add, grid_loc: [2, 3], grid_size: [1, 1], inputs: [multiply_382, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_384.dc.exp.0: {type: exp, grid_loc: [2, 4], grid_size: [1, 1], inputs: [add_383], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_384.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [softmax_384.dc.exp.0, lc.input_tensor.softmax_384.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_384.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_384.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_384.dc.multiply.3: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_384.dc.exp.0, softmax_384.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_388: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [layernorm_365.dc.add.10, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_395: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [softmax_384.dc.multiply.3, matmul_388], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_399: {type: matmul, grid_loc: [3, 3], grid_size: [1, 2], inputs: [matmul_395, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_365.dc.add.10_add_403: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_365.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_403: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_399, buffer_0_layernorm_365.dc.add.10_add_403], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_404.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [add_403, lc.input_tensor.layernorm_404.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_404.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_403, layernorm_404.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_404.dc.multiply.2: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_404.dc.subtract.1, layernorm_404.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_404.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_404.dc.multiply.2, lc.input_tensor.layernorm_404.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_404.dc.add.5: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_404.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_404.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_404.dc.sqrt.6: {type: sqrt, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_404.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_404.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_404.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_404.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_404.dc.reciprocal.7, lc.input_tensor.layernorm_404.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_404.dc.subtract.1_buffer_0_layernorm_404.dc.subtract.1_layernorm_404.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_404.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_404.dc.subtract.1_layernorm_404.dc.multiply.8: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_404.dc.subtract.1_buffer_0_layernorm_404.dc.subtract.1_layernorm_404.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_404.dc.multiply.8: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_404.dc.subtract.1_layernorm_404.dc.multiply.8, layernorm_404.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.6.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_404.dc.multiply.9: {type: multiply, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_404.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_404.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_404.dc.multiply.9, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_407: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [layernorm_404.dc.add.10, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_410: {type: gelu, grid_loc: [5, 6], grid_size: [1, 2], inputs: [matmul_407], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_413: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [gelu_410, layer.6.output.dense.weight, layer.6.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_404.dc.add.10_add_417: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_404.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_417: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [matmul_413, buffer_0_layernorm_404.dc.add.10_add_417], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_418.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_417, lc.input_tensor.layernorm_418.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_418.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [1, 1], inputs: [add_417, layernorm_418.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_418.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_418.dc.subtract.1, layernorm_418.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_418.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_418.dc.multiply.2, lc.input_tensor.layernorm_418.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_418.dc.add.5: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_418.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_418.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_418.dc.sqrt.6: {type: sqrt, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_418.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_418.dc.reciprocal.7: {type: reciprocal, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_418.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_418.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_418.dc.reciprocal.7, lc.input_tensor.layernorm_418.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_418.dc.subtract.1_buffer_0_layernorm_418.dc.subtract.1_layernorm_418.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_418.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_418.dc.subtract.1_layernorm_418.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_418.dc.subtract.1_buffer_0_layernorm_418.dc.subtract.1_layernorm_418.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_418.dc.multiply.8: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_418.dc.subtract.1_layernorm_418.dc.multiply.8, layernorm_418.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_418.dc.multiply.9: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_418.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_6:
    target_device: 0
    input_count: 2
    layernorm_418.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_418.dc.multiply.9_0, e2e_layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_421: {type: matmul, grid_loc: [0, 1], grid_size: [1, 2], inputs: [layernorm_418.dc.add.10, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_427: {type: matmul, grid_loc: [0, 3], grid_size: [1, 2], inputs: [layernorm_418.dc.add.10, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_433: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [matmul_421, matmul_427], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_435: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_433, input_1_multiply_435_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_436: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [multiply_435, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_437.dc.exp.0: {type: exp, grid_loc: [1, 0], grid_size: [1, 1], inputs: [add_436], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_437.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [softmax_437.dc.exp.0, lc.input_tensor.softmax_437.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_437.dc.reciprocal.2: {type: reciprocal, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_437.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_437.dc.multiply.3: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_437.dc.exp.0, softmax_437.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_441: {type: matmul, grid_loc: [1, 4], grid_size: [1, 2], inputs: [layernorm_418.dc.add.10, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_448: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [softmax_437.dc.multiply.3, matmul_441], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_452: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_448, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_418.dc.add.10_add_456: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_418.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_456: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_452, buffer_0_layernorm_418.dc.add.10_add_456], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_457.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_456, lc.input_tensor.layernorm_457.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_457.dc.subtract.1: {type: subtract, grid_loc: [2, 4], grid_size: [1, 1], inputs: [add_456, layernorm_457.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_457.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_457.dc.subtract.1, layernorm_457.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_457.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_457.dc.multiply.2, lc.input_tensor.layernorm_457.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_457.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_457.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_457.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_457.dc.sqrt.6: {type: sqrt, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_457.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_457.dc.reciprocal.7: {type: reciprocal, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_457.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_457.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_457.dc.reciprocal.7, lc.input_tensor.layernorm_457.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_457.dc.subtract.1_buffer_0_layernorm_457.dc.subtract.1_layernorm_457.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_457.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_457.dc.subtract.1_layernorm_457.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_457.dc.subtract.1_buffer_0_layernorm_457.dc.subtract.1_layernorm_457.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_457.dc.multiply.8: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_457.dc.subtract.1_layernorm_457.dc.multiply.8, layernorm_457.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.7.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_457.dc.multiply.9: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_457.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_457.dc.add.10: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_457.dc.multiply.9, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_460: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layernorm_457.dc.add.10, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_463: {type: gelu, grid_loc: [4, 2], grid_size: [1, 2], inputs: [matmul_460], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_466: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [gelu_463, layer.7.output.dense.weight, layer.7.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_457.dc.add.10_add_470: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_457.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_470: {type: add, grid_loc: [4, 5], grid_size: [1, 1], inputs: [matmul_466, buffer_0_layernorm_457.dc.add.10_add_470], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_471.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [add_470, lc.input_tensor.layernorm_471.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_471.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_470, layernorm_471.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_471.dc.multiply.2: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_471.dc.subtract.1, layernorm_471.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_471.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_471.dc.multiply.2, lc.input_tensor.layernorm_471.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_471.dc.add.5: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_471.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_471.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_471.dc.sqrt.6: {type: sqrt, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_471.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_471.dc.reciprocal.7: {type: reciprocal, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_471.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_471.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_471.dc.reciprocal.7, lc.input_tensor.layernorm_471.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_471.dc.subtract.1_buffer_0_layernorm_471.dc.subtract.1_layernorm_471.dc.multiply.8: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_471.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_471.dc.subtract.1_layernorm_471.dc.multiply.8: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_471.dc.subtract.1_buffer_0_layernorm_471.dc.subtract.1_layernorm_471.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_471.dc.multiply.8: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_471.dc.subtract.1_layernorm_471.dc.multiply.8, layernorm_471.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_471.dc.multiply.9: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_471.dc.multiply.8, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_471.dc.add.10: {type: add, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_471.dc.multiply.9, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_474: {type: matmul, grid_loc: [8, 5], grid_size: [1, 2], inputs: [layernorm_471.dc.add.10, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_480: {type: matmul, grid_loc: [9, 0], grid_size: [1, 2], inputs: [layernorm_471.dc.add.10, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_486: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [matmul_474, matmul_480], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_488: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_486, input_1_multiply_488_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_489: {type: add, grid_loc: [9, 3], grid_size: [1, 1], inputs: [multiply_488, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_490.dc.exp.0: {type: exp, grid_loc: [9, 4], grid_size: [1, 1], inputs: [add_489], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_490.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [softmax_490.dc.exp.0, lc.input_tensor.softmax_490.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_490.dc.reciprocal.2: {type: reciprocal, grid_loc: [9, 6], grid_size: [1, 1], inputs: [softmax_490.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_490.dc.multiply.3: {type: multiply, grid_loc: [9, 7], grid_size: [1, 1], inputs: [softmax_490.dc.exp.0, softmax_490.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}

  fwd_7:
    target_device: 0
    input_count: 2
    matmul_494: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_layernorm_471.dc.add.10_0, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias], t: 1, mblock: [2, 3],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 4}], attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_501: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_softmax_490.dc.multiply.3_0, matmul_494], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_505: {type: matmul, grid_loc: [0, 3], grid_size: [1, 2], inputs: [matmul_501, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_471.dc.add.10_add_509: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_layernorm_471.dc.add.10_0], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_509: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_505, buffer_0_layernorm_471.dc.add.10_add_509], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_510.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_509, lc.input_tensor.layernorm_510.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_510.dc.subtract.1: {type: subtract, grid_loc: [1, 0], grid_size: [1, 1], inputs: [add_509, layernorm_510.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_510.dc.multiply.2: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_510.dc.subtract.1, layernorm_510.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_510.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_510.dc.multiply.2, lc.input_tensor.layernorm_510.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_510.dc.add.5: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_510.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_510.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_510.dc.sqrt.6: {type: sqrt, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_510.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_510.dc.reciprocal.7: {type: reciprocal, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_510.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_510.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_510.dc.reciprocal.7, lc.input_tensor.layernorm_510.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_510.dc.subtract.1_buffer_0_layernorm_510.dc.subtract.1_layernorm_510.dc.multiply.8: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_510.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_510.dc.subtract.1_layernorm_510.dc.multiply.8: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_510.dc.subtract.1_buffer_0_layernorm_510.dc.subtract.1_layernorm_510.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_510.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_510.dc.subtract.1_layernorm_510.dc.multiply.8, layernorm_510.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.8.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_510.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_510.dc.multiply.8, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_510.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_510.dc.multiply.9, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_513: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layernorm_510.dc.add.10, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_516: {type: gelu, grid_loc: [2, 6], grid_size: [1, 2], inputs: [matmul_513], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_519: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [gelu_516, layer.8.output.dense.weight, layer.8.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_510.dc.add.10_add_523: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_510.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_523: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [matmul_519, buffer_0_layernorm_510.dc.add.10_add_523], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_524.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [add_523, lc.input_tensor.layernorm_524.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_524.dc.subtract.1: {type: subtract, grid_loc: [5, 3], grid_size: [1, 1], inputs: [add_523, layernorm_524.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_524.dc.multiply.2: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_524.dc.subtract.1, layernorm_524.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_524.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_524.dc.multiply.2, lc.input_tensor.layernorm_524.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_524.dc.add.5: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_524.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_524.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_524.dc.sqrt.6: {type: sqrt, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_524.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_524.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_524.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_524.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_524.dc.reciprocal.7, lc.input_tensor.layernorm_524.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_524.dc.subtract.1_buffer_0_layernorm_524.dc.subtract.1_layernorm_524.dc.multiply.8: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_524.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_524.dc.subtract.1_layernorm_524.dc.multiply.8: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_524.dc.subtract.1_buffer_0_layernorm_524.dc.subtract.1_layernorm_524.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_524.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_524.dc.subtract.1_layernorm_524.dc.multiply.8, layernorm_524.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_524.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_524.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_524.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_524.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_527: {type: matmul, grid_loc: [7, 1], grid_size: [1, 2], inputs: [layernorm_524.dc.add.10, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_533: {type: matmul, grid_loc: [7, 3], grid_size: [1, 2], inputs: [layernorm_524.dc.add.10, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_539: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_527, matmul_533], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_541: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_539, input_1_multiply_541_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_542: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [multiply_541, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_543.dc.exp.0: {type: exp, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_542], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_543.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [softmax_543.dc.exp.0, lc.input_tensor.softmax_543.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_543.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 2], grid_size: [1, 1], inputs: [softmax_543.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_543.dc.multiply.3: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [softmax_543.dc.exp.0, softmax_543.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_547: {type: matmul, grid_loc: [8, 4], grid_size: [1, 2], inputs: [layernorm_524.dc.add.10, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_554: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_543.dc.multiply.3, matmul_547], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_558: {type: matmul, grid_loc: [9, 0], grid_size: [1, 2], inputs: [matmul_554, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], input_0_tms: [
        hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_524.dc.add.10_add_562: {type: nop, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_524.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_562: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_558, buffer_0_layernorm_524.dc.add.10_add_562], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_563.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [add_562, lc.input_tensor.layernorm_563.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_563.dc.subtract.1: {type: subtract, grid_loc: [9, 4], grid_size: [1, 1], inputs: [add_562, layernorm_563.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_563.dc.multiply.2: {type: multiply, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_563.dc.subtract.1, layernorm_563.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_563.dc.subtract.1_buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8: {type: nop, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_563.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8: {type: nop, grid_loc: [9, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_563.dc.subtract.1_buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_8:
    target_device: 0
    input_count: 2
    layernorm_563.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_563.dc.multiply.2_0, lc.input_tensor.layernorm_563.dc.reduce_avg.3.0], t: 1, mblock: [2,
        1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_563.dc.add.5: {type: add, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_563.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_563.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_563.dc.sqrt.6: {type: sqrt, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_563.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_563.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_563.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_563.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_563.dc.reciprocal.7, lc.input_tensor.layernorm_563.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_563.dc.multiply.8: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8_0, layernorm_563.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.9.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_563.dc.multiply.9: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_563.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_563.dc.add.10: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_563.dc.multiply.9, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_566: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layernorm_563.dc.add.10, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_569: {type: gelu, grid_loc: [1, 2], grid_size: [1, 2], inputs: [matmul_566], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_572: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_569, layer.9.output.dense.weight, layer.9.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_563.dc.add.10_add_576: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_563.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_576: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [matmul_572, buffer_0_layernorm_563.dc.add.10_add_576], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_577.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_576, lc.input_tensor.layernorm_577.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_577.dc.subtract.1: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_576, layernorm_577.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_577.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_577.dc.subtract.1, layernorm_577.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_577.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_577.dc.multiply.2, lc.input_tensor.layernorm_577.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_577.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_577.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_577.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_577.dc.sqrt.6: {type: sqrt, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_577.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_577.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_577.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_577.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_577.dc.reciprocal.7, lc.input_tensor.layernorm_577.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_577.dc.subtract.1_buffer_0_layernorm_577.dc.subtract.1_layernorm_577.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_577.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_577.dc.subtract.1_layernorm_577.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_577.dc.subtract.1_buffer_0_layernorm_577.dc.subtract.1_layernorm_577.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_577.dc.multiply.8: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_577.dc.subtract.1_layernorm_577.dc.multiply.8, layernorm_577.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_577.dc.multiply.9: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_577.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_577.dc.add.10: {type: add, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_577.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_580: {type: matmul, grid_loc: [5, 5], grid_size: [1, 2], inputs: [layernorm_577.dc.add.10, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_586: {type: matmul, grid_loc: [6, 0], grid_size: [1, 2], inputs: [layernorm_577.dc.add.10, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_592: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [matmul_580, matmul_586], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_594: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [matmul_592, input_1_multiply_594_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_595: {type: add, grid_loc: [6, 3], grid_size: [1, 1], inputs: [multiply_594, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_596.dc.exp.0: {type: exp, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_595], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_596.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [softmax_596.dc.exp.0, lc.input_tensor.softmax_596.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_596.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 6], grid_size: [1, 1], inputs: [softmax_596.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_596.dc.multiply.3: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_596.dc.exp.0, softmax_596.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_600: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [layernorm_577.dc.add.10, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_607: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_596.dc.multiply.3, matmul_600], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_611: {type: matmul, grid_loc: [7, 3], grid_size: [1, 2], inputs: [matmul_607, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      input_0_tms: [hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_577.dc.add.10_add_615: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_577.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_615: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_611, buffer_0_layernorm_577.dc.add.10_add_615], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_616.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [add_615, lc.input_tensor.layernorm_616.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_616.dc.subtract.1: {type: subtract, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_615, layernorm_616.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_616.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_616.dc.subtract.1, layernorm_616.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_616.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_616.dc.multiply.2, lc.input_tensor.layernorm_616.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_616.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_616.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_616.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_616.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_616.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_616.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_616.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_616.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_616.dc.reciprocal.7, lc.input_tensor.layernorm_616.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_616.dc.subtract.1_buffer_0_layernorm_616.dc.subtract.1_layernorm_616.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_616.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_616.dc.subtract.1_layernorm_616.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_616.dc.subtract.1_buffer_0_layernorm_616.dc.subtract.1_layernorm_616.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_616.dc.multiply.8: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_616.dc.subtract.1_layernorm_616.dc.multiply.8, layernorm_616.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.10.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_616.dc.multiply.9: {type: multiply, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_616.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_616.dc.add.10: {type: add, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_616.dc.multiply.9, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}

  fwd_9:
    target_device: 0
    input_count: 2
    matmul_619: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_layernorm_616.dc.add.10_0, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_622: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_619], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_625: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_622, layer.10.output.dense.weight, layer.10.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true, m_k: 8, u_kt: 12}}
    buffer_0_layernorm_616.dc.add.10_add_629: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_layernorm_616.dc.add.10_0], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_629: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_625, buffer_0_layernorm_616.dc.add.10_add_629], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_630.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_629, lc.input_tensor.layernorm_630.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_630.dc.subtract.1: {type: subtract, grid_loc: [1, 5], grid_size: [1, 1], inputs: [add_629, layernorm_630.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_630.dc.multiply.2: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_630.dc.subtract.1, layernorm_630.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_630.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_630.dc.multiply.2, lc.input_tensor.layernorm_630.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_630.dc.add.5: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_630.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_630.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_630.dc.sqrt.6: {type: sqrt, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_630.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_630.dc.reciprocal.7: {type: reciprocal, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_630.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_630.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_630.dc.reciprocal.7, lc.input_tensor.layernorm_630.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_630.dc.subtract.1_buffer_0_layernorm_630.dc.subtract.1_layernorm_630.dc.multiply.8: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_630.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_630.dc.subtract.1_layernorm_630.dc.multiply.8: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_630.dc.subtract.1_buffer_0_layernorm_630.dc.subtract.1_layernorm_630.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_630.dc.multiply.8: {type: multiply, grid_loc: [3, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_630.dc.subtract.1_layernorm_630.dc.multiply.8, layernorm_630.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_630.dc.multiply.9: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_630.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_630.dc.add.10: {type: add, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_630.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_633: {type: matmul, grid_loc: [4, 3], grid_size: [1, 2], inputs: [layernorm_630.dc.add.10, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_639: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [layernorm_630.dc.add.10, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_645: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_633, matmul_639], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 2}}
    multiply_647: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [matmul_645, input_1_multiply_647_tile_bcast_tile_bcast], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_648: {type: add, grid_loc: [5, 1], grid_size: [1, 1], inputs: [multiply_647, attention_mask], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 12}]}
    softmax_649.dc.exp.0: {type: exp, grid_loc: [5, 5], grid_size: [1, 1], inputs: [add_648], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_649.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [softmax_649.dc.exp.0, lc.input_tensor.softmax_649.dc.reduce_sum.1.0], t: 12, mblock: [2, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}, broadcast: {
            z: 12}], attributes: {m_k: 1, u_kt: 4}}
    softmax_649.dc.reciprocal.2: {type: reciprocal, grid_loc: [5, 7], grid_size: [1, 1], inputs: [softmax_649.dc.reduce_sum.1.lc1], t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_649.dc.multiply.3: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_649.dc.exp.0, softmax_649.dc.reciprocal.2], t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        64, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 4}]}
    matmul_653: {type: matmul, grid_loc: [5, 2], grid_size: [1, 2], inputs: [layernorm_630.dc.add.10, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_660: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [softmax_649.dc.multiply.3, matmul_653], t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 12], attributes: {m_k: 1, u_kt: 4}}
    matmul_664: {type: matmul, grid_loc: [6, 2], grid_size: [1, 2], inputs: [matmul_660, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      input_0_tms: [hstack: 12], attributes: {bias: true, m_k: 12, u_kt: 2}}
    buffer_0_layernorm_630.dc.add.10_add_668: {type: nop, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_630.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        32], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_668: {type: add, grid_loc: [6, 4], grid_size: [1, 1], inputs: [matmul_664, buffer_0_layernorm_630.dc.add.10_add_668], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_669.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [add_668, lc.input_tensor.layernorm_669.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_669.dc.subtract.1: {type: subtract, grid_loc: [6, 6], grid_size: [1, 1], inputs: [add_668, layernorm_669.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_669.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_669.dc.subtract.1, layernorm_669.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_669.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_669.dc.multiply.2, lc.input_tensor.layernorm_669.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_669.dc.add.5: {type: add, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_669.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_669.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_669.dc.sqrt.6: {type: sqrt, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_669.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_669.dc.reciprocal.7: {type: reciprocal, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_669.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_669.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_669.dc.reciprocal.7, lc.input_tensor.layernorm_669.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_669.dc.subtract.1_buffer_0_layernorm_669.dc.subtract.1_layernorm_669.dc.multiply.8: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_669.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_669.dc.subtract.1_layernorm_669.dc.multiply.8: {type: nop, grid_loc: [7, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_669.dc.subtract.1_buffer_0_layernorm_669.dc.subtract.1_layernorm_669.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_669.dc.multiply.8: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_669.dc.subtract.1_layernorm_669.dc.multiply.8, layernorm_669.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.11.attention.output.LayerNorm.weight], t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_669.dc.multiply.9: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_669.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_669.dc.add.10: {type: add, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_669.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    matmul_672: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [layernorm_669.dc.add.10, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias], t: 1, mblock: [2, 3], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}],
      attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_675: {type: gelu, grid_loc: [8, 5], grid_size: [1, 2], inputs: [matmul_672], t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_0_layernorm_669.dc.add.10_add_682: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_669.dc.add.10], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_10:
    target_device: 0
    input_count: 2
    matmul_678: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_gelu_675_0, layer.11.output.dense.weight, layer.11.output.dense.bias], t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 4}], attributes: {bias: true,
        m_k: 8, u_kt: 12}}
    add_682: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_678, e2e_buffer_0_layernorm_669.dc.add.10_add_682_0], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_683.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_682, lc.input_tensor.layernorm_683.dc.reduce_avg.0.0], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {m_k: 1, u_kt: 24}}
    layernorm_683.dc.subtract.1: {type: subtract, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_682, layernorm_683.dc.reduce_avg.0.lc1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        112, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 24}]}
    layernorm_683.dc.multiply.2: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_683.dc.subtract.1, layernorm_683.dc.subtract.1], t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_683.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_683.dc.multiply.2, lc.input_tensor.layernorm_683.dc.reduce_avg.3.0], t: 1, mblock: [2, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 24}], attributes: {
        m_k: 1, u_kt: 24}}
    layernorm_683.dc.add.5: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_683.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_683.4], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_683.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_683.dc.add.5], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_683.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_683.dc.sqrt.6], t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_683.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_683.dc.reciprocal.7, lc.input_tensor.layernorm_683.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_683.dc.subtract.1_buffer_0_layernorm_683.dc.subtract.1_layernorm_683.dc.multiply.8: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_683.dc.subtract.1], t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [272], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_683.dc.subtract.1_layernorm_683.dc.multiply.8: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_683.dc.subtract.1_buffer_0_layernorm_683.dc.subtract.1_layernorm_683.dc.multiply.8],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_683.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_0_layernorm_683.dc.subtract.1_layernorm_683.dc.multiply.8, layernorm_683.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 24}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_683.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_683.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [2, 6], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 4}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
      t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_683.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_683.dc.multiply.9, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true, t: 1, mblock: [
        2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 4}]}


programs:
- run_fwd:
  - param: [$p_loop_count]
  - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $lptr_q3: 0, $gptr_q5: 0, $lptr_q5: 0, $gptr_q7: 0, $gptr_q9: 0, $lptr_q17: 0, $gptr_q20: 0, $gptr_q15: 0, $gptr_q19: 0, $lptr_q19: 0, $gptr_q3: 0,
      $gptr_q17: 0, $lptr_q15: 0, $lptr_q13: 0, $lptr_q7: 0, $lptr_q20: 0, $gptr_q11: 0, $lptr_q9: 0, $gptr_q13: 0, $lptr_q11: 0}
  - staticvar: {$gptr_q0: 0, $lptr_q0: 0, $lptr_q10: 0, $gptr_q12: 0, $gptr_q14_shadow: 0, $gptr_q2: 0, $gptr_q4_shadow: 0, $gptr_q10: 0, $gptr_q16_shadow: 0, $gptr_q16: 0, $lptr_q8: 0, $gptr_q1: 0, $lptr_q12: 0,
      $gptr_q8_shadow: 0, $lptr_q16: 0, $gptr_q14: 0, $gptr_q8: 0, $gptr_q2_shadow: 0, $lptr_q14: 0, $gptr_q18: 0, $lptr_q4: 0, $gptr_q10_shadow: 0, $lptr_q6: 0, $lptr_q18: 0, $gptr_q6_shadow: 0, $gptr_q12_shadow: 0,
      $gptr_q6: 0, $gptr_q4: 0, $gptr_q1_shadow: 0, $lptr_q2: 0, $lptr_q1: 0}
  - varinst: [$gptr_q16, set, $gptr_q16_shadow]
  - varinst: [$gptr_q14, set, $gptr_q14_shadow]
  - varinst: [$gptr_q12, set, $gptr_q12_shadow]
  - varinst: [$gptr_q10, set, $gptr_q10_shadow]
  - varinst: [$gptr_q8, set, $gptr_q8_shadow]
  - varinst: [$gptr_q6, set, $gptr_q6_shadow]
  - varinst: [$gptr_q4, set, $gptr_q4_shadow]
  - varinst: [$gptr_q2, set, $gptr_q2_shadow]
  - varinst: [$gptr_q1, set, $gptr_q1_shadow]
  - loop: $p_loop_count
  - allocate_queue: [e2e_layernorm_100.dc.add.10_0, e2e_softmax_119.dc.multiply.3_0, e2e_matmul_123_0]
  - execute: {graph_name: fwd_0, queue_settings: {hidden_states: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, attention_mask: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_64_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_66.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_86.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_86.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_86.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_86.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_100.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_100.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_100.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_100.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_117_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_119.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
  - allocate_queue: [e2e_layernorm_192.dc.add.5_0, e2e_buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8_0]
  - execute: {graph_name: fwd_1, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_layernorm_100.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_softmax_119.dc.multiply.3_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
        e2e_matmul_123_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_139.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_139.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_139.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_139.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_153.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_153.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_153.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.key.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_170_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_172.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_192.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_192.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_192.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_100.dc.add.10_0, e2e_softmax_119.dc.multiply.3_0, e2e_matmul_123_0]
  - varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_245.dc.add.10_0]
  - execute: {graph_name: fwd_2, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_layernorm_192.dc.add.5_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, lc.input_tensor.layernorm_192.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.intermediate.dense.bias: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_206.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_206.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_206.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_206.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_223_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_225.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_245.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_245.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_245.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_245.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_192.dc.add.5_0, e2e_buffer_0_layernorm_192.dc.subtract.1_layernorm_192.dc.multiply.8_0]
  - varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_298.dc.add.10_0, e2e_gelu_304_0]
  - execute: {graph_name: fwd_3, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_layernorm_245.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_259.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_259.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_259.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_259.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_276_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_278.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_298.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_298.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_298.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_298.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_245.dc.add.10_0]
  - varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_365.dc.subtract.1_0]
  - execute: {graph_name: fwd_4, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_layernorm_298.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, e2e_gelu_304_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
        layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_312.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_312.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_312.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_312.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_329_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_331.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_351.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_351.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_351.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_351.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.intermediate.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_365.dc.reduce_avg.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_298.dc.add.10_0, e2e_gelu_304_0]
  - varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q8, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_418.dc.multiply.9_0, e2e_layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_5, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10}, e2e_layernorm_365.dc.subtract.1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, lc.input_tensor.layernorm_365.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_365.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_365.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_382_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_384.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_404.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_404.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_404.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_404.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_418.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_418.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_418.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_418.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_365.dc.subtract.1_0]
  - varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q10, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_471.dc.add.10_0, e2e_softmax_490.dc.multiply.3_0]
  - execute: {graph_name: fwd_6, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}, e2e_layernorm_418.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_435_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_437.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_457.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_457.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_457.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_457.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_471.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_471.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_471.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_471.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.key.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_488_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_490.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_418.dc.multiply.9_0, e2e_layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q13, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q12, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q13, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_563.dc.multiply.2_0, e2e_buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8_0]
  - execute: {graph_name: fwd_7, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_layernorm_471.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15}, e2e_softmax_490.dc.multiply.3_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
        layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.value.bias: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_510.dc.reduce_avg.0.0: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_510.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_510.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_510.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_524.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_524.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_524.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_524.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.key.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_541_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_543.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_563.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_471.dc.add.10_0, e2e_softmax_490.dc.multiply.3_0]
  - varinst: [$gptr_q14_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q15, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q14, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q15, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_616.dc.add.10_0]
  - execute: {graph_name: fwd_8, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16}, e2e_layernorm_563.dc.multiply.2_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, e2e_buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, lc.input_tensor.layernorm_563.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        dc.input_tensor.layernorm_563.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_563.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_577.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_577.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_577.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_577.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.key.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_594_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_596.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_616.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_616.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_616.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_616.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_563.dc.multiply.2_0, e2e_buffer_0_layernorm_563.dc.subtract.1_layernorm_563.dc.multiply.8_0]
  - varinst: [$gptr_q16_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q17, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q16, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q17, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_gelu_675_0, e2e_buffer_0_layernorm_669.dc.add.10_add_682_0]
  - execute: {graph_name: fwd_9, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, e2e_layernorm_616.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19}, layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.output.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_630.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_630.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_630.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_630.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_647_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_649.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_669.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_669.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_669.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_669.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_616.dc.add.10_0]
  - varinst: [$gptr_q18, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q19, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q18, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q19, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: fwd_10, queue_settings: {e2e_gelu_675_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, e2e_buffer_0_layernorm_669.dc.add.10_add_682_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_683.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_683.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_683.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_683.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_gelu_675_0, e2e_buffer_0_layernorm_669.dc.add.10_add_682_0]
  - varinst: [$gptr_q20, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q20, incwrap, $c_microbatch_size, 4]
  - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 1
