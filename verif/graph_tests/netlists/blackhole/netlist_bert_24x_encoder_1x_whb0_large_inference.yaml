# git checkout 435f4d73
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole_B0-chip1-enc24-large]

devices:
  arch: blackhole

queues:

  # input
  hidden_states: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10000000]]}
  attention_mask: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10330040]]}

  # output
  bert_encoder.output_layernorm_1367: {input: layernorm_1367.dc.add.10, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10462080], [2, 0x104a60c0], [2, 0x104ea100], [2, 0x1052e140], [2, 0x10572180], [2, 0x105b61c0], [2, 0x105fa200], [2, 0x1063e240]]}
  layer.0.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10000000], [3, 0x10004440], [3, 0x10008880], [3, 0x1000ccc0]]}
  layer.0.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10000000], [1, 0x10044040], [1, 0x10088080], [1, 0x100cc0c0], [1, 0x10110100], [1, 0x10154140], [1, 0x10198180], [1, 0x101dc1c0]]}
  layer.0.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10011100],
      [3, 0x10015540], [3, 0x10019980], [3, 0x1001ddc0]]}
  layer.0.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10000000], [4, 0x10044040], [4, 0x10088080], [4, 0x100cc0c0], [4, 0x10110100], [4, 0x10154140], [4, 0x10198180], [4, 0x101dc1c0]]}
  layer.0.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10000000], [0, 0x10004440], [0, 0x10008880], [0, 0x1000ccc0]]}
  layer.0.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10220200], [4, 0x10264240], [4, 0x102a8280], [4, 0x102ec2c0], [4, 0x10330300], [4, 0x10374340], [4, 0x103b8380], [4, 0x103fc3c0]]}
  layer.0.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10440400], [4, 0x10444840], [4, 0x10448c80], [4, 0x1044d0c0]]}
  layer.0.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10220200]]}
  layer.0.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10022200]]}
  layer.0.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10451500], [4, 0x104d9540], [4, 0x10561580], [4, 0x105e95c0], [4, 0x10671600], [4, 0x106f9640], [4, 0x10781680], [4, 0x108096c0], [4, 0x10891700], [4, 0x10919740], [4, 0x109a1780], [4, 0x10a297c0],
      [4, 0x10ab1800], [4, 0x10b39840], [4, 0x10bc1880], [4, 0x10c498c0]]}
  layer.0.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10682280],
      [2, 0x106932c0], [2, 0x106a4300], [2, 0x106b5340]]}
  layer.0.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x106c6380],
      [2, 0x1074e3c0], [2, 0x107d6400], [2, 0x1085e440], [2, 0x108e6480], [2, 0x1096e4c0], [2, 0x109f6500], [2, 0x10a7e540], [2, 0x10b06580], [2, 0x10b8e5c0], [2, 0x10c16600], [2, 0x10c9e640], [2, 0x10d26680],
      [2, 0x10dae6c0], [2, 0x10e36700], [2, 0x10ebe740]]}
  layer.0.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10033240],
      [3, 0x10035480], [3, 0x100376c0], [3, 0x10039900], [3, 0x1003bb40], [3, 0x1003dd80], [3, 0x1003ffc0], [3, 0x10042200]]}
  layer.0.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10000000]]}
  layer.0.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10231240]]}
  layer.1.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10f46780], [2, 0x10f8a7c0], [2, 0x10fce800], [2, 0x11012840], [2, 0x11056880], [2, 0x1109a8c0], [2, 0x110de900], [2, 0x11122940]]}
  layer.1.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10cd1900], [4, 0x10cd5d40], [4, 0x10cda180], [4, 0x10cde5c0]]}
  layer.1.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10242280], [1, 0x102862c0], [1, 0x102ca300], [1, 0x1030e340], [1, 0x10352380], [1, 0x103963c0], [1, 0x103da400], [1, 0x1041e440]]}
  layer.1.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10011100],
      [0, 0x10015540], [0, 0x10019980], [0, 0x1001ddc0]]}
  layer.1.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10022200], [0, 0x10066240], [0, 0x100aa280], [0, 0x100ee2c0], [0, 0x10132300], [0, 0x10176340], [0, 0x101ba380], [0, 0x101fe3c0]]}
  layer.1.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11166980], [2, 0x1116adc0], [2, 0x1116f200], [2, 0x11173640]]}
  layer.1.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10242400], [0, 0x10286440], [0, 0x102ca480], [0, 0x1030e4c0], [0, 0x10352500], [0, 0x10396540], [0, 0x103da580], [0, 0x1041e5c0]]}
  layer.1.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11177a80], [2, 0x1117bec0], [2, 0x11180300], [2, 0x11184740]]}
  layer.1.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10ce2a00]]}
  layer.1.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10462600]]}
  layer.1.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10462480], [1, 0x104ea4c0], [1, 0x10572500], [1, 0x105fa540], [1, 0x10682580], [1, 0x1070a5c0], [1, 0x10792600], [1, 0x1081a640], [1, 0x108a2680], [1, 0x1092a6c0], [1, 0x109b2700], [1, 0x10a3a740],
      [1, 0x10ac2780], [1, 0x10b4a7c0], [1, 0x10bd2800], [1, 0x10c5a840]]}
  layer.1.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10011040],
      [5, 0x10022080], [5, 0x100330c0], [5, 0x10044100]]}
  layer.1.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10044440],
      [3, 0x100cc480], [3, 0x101544c0], [3, 0x101dc500], [3, 0x10264540], [3, 0x102ec580], [3, 0x103745c0], [3, 0x103fc600], [3, 0x10484640], [3, 0x1050c680], [3, 0x105946c0], [3, 0x1061c700], [3, 0x106a4740],
      [3, 0x1072c780], [3, 0x107b47c0], [3, 0x1083c800]]}
  layer.1.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10ce2880],
      [1, 0x10ce4ac0], [1, 0x10ce6d00], [1, 0x10ce8f40], [1, 0x10ceb180], [1, 0x10ced3c0], [1, 0x10cef600], [1, 0x10cf1840]]}
  layer.1.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11188b80]]}
  layer.1.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10cf3a40]]}
  layer.2.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10055140], [5, 0x10099180], [5, 0x100dd1c0], [5, 0x10121200], [5, 0x10165240], [5, 0x101a9280], [5, 0x101ed2c0], [5, 0x10231300]]}
  layer.2.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10cf3a80], [1, 0x10cf7ec0], [1, 0x10cfc300], [1, 0x10d00740]]}
  layer.2.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10275340], [5, 0x102b9380], [5, 0x102fd3c0], [5, 0x10341400], [5, 0x10385440], [5, 0x103c9480], [5, 0x1040d4c0], [5, 0x10451500]]}
  layer.2.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10d04b80],
      [1, 0x10d08fc0], [1, 0x10d0d400], [1, 0x10d11840]]}
  layer.2.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10d15c80], [1, 0x10d59cc0], [1, 0x10d9dd00], [1, 0x10de1d40], [1, 0x10e25d80], [1, 0x10e69dc0], [1, 0x10eade00], [1, 0x10ef1e40]]}
  layer.2.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x108c4840], [3, 0x108c8c80], [3, 0x108cd0c0], [3, 0x108d1500]]}
  layer.2.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10f35e80], [1, 0x10f79ec0], [1, 0x10fbdf00], [1, 0x11001f40], [1, 0x11045f80], [1, 0x11089fc0], [1, 0x110ce000], [1, 0x11112040]]}
  layer.2.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x108d5940], [3, 0x108d9d80], [3, 0x108de1c0], [3, 0x108e2600]]}
  layer.2.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x11156080]]}
  layer.2.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10473640]]}
  layer.2.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10d04a80], [4, 0x10d8cac0], [4, 0x10e14b00], [4, 0x10e9cb40], [4, 0x10f24b80], [4, 0x10facbc0], [4, 0x11034c00], [4, 0x110bcc40], [4, 0x11144c80], [4, 0x111cccc0], [4, 0x11254d00], [4, 0x112dcd40],
      [4, 0x11364d80], [4, 0x113ecdc0], [4, 0x11474e00], [4, 0x114fce40]]}
  layer.2.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11199bc0],
      [2, 0x111aac00], [2, 0x111bbc40], [2, 0x111ccc80]]}
  layer.2.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10484680],
      [0, 0x1050c6c0], [0, 0x10594700], [0, 0x1061c740], [0, 0x106a4780], [0, 0x1072c7c0], [0, 0x107b4800], [0, 0x1083c840], [0, 0x108c4880], [0, 0x1094c8c0], [0, 0x109d4900], [0, 0x10a5c940], [0, 0x10ae4980],
      [0, 0x10b6c9c0], [0, 0x10bf4a00], [0, 0x10c7ca40]]}
  layer.2.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11584e80],
      [4, 0x115870c0], [4, 0x11589300], [4, 0x1158b540], [4, 0x1158d780], [4, 0x1158f9c0], [4, 0x11591c00], [4, 0x11593e40]]}
  layer.2.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d04a80]]}
  layer.2.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x111670c0]]}
  layer.3.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x111ddcc0], [2, 0x11221d00], [2, 0x11265d40], [2, 0x112a9d80], [2, 0x112eddc0], [2, 0x11331e00], [2, 0x11375e40], [2, 0x113b9e80]]}
  layer.3.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x11596080], [4, 0x1159a4c0], [4, 0x1159e900], [4, 0x115a2d40]]}
  layer.3.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x113fdec0], [2, 0x11441f00], [2, 0x11485f40], [2, 0x114c9f80], [2, 0x1150dfc0], [2, 0x11552000], [2, 0x11596040], [2, 0x115da080]]}
  layer.3.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x115a7180],
      [4, 0x115ab5c0], [4, 0x115afa00], [4, 0x115b3e40]]}
  layer.3.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x115b8280], [4, 0x115fc2c0], [4, 0x11640300], [4, 0x11684340], [4, 0x116c8380], [4, 0x1170c3c0], [4, 0x11750400], [4, 0x11794440]]}
  layer.3.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10d15ac0], [0, 0x10d19f00], [0, 0x10d1e340], [0, 0x10d22780]]}
  layer.3.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x117d8480], [4, 0x1181c4c0], [4, 0x11860500], [4, 0x118a4540], [4, 0x118e8580], [4, 0x1192c5c0], [4, 0x11970600], [4, 0x119b4640]]}
  layer.3.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10d26bc0], [0, 0x10d2b000], [0, 0x10d2f440], [0, 0x10d33880]]}
  layer.3.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x119f8680]]}
  layer.3.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10d37cc0]]}
  layer.3.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x11178100], [1, 0x11200140], [1, 0x11288180], [1, 0x113101c0], [1, 0x11398200], [1, 0x11420240], [1, 0x114a8280], [1, 0x115302c0], [1, 0x115b8300], [1, 0x11640340], [1, 0x116c8380], [1, 0x117503c0],
      [1, 0x117d8400], [1, 0x11860440], [1, 0x118e8480], [1, 0x119704c0]]}
  layer.3.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10495540],
      [5, 0x104a6580], [5, 0x104b75c0], [5, 0x104c8600]]}
  layer.3.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x108e6a40],
      [3, 0x1096ea80], [3, 0x109f6ac0], [3, 0x10a7eb00], [3, 0x10b06b40], [3, 0x10b8eb80], [3, 0x10c16bc0], [3, 0x10c9ec00], [3, 0x10d26c40], [3, 0x10daec80], [3, 0x10e36cc0], [3, 0x10ebed00], [3, 0x10f46d40],
      [3, 0x10fced80], [3, 0x11056dc0], [3, 0x110dee00]]}
  layer.3.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11166e40],
      [3, 0x11169080], [3, 0x1116b2c0], [3, 0x1116d500], [3, 0x1116f740], [3, 0x11171980], [3, 0x11173bc0], [3, 0x11175e00]]}
  layer.3.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11a096c0]]}
  layer.3.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d48d00]]}
  layer.4.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x119f8500], [1, 0x11a3c540], [1, 0x11a80580], [1, 0x11ac45c0], [1, 0x11b08600], [1, 0x11b4c640], [1, 0x11b90680], [1, 0x11bd46c0]]}
  layer.4.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11178040], [3, 0x1117c480], [3, 0x111808c0], [3, 0x11184d00]]}
  layer.4.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x11c18700], [1, 0x11c5c740], [1, 0x11ca0780], [1, 0x11ce47c0], [1, 0x11d28800], [1, 0x11d6c840], [1, 0x11db0880], [1, 0x11df48c0]]}
  layer.4.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d59d40],
      [0, 0x10d5e180], [0, 0x10d625c0], [0, 0x10d66a00]]}
  layer.4.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x11e38900], [1, 0x11e7c940], [1, 0x11ec0980], [1, 0x11f049c0], [1, 0x11f48a00], [1, 0x11f8ca40], [1, 0x11fd0a80], [1, 0x12014ac0]]}
  layer.4.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11189140], [3, 0x1118d580], [3, 0x111919c0], [3, 0x11195e00]]}
  layer.4.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12058b00], [1, 0x1209cb40], [1, 0x120e0b80], [1, 0x12124bc0], [1, 0x12168c00], [1, 0x121acc40], [1, 0x121f0c80], [1, 0x12234cc0]]}
  layer.4.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x1119a240], [3, 0x1119e680], [3, 0x111a2ac0], [3, 0x111a6f00]]}
  layer.4.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x111ab340]]}
  layer.4.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x104d9640]]}
  layer.4.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10d6ae40], [0, 0x10df2e80], [0, 0x10e7aec0], [0, 0x10f02f00], [0, 0x10f8af40], [0, 0x11012f80], [0, 0x1109afc0], [0, 0x11123000], [0, 0x111ab040], [0, 0x11233080], [0, 0x112bb0c0], [0, 0x11343100],
      [0, 0x113cb140], [0, 0x11453180], [0, 0x114db1c0], [0, 0x11563200]]}
  layer.4.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11a1a700],
      [4, 0x11a2b740], [4, 0x11a3c780], [4, 0x11a4d7c0]]}
  layer.4.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1161e0c0],
      [2, 0x116a6100], [2, 0x1172e140], [2, 0x117b6180], [2, 0x1183e1c0], [2, 0x118c6200], [2, 0x1194e240], [2, 0x119d6280], [2, 0x11a5e2c0], [2, 0x11ae6300], [2, 0x11b6e340], [2, 0x11bf6380], [2, 0x11c7e3c0],
      [2, 0x11d06400], [2, 0x11d8e440], [2, 0x11e16480]]}
  layer.4.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x115eb240],
      [0, 0x115ed480], [0, 0x115ef6c0], [0, 0x115f1900], [0, 0x115f3b40], [0, 0x115f5d80], [0, 0x115f7fc0], [0, 0x115fa200]]}
  layer.4.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11e9e4c0]]}
  layer.4.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11a5e800]]}
  layer.5.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x104ea680], [5, 0x1052e6c0], [5, 0x10572700], [5, 0x105b6740], [5, 0x105fa780], [5, 0x1063e7c0], [5, 0x10682800], [5, 0x106c6840]]}
  layer.5.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x12278d00], [1, 0x1227d140], [1, 0x12281580], [1, 0x122859c0]]}
  layer.5.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1070a880], [5, 0x1074e8c0], [5, 0x10792900], [5, 0x107d6940], [5, 0x1081a980], [5, 0x1085e9c0], [5, 0x108a2a00], [5, 0x108e6a40]]}
  layer.5.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12289e00],
      [1, 0x1228e240], [1, 0x12292680], [1, 0x12296ac0]]}
  layer.5.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x115fc440], [0, 0x11640480], [0, 0x116844c0], [0, 0x116c8500], [0, 0x1170c540], [0, 0x11750580], [0, 0x117945c0], [0, 0x117d8600]]}
  layer.5.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1092aa80], [5, 0x1092eec0], [5, 0x10933300], [5, 0x10937740]]}
  layer.5.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x111bc380], [3, 0x112003c0], [3, 0x11244400], [3, 0x11288440], [3, 0x112cc480], [3, 0x113104c0], [3, 0x11354500], [3, 0x11398540]]}
  layer.5.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1093bb80], [5, 0x1093ffc0], [5, 0x10944400], [5, 0x10948840]]}
  layer.5.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x113dc580]]}
  layer.5.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1094cc80]]}
  layer.5.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1229af00], [1, 0x12322f40], [1, 0x123aaf80], [1, 0x12432fc0], [1, 0x124bb000], [1, 0x12543040], [1, 0x125cb080], [1, 0x126530c0], [1, 0x126db100], [1, 0x12763140], [1, 0x127eb180], [1, 0x128731c0],
      [1, 0x128fb200], [1, 0x12983240], [1, 0x12a0b280], [1, 0x12a932c0]]}
  layer.5.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11eaf500],
      [2, 0x11ec0540], [2, 0x11ed1580], [2, 0x11ee25c0]]}
  layer.5.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1181c640],
      [0, 0x118a4680], [0, 0x1192c6c0], [0, 0x119b4700], [0, 0x11a3c740], [0, 0x11ac4780], [0, 0x11b4c7c0], [0, 0x11bd4800], [0, 0x11c5c840], [0, 0x11ce4880], [0, 0x11d6c8c0], [0, 0x11df4900], [0, 0x11e7c940],
      [0, 0x11f04980], [0, 0x11f8c9c0], [0, 0x12014a00]]}
  layer.5.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11a6f840],
      [4, 0x11a71a80], [4, 0x11a73cc0], [4, 0x11a75f00], [4, 0x11a78140], [4, 0x11a7a380], [4, 0x11a7c5c0], [4, 0x11a7e800]]}
  layer.5.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1209ca40]]}
  layer.5.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1095dcc0]]}
  layer.6.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x120ada80], [0, 0x120f1ac0], [0, 0x12135b00], [0, 0x12179b40], [0, 0x121bdb80], [0, 0x12201bc0], [0, 0x12245c00], [0, 0x12289c40]]}
  layer.6.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x122cdc80], [0, 0x122d20c0], [0, 0x122d6500], [0, 0x122da940]]}
  layer.6.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x11a80a40], [4, 0x11ac4a80], [4, 0x11b08ac0], [4, 0x11b4cb00], [4, 0x11b90b40], [4, 0x11bd4b80], [4, 0x11c18bc0], [4, 0x11c5cc00]]}
  layer.6.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x122ded80],
      [0, 0x122e31c0], [0, 0x122e7600], [0, 0x122eba40]]}
  layer.6.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x122efe80], [0, 0x12333ec0], [0, 0x12377f00], [0, 0x123bbf40], [0, 0x123fff80], [0, 0x12443fc0], [0, 0x12488000], [0, 0x124cc040]]}
  layer.6.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11ef3600], [2, 0x11ef7a40], [2, 0x11efbe80], [2, 0x11f002c0]]}
  layer.6.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x12510080], [0, 0x125540c0], [0, 0x12598100], [0, 0x125dc140], [0, 0x12620180], [0, 0x126641c0], [0, 0x126a8200], [0, 0x126ec240]]}
  layer.6.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x12730280], [0, 0x127346c0], [0, 0x12738b00], [0, 0x1273cf40]]}
  layer.6.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x113ed5c0]]}
  layer.6.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1096ed00]]}
  layer.6.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x12741380], [0, 0x127c93c0], [0, 0x12851400], [0, 0x128d9440], [0, 0x12961480], [0, 0x129e94c0], [0, 0x12a71500], [0, 0x12af9540], [0, 0x12b81580], [0, 0x12c095c0], [0, 0x12c91600], [0, 0x12d19640],
      [0, 0x12da1680], [0, 0x12e296c0], [0, 0x12eb1700], [0, 0x12f39740]]}
  layer.6.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11ca0c40],
      [4, 0x11cb1c80], [4, 0x11cc2cc0], [4, 0x11cd3d00]]}
  layer.6.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11ce4d40],
      [4, 0x11d6cd80], [4, 0x11df4dc0], [4, 0x11e7ce00], [4, 0x11f04e40], [4, 0x11f8ce80], [4, 0x12014ec0], [4, 0x1209cf00], [4, 0x12124f40], [4, 0x121acf80], [4, 0x12234fc0], [4, 0x122bd000], [4, 0x12345040],
      [4, 0x123cd080], [4, 0x124550c0], [4, 0x124dd100]]}
  layer.6.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1097fd40],
      [5, 0x10981f80], [5, 0x109841c0], [5, 0x10986400], [5, 0x10988640], [5, 0x1098a880], [5, 0x1098cac0], [5, 0x1098ed00]]}
  layer.6.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12b1b300]]}
  layer.6.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x113fe600]]}
  layer.7.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x12565140], [4, 0x125a9180], [4, 0x125ed1c0], [4, 0x12631200], [4, 0x12675240], [4, 0x126b9280], [4, 0x126fd2c0], [4, 0x12741300]]}
  layer.7.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x12fc1780], [0, 0x12fc5bc0], [0, 0x12fca000], [0, 0x12fce440]]}
  layer.7.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x1140f640], [3, 0x11453680], [3, 0x114976c0], [3, 0x114db700], [3, 0x1151f740], [3, 0x11563780], [3, 0x115a77c0], [3, 0x115eb800]]}
  layer.7.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11f04700],
      [2, 0x11f08b40], [2, 0x11f0cf80], [2, 0x11f113c0]]}
  layer.7.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11f15800], [2, 0x11f59840], [2, 0x11f9d880], [2, 0x11fe18c0], [2, 0x12025900], [2, 0x12069940], [2, 0x120ad980], [2, 0x120f19c0]]}
  layer.7.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x12785340], [4, 0x12789780], [4, 0x1278dbc0], [4, 0x12792000]]}
  layer.7.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x12135a00], [2, 0x12179a40], [2, 0x121bda80], [2, 0x12201ac0], [2, 0x12245b00], [2, 0x12289b40], [2, 0x122cdb80], [2, 0x12311bc0]]}
  layer.7.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x12796440], [4, 0x1279a880], [4, 0x1279ecc0], [4, 0x127a3100]]}
  layer.7.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x1162f840]]}
  layer.7.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10990f40]]}
  layer.7.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x12fd2880], [0, 0x1305a8c0], [0, 0x130e2900], [0, 0x1316a940], [0, 0x131f2980], [0, 0x1327a9c0], [0, 0x13302a00], [0, 0x1338aa40], [0, 0x13412a80], [0, 0x1349aac0], [0, 0x13522b00], [0, 0x135aab40],
      [0, 0x13632b80], [0, 0x136babc0], [0, 0x13742c00], [0, 0x137cac40]]}
  layer.7.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x127a7540],
      [4, 0x127b8580], [4, 0x127c95c0], [4, 0x127da600]]}
  layer.7.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x12355c00],
      [2, 0x123ddc40], [2, 0x12465c80], [2, 0x124edcc0], [2, 0x12575d00], [2, 0x125fdd40], [2, 0x12685d80], [2, 0x1270ddc0], [2, 0x12795e00], [2, 0x1281de40], [2, 0x128a5e80], [2, 0x1292dec0], [2, 0x129b5f00],
      [2, 0x12a3df40], [2, 0x12ac5f80], [2, 0x12b4dfc0]]}
  layer.7.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x13852c80],
      [0, 0x13854ec0], [0, 0x13857100], [0, 0x13859340], [0, 0x1385b580], [0, 0x1385d7c0], [0, 0x1385fa00], [0, 0x13861c40]]}
  layer.7.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x12bd6000]]}
  layer.7.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x127eb640]]}
  layer.8.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x109a1f80], [5, 0x109e5fc0], [5, 0x10a2a000], [5, 0x10a6e040], [5, 0x10ab2080], [5, 0x10af60c0], [5, 0x10b3a100], [5, 0x10b7e140]]}
  layer.8.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x12b2c340], [1, 0x12b30780], [1, 0x12b34bc0], [1, 0x12b39000]]}
  layer.8.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10bc2180], [5, 0x10c061c0], [5, 0x10c4a200], [5, 0x10c8e240], [5, 0x10cd2280], [5, 0x10d162c0], [5, 0x10d5a300], [5, 0x10d9e340]]}
  layer.8.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12b3d440],
      [1, 0x12b41880], [1, 0x12b45cc0], [1, 0x12b4a100]]}
  layer.8.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x13863e80], [0, 0x138a7ec0], [0, 0x138ebf00], [0, 0x1392ff40], [0, 0x13973f80], [0, 0x139b7fc0], [0, 0x139fc000], [0, 0x13a40040]]}
  layer.8.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x127fc680], [4, 0x12800ac0], [4, 0x12804f00], [4, 0x12809340]]}
  layer.8.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12b4e540], [1, 0x12b92580], [1, 0x12bd65c0], [1, 0x12c1a600], [1, 0x12c5e640], [1, 0x12ca2680], [1, 0x12ce66c0], [1, 0x12d2a700]]}
  layer.8.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x11640880], [3, 0x11644cc0], [3, 0x11649100], [3, 0x1164d540]]}
  layer.8.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12d6e740]]}
  layer.8.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x11651980]]}
  layer.8.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x12be7040], [2, 0x12c6f080], [2, 0x12cf70c0], [2, 0x12d7f100], [2, 0x12e07140], [2, 0x12e8f180], [2, 0x12f171c0], [2, 0x12f9f200], [2, 0x13027240], [2, 0x130af280], [2, 0x131372c0], [2, 0x131bf300],
      [2, 0x13247340], [2, 0x132cf380], [2, 0x133573c0], [2, 0x133df400]]}
  layer.8.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13467440],
      [2, 0x13478480], [2, 0x134894c0], [2, 0x1349a500]]}
  layer.8.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x13a84080],
      [0, 0x13b0c0c0], [0, 0x13b94100], [0, 0x13c1c140], [0, 0x13ca4180], [0, 0x13d2c1c0], [0, 0x13db4200], [0, 0x13e3c240], [0, 0x13ec4280], [0, 0x13f4c2c0], [0, 0x13fd4300], [0, 0x1405c340], [0, 0x140e4380],
      [0, 0x1416c3c0], [0, 0x141f4400], [0, 0x1427c440]]}
  layer.8.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1280d780],
      [4, 0x1280f9c0], [4, 0x12811c00], [4, 0x12813e40], [4, 0x12816080], [4, 0x128182c0], [4, 0x1281a500], [4, 0x1281c740]]}
  layer.8.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x14304480]]}
  layer.8.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1281e980]]}
  layer.9.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x143154c0], [0, 0x14359500], [0, 0x1439d540], [0, 0x143e1580], [0, 0x144255c0], [0, 0x14469600], [0, 0x144ad640], [0, 0x144f1680]]}
  layer.9.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x134ab540], [2, 0x134af980], [2, 0x134b3dc0], [2, 0x134b8200]]}
  layer.9.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x145356c0], [0, 0x14579700], [0, 0x145bd740], [0, 0x14601780], [0, 0x146457c0], [0, 0x14689800], [0, 0x146cd840], [0, 0x14711880]]}
  layer.9.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x134bc640],
      [2, 0x134c0a80], [2, 0x134c4ec0], [2, 0x134c9300]]}
  layer.9.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x134cd740], [2, 0x13511780], [2, 0x135557c0], [2, 0x13599800], [2, 0x135dd840], [2, 0x13621880], [2, 0x136658c0], [2, 0x136a9900]]}
  layer.9.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x1282f9c0], [4, 0x12833e00], [4, 0x12838240], [4, 0x1283c680]]}
  layer.9.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x136ed940], [2, 0x13731980], [2, 0x137759c0], [2, 0x137b9a00], [2, 0x137fda40], [2, 0x13841a80], [2, 0x13885ac0], [2, 0x138c9b00]]}
  layer.9.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x12840ac0], [4, 0x12844f00], [4, 0x12849340], [4, 0x1284d780]]}
  layer.9.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1390db40]]}
  layer.9.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x12851bc0]]}
  layer.9.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10de2380], [5, 0x10e6a3c0], [5, 0x10ef2400], [5, 0x10f7a440], [5, 0x11002480], [5, 0x1108a4c0], [5, 0x11112500], [5, 0x1119a540], [5, 0x11222580], [5, 0x112aa5c0], [5, 0x11332600], [5, 0x113ba640],
      [5, 0x11442680], [5, 0x114ca6c0], [5, 0x11552700], [5, 0x115da740]]}
  layer.9.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x116629c0],
      [3, 0x11673a00], [3, 0x11684a40], [3, 0x11695a80]]}
  layer.9.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12d7f780],
      [1, 0x12e077c0], [1, 0x12e8f800], [1, 0x12f17840], [1, 0x12f9f880], [1, 0x130278c0], [1, 0x130af900], [1, 0x13137940], [1, 0x131bf980], [1, 0x132479c0], [1, 0x132cfa00], [1, 0x13357a40], [1, 0x133dfa80],
      [1, 0x13467ac0], [1, 0x134efb00], [1, 0x13577b40]]}
  layer.9.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11662780],
      [5, 0x116649c0], [5, 0x11666c00], [5, 0x11668e40], [5, 0x1166b080], [5, 0x1166d2c0], [5, 0x1166f500], [5, 0x11671740]]}
  layer.9.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x147558c0]]}
  layer.9.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1391eb80]]}
  layer.10.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x116a6ac0], [3, 0x116eab00], [3, 0x1172eb40], [3, 0x11772b80], [3, 0x117b6bc0], [3, 0x117fac00], [3, 0x1183ec40], [3, 0x11882c80]]}
  layer.10.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x11673980], [5, 0x11677dc0], [5, 0x1167c200], [5, 0x11680640]]}
  layer.10.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x118c6cc0], [3, 0x1190ad00], [3, 0x1194ed40], [3, 0x11992d80], [3, 0x119d6dc0], [3, 0x11a1ae00], [3, 0x11a5ee40], [3, 0x11aa2e80]]}
  layer.10.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x11684a80], [5, 0x11688ec0], [5, 0x1168d300], [5, 0x11691740]]}
  layer.10.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x11695b80], [5, 0x116d9bc0], [5, 0x1171dc00], [5, 0x11761c40], [5, 0x117a5c80], [5, 0x117e9cc0], [5, 0x1182dd00], [5, 0x11871d40]]}
  layer.10.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x135ffb80], [1, 0x13603fc0], [1, 0x13608400], [1, 0x1360c840]]}
  layer.10.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x118b5d80], [5, 0x118f9dc0], [5, 0x1193de00], [5, 0x11981e40], [5, 0x119c5e80], [5, 0x11a09ec0], [5, 0x11a4df00], [5, 0x11a91f40]]}
  layer.10.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x13610c80], [1, 0x136150c0], [1, 0x13619500], [1, 0x1361d940]]}
  layer.10.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x11ae6ec0]]}
  layer.10.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x12862c00]]}
  layer.10.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x11ad5f80], [5, 0x11b5dfc0], [5, 0x11be6000], [5, 0x11c6e040], [5, 0x11cf6080], [5, 0x11d7e0c0], [5, 0x11e06100], [5, 0x11e8e140], [5, 0x11f16180], [5, 0x11f9e1c0], [5, 0x12026200], [5, 0x120ae240],
      [5, 0x12136280], [5, 0x121be2c0], [5, 0x12246300], [5, 0x122ce340]]}
  layer.10.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11af7f00], [3, 0x11b08f40], [3, 0x11b19f80], [3, 0x11b2afc0]]}
  layer.10.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x13621d80],
      [1, 0x136a9dc0], [1, 0x13731e00], [1, 0x137b9e40], [1, 0x13841e80], [1, 0x138c9ec0], [1, 0x13951f00], [1, 0x139d9f40], [1, 0x13a61f80], [1, 0x13ae9fc0], [1, 0x13b72000], [1, 0x13bfa040], [1, 0x13c82080],
      [1, 0x13d0a0c0], [1, 0x13d92100], [1, 0x13e1a140]]}
  layer.10.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x12356380],
      [5, 0x123585c0], [5, 0x1235a800], [5, 0x1235ca40], [5, 0x1235ec80], [5, 0x12360ec0], [5, 0x12363100], [5, 0x12365340]]}
  layer.10.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x1392fbc0]]}
  layer.10.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12873c40]]}
  layer.11.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x12367580], [5, 0x123ab5c0], [5, 0x123ef600], [5, 0x12433640], [5, 0x12477680], [5, 0x124bb6c0], [5, 0x124ff700], [5, 0x12543740]]}
  layer.11.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x13ea2180], [1, 0x13ea65c0], [1, 0x13eaaa00], [1, 0x13eaee40]]}
  layer.11.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x12587780], [5, 0x125cb7c0], [5, 0x1260f800], [5, 0x12653840], [5, 0x12697880], [5, 0x126db8c0], [5, 0x1271f900], [5, 0x12763940]]}
  layer.11.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x13eb3280], [1, 0x13eb76c0], [1, 0x13ebbb00], [1, 0x13ebff40]]}
  layer.11.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x13ec4380], [1, 0x13f083c0], [1, 0x13f4c400], [1, 0x13f90440], [1, 0x13fd4480], [1, 0x140184c0], [1, 0x1405c500], [1, 0x140a0540]]}
  layer.11.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x14766900], [0, 0x1476ad40], [0, 0x1476f180], [0, 0x147735c0]]}
  layer.11.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x127a7980], [5, 0x127eb9c0], [5, 0x1282fa00], [5, 0x12873a40], [5, 0x128b7a80], [5, 0x128fbac0], [5, 0x1293fb00], [5, 0x12983b40]]}
  layer.11.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x140e4580], [1, 0x140e89c0], [1, 0x140ece00], [1, 0x140f1240]]}
  layer.11.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x129c7b80]]}
  layer.11.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x140f5680]]}
  layer.11.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x13940c00], [2, 0x139c8c40], [2, 0x13a50c80], [2, 0x13ad8cc0], [2, 0x13b60d00], [2, 0x13be8d40], [2, 0x13c70d80], [2, 0x13cf8dc0], [2, 0x13d80e00], [2, 0x13e08e40], [2, 0x13e90e80], [2, 0x13f18ec0],
      [2, 0x13fa0f00], [2, 0x14028f40], [2, 0x140b0f80], [2, 0x14138fc0]]}
  layer.11.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x141066c0], [1, 0x14117700], [1, 0x14128740], [1, 0x14139780]]}
  layer.11.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12884c80],
      [4, 0x1290ccc0], [4, 0x12994d00], [4, 0x12a1cd40], [4, 0x12aa4d80], [4, 0x12b2cdc0], [4, 0x12bb4e00], [4, 0x12c3ce40], [4, 0x12cc4e80], [4, 0x12d4cec0], [4, 0x12dd4f00], [4, 0x12e5cf40], [4, 0x12ee4f80],
      [4, 0x12f6cfc0], [4, 0x12ff5000], [4, 0x1307d040]]}
  layer.11.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x141c1000],
      [2, 0x141c3240], [2, 0x141c5480], [2, 0x141c76c0], [2, 0x141c9900], [2, 0x141cbb40], [2, 0x141cdd80], [2, 0x141cffc0]]}
  layer.11.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x13105080]]}
  layer.11.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x129d8bc0]]}
  layer.12.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14777a00], [0, 0x147bba40], [0, 0x147ffa80], [0, 0x14843ac0], [0, 0x14887b00], [0, 0x148cbb40], [0, 0x1490fb80], [0, 0x14953bc0]]}
  layer.12.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x131160c0], [4, 0x1311a500], [4, 0x1311e940], [4, 0x13122d80]]}
  layer.12.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x131271c0], [4, 0x1316b200], [4, 0x131af240], [4, 0x131f3280], [4, 0x132372c0], [4, 0x1327b300], [4, 0x132bf340], [4, 0x13303380]]}
  layer.12.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x14997c00], [0, 0x1499c040], [0, 0x149a0480], [0, 0x149a48c0]]}
  layer.12.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x149a8d00], [0, 0x149ecd40], [0, 0x14a30d80], [0, 0x14a74dc0], [0, 0x14ab8e00], [0, 0x14afce40], [0, 0x14b40e80], [0, 0x14b84ec0]]}
  layer.12.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x141d2200], [2, 0x141d6640], [2, 0x141daa80], [2, 0x141deec0]]}
  layer.12.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14bc8f00], [0, 0x14c0cf40], [0, 0x14c50f80], [0, 0x14c94fc0], [0, 0x14cd9000], [0, 0x14d1d040], [0, 0x14d61080], [0, 0x14da50c0]]}
  layer.12.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x141e3300], [2, 0x141e7740], [2, 0x141ebb80], [2, 0x141effc0]]}
  layer.12.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14de9100]]}
  layer.12.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x141f4400]]}
  layer.12.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x11b3c000], [3, 0x11bc4040], [3, 0x11c4c080], [3, 0x11cd40c0], [3, 0x11d5c100], [3, 0x11de4140], [3, 0x11e6c180], [3, 0x11ef41c0], [3, 0x11f7c200], [3, 0x12004240], [3, 0x1208c280], [3, 0x121142c0],
      [3, 0x1219c300], [3, 0x12224340], [3, 0x122ac380], [3, 0x123343c0]]}
  layer.12.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1414a7c0], [1, 0x1415b800], [1, 0x1416c840], [1, 0x1417d880]]}
  layer.12.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x129e9c00],
      [5, 0x12a71c40], [5, 0x12af9c80], [5, 0x12b81cc0], [5, 0x12c09d00], [5, 0x12c91d40], [5, 0x12d19d80], [5, 0x12da1dc0], [5, 0x12e29e00], [5, 0x12eb1e40], [5, 0x12f39e80], [5, 0x12fc1ec0], [5, 0x13049f00],
      [5, 0x130d1f40], [5, 0x13159f80], [5, 0x131e1fc0]]}
  layer.12.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x123bc400],
      [3, 0x123be640], [3, 0x123c0880], [3, 0x123c2ac0], [3, 0x123c4d00], [3, 0x123c6f40], [3, 0x123c9180], [3, 0x123cb3c0]]}
  layer.12.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x14205440]]}
  layer.12.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x133473c0]]}
  layer.13.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1326a000], [5, 0x132ae040], [5, 0x132f2080], [5, 0x133360c0], [5, 0x1337a100], [5, 0x133be140], [5, 0x13402180], [5, 0x134461c0]]}
  layer.13.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x1418e8c0], [1, 0x14192d00], [1, 0x14197140], [1, 0x1419b580]]}
  layer.13.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1348a200], [5, 0x134ce240], [5, 0x13512280], [5, 0x135562c0], [5, 0x1359a300], [5, 0x135de340], [5, 0x13622380], [5, 0x136663c0]]}
  layer.13.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1419f9c0], [1, 0x141a3e00], [1, 0x141a8240], [1, 0x141ac680]]}
  layer.13.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x141b0ac0], [1, 0x141f4b00], [1, 0x14238b40], [1, 0x1427cb80], [1, 0x142c0bc0], [1, 0x14304c00], [1, 0x14348c40], [1, 0x1438cc80]]}
  layer.13.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x123cd600], [3, 0x123d1a40], [3, 0x123d5e80], [3, 0x123da2c0]]}
  layer.13.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x143d0cc0], [1, 0x14414d00], [1, 0x14458d40], [1, 0x1449cd80], [1, 0x144e0dc0], [1, 0x14524e00], [1, 0x14568e40], [1, 0x145ace80]]}
  layer.13.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x123de700], [3, 0x123e2b40], [3, 0x123e6f80], [3, 0x123eb3c0]]}
  layer.13.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x145f0ec0]]}
  layer.13.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x14601f00]]}
  layer.13.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x14216480], [2, 0x1429e4c0], [2, 0x14326500], [2, 0x143ae540], [2, 0x14436580], [2, 0x144be5c0], [2, 0x14546600], [2, 0x145ce640], [2, 0x14656680], [2, 0x146de6c0], [2, 0x14766700], [2, 0x147ee740],
      [2, 0x14876780], [2, 0x148fe7c0], [2, 0x14986800], [2, 0x14a0e840]]}
  layer.13.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x14dfa140], [0, 0x14e0b180], [0, 0x14e1c1c0], [0, 0x14e2d200]]}
  layer.13.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x13358400],
      [4, 0x133e0440], [4, 0x13468480], [4, 0x134f04c0], [4, 0x13578500], [4, 0x13600540], [4, 0x13688580], [4, 0x137105c0], [4, 0x13798600], [4, 0x13820640], [4, 0x138a8680], [4, 0x139306c0], [4, 0x139b8700],
      [4, 0x13a40740], [4, 0x13ac8780], [4, 0x13b507c0]]}
  layer.13.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14a96880],
      [2, 0x14a98ac0], [2, 0x14a9ad00], [2, 0x14a9cf40], [2, 0x14a9f180], [2, 0x14aa13c0], [2, 0x14aa3600], [2, 0x14aa5840]]}
  layer.13.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x136aa400]]}
  layer.13.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x14612f40]]}
  layer.14.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14aa7a80], [2, 0x14aebac0], [2, 0x14b2fb00], [2, 0x14b73b40], [2, 0x14bb7b80], [2, 0x14bfbbc0], [2, 0x14c3fc00], [2, 0x14c83c40]]}
  layer.14.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x13bd8800], [4, 0x13bdcc40], [4, 0x13be1080], [4, 0x13be54c0]]}
  layer.14.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x14cc7c80], [2, 0x14d0bcc0], [2, 0x14d4fd00], [2, 0x14d93d40], [2, 0x14dd7d80], [2, 0x14e1bdc0], [2, 0x14e5fe00], [2, 0x14ea3e40]]}
  layer.14.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x13be9900], [4, 0x13bedd40], [4, 0x13bf2180], [4, 0x13bf65c0]]}
  layer.14.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x13bfaa00], [4, 0x13c3ea40], [4, 0x13c82a80], [4, 0x13cc6ac0], [4, 0x13d0ab00], [4, 0x13d4eb40], [4, 0x13d92b80], [4, 0x13dd6bc0]]}
  layer.14.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x13e1ac00], [4, 0x13e1f040], [4, 0x13e23480], [4, 0x13e278c0]]}
  layer.14.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x123ef800], [3, 0x12433840], [3, 0x12477880], [3, 0x124bb8c0], [3, 0x124ff900], [3, 0x12543940], [3, 0x12587980], [3, 0x125cb9c0]]}
  layer.14.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x136bb440], [5, 0x136bf880], [5, 0x136c3cc0], [5, 0x136c8100]]}
  layer.14.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x1260fa00]]}
  layer.14.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x136cc540]]}
  layer.14.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x14e3e240], [0, 0x14ec6280], [0, 0x14f4e2c0], [0, 0x14fd6300], [0, 0x1505e340], [0, 0x150e6380], [0, 0x1516e3c0], [0, 0x151f6400], [0, 0x1527e440], [0, 0x15306480], [0, 0x1538e4c0], [0, 0x15416500],
      [0, 0x1549e540], [0, 0x15526580], [0, 0x155ae5c0], [0, 0x15636600]]}
  layer.14.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x136dd580], [5, 0x136ee5c0], [5, 0x136ff600], [5, 0x13710640]]}
  layer.14.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14ee7e80],
      [2, 0x14f6fec0], [2, 0x14ff7f00], [2, 0x1507ff40], [2, 0x15107f80], [2, 0x1518ffc0], [2, 0x15218000], [2, 0x152a0040], [2, 0x15328080], [2, 0x153b00c0], [2, 0x15438100], [2, 0x154c0140], [2, 0x15548180],
      [2, 0x155d01c0], [2, 0x15658200], [2, 0x156e0240]]}
  layer.14.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x156be640],
      [0, 0x156c0880], [0, 0x156c2ac0], [0, 0x156c4d00], [0, 0x156c6f40], [0, 0x156c9180], [0, 0x156cb3c0], [0, 0x156cd600]]}
  layer.14.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x15768280]]}
  layer.14.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x13e2bd00]]}
  layer.15.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x12620a40], [3, 0x12664a80], [3, 0x126a8ac0], [3, 0x126ecb00], [3, 0x12730b40], [3, 0x12774b80], [3, 0x127b8bc0], [3, 0x127fcc00]]}
  layer.15.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x13e3cd40], [4, 0x13e41180], [4, 0x13e455c0], [4, 0x13e49a00]]}
  layer.15.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x13e4de40], [4, 0x13e91e80], [4, 0x13ed5ec0], [4, 0x13f19f00], [4, 0x13f5df40], [4, 0x13fa1f80], [4, 0x13fe5fc0], [4, 0x1402a000]]}
  layer.15.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x156cf840], [0, 0x156d3c80], [0, 0x156d80c0], [0, 0x156dc500]]}
  layer.15.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x156e0940], [0, 0x15724980], [0, 0x157689c0], [0, 0x157aca00], [0, 0x157f0a40], [0, 0x15834a80], [0, 0x15878ac0], [0, 0x158bcb00]]}
  layer.15.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x157792c0], [2, 0x1577d700], [2, 0x15781b40], [2, 0x15785f80]]}
  layer.15.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x15900b40], [0, 0x15944b80], [0, 0x15988bc0], [0, 0x159ccc00], [0, 0x15a10c40], [0, 0x15a54c80], [0, 0x15a98cc0], [0, 0x15adcd00]]}
  layer.15.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1578a3c0], [2, 0x1578e800], [2, 0x15792c40], [2, 0x15797080]]}
  layer.15.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x15b20d40]]}
  layer.15.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1579b4c0]]}
  layer.15.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x12840c40], [3, 0x128c8c80], [3, 0x12950cc0], [3, 0x129d8d00], [3, 0x12a60d40], [3, 0x12ae8d80], [3, 0x12b70dc0], [3, 0x12bf8e00], [3, 0x12c80e40], [3, 0x12d08e80], [3, 0x12d90ec0], [3, 0x12e18f00],
      [3, 0x12ea0f40], [3, 0x12f28f80], [3, 0x12fb0fc0], [3, 0x13039000]]}
  layer.15.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x14623f80], [1, 0x14634fc0], [1, 0x14646000], [1, 0x14657040]]}
  layer.15.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13721680],
      [5, 0x137a96c0], [5, 0x13831700], [5, 0x138b9740], [5, 0x13941780], [5, 0x139c97c0], [5, 0x13a51800], [5, 0x13ad9840], [5, 0x13b61880], [5, 0x13be98c0], [5, 0x13c71900], [5, 0x13cf9940], [5, 0x13d81980],
      [5, 0x13e099c0], [5, 0x13e91a00], [5, 0x13f19a40]]}
  layer.15.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x130c1040],
      [3, 0x130c3280], [3, 0x130c54c0], [3, 0x130c7700], [3, 0x130c9940], [3, 0x130cbb80], [3, 0x130cddc0], [3, 0x130d0000]]}
  layer.15.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x15b31d80]]}
  layer.15.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x157ac500]]}
  layer.16.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x130d2240], [3, 0x13116280], [3, 0x1315a2c0], [3, 0x1319e300], [3, 0x131e2340], [3, 0x13226380], [3, 0x1326a3c0], [3, 0x132ae400]]}
  layer.16.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x13fa1a80], [5, 0x13fa5ec0], [5, 0x13faa300], [5, 0x13fae740]]}
  layer.16.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x132f2440], [3, 0x13336480], [3, 0x1337a4c0], [3, 0x133be500], [3, 0x13402540], [3, 0x13446580], [3, 0x1348a5c0], [3, 0x134ce600]]}
  layer.16.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x13fb2b80], [5, 0x13fb6fc0], [5, 0x13fbb400], [5, 0x13fbf840]]}
  layer.16.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13fc3c80], [5, 0x14007cc0], [5, 0x1404bd00], [5, 0x1408fd40], [5, 0x140d3d80], [5, 0x14117dc0], [5, 0x1415be00], [5, 0x1419fe40]]}
  layer.16.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x14668080], [1, 0x1466c4c0], [1, 0x14670900], [1, 0x14674d40]]}
  layer.16.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x141e3e80], [5, 0x14227ec0], [5, 0x1426bf00], [5, 0x142aff40], [5, 0x142f3f80], [5, 0x14337fc0], [5, 0x1437c000], [5, 0x143c0040]]}
  layer.16.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x14679180], [1, 0x1467d5c0], [1, 0x14681a00], [1, 0x14685e40]]}
  layer.16.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x15b42dc0]]}
  layer.16.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x1406e040]]}
  layer.16.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x14404080], [5, 0x1448c0c0], [5, 0x14514100], [5, 0x1459c140], [5, 0x14624180], [5, 0x146ac1c0], [5, 0x14734200], [5, 0x147bc240], [5, 0x14844280], [5, 0x148cc2c0], [5, 0x14954300], [5, 0x149dc340],
      [5, 0x14a64380], [5, 0x14aec3c0], [5, 0x14b74400], [5, 0x14bfc440]]}
  layer.16.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x13512640], [3, 0x13523680], [3, 0x135346c0], [3, 0x13545700]]}
  layer.16.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1468a280],
      [1, 0x147122c0], [1, 0x1479a300], [1, 0x14822340], [1, 0x148aa380], [1, 0x149323c0], [1, 0x149ba400], [1, 0x14a42440], [1, 0x14aca480], [1, 0x14b524c0], [1, 0x14bda500], [1, 0x14c62540], [1, 0x14cea580],
      [1, 0x14d725c0], [1, 0x14dfa600], [1, 0x14e82640]]}
  layer.16.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x14c84480],
      [5, 0x14c866c0], [5, 0x14c88900], [5, 0x14c8ab40], [5, 0x14c8cd80], [5, 0x14c8efc0], [5, 0x14c91200], [5, 0x14c93440]]}
  layer.16.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x14f0a680]]}
  layer.16.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x13556740]]}
  layer.17.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x1407f080], [4, 0x140c30c0], [4, 0x14107100], [4, 0x1414b140], [4, 0x1418f180], [4, 0x141d31c0], [4, 0x14217200], [4, 0x1425b240]]}
  layer.17.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x15b53e00], [0, 0x15b58240], [0, 0x15b5c680], [0, 0x15b60ac0]]}
  layer.17.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x1429f280], [4, 0x142e32c0], [4, 0x14327300], [4, 0x1436b340], [4, 0x143af380], [4, 0x143f33c0], [4, 0x14437400], [4, 0x1447b440]]}
  layer.17.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x15b64f00], [0, 0x15b69340], [0, 0x15b6d780], [0, 0x15b71bc0]]}
  layer.17.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x15b76000], [0, 0x15bba040], [0, 0x15bfe080], [0, 0x15c420c0], [0, 0x15c86100], [0, 0x15cca140], [0, 0x15d0e180], [0, 0x15d521c0]]}
  layer.17.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x144bf480], [4, 0x144c38c0], [4, 0x144c7d00], [4, 0x144cc140]]}
  layer.17.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x13567780], [3, 0x135ab7c0], [3, 0x135ef800], [3, 0x13633840], [3, 0x13677880], [3, 0x136bb8c0], [3, 0x136ff900], [3, 0x13743940]]}
  layer.17.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x14c95680], [5, 0x14c99ac0], [5, 0x14c9df00], [5, 0x14ca2340]]}
  layer.17.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x13787980]]}
  layer.17.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x14ca6780]]}
  layer.17.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x15d96200], [0, 0x15e1e240], [0, 0x15ea6280], [0, 0x15f2e2c0], [0, 0x15fb6300], [0, 0x1603e340], [0, 0x160c6380], [0, 0x1614e3c0], [0, 0x161d6400], [0, 0x1625e440], [0, 0x162e6480], [0, 0x1636e4c0],
      [0, 0x163f6500], [0, 0x1647e540], [0, 0x16506580], [0, 0x1658e5c0]]}
  layer.17.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x14cb77c0], [5, 0x14cc8800], [5, 0x14cd9840], [5, 0x14cea880]]}
  layer.17.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x157bd540],
      [2, 0x15845580], [2, 0x158cd5c0], [2, 0x15955600], [2, 0x159dd640], [2, 0x15a65680], [2, 0x15aed6c0], [2, 0x15b75700], [2, 0x15bfd740], [2, 0x15c85780], [2, 0x15d0d7c0], [2, 0x15d95800], [2, 0x15e1d840],
      [2, 0x15ea5880], [2, 0x15f2d8c0], [2, 0x15fb5900]]}
  layer.17.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x16616600],
      [0, 0x16618840], [0, 0x1661aa80], [0, 0x1661ccc0], [0, 0x1661ef00], [0, 0x16621140], [0, 0x16623380], [0, 0x166255c0]]}
  layer.17.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x1603d940]]}
  layer.17.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x137989c0]]}
  layer.18.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x137a9a00], [3, 0x137eda40], [3, 0x13831a80], [3, 0x13875ac0], [3, 0x138b9b00], [3, 0x138fdb40], [3, 0x13941b80], [3, 0x13985bc0]]}
  layer.18.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x16627800], [0, 0x1662bc40], [0, 0x16630080], [0, 0x166344c0]]}
  layer.18.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x144d0580], [4, 0x145145c0], [4, 0x14558600], [4, 0x1459c640], [4, 0x145e0680], [4, 0x146246c0], [4, 0x14668700], [4, 0x146ac740]]}
  layer.18.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x16638900], [0, 0x1663cd40], [0, 0x16641180], [0, 0x166455c0]]}
  layer.18.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x16649a00], [0, 0x1668da40], [0, 0x166d1a80], [0, 0x16715ac0], [0, 0x16759b00], [0, 0x1679db40], [0, 0x167e1b80], [0, 0x16825bc0]]}
  layer.18.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x1604e980], [2, 0x16052dc0], [2, 0x16057200], [2, 0x1605b640]]}
  layer.18.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x16869c00], [0, 0x168adc40], [0, 0x168f1c80], [0, 0x16935cc0], [0, 0x16979d00], [0, 0x169bdd40], [0, 0x16a01d80], [0, 0x16a45dc0]]}
  layer.18.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1605fa80], [2, 0x16063ec0], [2, 0x16068300], [2, 0x1606c740]]}
  layer.18.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x16a89e00]]}
  layer.18.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x16070b80]]}
  layer.18.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x139c9c00], [3, 0x13a51c40], [3, 0x13ad9c80], [3, 0x13b61cc0], [3, 0x13be9d00], [3, 0x13c71d40], [3, 0x13cf9d80], [3, 0x13d81dc0], [3, 0x13e09e00], [3, 0x13e91e40], [3, 0x13f19e80], [3, 0x13fa1ec0],
      [3, 0x14029f00], [3, 0x140b1f40], [3, 0x14139f80], [3, 0x141c1fc0]]}
  layer.18.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x14f1b6c0], [1, 0x14f2c700], [1, 0x14f3d740], [1, 0x14f4e780]]}
  layer.18.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x14cfb8c0],
      [5, 0x14d83900], [5, 0x14e0b940], [5, 0x14e93980], [5, 0x14f1b9c0], [5, 0x14fa3a00], [5, 0x1502ba40], [5, 0x150b3a80], [5, 0x1513bac0], [5, 0x151c3b00], [5, 0x1524bb40], [5, 0x152d3b80], [5, 0x1535bbc0],
      [5, 0x153e3c00], [5, 0x1546bc40], [5, 0x154f3c80]]}
  layer.18.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1424a000],
      [3, 0x1424c240], [3, 0x1424e480], [3, 0x142506c0], [3, 0x14252900], [3, 0x14254b40], [3, 0x14256d80], [3, 0x14258fc0]]}
  layer.18.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x16a9ae40]]}
  layer.18.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16081bc0]]}
  layer.19.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x1425b200], [3, 0x1429f240], [3, 0x142e3280], [3, 0x143272c0], [3, 0x1436b300], [3, 0x143af340], [3, 0x143f3380], [3, 0x144373c0]]}
  layer.19.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1557bcc0], [5, 0x15580100], [5, 0x15584540], [5, 0x15588980]]}
  layer.19.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1447b400], [3, 0x144bf440], [3, 0x14503480], [3, 0x145474c0], [3, 0x1458b500], [3, 0x145cf540], [3, 0x14613580], [3, 0x146575c0]]}
  layer.19.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1558cdc0], [5, 0x15591200], [5, 0x15595640], [5, 0x15599a80]]}
  layer.19.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1559dec0], [5, 0x155e1f00], [5, 0x15625f40], [5, 0x15669f80], [5, 0x156adfc0], [5, 0x156f2000], [5, 0x15736040], [5, 0x1577a080]]}
  layer.19.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x14f5f7c0], [1, 0x14f63c00], [1, 0x14f68040], [1, 0x14f6c480]]}
  layer.19.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x157be0c0], [5, 0x15802100], [5, 0x15846140], [5, 0x1588a180], [5, 0x158ce1c0], [5, 0x15912200], [5, 0x15956240], [5, 0x1599a280]]}
  layer.19.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x14f708c0], [1, 0x14f74d00], [1, 0x14f79140], [1, 0x14f7d580]]}
  layer.19.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x14f819c0]]}
  layer.19.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x14f92a00]]}
  layer.19.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x16092c00], [2, 0x1611ac40], [2, 0x161a2c80], [2, 0x1622acc0], [2, 0x162b2d00], [2, 0x1633ad40], [2, 0x163c2d80], [2, 0x1644adc0], [2, 0x164d2e00], [2, 0x1655ae40], [2, 0x165e2e80], [2, 0x1666aec0],
      [2, 0x166f2f00], [2, 0x1677af40], [2, 0x16802f80], [2, 0x1688afc0]]}
  layer.19.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x16aabe80], [0, 0x16abcec0], [0, 0x16acdf00], [0, 0x16adef40]]}
  layer.19.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x146f0780],
      [4, 0x147787c0], [4, 0x14800800], [4, 0x14888840], [4, 0x14910880], [4, 0x149988c0], [4, 0x14a20900], [4, 0x14aa8940], [4, 0x14b30980], [4, 0x14bb89c0], [4, 0x14c40a00], [4, 0x14cc8a40], [4, 0x14d50a80],
      [4, 0x14dd8ac0], [4, 0x14e60b00], [4, 0x14ee8b40]]}
  layer.19.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16913000],
      [2, 0x16915240], [2, 0x16917480], [2, 0x169196c0], [2, 0x1691b900], [2, 0x1691db40], [2, 0x1691fd80], [2, 0x16921fc0]]}
  layer.19.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x159de2c0]]}
  layer.19.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x14fa3a40]]}
  layer.20.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x16924200], [2, 0x16968240], [2, 0x169ac280], [2, 0x169f02c0], [2, 0x16a34300], [2, 0x16a78340], [2, 0x16abc380], [2, 0x16b003c0]]}
  layer.20.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x14f70b80], [4, 0x14f74fc0], [4, 0x14f79400], [4, 0x14f7d840]]}
  layer.20.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x16b44400], [2, 0x16b88440], [2, 0x16bcc480], [2, 0x16c104c0], [2, 0x16c54500], [2, 0x16c98540], [2, 0x16cdc580], [2, 0x16d205c0]]}
  layer.20.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x14f81c80], [4, 0x14f860c0], [4, 0x14f8a500], [4, 0x14f8e940]]}
  layer.20.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x14f92d80], [4, 0x14fd6dc0], [4, 0x1501ae00], [4, 0x1505ee40], [4, 0x150a2e80], [4, 0x150e6ec0], [4, 0x1512af00], [4, 0x1516ef40]]}
  layer.20.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x151b2f80], [4, 0x151b73c0], [4, 0x151bb800], [4, 0x151bfc40]]}
  layer.20.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x1469b600], [3, 0x146df640], [3, 0x14723680], [3, 0x147676c0], [3, 0x147ab700], [3, 0x147ef740], [3, 0x14833780], [3, 0x148777c0]]}
  layer.20.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x159ef300], [5, 0x159f3740], [5, 0x159f7b80], [5, 0x159fbfc0]]}
  layer.20.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x148bb800]]}
  layer.20.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x15a00400]]}
  layer.20.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x16aeff80], [0, 0x16b77fc0], [0, 0x16c00000], [0, 0x16c88040], [0, 0x16d10080], [0, 0x16d980c0], [0, 0x16e20100], [0, 0x16ea8140], [0, 0x16f30180], [0, 0x16fb81c0], [0, 0x17040200], [0, 0x170c8240],
      [0, 0x17150280], [0, 0x171d82c0], [0, 0x17260300], [0, 0x172e8340]]}
  layer.20.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x15a11440], [5, 0x15a22480], [5, 0x15a334c0], [5, 0x15a44500]]}
  layer.20.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16d64600],
      [2, 0x16dec640], [2, 0x16e74680], [2, 0x16efc6c0], [2, 0x16f84700], [2, 0x1700c740], [2, 0x17094780], [2, 0x1711c7c0], [2, 0x171a4800], [2, 0x1722c840], [2, 0x172b4880], [2, 0x1733c8c0], [2, 0x173c4900],
      [2, 0x1744c940], [2, 0x174d4980], [2, 0x1755c9c0]]}
  layer.20.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x17370380],
      [0, 0x173725c0], [0, 0x17374800], [0, 0x17376a40], [0, 0x17378c80], [0, 0x1737aec0], [0, 0x1737d100], [0, 0x1737f340]]}
  layer.20.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x175e4a00]]}
  layer.20.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x151c4080]]}
  layer.21.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x148cc840], [3, 0x14910880], [3, 0x149548c0], [3, 0x14998900], [3, 0x149dc940], [3, 0x14a20980], [3, 0x14a649c0], [3, 0x14aa8a00]]}
  layer.21.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x17381580], [0, 0x173859c0], [0, 0x17389e00], [0, 0x1738e240]]}
  layer.21.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x151d50c0], [4, 0x15219100], [4, 0x1525d140], [4, 0x152a1180], [4, 0x152e51c0], [4, 0x15329200], [4, 0x1536d240], [4, 0x153b1280]]}
  layer.21.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x17392680], [0, 0x17396ac0], [0, 0x1739af00], [0, 0x1739f340]]}
  layer.21.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x173a3780], [0, 0x173e77c0], [0, 0x1742b800], [0, 0x1746f840], [0, 0x174b3880], [0, 0x174f78c0], [0, 0x1753b900], [0, 0x1757f940]]}
  layer.21.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x175f5a40], [2, 0x175f9e80], [2, 0x175fe2c0], [2, 0x17602700]]}
  layer.21.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x175c3980], [0, 0x176079c0], [0, 0x1764ba00], [0, 0x1768fa40], [0, 0x176d3a80], [0, 0x17717ac0], [0, 0x1775bb00], [0, 0x1779fb40]]}
  layer.21.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x17606b40], [2, 0x1760af80], [2, 0x1760f3c0], [2, 0x17613800]]}
  layer.21.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x177e3b80]]}
  layer.21.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x17617c40]]}
  layer.21.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x14aeca40], [3, 0x14b74a80], [3, 0x14bfcac0], [3, 0x14c84b00], [3, 0x14d0cb40], [3, 0x14d94b80], [3, 0x14e1cbc0], [3, 0x14ea4c00], [3, 0x14f2cc40], [3, 0x14fb4c80], [3, 0x1503ccc0], [3, 0x150c4d00],
      [3, 0x1514cd40], [3, 0x151d4d80], [3, 0x1525cdc0], [3, 0x152e4e00]]}
  layer.21.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x14fb4a80], [1, 0x14fc5ac0], [1, 0x14fd6b00], [1, 0x14fe7b40]]}
  layer.21.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x15a55540],
      [5, 0x15add580], [5, 0x15b655c0], [5, 0x15bed600], [5, 0x15c75640], [5, 0x15cfd680], [5, 0x15d856c0], [5, 0x15e0d700], [5, 0x15e95740], [5, 0x15f1d780], [5, 0x15fa57c0], [5, 0x1602d800], [5, 0x160b5840],
      [5, 0x1613d880], [5, 0x161c58c0], [5, 0x1624d900]]}
  layer.21.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1536ce40],
      [3, 0x1536f080], [3, 0x153712c0], [3, 0x15373500], [3, 0x15375740], [3, 0x15377980], [3, 0x15379bc0], [3, 0x1537be00]]}
  layer.21.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x177f4bc0]]}
  layer.21.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x17628c80]]}
  layer.22.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x1537e040], [3, 0x153c2080], [3, 0x154060c0], [3, 0x1544a100], [3, 0x1548e140], [3, 0x154d2180], [3, 0x155161c0], [3, 0x1555a200]]}
  layer.22.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x162d5940], [5, 0x162d9d80], [5, 0x162de1c0], [5, 0x162e2600]]}
  layer.22.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1559e240], [3, 0x155e2280], [3, 0x156262c0], [3, 0x1566a300], [3, 0x156ae340], [3, 0x156f2380], [3, 0x157363c0], [3, 0x1577a400]]}
  layer.22.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x162e6a40], [5, 0x162eae80], [5, 0x162ef2c0], [5, 0x162f3700]]}
  layer.22.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x162f7b40], [5, 0x1633bb80], [5, 0x1637fbc0], [5, 0x163c3c00], [5, 0x16407c40], [5, 0x1644bc80], [5, 0x1648fcc0], [5, 0x164d3d00]]}
  layer.22.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x14ff8b80], [1, 0x14ffcfc0], [1, 0x15001400], [1, 0x15005840]]}
  layer.22.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x16517d40], [5, 0x1655bd80], [5, 0x1659fdc0], [5, 0x165e3e00], [5, 0x16627e40], [5, 0x1666be80], [5, 0x166afec0], [5, 0x166f3f00]]}
  layer.22.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x15009c80], [1, 0x1500e0c0], [1, 0x15012500], [1, 0x15016940]]}
  layer.22.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x1501ad80]]}
  layer.22.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x1502bdc0]]}
  layer.22.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x17639cc0], [2, 0x176c1d00], [2, 0x17749d40], [2, 0x177d1d80], [2, 0x17859dc0], [2, 0x178e1e00], [2, 0x17969e40], [2, 0x179f1e80], [2, 0x17a79ec0], [2, 0x17b01f00], [2, 0x17b89f40], [2, 0x17c11f80],
      [2, 0x17c99fc0], [2, 0x17d22000], [2, 0x17daa040], [2, 0x17e32080]]}
  layer.22.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x17805c00], [0, 0x17816c40], [0, 0x17827c80], [0, 0x17838cc0]]}
  layer.22.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x153f52c0],
      [4, 0x1547d300], [4, 0x15505340], [4, 0x1558d380], [4, 0x156153c0], [4, 0x1569d400], [4, 0x15725440], [4, 0x157ad480], [4, 0x158354c0], [4, 0x158bd500], [4, 0x15945540], [4, 0x159cd580], [4, 0x15a555c0],
      [4, 0x15add600], [4, 0x15b65640], [4, 0x15bed680]]}
  layer.22.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x17eba0c0],
      [2, 0x17ebc300], [2, 0x17ebe540], [2, 0x17ec0780], [2, 0x17ec29c0], [2, 0x17ec4c00], [2, 0x17ec6e40], [2, 0x17ec9080]]}
  layer.22.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x16737f40]]}
  layer.22.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1503ce00]]}
  layer.23.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x17ecb2c0], [2, 0x17f0f300], [2, 0x17f53340], [2, 0x17f97380], [2, 0x17fdb3c0], [2, 0x1801f400], [2, 0x18063440], [2, 0x180a7480]]}
  layer.23.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x15c756c0], [4, 0x15c79b00], [4, 0x15c7df40], [4, 0x15c82380]]}
  layer.23.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x180eb4c0], [2, 0x1812f500], [2, 0x18173540], [2, 0x181b7580], [2, 0x181fb5c0], [2, 0x1823f600], [2, 0x18283640], [2, 0x182c7680]]}
  layer.23.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15c867c0], [4, 0x15c8ac00], [4, 0x15c8f040], [4, 0x15c93480]]}
  layer.23.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x15c978c0], [4, 0x15cdb900], [4, 0x15d1f940], [4, 0x15d63980], [4, 0x15da79c0], [4, 0x15deba00], [4, 0x15e2fa40], [4, 0x15e73a80]]}
  layer.23.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x15eb7ac0], [4, 0x15ebbf00], [4, 0x15ec0340], [4, 0x15ec4780]]}
  layer.23.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x157be440], [3, 0x15802480], [3, 0x158464c0], [3, 0x1588a500], [3, 0x158ce540], [3, 0x15912580], [3, 0x159565c0], [3, 0x1599a600]]}
  layer.23.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x16748f80], [5, 0x1674d3c0], [5, 0x16751800], [5, 0x16755c40]]}
  layer.23.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x159de640]]}
  layer.23.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1675a080]]}
  layer.23.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x17849d00], [0, 0x178d1d40], [0, 0x17959d80], [0, 0x179e1dc0], [0, 0x17a69e00], [0, 0x17af1e40], [0, 0x17b79e80], [0, 0x17c01ec0], [0, 0x17c89f00], [0, 0x17d11f40], [0, 0x17d99f80], [0, 0x17e21fc0],
      [0, 0x17eaa000], [0, 0x17f32040], [0, 0x17fba080], [0, 0x180420c0]]}
  layer.23.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1676b0c0], [5, 0x1677c100], [5, 0x1678d140], [5, 0x1679e180]]}
  layer.23.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1830b6c0],
      [2, 0x18393700], [2, 0x1841b740], [2, 0x184a3780], [2, 0x1852b7c0], [2, 0x185b3800], [2, 0x1863b840], [2, 0x186c3880], [2, 0x1874b8c0], [2, 0x187d3900], [2, 0x1885b940], [2, 0x188e3980], [2, 0x1896b9c0],
      [2, 0x189f3a00], [2, 0x18a7ba40], [2, 0x18b03a80]]}
  layer.23.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x180ca100],
      [0, 0x180cc340], [0, 0x180ce580], [0, 0x180d07c0], [0, 0x180d2a00], [0, 0x180d4c40], [0, 0x180d6e80], [0, 0x180d90c0]]}
  layer.23.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18b8bac0]]}
  layer.23.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x15ec8bc0]]}

  # constant
  input_1_multiply_112_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1504de40]]}
  lc.input_tensor.softmax_114.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x159ef680]]}
  lc.input_tensor.layernorm_134.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1504e700]]}
  lc.input_tensor.layernorm_134.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18b9cb00]]}
  dc.input_tensor.layernorm_134.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x159eff40], [3, 0x159f3280]]}
  lc.input_tensor.layernorm_134.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167af1c0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x180db300]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18b9d3c0]]}
  lc.input_tensor.layernorm_148.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167afa80]]}
  lc.input_tensor.layernorm_148.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180dbbc0]]}
  dc.input_tensor.layernorm_148.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1504efc0], [1, 0x15052300]]}
  lc.input_tensor.layernorm_148.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x159f65c0]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15ed9c00]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180dc480]]}
  input_1_multiply_165_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15eda4c0]]}
  lc.input_tensor.softmax_167.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167b0340]]}
  lc.input_tensor.layernorm_187.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180dcd40]]}
  lc.input_tensor.layernorm_187.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15055640]]}
  dc.input_tensor.layernorm_187.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18b9dc80], [2, 0x18ba0fc0]]}
  lc.input_tensor.layernorm_187.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180dd600]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x159f6e80]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x167b0c00]]}
  lc.input_tensor.layernorm_201.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x159f7740]]}
  lc.input_tensor.layernorm_201.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15edad80]]}
  dc.input_tensor.layernorm_201.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x15055f00], [1, 0x15059240]]}
  lc.input_tensor.layernorm_201.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180ddec0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1505c580]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x159f8000]]}
  input_1_multiply_218_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167b14c0]]}
  lc.input_tensor.softmax_220.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x159f88c0]]}
  lc.input_tensor.layernorm_240.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1505ce40]]}
  lc.input_tensor.layernorm_240.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18ba4300]]}
  dc.input_tensor.layernorm_240.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x159f9180], [3, 0x159fc4c0]]}
  lc.input_tensor.layernorm_240.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167b1d80]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x180de780]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18ba4bc0]]}
  lc.input_tensor.layernorm_254.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180df040]]}
  lc.input_tensor.layernorm_254.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1505d700]]}
  dc.input_tensor.layernorm_254.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18ba5480], [2, 0x18ba87c0]]}
  lc.input_tensor.layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15edb640]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167b2640]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180df900]]}
  input_1_multiply_271_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18babb00]]}
  lc.input_tensor.softmax_273.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x159ff800]]}
  lc.input_tensor.layernorm_293.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180e01c0]]}
  lc.input_tensor.layernorm_293.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167b2f00]]}
  dc.input_tensor.layernorm_293.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x180e0a80], [0, 0x180e3dc0]]}
  lc.input_tensor.layernorm_293.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bac3c0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x15a000c0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x167b37c0]]}
  lc.input_tensor.layernorm_307.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15edbf00]]}
  lc.input_tensor.layernorm_307.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167b4080]]}
  dc.input_tensor.layernorm_307.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x180e7100], [0, 0x180ea440]]}
  lc.input_tensor.layernorm_307.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bacc80]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a00980]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167b4940]]}
  input_1_multiply_324_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167b5200]]}
  lc.input_tensor.softmax_326.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180ed780]]}
  lc.input_tensor.layernorm_346.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1505dfc0]]}
  lc.input_tensor.layernorm_346.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bad540]]}
  dc.input_tensor.layernorm_346.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x15a01240], [3, 0x15a04580]]}
  lc.input_tensor.layernorm_346.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167b5ac0]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15edc7c0]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15edd080]]}
  lc.input_tensor.layernorm_360.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bade00]]}
  lc.input_tensor.layernorm_360.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a078c0]]}
  dc.input_tensor.layernorm_360.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15edd940], [4, 0x15ee0c80]]}
  lc.input_tensor.layernorm_360.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bae6c0]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1505e880]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a08180]]}
  input_1_multiply_377_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167b6380]]}
  lc.input_tensor.softmax_379.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180ee040]]}
  lc.input_tensor.layernorm_399.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a08a40]]}
  lc.input_tensor.layernorm_399.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15ee3fc0]]}
  dc.input_tensor.layernorm_399.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x167b6c40], [5, 0x167b9f80]]}
  lc.input_tensor.layernorm_399.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1505f140]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18baef80]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15ee4880]]}
  lc.input_tensor.layernorm_413.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180ee900]]}
  lc.input_tensor.layernorm_413.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1505fa00]]}
  dc.input_tensor.layernorm_413.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18baf840], [2, 0x18bb2b80]]}
  lc.input_tensor.layernorm_413.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15ee5140]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167bd2c0]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x150602c0]]}
  input_1_multiply_430_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15ee5a00]]}
  lc.input_tensor.softmax_432.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167bdb80]]}
  lc.input_tensor.layernorm_452.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a09300]]}
  lc.input_tensor.layernorm_452.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15ee62c0]]}
  dc.input_tensor.layernorm_452.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x167be440], [5, 0x167c1780]]}
  lc.input_tensor.layernorm_452.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15060b80]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18bb5ec0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15ee6b80]]}
  lc.input_tensor.layernorm_466.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15061440]]}
  lc.input_tensor.layernorm_466.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bb6780]]}
  dc.input_tensor.layernorm_466.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x15a09bc0], [3, 0x15a0cf00]]}
  lc.input_tensor.layernorm_466.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167c4ac0]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180ef1c0]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bb7040]]}
  input_1_multiply_483_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180efa80]]}
  lc.input_tensor.softmax_485.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15061d00]]}
  lc.input_tensor.layernorm_505.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bb7900]]}
  lc.input_tensor.layernorm_505.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a10240]]}
  dc.input_tensor.layernorm_505.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15ee7440], [4, 0x15eea780]]}
  lc.input_tensor.layernorm_505.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bb81c0]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x180f0340]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15eedac0]]}
  lc.input_tensor.layernorm_519.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bb8a80]]}
  lc.input_tensor.layernorm_519.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a10b00]]}
  dc.input_tensor.layernorm_519.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15eee380], [4, 0x15ef16c0]]}
  lc.input_tensor.layernorm_519.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180f0c00]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x150625c0]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a113c0]]}
  input_1_multiply_536_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167c5380]]}
  lc.input_tensor.softmax_538.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bb9340]]}
  lc.input_tensor.layernorm_558.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15062e80]]}
  lc.input_tensor.layernorm_558.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bb9c00]]}
  dc.input_tensor.layernorm_558.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x15a11c80], [3, 0x15a14fc0]]}
  lc.input_tensor.layernorm_558.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167c5c40]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x180f14c0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18bba4c0]]}
  lc.input_tensor.layernorm_572.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180f1d80]]}
  lc.input_tensor.layernorm_572.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15063740]]}
  dc.input_tensor.layernorm_572.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18bbad80], [2, 0x18bbe0c0]]}
  lc.input_tensor.layernorm_572.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15ef4a00]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167c6500]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15064000]]}
  input_1_multiply_589_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180f2640]]}
  lc.input_tensor.softmax_591.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x150648c0]]}
  lc.input_tensor.layernorm_611.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bc1400]]}
  lc.input_tensor.layernorm_611.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167c6dc0]]}
  dc.input_tensor.layernorm_611.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15ef52c0], [4, 0x15ef8600]]}
  lc.input_tensor.layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180f2f00]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x15065180]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x15a18300]]}
  lc.input_tensor.layernorm_625.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a18bc0]]}
  lc.input_tensor.layernorm_625.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15065a40]]}
  dc.input_tensor.layernorm_625.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18bc1cc0], [2, 0x18bc5000]]}
  lc.input_tensor.layernorm_625.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15efb940]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167c7680]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15066300]]}
  input_1_multiply_642_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180f37c0]]}
  lc.input_tensor.softmax_644.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15efc200]]}
  lc.input_tensor.layernorm_664.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167c7f40]]}
  lc.input_tensor.layernorm_664.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180f4080]]}
  dc.input_tensor.layernorm_664.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x15066bc0], [1, 0x15069f00]]}
  lc.input_tensor.layernorm_664.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a19480]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15efcac0]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x15a19d40]]}
  lc.input_tensor.layernorm_678.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1506d240]]}
  lc.input_tensor.layernorm_678.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bc8340]]}
  dc.input_tensor.layernorm_678.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x15a1a600], [3, 0x15a1d940]]}
  lc.input_tensor.layernorm_678.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167c8800]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bc8c00]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a20c80]]}
  input_1_multiply_695_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167c90c0]]}
  lc.input_tensor.softmax_697.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180f4940]]}
  lc.input_tensor.layernorm_717.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167c9980]]}
  lc.input_tensor.layernorm_717.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180f5200]]}
  dc.input_tensor.layernorm_717.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1506db00], [1, 0x15070e40]]}
  lc.input_tensor.layernorm_717.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a21540]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15efd380]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x180f5ac0]]}
  lc.input_tensor.layernorm_731.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15efdc40]]}
  lc.input_tensor.layernorm_731.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167ca240]]}
  dc.input_tensor.layernorm_731.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x180f6380], [0, 0x180f96c0]]}
  lc.input_tensor.layernorm_731.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bc94c0]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a21e00]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167cab00]]}
  input_1_multiply_748_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15efe500]]}
  lc.input_tensor.softmax_750.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167cb3c0]]}
  lc.input_tensor.layernorm_770.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180fca00]]}
  lc.input_tensor.layernorm_770.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15074180]]}
  dc.input_tensor.layernorm_770.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18bc9d80], [2, 0x18bcd0c0]]}
  lc.input_tensor.layernorm_770.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15efedc0]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x167cbc80]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x15074a40]]}
  lc.input_tensor.layernorm_784.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167cc540]]}
  lc.input_tensor.layernorm_784.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a226c0]]}
  dc.input_tensor.layernorm_784.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15eff680], [4, 0x15f029c0]]}
  lc.input_tensor.layernorm_784.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x180fd2c0]]}
  lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15075300]]}
  lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a22f80]]}
  input_1_multiply_801_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bd0400]]}
  lc.input_tensor.softmax_803.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x180fdb80]]}
  lc.input_tensor.layernorm_823.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15075bc0]]}
  lc.input_tensor.layernorm_823.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bd0cc0]]}
  dc.input_tensor.layernorm_823.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x15a23840], [3, 0x15a26b80]]}
  lc.input_tensor.layernorm_823.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167cce00]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x180fe440]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x180fed00]]}
  lc.input_tensor.layernorm_837.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f05d00]]}
  lc.input_tensor.layernorm_837.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167cd6c0]]}
  dc.input_tensor.layernorm_837.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x180ff5c0], [0, 0x18102900]]}
  lc.input_tensor.layernorm_837.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bd1580]]}
  lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167cdf80]]}
  lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18105c40]]}
  input_1_multiply_854_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bd1e40]]}
  lc.input_tensor.softmax_856.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a29ec0]]}
  lc.input_tensor.layernorm_876.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a2a780]]}
  lc.input_tensor.layernorm_876.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f065c0]]}
  dc.input_tensor.layernorm_876.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x167ce840], [5, 0x167d1b80]]}
  lc.input_tensor.layernorm_876.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15076480]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18bd2700]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15f06e80]]}
  lc.input_tensor.layernorm_890.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bd2fc0]]}
  lc.input_tensor.layernorm_890.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a2b040]]}
  dc.input_tensor.layernorm_890.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15f07740], [4, 0x15f0aa80]]}
  lc.input_tensor.layernorm_890.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18106500]]}
  lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15076d40]]}
  lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a2b900]]}
  input_1_multiply_907_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f0ddc0]]}
  lc.input_tensor.softmax_909.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167d4ec0]]}
  lc.input_tensor.layernorm_929.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18106dc0]]}
  lc.input_tensor.layernorm_929.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bd3880]]}
  dc.input_tensor.layernorm_929.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18bd4140], [2, 0x18bd7480]]}
  lc.input_tensor.layernorm_929.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15f0e680]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x167d5780]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x15077600]]}
  lc.input_tensor.layernorm_943.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15077ec0]]}
  lc.input_tensor.layernorm_943.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18107680]]}
  dc.input_tensor.layernorm_943.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x18bda7c0], [2, 0x18bddb00]]}
  lc.input_tensor.layernorm_943.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15f0ef40]]}
  lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167d6040]]}
  lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15078780]]}
  input_1_multiply_960_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a2c1c0]]}
  lc.input_tensor.softmax_962.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f0f800]]}
  lc.input_tensor.layernorm_982.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167d6900]]}
  lc.input_tensor.layernorm_982.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18107f40]]}
  dc.input_tensor.layernorm_982.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x15079040], [1, 0x1507c380]]}
  lc.input_tensor.layernorm_982.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a2ca80]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x1507f6c0]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x15a2d340]]}
  lc.input_tensor.layernorm_996.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1507ff80]]}
  lc.input_tensor.layernorm_996.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18be0e40]]}
  dc.input_tensor.layernorm_996.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x15a2dc00], [3, 0x15a30f40]]}
  lc.input_tensor.layernorm_996.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167d71c0]]}
  lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18be1700]]}
  lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18be1fc0]]}
  input_1_multiply_1013_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f100c0]]}
  lc.input_tensor.softmax_1015.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167d7a80]]}
  lc.input_tensor.layernorm_1035.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a34280]]}
  lc.input_tensor.layernorm_1035.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f10980]]}
  dc.input_tensor.layernorm_1035.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x167d8340], [5, 0x167db680]]}
  lc.input_tensor.layernorm_1035.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15080840]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18be2880]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15f11240]]}
  lc.input_tensor.layernorm_1049.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18be3140]]}
  lc.input_tensor.layernorm_1049.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a34b40]]}
  dc.input_tensor.layernorm_1049.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x15f11b00], [4, 0x15f14e40]]}
  lc.input_tensor.layernorm_1049.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18108800]]}
  lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15081100]]}
  lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a35400]]}
  input_1_multiply_1066_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f18180]]}
  lc.input_tensor.softmax_1068.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167de9c0]]}
  lc.input_tensor.layernorm_1088.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x181090c0]]}
  lc.input_tensor.layernorm_1088.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167df280]]}
  dc.input_tensor.layernorm_1088.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x18be3a00], [2, 0x18be6d40]]}
  lc.input_tensor.layernorm_1088.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15f18a40]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x167dfb40]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x150819c0]]}
  lc.input_tensor.layernorm_1102.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a35cc0]]}
  lc.input_tensor.layernorm_1102.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15082280]]}
  dc.input_tensor.layernorm_1102.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x18bea080], [2, 0x18bed3c0]]}
  lc.input_tensor.layernorm_1102.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15f19300]]}
  lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167e0400]]}
  lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15082b40]]}
  input_1_multiply_1119_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18109980]]}
  lc.input_tensor.softmax_1121.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f19bc0]]}
  lc.input_tensor.layernorm_1141.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167e0cc0]]}
  lc.input_tensor.layernorm_1141.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1810a240]]}
  dc.input_tensor.layernorm_1141.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x15083400], [1, 0x15086740]]}
  lc.input_tensor.layernorm_1141.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a36580]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15f1a480]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x1810ab00]]}
  lc.input_tensor.layernorm_1155.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f1ad40]]}
  lc.input_tensor.layernorm_1155.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167e1580]]}
  dc.input_tensor.layernorm_1155.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x1810b3c0], [0, 0x1810e700]]}
  lc.input_tensor.layernorm_1155.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bf0700]]}
  lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167e1e40]]}
  lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18111a40]]}
  input_1_multiply_1172_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bf0fc0]]}
  lc.input_tensor.softmax_1174.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a36e40]]}
  lc.input_tensor.layernorm_1194.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a37700]]}
  lc.input_tensor.layernorm_1194.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f1b600]]}
  dc.input_tensor.layernorm_1194.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x167e2700], [5, 0x167e5a40]]}
  lc.input_tensor.layernorm_1194.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15089a80]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18bf1880]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15f1bec0]]}
  lc.input_tensor.layernorm_1208.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bf2140]]}
  lc.input_tensor.layernorm_1208.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a37fc0]]}
  dc.input_tensor.layernorm_1208.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x15f1c780], [4, 0x15f1fac0]]}
  lc.input_tensor.layernorm_1208.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18112300]]}
  lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1508a340]]}
  lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a38880]]}
  input_1_multiply_1225_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f22e00]]}
  lc.input_tensor.softmax_1227.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167e8d80]]}
  lc.input_tensor.layernorm_1247.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18112bc0]]}
  lc.input_tensor.layernorm_1247.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167e9640]]}
  dc.input_tensor.layernorm_1247.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x18bf2a00], [2, 0x18bf5d40]]}
  lc.input_tensor.layernorm_1247.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15f236c0]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x167e9f00]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x1508ac00]]}
  lc.input_tensor.layernorm_1261.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a39140]]}
  lc.input_tensor.layernorm_1261.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1508b4c0]]}
  dc.input_tensor.layernorm_1261.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x18bf9080], [2, 0x18bfc3c0]]}
  lc.input_tensor.layernorm_1261.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15f23f80]]}
  lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167ea7c0]]}
  lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1508bd80]]}
  input_1_multiply_1278_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18113480]]}
  lc.input_tensor.softmax_1280.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f24840]]}
  lc.input_tensor.layernorm_1300.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167eb080]]}
  lc.input_tensor.layernorm_1300.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18113d40]]}
  dc.input_tensor.layernorm_1300.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x1508c640], [1, 0x1508f980]]}
  lc.input_tensor.layernorm_1300.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a39a00]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15f25100]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x18114600]]}
  lc.input_tensor.layernorm_1314.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f259c0]]}
  lc.input_tensor.layernorm_1314.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x167eb940]]}
  dc.input_tensor.layernorm_1314.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x18114ec0], [0, 0x18118200]]}
  lc.input_tensor.layernorm_1314.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18bff700]]}
  lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x167ec200]]}
  lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1811b540]]}
  input_1_multiply_1331_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18bfffc0]]}
  lc.input_tensor.softmax_1333.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a3a2c0]]}
  lc.input_tensor.layernorm_1353.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a3ab80]]}
  lc.input_tensor.layernorm_1353.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x15f26280]]}
  dc.input_tensor.layernorm_1353.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x167ecac0], [5, 0x167efe00]]}
  lc.input_tensor.layernorm_1353.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15092cc0]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x18c00880]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x15f26b40]]}
  lc.input_tensor.layernorm_1367.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18c01140]]}
  lc.input_tensor.layernorm_1367.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15a3b440]]}
  dc.input_tensor.layernorm_1367.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x15f27400], [4, 0x15f2a740]]}
  lc.input_tensor.layernorm_1367.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1811be00]]}
  lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15093580]]}
  lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a3bd00]]}

  # epoch_to_epoch
  e2e_layernorm_134.dc.multiply.9_0: {input: layernorm_134.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x15a3c5c0], [3, 0x15b08600]]}
  e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8],
    ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x18c01a00]]}
  e2e_layernorm_148.dc.multiply.8_0: {input: layernorm_148.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x15f2da80], [4, 0x15ff9ac0]]}
  e2e_layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {input: layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4],
    ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x167f3140]]}
  e2e_layernorm_187.dc.reciprocal.7_0: {input: layernorm_187.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15093e40], [1, 0x1509a480]]}
  e2e_buffer_0_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8_0: {input: buffer_0_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1811c6c0], [0, 0x181e8700]]}
  e2e_layernorm_201.dc.add.5_0: {input: layernorm_201.dc.add.5, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18c23a40], [2, 0x18c2a080]]}
  e2e_buffer_0_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8_0: {input: buffer_0_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x15bd4640], [3, 0x15ca0680]]}
  e2e_layernorm_240.dc.subtract.1_0: {input: layernorm_240.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x160c5b00], [4, 0x16191b40]]}
  e2e_buffer_0_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8_0: {input: buffer_0_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x16815180], [5, 0x168e11c0]]}
  e2e_gelu_246_0: {input: gelu_246, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x150a0ac0],
      [1, 0x1516cb00], [1, 0x15238b40], [1, 0x15304b80], [1, 0x153d0bc0], [1, 0x1549cc00], [1, 0x15568c40], [1, 0x15634c80]]}
  e2e_layernorm_240.dc.add.10_0: {input: layernorm_240.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x182b4740], [0, 0x18380780]]}
  e2e_softmax_273.dc.exp.0_0: {input: softmax_273.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x15d6c6c0], [3, 0x15fd0700], [3, 0x16234740], [3, 0x16498780]]}
  e2e_softmax_273.dc.reciprocal.2_0: {input: softmax_273.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x1625db80], [4, 0x162c3bc0]]}
  e2e_layernorm_254.dc.add.10_0: {input: layernorm_254.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x18c306c0], [2, 0x18cfc700]]}
  e2e_layernorm_293.dc.add.10_0: {input: layernorm_293.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x169ad200], [5, 0x16a79240]]}
  e2e_layernorm_307.dc.multiply.9_0: {input: layernorm_307.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1844c7c0], [0, 0x18518800]]}
  e2e_layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x15700cc0]]}
  e2e_layernorm_346.dc.multiply.8_0: {input: layernorm_346.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x18dc8740], [2, 0x18e94780]]}
  e2e_layernorm_360.dc.reciprocal.7_0: {input: layernorm_360.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x166fc7c0], [3, 0x16702e00]]}
  e2e_buffer_0_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8_0: {input: buffer_0_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x16329c00], [4, 0x163f5c40]]}
  e2e_layernorm_399.dc.reduce_avg.3.lc1_0: {input: layernorm_399.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x16b45280], [5, 0x16b4b8c0]]}
  e2e_buffer_0_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8_0: {input: buffer_0_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x185e4840], [0, 0x186b0880]]}
  e2e_layernorm_413.dc.subtract.1_0: {input: layernorm_413.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15722d00], [1, 0x157eed40]]}
  e2e_buffer_0_layernorm_413.dc.subtract.1_buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8_0: {input: buffer_0_layernorm_413.dc.subtract.1_buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8,
    type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x18f607c0], [2, 0x1902c800]]}
  e2e_add_451_0: {input: add_451, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x16709440], [
        3, 0x167d5480]]}
  e2e_gelu_458_0: {input: gelu_458, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x16b51f00],
      [5, 0x16c1df40], [5, 0x16ce9f80], [5, 0x16db5fc0], [5, 0x16e82000], [5, 0x16f4e040], [5, 0x1701a080], [5, 0x170e60c0]]}
  e2e_layernorm_452.dc.add.10_0: {input: layernorm_452.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x164c1c80], [4, 0x1658dcc0]]}
  e2e_softmax_485.dc.exp.0_0: {input: softmax_485.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x158bad80], [1, 0x15b1edc0], [1, 0x15d82e00], [1, 0x15fe6e40]]}
  e2e_softmax_485.dc.reciprocal.2_0: {input: softmax_485.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1877c8c0], [0, 0x187e2900]]}
  e2e_layernorm_466.dc.add.10_0: {input: layernorm_466.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x190f8840], [2, 0x191c4880]]}
  e2e_layernorm_505.dc.add.10_0: {input: layernorm_505.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x168a14c0], [3, 0x1696d500]]}
  e2e_layernorm_519.dc.multiply.9_0: {input: layernorm_519.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x16659d00], [4, 0x16725d40]]}
  e2e_layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x171b2100]]}
  e2e_layernorm_558.dc.multiply.8_0: {input: layernorm_558.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18848940], [0, 0x18914980]]}
  e2e_layernorm_572.dc.reciprocal.7_0: {input: layernorm_572.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1624ae80], [1, 0x162514c0]]}
  e2e_buffer_0_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8_0: {input: buffer_0_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x192908c0], [2, 0x1935c900]]}
  e2e_layernorm_611.dc.reduce_avg.3.lc1_0: {input: layernorm_611.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x16a39540], [3, 0x16a3fb80]]}
  e2e_buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8_0: {input: buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x167f1d80], [4, 0x168bddc0]]}
  e2e_layernorm_625.dc.subtract.1_0: {input: layernorm_625.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x171d4140], [5, 0x172a0180]]}
  e2e_buffer_0_layernorm_625.dc.subtract.1_buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8_0: {input: buffer_0_layernorm_625.dc.subtract.1_buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8,
    type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x189e09c0], [0, 0x18aaca00]]}
  e2e_add_663_0: {input: add_663, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x16257b00], [
        1, 0x16323b40]]}
  e2e_gelu_670_0: {input: gelu_670, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x16a461c0],
      [3, 0x16b12200], [3, 0x16bde240], [3, 0x16caa280], [3, 0x16d762c0], [3, 0x16e42300], [3, 0x16f0e340], [3, 0x16fda380]]}
  e2e_layernorm_664.dc.add.10_0: {input: layernorm_664.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x19428940], [2, 0x194f4980]]}
  e2e_softmax_697.dc.exp.0_0: {input: softmax_697.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1736c1c0], [5, 0x175d0200], [5, 0x17834240], [5, 0x17a98280]]}
  e2e_softmax_697.dc.reciprocal.2_0: {input: softmax_697.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18b78a40], [0, 0x18bdea80]]}
  e2e_layernorm_678.dc.add.10_0: {input: layernorm_678.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x16989e00], [4, 0x16a55e40]]}
  e2e_layernorm_717.dc.add.10_0: {input: layernorm_717.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x163efb80], [1, 0x164bbbc0]]}
  e2e_layernorm_731.dc.multiply.9_0: {input: layernorm_731.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x195c09c0], [2, 0x1968ca00]]}
  e2e_layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x170a63c0]]}
  e2e_layernorm_770.dc.multiply.8_0: {input: layernorm_770.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x16b21e80], [4, 0x16bedec0]]}
  e2e_layernorm_784.dc.reciprocal.7_0: {input: layernorm_784.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x17cfc2c0], [5, 0x17d02900]]}
  e2e_buffer_0_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8_0: {input: buffer_0_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x18c44ac0], [0, 0x18d10b00]]}
  e2e_layernorm_823.dc.reduce_avg.3.lc1_0: {input: layernorm_823.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x16587c00], [1, 0x1658e240]]}
  e2e_buffer_0_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8_0: {input: buffer_0_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x19758a40], [2, 0x19824a80]]}
  e2e_layernorm_837.dc.subtract.1_0: {input: layernorm_837.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x170c8400], [3, 0x17194440]]}
  e2e_buffer_0_layernorm_837.dc.subtract.1_buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8_0: {input: buffer_0_layernorm_837.dc.subtract.1_buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8,
    type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x16cb9f00], [4, 0x16d85f40]]}
  e2e_add_875_0: {input: add_875, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x17d08f40], [
        5, 0x17dd4f80]]}
  e2e_gelu_882_0: {input: gelu_882, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x16594880],
      [1, 0x166608c0], [1, 0x1672c900], [1, 0x167f8940], [1, 0x168c4980], [1, 0x169909c0], [1, 0x16a5ca00], [1, 0x16b28a40]]}
  e2e_layernorm_876.dc.add.10_0: {input: layernorm_876.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x18ddcb40], [0, 0x18ea8b80]]}
  e2e_softmax_909.dc.exp.0_0: {input: softmax_909.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17260480], [3, 0x174c44c0], [3, 0x17728500], [3, 0x1798c540]]}
  e2e_softmax_909.dc.reciprocal.2_0: {input: softmax_909.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x16e51f80], [4, 0x16eb7fc0]]}
  e2e_layernorm_890.dc.add.10_0: {input: layernorm_890.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x198f0ac0], [2, 0x199bcb00]]}
  e2e_layernorm_929.dc.add.10_0: {input: layernorm_929.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x17ea0fc0], [5, 0x17f6d000]]}
  e2e_layernorm_943.dc.multiply.9_0: {input: layernorm_943.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x18f74bc0], [0, 0x19040c00]]}
  e2e_layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x16bf4a80]]}
  e2e_layernorm_982.dc.multiply.8_0: {input: layernorm_982.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x19a88b40], [2, 0x19b54b80]]}
  e2e_layernorm_996.dc.reciprocal.7_0: {input: layernorm_996.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17bf0580], [3, 0x17bf6bc0]]}
  e2e_buffer_0_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8_0: {input: buffer_0_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [
      3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x16f1e000], [4, 0x16fea040]]}
  e2e_layernorm_1035.dc.reduce_avg.3.lc1_0: {input: layernorm_1035.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x18039040], [5, 0x1803f680]]}
  e2e_buffer_0_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8_0: {input: buffer_0_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1910cc40], [0, 0x191d8c80]]}
  e2e_layernorm_1049.dc.subtract.1_0: {input: layernorm_1049.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x16c16ac0], [1, 0x16ce2b00]]}
  e2e_buffer_0_layernorm_1049.dc.subtract.1_buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8_0: {input: buffer_0_layernorm_1049.dc.subtract.1_buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8,
    type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x19c20bc0], [2, 0x19cecc00]]}
  e2e_add_1087_0: {input: add_1087, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x17bfd200],
      [3, 0x17cc9240]]}
  e2e_gelu_1094_0: {input: gelu_1094, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x170b6080],
      [4, 0x171820c0], [4, 0x1724e100], [4, 0x1731a140], [4, 0x173e6180], [4, 0x174b21c0], [4, 0x1757e200], [4, 0x1764a240]]}
  e2e_layernorm_1088.dc.add.10_0: {input: layernorm_1088.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x18045cc0], [5, 0x18111d00]]}
  e2e_softmax_1121.dc.exp.0_0: {input: softmax_1121.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x16daeb40], [1, 0x17012b80], [1, 0x17276bc0], [1, 0x174dac00]]}
  e2e_softmax_1121.dc.reciprocal.2_0: {input: softmax_1121.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x19db8c40], [2, 0x19e1ec80]]}
  e2e_layernorm_1102.dc.add.10_0: {input: layernorm_1102.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x192a4cc0], [0, 0x19370d00]]}
  e2e_layernorm_1141.dc.add.10_0: {input: layernorm_1141.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17d95280], [3, 0x17e612c0]]}
  e2e_layernorm_1155.dc.multiply.9_0: {input: layernorm_1155.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x17716280], [4, 0x177e22c0]]}
  e2e_layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x181ddd40]]}
  e2e_layernorm_1194.dc.multiply.8_0: {input: layernorm_1194.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1943cd40], [0, 0x19508d80]]}
  e2e_layernorm_1208.dc.reciprocal.7_0: {input: layernorm_1208.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1773ec40], [1, 0x17745280]]}
  e2e_buffer_0_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8_0: {input: buffer_0_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x19e84cc0], [2, 0x19f50d00]]}
  e2e_layernorm_1247.dc.reduce_avg.3.lc1_0: {input: layernorm_1247.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x178ae300], [4, 0x178b4940]]}
  e2e_buffer_0_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8_0: {input: buffer_0_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x17f2d300], [3, 0x17ff9340]]}
  e2e_layernorm_1261.dc.subtract.1_0: {input: layernorm_1261.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x181ffd80], [5, 0x182cbdc0]]}
  e2e_buffer_0_layernorm_1261.dc.subtract.1_buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8_0: {input: buffer_0_layernorm_1261.dc.subtract.1_buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8,
    type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x195d4dc0], [0, 0x196a0e00]]}
  e2e_add_1299_0: {input: add_1299, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1774b8c0],
      [1, 0x17817900]]}
  e2e_gelu_1306_0: {input: gelu_1306, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x180c5380],
      [3, 0x181913c0], [3, 0x1825d400], [3, 0x18329440], [3, 0x183f5480], [3, 0x184c14c0], [3, 0x1858d500], [3, 0x18659540]]}
  e2e_layernorm_1300.dc.add.10_0: {input: layernorm_1300.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1a01cd40], [2, 0x1a0e8d80]]}
  e2e_add_1332_0: {input: add_1332, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x18397e00],
      [5, 0x1885fe40]]}
  e2e_matmul_1337_0: {input: matmul_1337, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1976ce40],
      [0, 0x1979fe80], [0, 0x197d2ec0], [0, 0x19805f00], [0, 0x19838f40], [0, 0x1986bf80], [0, 0x1989efc0], [0, 0x198d2000]]}
  e2e_layernorm_1314.dc.add.10_0: {input: layernorm_1314.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x178baf80], [4, 0x17986fc0]]}
  e2e_layernorm_1353.dc.add.10_0: {input: layernorm_1353.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x178e3940], [1, 0x179af980]]}
  e2e_layernorm_1367.dc.multiply.9_0: {input: layernorm_1367.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1a1b4dc0], [2, 0x1a280e00]]}
  e2e_layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {input: layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x18725580]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 2
    matmul_98: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {
        bias: true, m_k: 4, u_kt: 8}}
    matmul_104: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 4, u_kt: 8}}
    matmul_110: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_98, matmul_104], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_112: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_110, input_1_multiply_112_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_113: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [multiply_112, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_114.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_113], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_114.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_114.dc.exp.0, lc.input_tensor.softmax_114.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_114.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_114.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_114.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_114.dc.exp.0, softmax_114.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_118: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {
        bias: true, m_k: 4, u_kt: 8}}
    matmul_125: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_114.dc.multiply.3, matmul_118], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_129: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_125, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_133: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_129, hidden_states], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_134.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_133, lc.input_tensor.layernorm_134.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_134.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_133, layernorm_134.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_134.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_134.dc.subtract.1, layernorm_134.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_134.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_134.dc.multiply.2, lc.input_tensor.layernorm_134.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_134.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_134.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_134.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_134.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_134.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_134.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_134.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_134.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_134.dc.reciprocal.7, lc.input_tensor.layernorm_134.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_134.dc.subtract.1_buffer_1_layernorm_134.dc.subtract.1_layernorm_134.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_134.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_134.dc.subtract.1_layernorm_134.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_134.dc.subtract.1_buffer_1_layernorm_134.dc.subtract.1_layernorm_134.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_134.dc.subtract.1_layernorm_134.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_134.dc.subtract.1_layernorm_134.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_134.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_134.dc.subtract.1_layernorm_134.dc.multiply.8, layernorm_134.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_134.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_134.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_1:
    target_device: 0
    input_count: 2
    layernorm_134.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_134.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    matmul_137: {type: matmul, grid_loc: [0, 1], grid_size: [6, 4], inputs: [layernorm_134.dc.add.10, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_140: {type: gelu, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_137], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_143: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_140, layer.0.output.dense.weight, layer.0.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_134.dc.add.10_add_147: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_134.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_147: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_143, buffer_0_layernorm_134.dc.add.10_add_147], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_148.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_147, lc.input_tensor.layernorm_148.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_148.dc.subtract.1: {type: subtract, grid_loc: [2, 0], grid_size: [2, 1], inputs: [add_147, layernorm_148.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_148.dc.multiply.2: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_148.dc.subtract.1, layernorm_148.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_148.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_148.dc.multiply.2, lc.input_tensor.layernorm_148.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_148.dc.add.5: {type: add, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_148.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_148.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_148.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_148.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_148.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_148.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_148.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_148.dc.reciprocal.7, lc.input_tensor.layernorm_148.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_148.dc.subtract.1_buffer_1_layernorm_148.dc.subtract.1_layernorm_148.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_148.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_148.dc.subtract.1_layernorm_148.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_148.dc.subtract.1_buffer_1_layernorm_148.dc.subtract.1_layernorm_148.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_148.dc.subtract.1_layernorm_148.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_148.dc.subtract.1_layernorm_148.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_148.dc.multiply.8: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_148.dc.subtract.1_layernorm_148.dc.multiply.8, layernorm_148.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_2:
    target_device: 0
    input_count: 2
    layernorm_148.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_148.dc.multiply.8_0, e2e_layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_148.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_148.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_151: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_148.dc.add.10, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_157: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_148.dc.add.10, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_163: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_151, matmul_157], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_165: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_163, input_1_multiply_165_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_166: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_165, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_167.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_166], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_167.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_167.dc.exp.0, lc.input_tensor.softmax_167.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_167.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_167.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_167.dc.multiply.3: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_167.dc.exp.0, softmax_167.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_171: {type: matmul, grid_loc: [4, 3], grid_size: [2, 4], inputs: [layernorm_148.dc.add.10, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_178: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [softmax_167.dc.multiply.3, matmul_171], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_182: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_178, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_148.dc.add.10_add_186: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_148.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_186: {type: add, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_182, buffer_0_layernorm_148.dc.add.10_add_186], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_187.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_186, lc.input_tensor.layernorm_187.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_187.dc.subtract.1: {type: subtract, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_186, layernorm_187.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_187.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_187.dc.subtract.1, layernorm_187.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_187.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_187.dc.multiply.2, lc.input_tensor.layernorm_187.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_187.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_187.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_187.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_187.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_187.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_187.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_187.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_0_layernorm_187.dc.subtract.1_buffer_1_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_187.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_187.dc.subtract.1_buffer_1_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3:
    target_device: 0
    input_count: 2
    layernorm_187.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_187.dc.reciprocal.7_0, lc.input_tensor.layernorm_187.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_187.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8_0, layernorm_187.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_187.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_187.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_187.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_187.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_190: {type: matmul, grid_loc: [2, 0], grid_size: [6, 4], inputs: [layernorm_187.dc.add.10, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_193: {type: gelu, grid_loc: [2, 4], grid_size: [2, 4], inputs: [matmul_190], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_196: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_193, layer.1.output.dense.weight, layer.1.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_187.dc.add.10_add_200: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_187.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_200: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_196, buffer_0_layernorm_187.dc.add.10_add_200], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_201.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_200, lc.input_tensor.layernorm_201.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_201.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_200, layernorm_201.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_201.dc.multiply.2: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_201.dc.subtract.1, layernorm_201.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_201.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_201.dc.multiply.2, lc.input_tensor.layernorm_201.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_201.dc.add.5: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_201.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_201.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_201.dc.subtract.1_buffer_1_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_201.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_201.dc.subtract.1_buffer_1_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_4:
    target_device: 0
    input_count: 2
    layernorm_201.dc.sqrt.6: {type: sqrt, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_201.dc.add.5_0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_201.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_201.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_201.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_201.dc.reciprocal.7, lc.input_tensor.layernorm_201.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_201.dc.multiply.8: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8_0, layernorm_201.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_201.dc.multiply.9: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_201.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_201.dc.add.10: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_201.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_204: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_201.dc.add.10, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_210: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_201.dc.add.10, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_216: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [matmul_204, matmul_210], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_218: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_216, input_1_multiply_218_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_219: {type: add, grid_loc: [4, 2], grid_size: [2, 1], inputs: [multiply_218, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_220.dc.exp.0: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [add_219], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_220.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_220.dc.exp.0, lc.input_tensor.softmax_220.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_220.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [softmax_220.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_220.dc.multiply.3: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [softmax_220.dc.exp.0, softmax_220.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_224: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_201.dc.add.10, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_231: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_220.dc.multiply.3, matmul_224], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_235: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_231, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_201.dc.add.10_add_239: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_201.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_239: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_235, buffer_0_layernorm_201.dc.add.10_add_239], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_240.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_239, lc.input_tensor.layernorm_240.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_240.dc.subtract.1: {type: subtract, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_239, layernorm_240.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_0_layernorm_240.dc.subtract.1_buffer_1_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_240.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_240.dc.subtract.1_buffer_1_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_5:
    target_device: 0
    input_count: 2
    layernorm_240.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_240.dc.subtract.1_0, e2e_layernorm_240.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_240.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_240.dc.multiply.2, lc.input_tensor.layernorm_240.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_240.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_240.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_240.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_240.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_240.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_240.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_240.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_240.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_240.dc.reciprocal.7, lc.input_tensor.layernorm_240.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_240.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8_0, layernorm_240.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.2.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_240.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_240.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_240.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_240.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_243: {type: matmul, grid_loc: [2, 2], grid_size: [6, 4], inputs: [layernorm_240.dc.add.10, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_246: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_243], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_6:
    target_device: 0
    input_count: 2
    matmul_249: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_246_0, layer.2.output.dense.weight, layer.2.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_240.dc.add.10_add_253: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_240.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_253: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_249, buffer_0_layernorm_240.dc.add.10_add_253], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_254.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_253, lc.input_tensor.layernorm_254.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_254.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_253, layernorm_254.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_254.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_254.dc.subtract.1, layernorm_254.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_254.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_254.dc.multiply.2, lc.input_tensor.layernorm_254.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_254.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_254.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_254.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_254.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_254.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_254.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_254.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_254.dc.reciprocal.7, lc.input_tensor.layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_254.dc.subtract.1_buffer_1_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_254.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_254.dc.subtract.1_buffer_1_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_254.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_254.dc.subtract.1_layernorm_254.dc.multiply.8, layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_254.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_254.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_254.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_254.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_257: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_254.dc.add.10, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_263: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_254.dc.add.10, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_269: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_257, matmul_263], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_271: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_269, input_1_multiply_271_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_272: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_271, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_273.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_272], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_273.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_273.dc.exp.0, lc.input_tensor.softmax_273.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_273.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_273.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_7:
    target_device: 0
    input_count: 2
    softmax_273.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_273.dc.exp.0_0, e2e_softmax_273.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_277: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_254.dc.add.10_0, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias], t: 1, mblock: [3, 2],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_284: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [softmax_273.dc.multiply.3, matmul_277], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_288: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_284, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_254.dc.add.10_add_292: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_254.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_292: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_288, buffer_0_layernorm_254.dc.add.10_add_292], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_293.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_292, lc.input_tensor.layernorm_293.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_293.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_292, layernorm_293.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_293.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_293.dc.subtract.1, layernorm_293.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_293.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_293.dc.multiply.2, lc.input_tensor.layernorm_293.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_293.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_293.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_293.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_293.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_293.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_293.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_293.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_293.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_293.dc.reciprocal.7, lc.input_tensor.layernorm_293.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_293.dc.subtract.1_buffer_1_layernorm_293.dc.subtract.1_layernorm_293.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_293.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_293.dc.subtract.1_layernorm_293.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_293.dc.subtract.1_buffer_1_layernorm_293.dc.subtract.1_layernorm_293.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_293.dc.subtract.1_layernorm_293.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_293.dc.subtract.1_layernorm_293.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_293.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_293.dc.subtract.1_layernorm_293.dc.multiply.8, layernorm_293.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.3.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_293.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_293.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_293.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_293.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_8:
    target_device: 0
    input_count: 2
    matmul_296: {type: matmul, grid_loc: [0, 0], grid_size: [6, 4], inputs: [e2e_layernorm_293.dc.add.10_0, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_299: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [matmul_296], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_302: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_299, layer.3.output.dense.weight, layer.3.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_293.dc.add.10_add_306: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_293.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_306: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_302, buffer_0_layernorm_293.dc.add.10_add_306], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_307.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [add_306, lc.input_tensor.layernorm_307.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_307.dc.subtract.1: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_306, layernorm_307.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_307.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_307.dc.subtract.1, layernorm_307.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_307.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_307.dc.multiply.2, lc.input_tensor.layernorm_307.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_307.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_307.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_307.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_307.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_307.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_307.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_307.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_307.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_307.dc.reciprocal.7, lc.input_tensor.layernorm_307.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_307.dc.subtract.1_buffer_1_layernorm_307.dc.subtract.1_layernorm_307.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_307.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_307.dc.subtract.1_layernorm_307.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_307.dc.subtract.1_buffer_1_layernorm_307.dc.subtract.1_layernorm_307.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_307.dc.subtract.1_layernorm_307.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_307.dc.subtract.1_layernorm_307.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_307.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_307.dc.subtract.1_layernorm_307.dc.multiply.8, layernorm_307.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_307.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_307.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_9:
    target_device: 0
    input_count: 2
    layernorm_307.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_307.dc.multiply.9_0, e2e_layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_310: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [layernorm_307.dc.add.10, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_316: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_307.dc.add.10, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_322: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_310, matmul_316], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_324: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_322, input_1_multiply_324_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_325: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_324, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_326.dc.exp.0: {type: exp, grid_loc: [2, 4], grid_size: [2, 2], inputs: [add_325], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_326.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_326.dc.exp.0, lc.input_tensor.softmax_326.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_326.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_326.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_326.dc.multiply.3: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_326.dc.exp.0, softmax_326.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_330: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [layernorm_307.dc.add.10, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_337: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_326.dc.multiply.3, matmul_330], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_341: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_337, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_307.dc.add.10_add_345: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_307.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_345: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_341, buffer_0_layernorm_307.dc.add.10_add_345], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_346.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_345, lc.input_tensor.layernorm_346.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_346.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [2, 1], inputs: [add_345, layernorm_346.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_346.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_346.dc.subtract.1, layernorm_346.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_346.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_346.dc.multiply.2, lc.input_tensor.layernorm_346.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_346.dc.add.5: {type: add, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_346.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_346.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_346.dc.sqrt.6: {type: sqrt, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_346.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_346.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_346.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_346.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_346.dc.reciprocal.7, lc.input_tensor.layernorm_346.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_346.dc.subtract.1_buffer_1_layernorm_346.dc.subtract.1_layernorm_346.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_346.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_346.dc.subtract.1_layernorm_346.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_346.dc.subtract.1_buffer_1_layernorm_346.dc.subtract.1_layernorm_346.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_346.dc.subtract.1_layernorm_346.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_346.dc.subtract.1_layernorm_346.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_346.dc.multiply.8: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_346.dc.subtract.1_layernorm_346.dc.multiply.8, layernorm_346.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}

  fwd_10:
    target_device: 0
    input_count: 2
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.4.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_346.dc.multiply.9: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_layernorm_346.dc.multiply.8_0, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_346.dc.add.10: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_346.dc.multiply.9, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_349: {type: matmul, grid_loc: [0, 4], grid_size: [6, 4], inputs: [layernorm_346.dc.add.10, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_352: {type: gelu, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_349], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_355: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_352, layer.4.output.dense.weight, layer.4.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_346.dc.add.10_add_359: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_346.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_359: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_355, buffer_0_layernorm_346.dc.add.10_add_359], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_360.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_359, lc.input_tensor.layernorm_360.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_360.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_359, layernorm_360.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_360.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_360.dc.subtract.1, layernorm_360.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_360.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_360.dc.multiply.2, lc.input_tensor.layernorm_360.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_360.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_360.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_360.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_360.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_360.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_360.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_360.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_0_layernorm_360.dc.subtract.1_buffer_1_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_360.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_360.dc.subtract.1_buffer_1_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_11:
    target_device: 0
    input_count: 2
    layernorm_360.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_360.dc.reciprocal.7_0, lc.input_tensor.layernorm_360.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_360.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8_0, layernorm_360.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_360.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_360.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_360.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_360.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_363: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_360.dc.add.10, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_369: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_360.dc.add.10, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_375: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_363, matmul_369], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_377: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_375, input_1_multiply_377_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_378: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_377, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_379.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_378], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_379.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_379.dc.exp.0, lc.input_tensor.softmax_379.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_379.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_379.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_379.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_379.dc.exp.0, softmax_379.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_383: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_360.dc.add.10, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_390: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [softmax_379.dc.multiply.3, matmul_383], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_394: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_390, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_360.dc.add.10_add_398: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_360.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_398: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_394, buffer_0_layernorm_360.dc.add.10_add_398], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_399.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_398, lc.input_tensor.layernorm_399.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_399.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_398, layernorm_399.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_399.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_399.dc.subtract.1, layernorm_399.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_399.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_399.dc.multiply.2, lc.input_tensor.layernorm_399.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    buffer_0_layernorm_399.dc.subtract.1_buffer_1_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_399.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_399.dc.subtract.1_buffer_1_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_12:
    target_device: 0
    input_count: 2
    layernorm_399.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_399.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_399.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_399.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_399.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_399.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_399.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_399.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_399.dc.reciprocal.7, lc.input_tensor.layernorm_399.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_399.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8_0, layernorm_399.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.5.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_399.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_399.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_399.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_399.dc.multiply.9, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_402: {type: matmul, grid_loc: [2, 0], grid_size: [6, 4], inputs: [layernorm_399.dc.add.10, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_405: {type: gelu, grid_loc: [3, 4], grid_size: [2, 4], inputs: [matmul_402], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_408: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_405, layer.5.output.dense.weight, layer.5.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_399.dc.add.10_add_412: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_399.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_412: {type: add, grid_loc: [5, 4], grid_size: [2, 1], inputs: [matmul_408, buffer_0_layernorm_399.dc.add.10_add_412], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_413.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [add_412, lc.input_tensor.layernorm_413.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_413.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [2, 1], inputs: [add_412, layernorm_413.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_0_layernorm_413.dc.subtract.1_buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [layernorm_413.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_13:
    target_device: 0
    input_count: 2
    layernorm_413.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_413.dc.subtract.1_0, e2e_layernorm_413.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_413.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_413.dc.multiply.2, lc.input_tensor.layernorm_413.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_413.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_413.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_413.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_413.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_413.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_413.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_413.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_413.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_413.dc.reciprocal.7, lc.input_tensor.layernorm_413.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_413.dc.subtract.1_buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_413.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8, layernorm_413.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_413.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_413.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_413.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_413.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_416: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_413.dc.add.10, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_422: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_413.dc.add.10, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_428: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_416, matmul_422], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_430: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_428, input_1_multiply_430_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_431: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [multiply_430, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_432.dc.exp.0: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [add_431], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_432.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_432.dc.exp.0, lc.input_tensor.softmax_432.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_432.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [softmax_432.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_432.dc.multiply.3: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_432.dc.exp.0, softmax_432.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_436: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_413.dc.add.10, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_443: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [softmax_432.dc.multiply.3, matmul_436], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_447: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [matmul_443, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_413.dc.add.10_add_451: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_413.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_451: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [matmul_447, buffer_0_layernorm_413.dc.add.10_add_451], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_14:
    target_device: 0
    input_count: 2
    layernorm_452.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_451_0, lc.input_tensor.layernorm_452.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_452.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_451_0, layernorm_452.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_452.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_452.dc.subtract.1, layernorm_452.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_452.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_452.dc.multiply.2, lc.input_tensor.layernorm_452.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_452.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_452.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_452.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_452.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_452.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_452.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_452.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_452.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_452.dc.reciprocal.7, lc.input_tensor.layernorm_452.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_452.dc.subtract.1_buffer_1_layernorm_452.dc.subtract.1_layernorm_452.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_452.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_452.dc.subtract.1_layernorm_452.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_452.dc.subtract.1_buffer_1_layernorm_452.dc.subtract.1_layernorm_452.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_452.dc.subtract.1_layernorm_452.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_452.dc.subtract.1_layernorm_452.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_452.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_452.dc.subtract.1_layernorm_452.dc.multiply.8, layernorm_452.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.6.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_452.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_452.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_452.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_452.dc.multiply.9, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_455: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [layernorm_452.dc.add.10, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_458: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_455], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_15:
    target_device: 0
    input_count: 2
    matmul_461: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_458_0, layer.6.output.dense.weight, layer.6.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_452.dc.add.10_add_465: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_452.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_465: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_461, buffer_0_layernorm_452.dc.add.10_add_465], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_466.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_465, lc.input_tensor.layernorm_466.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_466.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_465, layernorm_466.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_466.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_466.dc.subtract.1, layernorm_466.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_466.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_466.dc.multiply.2, lc.input_tensor.layernorm_466.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_466.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_466.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_466.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_466.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_466.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_466.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_466.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_466.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_466.dc.reciprocal.7, lc.input_tensor.layernorm_466.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_466.dc.subtract.1_buffer_1_layernorm_466.dc.subtract.1_layernorm_466.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_466.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_466.dc.subtract.1_layernorm_466.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_466.dc.subtract.1_buffer_1_layernorm_466.dc.subtract.1_layernorm_466.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_466.dc.subtract.1_layernorm_466.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_466.dc.subtract.1_layernorm_466.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_466.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_466.dc.subtract.1_layernorm_466.dc.multiply.8, layernorm_466.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_466.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_466.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_466.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_466.dc.multiply.9, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_469: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_466.dc.add.10, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_475: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_466.dc.add.10, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_481: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_469, matmul_475], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_483: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_481, input_1_multiply_483_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_484: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_483, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_485.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_484], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_485.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_485.dc.exp.0, lc.input_tensor.softmax_485.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_485.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_485.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_16:
    target_device: 0
    input_count: 2
    softmax_485.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_485.dc.exp.0_0, e2e_softmax_485.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_489: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_466.dc.add.10_0, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias], t: 1, mblock: [3, 2],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_496: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [softmax_485.dc.multiply.3, matmul_489], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_500: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_496, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_466.dc.add.10_add_504: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_466.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_504: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_500, buffer_0_layernorm_466.dc.add.10_add_504], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_505.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_504, lc.input_tensor.layernorm_505.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_505.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_504, layernorm_505.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_505.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_505.dc.subtract.1, layernorm_505.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_505.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_505.dc.multiply.2, lc.input_tensor.layernorm_505.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_505.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_505.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_505.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_505.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_505.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_505.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_505.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_505.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_505.dc.reciprocal.7, lc.input_tensor.layernorm_505.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_505.dc.subtract.1_buffer_1_layernorm_505.dc.subtract.1_layernorm_505.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_505.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_505.dc.subtract.1_layernorm_505.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_505.dc.subtract.1_buffer_1_layernorm_505.dc.subtract.1_layernorm_505.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_505.dc.subtract.1_layernorm_505.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_505.dc.subtract.1_layernorm_505.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_505.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_505.dc.subtract.1_layernorm_505.dc.multiply.8, layernorm_505.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.7.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_505.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_505.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_505.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_505.dc.multiply.9, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_17:
    target_device: 0
    input_count: 2
    matmul_508: {type: matmul, grid_loc: [0, 0], grid_size: [6, 4], inputs: [e2e_layernorm_505.dc.add.10_0, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_511: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [matmul_508], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_514: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_511, layer.7.output.dense.weight, layer.7.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_505.dc.add.10_add_518: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_505.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_518: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_514, buffer_0_layernorm_505.dc.add.10_add_518], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_519.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [add_518, lc.input_tensor.layernorm_519.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_519.dc.subtract.1: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_518, layernorm_519.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_519.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_519.dc.subtract.1, layernorm_519.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_519.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_519.dc.multiply.2, lc.input_tensor.layernorm_519.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_519.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_519.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_519.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_519.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_519.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_519.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_519.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_519.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_519.dc.reciprocal.7, lc.input_tensor.layernorm_519.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_519.dc.subtract.1_buffer_1_layernorm_519.dc.subtract.1_layernorm_519.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_519.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_519.dc.subtract.1_layernorm_519.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_519.dc.subtract.1_buffer_1_layernorm_519.dc.subtract.1_layernorm_519.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_519.dc.subtract.1_layernorm_519.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_519.dc.subtract.1_layernorm_519.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_519.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_519.dc.subtract.1_layernorm_519.dc.multiply.8, layernorm_519.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_519.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_519.dc.multiply.8, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_18:
    target_device: 0
    input_count: 2
    layernorm_519.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_519.dc.multiply.9_0, e2e_layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_522: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [layernorm_519.dc.add.10, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_528: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_519.dc.add.10, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_534: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_522, matmul_528], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_536: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_534, input_1_multiply_536_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_537: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_536, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_538.dc.exp.0: {type: exp, grid_loc: [2, 4], grid_size: [2, 2], inputs: [add_537], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_538.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_538.dc.exp.0, lc.input_tensor.softmax_538.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_538.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_538.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_538.dc.multiply.3: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_538.dc.exp.0, softmax_538.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_542: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [layernorm_519.dc.add.10, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_549: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_538.dc.multiply.3, matmul_542], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_553: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_549, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_519.dc.add.10_add_557: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_519.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_557: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_553, buffer_0_layernorm_519.dc.add.10_add_557], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_558.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_557, lc.input_tensor.layernorm_558.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_558.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [2, 1], inputs: [add_557, layernorm_558.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_558.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_558.dc.subtract.1, layernorm_558.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_558.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_558.dc.multiply.2, lc.input_tensor.layernorm_558.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_558.dc.add.5: {type: add, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_558.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_558.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_558.dc.sqrt.6: {type: sqrt, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_558.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_558.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_558.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_558.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_558.dc.reciprocal.7, lc.input_tensor.layernorm_558.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_558.dc.subtract.1_buffer_1_layernorm_558.dc.subtract.1_layernorm_558.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_558.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_558.dc.subtract.1_layernorm_558.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_558.dc.subtract.1_buffer_1_layernorm_558.dc.subtract.1_layernorm_558.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_558.dc.subtract.1_layernorm_558.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_558.dc.subtract.1_layernorm_558.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_558.dc.multiply.8: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_558.dc.subtract.1_layernorm_558.dc.multiply.8, layernorm_558.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}

  fwd_19:
    target_device: 0
    input_count: 2
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.8.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_558.dc.multiply.9: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_layernorm_558.dc.multiply.8_0, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_558.dc.add.10: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_558.dc.multiply.9, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_561: {type: matmul, grid_loc: [0, 4], grid_size: [6, 4], inputs: [layernorm_558.dc.add.10, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_564: {type: gelu, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_561], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_567: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_564, layer.8.output.dense.weight, layer.8.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_558.dc.add.10_add_571: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_558.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_571: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_567, buffer_0_layernorm_558.dc.add.10_add_571], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_572.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_571, lc.input_tensor.layernorm_572.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_572.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_571, layernorm_572.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_572.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_572.dc.subtract.1, layernorm_572.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_572.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_572.dc.multiply.2, lc.input_tensor.layernorm_572.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_572.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_572.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_572.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_572.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_572.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_572.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_572.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_0_layernorm_572.dc.subtract.1_buffer_1_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_572.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_572.dc.subtract.1_buffer_1_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_20:
    target_device: 0
    input_count: 2
    layernorm_572.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_572.dc.reciprocal.7_0, lc.input_tensor.layernorm_572.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_572.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8_0, layernorm_572.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_572.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_572.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_572.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_572.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_575: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_572.dc.add.10, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_581: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_572.dc.add.10, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_587: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_575, matmul_581], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_589: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_587, input_1_multiply_589_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_590: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_589, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_591.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_590], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_591.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_591.dc.exp.0, lc.input_tensor.softmax_591.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_591.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_591.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_591.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_591.dc.exp.0, softmax_591.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_595: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_572.dc.add.10, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_602: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [softmax_591.dc.multiply.3, matmul_595], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_606: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_602, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], input_0_tms: [
        hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_572.dc.add.10_add_610: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_572.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_610: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_606, buffer_0_layernorm_572.dc.add.10_add_610], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_611.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_610, lc.input_tensor.layernorm_611.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_611.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_610, layernorm_611.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_611.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_611.dc.subtract.1, layernorm_611.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_611.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_611.dc.multiply.2, lc.input_tensor.layernorm_611.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    buffer_0_layernorm_611.dc.subtract.1_buffer_1_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_611.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_611.dc.subtract.1_buffer_1_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_21:
    target_device: 0
    input_count: 2
    layernorm_611.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_611.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_611.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_611.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_611.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_611.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_611.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_611.dc.reciprocal.7, lc.input_tensor.layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_611.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8_0, layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.9.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_611.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_611.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_611.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_611.dc.multiply.9, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_614: {type: matmul, grid_loc: [2, 0], grid_size: [6, 4], inputs: [layernorm_611.dc.add.10, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_617: {type: gelu, grid_loc: [3, 4], grid_size: [2, 4], inputs: [matmul_614], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_620: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_617, layer.9.output.dense.weight, layer.9.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_611.dc.add.10_add_624: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_611.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_624: {type: add, grid_loc: [5, 4], grid_size: [2, 1], inputs: [matmul_620, buffer_0_layernorm_611.dc.add.10_add_624], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_625.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [add_624, lc.input_tensor.layernorm_625.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_625.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [2, 1], inputs: [add_624, layernorm_625.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_0_layernorm_625.dc.subtract.1_buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [layernorm_625.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_22:
    target_device: 0
    input_count: 2
    layernorm_625.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_625.dc.subtract.1_0, e2e_layernorm_625.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_625.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_625.dc.multiply.2, lc.input_tensor.layernorm_625.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_625.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_625.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_625.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_625.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_625.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_625.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_625.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_625.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_625.dc.reciprocal.7, lc.input_tensor.layernorm_625.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_625.dc.subtract.1_buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_625.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8, layernorm_625.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_625.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_625.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_625.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_625.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_628: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_625.dc.add.10, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_634: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_625.dc.add.10, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_640: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_628, matmul_634], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_642: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_640, input_1_multiply_642_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_643: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [multiply_642, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_644.dc.exp.0: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [add_643], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_644.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_644.dc.exp.0, lc.input_tensor.softmax_644.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_644.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [softmax_644.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_644.dc.multiply.3: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_644.dc.exp.0, softmax_644.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_648: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_625.dc.add.10, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_655: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [softmax_644.dc.multiply.3, matmul_648], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_659: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [matmul_655, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_625.dc.add.10_add_663: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_625.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_663: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [matmul_659, buffer_0_layernorm_625.dc.add.10_add_663], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_23:
    target_device: 0
    input_count: 2
    layernorm_664.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_663_0, lc.input_tensor.layernorm_664.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_664.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_663_0, layernorm_664.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_664.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_664.dc.subtract.1, layernorm_664.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_664.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_664.dc.multiply.2, lc.input_tensor.layernorm_664.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_664.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_664.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_664.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_664.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_664.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_664.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_664.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_664.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_664.dc.reciprocal.7, lc.input_tensor.layernorm_664.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_664.dc.subtract.1_buffer_1_layernorm_664.dc.subtract.1_layernorm_664.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_664.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_664.dc.subtract.1_layernorm_664.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_664.dc.subtract.1_buffer_1_layernorm_664.dc.subtract.1_layernorm_664.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_664.dc.subtract.1_layernorm_664.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_664.dc.subtract.1_layernorm_664.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_664.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_664.dc.subtract.1_layernorm_664.dc.multiply.8, layernorm_664.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.10.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_664.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_664.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_664.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_664.dc.multiply.9, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_667: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [layernorm_664.dc.add.10, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_670: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_667], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_24:
    target_device: 0
    input_count: 2
    matmul_673: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_670_0, layer.10.output.dense.weight, layer.10.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_664.dc.add.10_add_677: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_664.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_677: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_673, buffer_0_layernorm_664.dc.add.10_add_677], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_678.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_677, lc.input_tensor.layernorm_678.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_678.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_677, layernorm_678.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_678.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_678.dc.subtract.1, layernorm_678.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_678.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_678.dc.multiply.2, lc.input_tensor.layernorm_678.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_678.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_678.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_678.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_678.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_678.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_678.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_678.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_678.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_678.dc.reciprocal.7, lc.input_tensor.layernorm_678.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_678.dc.subtract.1_buffer_1_layernorm_678.dc.subtract.1_layernorm_678.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_678.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_678.dc.subtract.1_layernorm_678.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_678.dc.subtract.1_buffer_1_layernorm_678.dc.subtract.1_layernorm_678.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_678.dc.subtract.1_layernorm_678.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_678.dc.subtract.1_layernorm_678.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_678.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_678.dc.subtract.1_layernorm_678.dc.multiply.8, layernorm_678.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_678.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_678.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_678.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_678.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_681: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_678.dc.add.10, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_687: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_678.dc.add.10, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_693: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_681, matmul_687], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_695: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_693, input_1_multiply_695_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_696: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_695, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_697.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_696], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_697.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_697.dc.exp.0, lc.input_tensor.softmax_697.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_697.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_697.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_25:
    target_device: 0
    input_count: 2
    softmax_697.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_697.dc.exp.0_0, e2e_softmax_697.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_701: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_678.dc.add.10_0, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias], t: 1, mblock: [3, 2],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_708: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [softmax_697.dc.multiply.3, matmul_701], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_712: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_708, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_678.dc.add.10_add_716: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_678.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_716: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_712, buffer_0_layernorm_678.dc.add.10_add_716], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_717.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_716, lc.input_tensor.layernorm_717.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_717.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_716, layernorm_717.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_717.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_717.dc.subtract.1, layernorm_717.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_717.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_717.dc.multiply.2, lc.input_tensor.layernorm_717.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_717.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_717.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_717.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_717.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_717.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_717.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_717.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_717.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_717.dc.reciprocal.7, lc.input_tensor.layernorm_717.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_717.dc.subtract.1_buffer_1_layernorm_717.dc.subtract.1_layernorm_717.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_717.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_717.dc.subtract.1_layernorm_717.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_717.dc.subtract.1_buffer_1_layernorm_717.dc.subtract.1_layernorm_717.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_717.dc.subtract.1_layernorm_717.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_717.dc.subtract.1_layernorm_717.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_717.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_717.dc.subtract.1_layernorm_717.dc.multiply.8, layernorm_717.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.11.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_717.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_717.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_717.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_717.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_26:
    target_device: 0
    input_count: 2
    matmul_720: {type: matmul, grid_loc: [0, 0], grid_size: [6, 4], inputs: [e2e_layernorm_717.dc.add.10_0, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_723: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [matmul_720], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_726: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_723, layer.11.output.dense.weight, layer.11.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_717.dc.add.10_add_730: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_717.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_730: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_726, buffer_0_layernorm_717.dc.add.10_add_730], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_731.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [add_730, lc.input_tensor.layernorm_731.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_731.dc.subtract.1: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_730, layernorm_731.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_731.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_731.dc.subtract.1, layernorm_731.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_731.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_731.dc.multiply.2, lc.input_tensor.layernorm_731.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_731.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_731.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_731.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_731.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_731.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_731.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_731.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_731.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_731.dc.reciprocal.7, lc.input_tensor.layernorm_731.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_731.dc.subtract.1_buffer_1_layernorm_731.dc.subtract.1_layernorm_731.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_731.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_731.dc.subtract.1_layernorm_731.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_731.dc.subtract.1_buffer_1_layernorm_731.dc.subtract.1_layernorm_731.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_731.dc.subtract.1_layernorm_731.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_731.dc.subtract.1_layernorm_731.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_731.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_731.dc.subtract.1_layernorm_731.dc.multiply.8, layernorm_731.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_731.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_731.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_27:
    target_device: 0
    input_count: 2
    layernorm_731.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_731.dc.multiply.9_0, e2e_layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_734: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [layernorm_731.dc.add.10, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_740: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_731.dc.add.10, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_746: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_734, matmul_740], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_748: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_746, input_1_multiply_748_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_749: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_748, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_750.dc.exp.0: {type: exp, grid_loc: [2, 4], grid_size: [2, 2], inputs: [add_749], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_750.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_750.dc.exp.0, lc.input_tensor.softmax_750.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_750.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_750.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_750.dc.multiply.3: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_750.dc.exp.0, softmax_750.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_754: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [layernorm_731.dc.add.10, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_761: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_750.dc.multiply.3, matmul_754], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_765: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_761, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_731.dc.add.10_add_769: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_731.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_769: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_765, buffer_0_layernorm_731.dc.add.10_add_769], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_770.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_769, lc.input_tensor.layernorm_770.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_770.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [2, 1], inputs: [add_769, layernorm_770.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_770.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_770.dc.subtract.1, layernorm_770.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_770.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_770.dc.multiply.2, lc.input_tensor.layernorm_770.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_770.dc.add.5: {type: add, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_770.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_770.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_770.dc.sqrt.6: {type: sqrt, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_770.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_770.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_770.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_770.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_770.dc.reciprocal.7, lc.input_tensor.layernorm_770.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_770.dc.subtract.1_buffer_1_layernorm_770.dc.subtract.1_layernorm_770.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_770.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_770.dc.subtract.1_layernorm_770.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_770.dc.subtract.1_buffer_1_layernorm_770.dc.subtract.1_layernorm_770.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_770.dc.subtract.1_layernorm_770.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_770.dc.subtract.1_layernorm_770.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_770.dc.multiply.8: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_770.dc.subtract.1_layernorm_770.dc.multiply.8, layernorm_770.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}

  fwd_28:
    target_device: 0
    input_count: 2
    layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.12.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_770.dc.multiply.9: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_layernorm_770.dc.multiply.8_0, layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_770.dc.add.10: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_770.dc.multiply.9, layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_773: {type: matmul, grid_loc: [0, 4], grid_size: [6, 4], inputs: [layernorm_770.dc.add.10, layer.12.intermediate.dense.weight, layer.12.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_776: {type: gelu, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_773], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_779: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_776, layer.12.output.dense.weight, layer.12.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_770.dc.add.10_add_783: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_770.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_783: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_779, buffer_0_layernorm_770.dc.add.10_add_783], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_784.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_783, lc.input_tensor.layernorm_784.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_784.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_783, layernorm_784.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_784.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_784.dc.subtract.1, layernorm_784.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_784.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_784.dc.multiply.2, lc.input_tensor.layernorm_784.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_784.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_784.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_784.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_784.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_784.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_784.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_784.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_0_layernorm_784.dc.subtract.1_buffer_1_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_784.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_784.dc.subtract.1_buffer_1_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_29:
    target_device: 0
    input_count: 2
    layernorm_784.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_784.dc.reciprocal.7_0, lc.input_tensor.layernorm_784.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_784.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8_0, layernorm_784.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_784.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_784.dc.multiply.8, layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_784.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_784.dc.multiply.9, layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_787: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_784.dc.add.10, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_793: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_784.dc.add.10, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_799: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_787, matmul_793], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_801: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_799, input_1_multiply_801_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_802: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_801, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_803.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_802], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_803.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_803.dc.exp.0, lc.input_tensor.softmax_803.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_803.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_803.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_803.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_803.dc.exp.0, softmax_803.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_807: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_784.dc.add.10, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_814: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [softmax_803.dc.multiply.3, matmul_807], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_818: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_814, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_784.dc.add.10_add_822: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_784.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_822: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_818, buffer_0_layernorm_784.dc.add.10_add_822], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_823.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_822, lc.input_tensor.layernorm_823.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_823.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_822, layernorm_823.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_823.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_823.dc.subtract.1, layernorm_823.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_823.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_823.dc.multiply.2, lc.input_tensor.layernorm_823.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    buffer_0_layernorm_823.dc.subtract.1_buffer_1_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_823.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_823.dc.subtract.1_buffer_1_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_30:
    target_device: 0
    input_count: 2
    layernorm_823.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_823.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_823.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_823.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_823.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_823.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_823.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_823.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_823.dc.reciprocal.7, lc.input_tensor.layernorm_823.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_823.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8_0, layernorm_823.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.13.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_823.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_823.dc.multiply.8, layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_823.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_823.dc.multiply.9, layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_826: {type: matmul, grid_loc: [2, 0], grid_size: [6, 4], inputs: [layernorm_823.dc.add.10, layer.13.intermediate.dense.weight, layer.13.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_829: {type: gelu, grid_loc: [3, 4], grid_size: [2, 4], inputs: [matmul_826], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_832: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_829, layer.13.output.dense.weight, layer.13.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_823.dc.add.10_add_836: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_823.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_836: {type: add, grid_loc: [5, 4], grid_size: [2, 1], inputs: [matmul_832, buffer_0_layernorm_823.dc.add.10_add_836], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_837.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [add_836, lc.input_tensor.layernorm_837.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_837.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [2, 1], inputs: [add_836, layernorm_837.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_0_layernorm_837.dc.subtract.1_buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [layernorm_837.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_31:
    target_device: 0
    input_count: 2
    layernorm_837.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_837.dc.subtract.1_0, e2e_layernorm_837.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_837.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_837.dc.multiply.2, lc.input_tensor.layernorm_837.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_837.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_837.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_837.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_837.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_837.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_837.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_837.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_837.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_837.dc.reciprocal.7, lc.input_tensor.layernorm_837.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_837.dc.subtract.1_buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_837.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8, layernorm_837.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_837.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_837.dc.multiply.8, layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_837.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_837.dc.multiply.9, layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_840: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_837.dc.add.10, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_846: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_837.dc.add.10, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_852: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_840, matmul_846], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_854: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_852, input_1_multiply_854_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_855: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [multiply_854, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_856.dc.exp.0: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [add_855], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_856.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_856.dc.exp.0, lc.input_tensor.softmax_856.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_856.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [softmax_856.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_856.dc.multiply.3: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_856.dc.exp.0, softmax_856.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_860: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_837.dc.add.10, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_867: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [softmax_856.dc.multiply.3, matmul_860], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_871: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [matmul_867, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_837.dc.add.10_add_875: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_837.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_875: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [matmul_871, buffer_0_layernorm_837.dc.add.10_add_875], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_32:
    target_device: 0
    input_count: 2
    layernorm_876.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_875_0, lc.input_tensor.layernorm_876.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_876.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_875_0, layernorm_876.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_876.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_876.dc.subtract.1, layernorm_876.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_876.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_876.dc.multiply.2, lc.input_tensor.layernorm_876.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_876.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_876.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_876.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_876.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_876.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_876.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_876.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_876.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_876.dc.reciprocal.7, lc.input_tensor.layernorm_876.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_876.dc.subtract.1_buffer_1_layernorm_876.dc.subtract.1_layernorm_876.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_876.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_876.dc.subtract.1_layernorm_876.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_876.dc.subtract.1_buffer_1_layernorm_876.dc.subtract.1_layernorm_876.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_876.dc.subtract.1_layernorm_876.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_876.dc.subtract.1_layernorm_876.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_876.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_876.dc.subtract.1_layernorm_876.dc.multiply.8, layernorm_876.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.14.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_876.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_876.dc.multiply.8, layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_876.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_876.dc.multiply.9, layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_879: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [layernorm_876.dc.add.10, layer.14.intermediate.dense.weight, layer.14.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_882: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_879], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_33:
    target_device: 0
    input_count: 2
    matmul_885: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_882_0, layer.14.output.dense.weight, layer.14.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_876.dc.add.10_add_889: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_876.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_889: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_885, buffer_0_layernorm_876.dc.add.10_add_889], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_890.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_889, lc.input_tensor.layernorm_890.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_890.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_889, layernorm_890.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_890.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_890.dc.subtract.1, layernorm_890.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_890.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_890.dc.multiply.2, lc.input_tensor.layernorm_890.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_890.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_890.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_890.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_890.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_890.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_890.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_890.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_890.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_890.dc.reciprocal.7, lc.input_tensor.layernorm_890.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_890.dc.subtract.1_buffer_1_layernorm_890.dc.subtract.1_layernorm_890.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_890.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_890.dc.subtract.1_layernorm_890.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_890.dc.subtract.1_buffer_1_layernorm_890.dc.subtract.1_layernorm_890.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_890.dc.subtract.1_layernorm_890.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_890.dc.subtract.1_layernorm_890.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_890.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_890.dc.subtract.1_layernorm_890.dc.multiply.8, layernorm_890.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_890.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_890.dc.multiply.8, layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_890.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_890.dc.multiply.9, layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_893: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_890.dc.add.10, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_899: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_890.dc.add.10, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_905: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_893, matmul_899], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_907: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_905, input_1_multiply_907_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_908: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_907, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_909.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_908], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_909.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_909.dc.exp.0, lc.input_tensor.softmax_909.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_909.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_909.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_34:
    target_device: 0
    input_count: 2
    softmax_909.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_909.dc.exp.0_0, e2e_softmax_909.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_913: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_890.dc.add.10_0, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias], t: 1, mblock: [3, 2],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_920: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [softmax_909.dc.multiply.3, matmul_913], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_924: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_920, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_890.dc.add.10_add_928: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_890.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_928: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_924, buffer_0_layernorm_890.dc.add.10_add_928], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_929.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_928, lc.input_tensor.layernorm_929.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_929.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_928, layernorm_929.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_929.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_929.dc.subtract.1, layernorm_929.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_929.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_929.dc.multiply.2, lc.input_tensor.layernorm_929.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_929.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_929.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_929.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_929.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_929.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_929.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_929.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_929.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_929.dc.reciprocal.7, lc.input_tensor.layernorm_929.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_929.dc.subtract.1_buffer_1_layernorm_929.dc.subtract.1_layernorm_929.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_929.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_929.dc.subtract.1_layernorm_929.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_929.dc.subtract.1_buffer_1_layernorm_929.dc.subtract.1_layernorm_929.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_929.dc.subtract.1_layernorm_929.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_929.dc.subtract.1_layernorm_929.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_929.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_929.dc.subtract.1_layernorm_929.dc.multiply.8, layernorm_929.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.15.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_929.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_929.dc.multiply.8, layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_929.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_929.dc.multiply.9, layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_35:
    target_device: 0
    input_count: 2
    matmul_932: {type: matmul, grid_loc: [0, 0], grid_size: [6, 4], inputs: [e2e_layernorm_929.dc.add.10_0, layer.15.intermediate.dense.weight, layer.15.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_935: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [matmul_932], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_938: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_935, layer.15.output.dense.weight, layer.15.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_929.dc.add.10_add_942: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_929.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_942: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_938, buffer_0_layernorm_929.dc.add.10_add_942], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_943.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [add_942, lc.input_tensor.layernorm_943.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_943.dc.subtract.1: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_942, layernorm_943.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_943.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_943.dc.subtract.1, layernorm_943.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_943.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_943.dc.multiply.2, lc.input_tensor.layernorm_943.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_943.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_943.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_943.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_943.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_943.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_943.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_943.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_943.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_943.dc.reciprocal.7, lc.input_tensor.layernorm_943.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_943.dc.subtract.1_buffer_1_layernorm_943.dc.subtract.1_layernorm_943.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_943.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_943.dc.subtract.1_layernorm_943.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_943.dc.subtract.1_buffer_1_layernorm_943.dc.subtract.1_layernorm_943.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_943.dc.subtract.1_layernorm_943.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_943.dc.subtract.1_layernorm_943.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_943.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_943.dc.subtract.1_layernorm_943.dc.multiply.8, layernorm_943.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_943.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_943.dc.multiply.8, layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_36:
    target_device: 0
    input_count: 2
    layernorm_943.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_943.dc.multiply.9_0, e2e_layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_946: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [layernorm_943.dc.add.10, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_952: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_943.dc.add.10, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_958: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_946, matmul_952], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_960: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_958, input_1_multiply_960_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_961: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_960, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_962.dc.exp.0: {type: exp, grid_loc: [2, 4], grid_size: [2, 2], inputs: [add_961], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_962.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_962.dc.exp.0, lc.input_tensor.softmax_962.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_962.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_962.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_962.dc.multiply.3: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_962.dc.exp.0, softmax_962.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_966: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [layernorm_943.dc.add.10, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_973: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_962.dc.multiply.3, matmul_966], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_977: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_973, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_943.dc.add.10_add_981: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_943.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_981: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_977, buffer_0_layernorm_943.dc.add.10_add_981], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_982.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_981, lc.input_tensor.layernorm_982.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_982.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [2, 1], inputs: [add_981, layernorm_982.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_982.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_982.dc.subtract.1, layernorm_982.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_982.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_982.dc.multiply.2, lc.input_tensor.layernorm_982.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_982.dc.add.5: {type: add, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_982.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_982.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_982.dc.sqrt.6: {type: sqrt, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_982.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_982.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_982.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_982.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_982.dc.reciprocal.7, lc.input_tensor.layernorm_982.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_982.dc.subtract.1_buffer_1_layernorm_982.dc.subtract.1_layernorm_982.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_982.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_982.dc.subtract.1_layernorm_982.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_982.dc.subtract.1_buffer_1_layernorm_982.dc.subtract.1_layernorm_982.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_982.dc.subtract.1_layernorm_982.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_982.dc.subtract.1_layernorm_982.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_982.dc.multiply.8: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_982.dc.subtract.1_layernorm_982.dc.multiply.8, layernorm_982.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}

  fwd_37:
    target_device: 0
    input_count: 2
    layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.16.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_982.dc.multiply.9: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_layernorm_982.dc.multiply.8_0, layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_982.dc.add.10: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_982.dc.multiply.9, layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_985: {type: matmul, grid_loc: [0, 4], grid_size: [6, 4], inputs: [layernorm_982.dc.add.10, layer.16.intermediate.dense.weight, layer.16.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_988: {type: gelu, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_985], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_991: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_988, layer.16.output.dense.weight, layer.16.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_982.dc.add.10_add_995: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_982.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_995: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_991, buffer_0_layernorm_982.dc.add.10_add_995], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_996.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_995, lc.input_tensor.layernorm_996.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_996.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_995, layernorm_996.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_996.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_996.dc.subtract.1, layernorm_996.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_996.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_996.dc.multiply.2, lc.input_tensor.layernorm_996.dc.reduce_avg.3.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_996.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_996.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_996.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_996.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_996.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_996.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_996.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_0_layernorm_996.dc.subtract.1_buffer_1_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_996.dc.subtract.1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_996.dc.subtract.1_buffer_1_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_38:
    target_device: 0
    input_count: 2
    layernorm_996.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_996.dc.reciprocal.7_0, lc.input_tensor.layernorm_996.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_996.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8_0, layernorm_996.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_996.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_996.dc.multiply.8, layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_996.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_996.dc.multiply.9, layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_999: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_996.dc.add.10, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1005: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_996.dc.add.10, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1011: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_999, matmul_1005], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_1013: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_1011, input_1_multiply_1013_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1014: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_1013, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_1015.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_1014], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1015.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_1015.dc.exp.0, lc.input_tensor.softmax_1015.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_1015.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_1015.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1015.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_1015.dc.exp.0, softmax_1015.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_1019: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_996.dc.add.10, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1026: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [softmax_1015.dc.multiply.3, matmul_1019], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_1030: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_1026, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_996.dc.add.10_add_1034: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_996.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1034: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_1030, buffer_0_layernorm_996.dc.add.10_add_1034], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1035.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_1034, lc.input_tensor.layernorm_1035.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1035.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_1034, layernorm_1035.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1035.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1035.dc.subtract.1, layernorm_1035.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1035.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_1035.dc.multiply.2, lc.input_tensor.layernorm_1035.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    buffer_0_layernorm_1035.dc.subtract.1_buffer_1_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1035.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_1035.dc.subtract.1_buffer_1_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_39:
    target_device: 0
    input_count: 2
    layernorm_1035.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1035.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_1035.4], t: 1, mblock: [3, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1035.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_1035.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1035.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1035.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1035.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_1035.dc.reciprocal.7, lc.input_tensor.layernorm_1035.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1035.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8_0, layernorm_1035.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.17.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_1035.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_1035.dc.multiply.8, layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1035.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_1035.dc.multiply.9, layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1038: {type: matmul, grid_loc: [2, 0], grid_size: [6, 4], inputs: [layernorm_1035.dc.add.10, layer.17.intermediate.dense.weight, layer.17.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1041: {type: gelu, grid_loc: [3, 4], grid_size: [2, 4], inputs: [matmul_1038], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_1044: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_1041, layer.17.output.dense.weight, layer.17.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_1035.dc.add.10_add_1048: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_1035.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1048: {type: add, grid_loc: [5, 4], grid_size: [2, 1], inputs: [matmul_1044, buffer_0_layernorm_1035.dc.add.10_add_1048], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1049.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [add_1048, lc.input_tensor.layernorm_1049.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1049.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [2, 1], inputs: [add_1048, layernorm_1049.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_0_layernorm_1049.dc.subtract.1_buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [layernorm_1049.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_40:
    target_device: 0
    input_count: 2
    layernorm_1049.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_1049.dc.subtract.1_0, e2e_layernorm_1049.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1049.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_1049.dc.multiply.2, lc.input_tensor.layernorm_1049.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1049.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_1049.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1049.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1049.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_1049.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1049.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_1049.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1049.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_1049.dc.reciprocal.7, lc.input_tensor.layernorm_1049.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_1049.dc.subtract.1_buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1049.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8, layernorm_1049.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1049.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_1049.dc.multiply.8, layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1049.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_1049.dc.multiply.9, layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1052: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_1049.dc.add.10, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1058: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_1049.dc.add.10, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1064: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_1052, matmul_1058], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_1066: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_1064, input_1_multiply_1066_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1067: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [multiply_1066, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_1068.dc.exp.0: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [add_1067], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1068.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_1068.dc.exp.0, lc.input_tensor.softmax_1068.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_1068.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [softmax_1068.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1068.dc.multiply.3: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_1068.dc.exp.0, softmax_1068.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_1072: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_1049.dc.add.10, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1079: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [softmax_1068.dc.multiply.3, matmul_1072], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_1083: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [matmul_1079, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_1049.dc.add.10_add_1087: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_1049.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1087: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [matmul_1083, buffer_0_layernorm_1049.dc.add.10_add_1087], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_41:
    target_device: 0
    input_count: 2
    layernorm_1088.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_1087_0, lc.input_tensor.layernorm_1088.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_1088.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_1087_0, layernorm_1088.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1088.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_1088.dc.subtract.1, layernorm_1088.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1088.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_1088.dc.multiply.2, lc.input_tensor.layernorm_1088.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1088.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_1088.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1088.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1088.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_1088.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1088.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_1088.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1088.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_1088.dc.reciprocal.7, lc.input_tensor.layernorm_1088.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1088.dc.subtract.1_buffer_1_layernorm_1088.dc.subtract.1_layernorm_1088.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1088.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1088.dc.subtract.1_layernorm_1088.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_1088.dc.subtract.1_buffer_1_layernorm_1088.dc.subtract.1_layernorm_1088.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1088.dc.subtract.1_layernorm_1088.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_1088.dc.subtract.1_layernorm_1088.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1088.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_1088.dc.subtract.1_layernorm_1088.dc.multiply.8, layernorm_1088.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.18.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_1088.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_1088.dc.multiply.8, layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1088.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_1088.dc.multiply.9, layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1091: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [layernorm_1088.dc.add.10, layer.18.intermediate.dense.weight, layer.18.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1094: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_1091], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_42:
    target_device: 0
    input_count: 2
    matmul_1097: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_1094_0, layer.18.output.dense.weight, layer.18.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_1088.dc.add.10_add_1101: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1088.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1101: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_1097, buffer_0_layernorm_1088.dc.add.10_add_1101], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1102.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_1101, lc.input_tensor.layernorm_1102.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1102.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_1101, layernorm_1102.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1102.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_1102.dc.subtract.1, layernorm_1102.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1102.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1102.dc.multiply.2, lc.input_tensor.layernorm_1102.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1102.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_1102.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1102.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1102.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_1102.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1102.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_1102.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1102.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_1102.dc.reciprocal.7, lc.input_tensor.layernorm_1102.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1102.dc.subtract.1_buffer_1_layernorm_1102.dc.subtract.1_layernorm_1102.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_1102.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1102.dc.subtract.1_layernorm_1102.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_1102.dc.subtract.1_buffer_1_layernorm_1102.dc.subtract.1_layernorm_1102.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1102.dc.subtract.1_layernorm_1102.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_1102.dc.subtract.1_layernorm_1102.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1102.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_1102.dc.subtract.1_layernorm_1102.dc.multiply.8, layernorm_1102.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1102.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_1102.dc.multiply.8, layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1102.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1102.dc.multiply.9, layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1105: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_1102.dc.add.10, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1111: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_1102.dc.add.10, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1117: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_1105, matmul_1111], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_1119: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_1117, input_1_multiply_1119_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1120: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_1119, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_1121.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_1120], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1121.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_1121.dc.exp.0, lc.input_tensor.softmax_1121.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_1121.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_1121.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_43:
    target_device: 0
    input_count: 2
    softmax_1121.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_1121.dc.exp.0_0, e2e_softmax_1121.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_1125: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_1102.dc.add.10_0, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias], t: 1, mblock: [3,
        2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [
        broadcast: {r: 12}], attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1132: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [softmax_1121.dc.multiply.3, matmul_1125], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_1136: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_1132, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_1102.dc.add.10_add_1140: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_1102.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1140: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_1136, buffer_0_layernorm_1102.dc.add.10_add_1140], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1141.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_1140, lc.input_tensor.layernorm_1141.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1141.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_1140, layernorm_1141.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1141.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_1141.dc.subtract.1, layernorm_1141.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1141.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_1141.dc.multiply.2, lc.input_tensor.layernorm_1141.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1141.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_1141.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1141.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1141.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_1141.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1141.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_1141.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1141.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_1141.dc.reciprocal.7, lc.input_tensor.layernorm_1141.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1141.dc.subtract.1_buffer_1_layernorm_1141.dc.subtract.1_layernorm_1141.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_1141.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1141.dc.subtract.1_layernorm_1141.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_1141.dc.subtract.1_buffer_1_layernorm_1141.dc.subtract.1_layernorm_1141.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1141.dc.subtract.1_layernorm_1141.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_1141.dc.subtract.1_layernorm_1141.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1141.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_1141.dc.subtract.1_layernorm_1141.dc.multiply.8, layernorm_1141.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.19.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_1141.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1141.dc.multiply.8, layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1141.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1141.dc.multiply.9, layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_44:
    target_device: 0
    input_count: 2
    matmul_1144: {type: matmul, grid_loc: [0, 0], grid_size: [6, 4], inputs: [e2e_layernorm_1141.dc.add.10_0, layer.19.intermediate.dense.weight, layer.19.intermediate.dense.bias], t: 1, mblock: [1, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1147: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [matmul_1144], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_1150: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_1147, layer.19.output.dense.weight, layer.19.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_1141.dc.add.10_add_1154: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [e2e_layernorm_1141.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1154: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_1150, buffer_0_layernorm_1141.dc.add.10_add_1154], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1155.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [add_1154, lc.input_tensor.layernorm_1155.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1155.dc.subtract.1: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_1154, layernorm_1155.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1155.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_1155.dc.subtract.1, layernorm_1155.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1155.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1155.dc.multiply.2, lc.input_tensor.layernorm_1155.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1155.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_1155.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1155.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1155.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1155.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1155.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1155.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1155.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1155.dc.reciprocal.7, lc.input_tensor.layernorm_1155.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1155.dc.subtract.1_buffer_1_layernorm_1155.dc.subtract.1_layernorm_1155.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_1155.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1155.dc.subtract.1_layernorm_1155.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_1155.dc.subtract.1_buffer_1_layernorm_1155.dc.subtract.1_layernorm_1155.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1155.dc.subtract.1_layernorm_1155.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_1155.dc.subtract.1_layernorm_1155.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1155.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_1155.dc.subtract.1_layernorm_1155.dc.multiply.8, layernorm_1155.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1155.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_1155.dc.multiply.8, layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_45:
    target_device: 0
    input_count: 2
    layernorm_1155.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1155.dc.multiply.9_0, e2e_layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1158: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [layernorm_1155.dc.add.10, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1164: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_1155.dc.add.10, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1170: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_1158, matmul_1164], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_1172: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_1170, input_1_multiply_1172_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1173: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_1172, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_1174.dc.exp.0: {type: exp, grid_loc: [2, 4], grid_size: [2, 2], inputs: [add_1173], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1174.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_1174.dc.exp.0, lc.input_tensor.softmax_1174.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_1174.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_1174.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1174.dc.multiply.3: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_1174.dc.exp.0, softmax_1174.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_1178: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [layernorm_1155.dc.add.10, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1185: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_1174.dc.multiply.3, matmul_1178], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_1189: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_1185, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_1155.dc.add.10_add_1193: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_1155.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1193: {type: add, grid_loc: [4, 7], grid_size: [2, 1], inputs: [matmul_1189, buffer_0_layernorm_1155.dc.add.10_add_1193], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1194.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_1193, lc.input_tensor.layernorm_1194.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1194.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [2, 1], inputs: [add_1193, layernorm_1194.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1194.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_1194.dc.subtract.1, layernorm_1194.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1194.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1194.dc.multiply.2, lc.input_tensor.layernorm_1194.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1194.dc.add.5: {type: add, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1194.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1194.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1194.dc.sqrt.6: {type: sqrt, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1194.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1194.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_1194.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1194.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1194.dc.reciprocal.7, lc.input_tensor.layernorm_1194.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1194.dc.subtract.1_buffer_1_layernorm_1194.dc.subtract.1_layernorm_1194.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_1194.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1194.dc.subtract.1_layernorm_1194.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_1194.dc.subtract.1_buffer_1_layernorm_1194.dc.subtract.1_layernorm_1194.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1194.dc.subtract.1_layernorm_1194.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_1_layernorm_1194.dc.subtract.1_layernorm_1194.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1194.dc.multiply.8: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_1194.dc.subtract.1_layernorm_1194.dc.multiply.8, layernorm_1194.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}

  fwd_46:
    target_device: 0
    input_count: 2
    layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.20.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_1194.dc.multiply.9: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_layernorm_1194.dc.multiply.8_0, layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1194.dc.add.10: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_1194.dc.multiply.9, layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1197: {type: matmul, grid_loc: [0, 4], grid_size: [6, 4], inputs: [layernorm_1194.dc.add.10, layer.20.intermediate.dense.weight, layer.20.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1200: {type: gelu, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_1197], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_1203: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_1200, layer.20.output.dense.weight, layer.20.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_1194.dc.add.10_add_1207: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1194.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1207: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_1203, buffer_0_layernorm_1194.dc.add.10_add_1207], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1208.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_1207, lc.input_tensor.layernorm_1208.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1208.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_1207, layernorm_1208.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1208.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1208.dc.subtract.1, layernorm_1208.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1208.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1208.dc.multiply.2, lc.input_tensor.layernorm_1208.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1208.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_1208.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1208.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1208.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1208.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1208.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_1208.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_0_layernorm_1208.dc.subtract.1_buffer_1_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1208.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_1208.dc.subtract.1_buffer_1_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_47:
    target_device: 0
    input_count: 2
    layernorm_1208.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1208.dc.reciprocal.7_0, lc.input_tensor.layernorm_1208.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1208.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8_0, layernorm_1208.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1208.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_1208.dc.multiply.8, layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1208.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_1208.dc.multiply.9, layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1211: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_1208.dc.add.10, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1217: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_1208.dc.add.10, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1223: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_1211, matmul_1217], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_1225: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_1223, input_1_multiply_1225_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1226: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_1225, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_1227.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_1226], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1227.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_1227.dc.exp.0, lc.input_tensor.softmax_1227.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_1227.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_1227.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1227.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_1227.dc.exp.0, softmax_1227.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_1231: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_1208.dc.add.10, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1238: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [softmax_1227.dc.multiply.3, matmul_1231], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_1242: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_1238, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_1208.dc.add.10_add_1246: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_1208.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1246: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_1242, buffer_0_layernorm_1208.dc.add.10_add_1246], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1247.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_1246, lc.input_tensor.layernorm_1247.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1247.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_1246, layernorm_1247.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1247.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_1247.dc.subtract.1, layernorm_1247.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1247.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_1247.dc.multiply.2, lc.input_tensor.layernorm_1247.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    buffer_0_layernorm_1247.dc.subtract.1_buffer_1_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1247.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_1247.dc.subtract.1_buffer_1_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_48:
    target_device: 0
    input_count: 2
    layernorm_1247.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1247.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_1247.4], t: 1, mblock: [3, 1], ublock: [2, 1],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1247.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_1247.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1247.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1247.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1247.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_1247.dc.reciprocal.7, lc.input_tensor.layernorm_1247.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1247.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8_0, layernorm_1247.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.21.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_1247.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_1247.dc.multiply.8, layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1247.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_1247.dc.multiply.9, layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1250: {type: matmul, grid_loc: [2, 0], grid_size: [6, 4], inputs: [layernorm_1247.dc.add.10, layer.21.intermediate.dense.weight, layer.21.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1253: {type: gelu, grid_loc: [3, 4], grid_size: [2, 4], inputs: [matmul_1250], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_1256: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_1253, layer.21.output.dense.weight, layer.21.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_1247.dc.add.10_add_1260: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_1247.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        120], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1260: {type: add, grid_loc: [5, 4], grid_size: [2, 1], inputs: [matmul_1256, buffer_0_layernorm_1247.dc.add.10_add_1260], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 88], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1261.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [2, 1], inputs: [add_1260, lc.input_tensor.layernorm_1261.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1261.dc.subtract.1: {type: subtract, grid_loc: [5, 6], grid_size: [2, 1], inputs: [add_1260, layernorm_1261.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_0_layernorm_1261.dc.subtract.1_buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [layernorm_1261.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_49:
    target_device: 0
    input_count: 2
    layernorm_1261.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_1261.dc.subtract.1_0, e2e_layernorm_1261.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1261.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_1261.dc.multiply.2, lc.input_tensor.layernorm_1261.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1261.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_1261.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1261.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1261.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_1261.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1261.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_1261.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1261.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_1261.dc.reciprocal.7, lc.input_tensor.layernorm_1261.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_1261.dc.subtract.1_buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1261.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8, layernorm_1261.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1261.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_1261.dc.multiply.8, layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1261.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_1261.dc.multiply.9, layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1264: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_1261.dc.add.10, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1270: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_1261.dc.add.10, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1276: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_1264, matmul_1270], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_1278: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_1276, input_1_multiply_1278_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1279: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [multiply_1278, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_1280.dc.exp.0: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [add_1279], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1280.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_1280.dc.exp.0, lc.input_tensor.softmax_1280.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_1280.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [softmax_1280.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1280.dc.multiply.3: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_1280.dc.exp.0, softmax_1280.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_1284: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_1261.dc.add.10, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1291: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [softmax_1280.dc.multiply.3, matmul_1284], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_1295: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [matmul_1291, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_1261.dc.add.10_add_1299: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_1261.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1299: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [matmul_1295, buffer_0_layernorm_1261.dc.add.10_add_1299], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_50:
    target_device: 0
    input_count: 2
    layernorm_1300.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_1299_0, lc.input_tensor.layernorm_1300.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_1300.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_1299_0, layernorm_1300.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1300.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_1300.dc.subtract.1, layernorm_1300.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1300.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_1300.dc.multiply.2, lc.input_tensor.layernorm_1300.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1300.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_1300.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1300.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1300.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_1300.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1300.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_1300.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1300.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_1300.dc.reciprocal.7, lc.input_tensor.layernorm_1300.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1300.dc.subtract.1_buffer_1_layernorm_1300.dc.subtract.1_layernorm_1300.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_1300.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1300.dc.subtract.1_layernorm_1300.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_1300.dc.subtract.1_buffer_1_layernorm_1300.dc.subtract.1_layernorm_1300.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1300.dc.subtract.1_layernorm_1300.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_1300.dc.subtract.1_layernorm_1300.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1300.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_1300.dc.subtract.1_layernorm_1300.dc.multiply.8, layernorm_1300.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.22.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_1300.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_1300.dc.multiply.8, layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1300.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_1300.dc.multiply.9, layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1303: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [layernorm_1300.dc.add.10, layer.22.intermediate.dense.weight, layer.22.intermediate.dense.bias], t: 1, mblock: [1, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1306: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_1303], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_51:
    target_device: 0
    input_count: 2
    matmul_1309: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_1306_0, layer.22.output.dense.weight, layer.22.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_1300.dc.add.10_add_1313: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1300.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1313: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_1309, buffer_0_layernorm_1300.dc.add.10_add_1313], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1314.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_1313, lc.input_tensor.layernorm_1314.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1314.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_1313, layernorm_1314.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1314.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_1314.dc.subtract.1, layernorm_1314.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1314.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1314.dc.multiply.2, lc.input_tensor.layernorm_1314.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1314.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_1314.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1314.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1314.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_1314.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1314.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_1314.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1314.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_1314.dc.reciprocal.7, lc.input_tensor.layernorm_1314.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1314.dc.subtract.1_buffer_1_layernorm_1314.dc.subtract.1_layernorm_1314.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_1314.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1314.dc.subtract.1_layernorm_1314.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_1314.dc.subtract.1_buffer_1_layernorm_1314.dc.subtract.1_layernorm_1314.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1314.dc.subtract.1_layernorm_1314.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_1314.dc.subtract.1_layernorm_1314.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1314.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_1314.dc.subtract.1_layernorm_1314.dc.multiply.8, layernorm_1314.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1314.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_1314.dc.multiply.8, layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1314.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_1314.dc.multiply.9, layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_1317: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_1314.dc.add.10, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1323: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_1314.dc.add.10, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_1329: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_1317, matmul_1323], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_1331: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_1329, input_1_multiply_1331_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_1332: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_1331, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    matmul_1337: {type: matmul, grid_loc: [8, 4], grid_size: [2, 4], inputs: [layernorm_1314.dc.add.10, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_52:
    target_device: 0
    input_count: 2
    softmax_1333.dc.exp.0: {type: exp, grid_loc: [0, 1], grid_size: [2, 2], inputs: [e2e_add_1332_0], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1333.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [softmax_1333.dc.exp.0, lc.input_tensor.softmax_1333.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_1333.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [softmax_1333.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_1333.dc.multiply.3: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [softmax_1333.dc.exp.0, softmax_1333.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_1344: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [softmax_1333.dc.multiply.3, e2e_matmul_1337_0], t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 2, u_kt: 6}}
    matmul_1348: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_1344, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_1314.dc.add.10_add_1352: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1314.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1352: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_1348, buffer_0_layernorm_1314.dc.add.10_add_1352], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1353.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [add_1352, lc.input_tensor.layernorm_1353.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1353.dc.subtract.1: {type: subtract, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_1352, layernorm_1353.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1353.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_1353.dc.subtract.1, layernorm_1353.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1353.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_1353.dc.multiply.2, lc.input_tensor.layernorm_1353.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1353.dc.add.5: {type: add, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_1353.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1353.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1353.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_1353.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1353.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_1353.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1353.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_1353.dc.reciprocal.7, lc.input_tensor.layernorm_1353.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1353.dc.subtract.1_buffer_1_layernorm_1353.dc.subtract.1_layernorm_1353.dc.multiply.8: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_1353.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1353.dc.subtract.1_layernorm_1353.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_1353.dc.subtract.1_buffer_1_layernorm_1353.dc.subtract.1_layernorm_1353.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1353.dc.subtract.1_layernorm_1353.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_1353.dc.subtract.1_layernorm_1353.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1353.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_1353.dc.subtract.1_layernorm_1353.dc.multiply.8, layernorm_1353.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.23.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_1353.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_1353.dc.multiply.8, layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1353.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_1353.dc.multiply.9, layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_53:
    target_device: 0
    input_count: 2
    matmul_1356: {type: matmul, grid_loc: [0, 0], grid_size: [6, 4], inputs: [e2e_layernorm_1353.dc.add.10_0, layer.23.intermediate.dense.weight, layer.23.intermediate.dense.bias], t: 1, mblock: [1, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_1359: {type: gelu, grid_loc: [2, 4], grid_size: [2, 4], inputs: [matmul_1356], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_1362: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_1359, layer.23.output.dense.weight, layer.23.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 8}}
    buffer_0_layernorm_1353.dc.add.10_add_1366: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_layernorm_1353.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1366: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_1362, buffer_0_layernorm_1353.dc.add.10_add_1366], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1367.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_1366, lc.input_tensor.layernorm_1367.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_1367.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_1366, layernorm_1367.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        104, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_1367.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_1367.dc.subtract.1, layernorm_1367.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1367.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_1367.dc.multiply.2, lc.input_tensor.layernorm_1367.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}],
      attributes: {m_k: 1, u_kt: 32}}
    layernorm_1367.dc.add.5: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_1367.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1367.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1367.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_1367.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1367.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_1367.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_1367.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_1367.dc.reciprocal.7, lc.input_tensor.layernorm_1367.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_0_layernorm_1367.dc.subtract.1_buffer_1_layernorm_1367.dc.subtract.1_layernorm_1367.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_1367.dc.subtract.1], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_1367.dc.subtract.1_layernorm_1367.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_layernorm_1367.dc.subtract.1_buffer_1_layernorm_1367.dc.subtract.1_layernorm_1367.dc.multiply.8],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_1367.dc.subtract.1_layernorm_1367.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_1367.dc.subtract.1_layernorm_1367.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1367.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_1367.dc.subtract.1_layernorm_1367.dc.multiply.8, layernorm_1367.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_1367.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_1367.dc.multiply.8, layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}

  fwd_54:
    target_device: 0
    input_count: 2
    layernorm_1367.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_1367.dc.multiply.9_0, e2e_layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0], untilize_output: true,
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {r: 12}]}


programs:
- run_fwd:
  - param: [$p_loop_count]
  - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0, $gptr_q41: 0, $gptr_q40: 0, $lptr_q38: 0, $gptr_q37: 0, $gptr_q36: 0, $lptr_q36: 0, $lptr_q34: 0, $gptr_q33: 0, $lptr_q33: 0, $gptr_q31: 0, $lptr_q31: 0,
      $gptr_q28: 0, $gptr_q27: 0, $gptr_q25: 0, $gptr_q30: 0, $lptr_q25: 0, $lptr_q23: 0, $lptr_q21: 0, $gptr_q20: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q5: 0, $lptr_q5: 0, $gptr_q38: 0, $gptr_q7: 0, $lptr_q40: 0,
      $gptr_q8: 0, $lptr_q8: 0, $gptr_q10: 0, $lptr_q10: 0, $gptr_q11: 0, $lptr_q28: 0, $lptr_q27: 0, $lptr_q11: 0, $lptr_q59: 0, $gptr_q14: 0, $gptr_q62: 0, $gptr_q12: 0, $lptr_q63: 0, $gptr_q59: 0, $lptr_q60: 0,
      $gptr_q34: 0, $lptr_q64: 0, $gptr_q43: 0, $lptr_q54: 0, $lptr_q66: 0, $gptr_q64: 0, $gptr_q67: 0, $gptr_q24: 0, $lptr_q20: 0, $lptr_q18: 0, $lptr_q67: 0, $lptr_q69: 0, $lptr_q7: 0, $gptr_q76: 0, $lptr_q53: 0,
      $lptr_q4: 0, $gptr_q66: 0, $gptr_q73: 0, $gptr_q57: 0, $lptr_q77: 0, $gptr_q60: 0, $lptr_q49: 0, $gptr_q69: 0, $lptr_q24: 0, $gptr_q70: 0, $gptr_q77: 0, $lptr_q50: 0, $lptr_q30: 0, $gptr_q72: 0, $lptr_q73: 0,
      $gptr_q78: 0, $lptr_q76: 0, $lptr_q62: 0, $gptr_q17: 0, $gptr_q75: 0, $gptr_q63: 0, $lptr_q56: 0, $lptr_q57: 0, $gptr_q56: 0, $lptr_q72: 0, $lptr_q51: 0, $gptr_q51: 0, $gptr_q49: 0, $lptr_q47: 0,
      $gptr_q53: 0, $gptr_q54: 0, $gptr_q47: 0, $gptr_q21: 0, $lptr_q78: 0, $gptr_q50: 0, $lptr_q46: 0, $lptr_q12: 0, $gptr_q46: 0, $gptr_q18: 0, $gptr_q4: 0, $lptr_q70: 0, $lptr_q43: 0, $lptr_q44: 0, $gptr_q44: 0,
      $lptr_q41: 0, $lptr_q14: 0, $lptr_q17: 0, $gptr_q23: 0, $lptr_q15: 0, $lptr_q37: 0, $lptr_q75: 0, $gptr_q15: 0}
  - staticvar: {$gptr_q39: 0, $lptr_q39: 0, $gptr_q35_shadow: 0, $gptr_q35: 0, $gptr_q32: 0, $lptr_q32: 0, $gptr_q26_shadow: 0, $gptr_q48_shadow: 0, $lptr_q3: 0, $lptr_q29: 0, $lptr_q0: 0, $lptr_q48: 0,
      $lptr_q52: 0, $gptr_q52: 0, $gptr_q55_shadow: 0, $gptr_q48: 0, $gptr_q58: 0, $gptr_q29_shadow: 0, $lptr_q74: 0, $gptr_q45_shadow: 0, $lptr_q16: 0, $lptr_q26: 0, $gptr_q61_shadow: 0, $gptr_q52_shadow: 0,
      $gptr_q74: 0, $gptr_q3_shadow: 0, $lptr_q45: 0, $lptr_q61: 0, $gptr_q58_shadow: 0, $lptr_q68: 0, $gptr_q22_shadow: 0, $gptr_q55: 0, $lptr_q55: 0, $lptr_q58: 0, $gptr_q26: 0, $gptr_q71_shadow: 0, $lptr_q9: 0,
      $gptr_q1: 0, $gptr_q68_shadow: 0, $gptr_q29: 0, $lptr_q65: 0, $gptr_q3: 0, $lptr_q22: 0, $gptr_q42_shadow: 0, $gptr_q45: 0, $gptr_q65: 0, $lptr_q35: 0, $gptr_q6: 0, $gptr_q1_shadow: 0, $gptr_q16: 0,
      $gptr_q39_shadow: 0, $lptr_q13: 0, $gptr_q32_shadow: 0, $gptr_q61: 0, $lptr_q71: 0, $gptr_q16_shadow: 0, $gptr_q42: 0, $gptr_q13: 0, $gptr_q65_shadow: 0, $gptr_q9: 0, $gptr_q71: 0, $gptr_q68: 0, $gptr_q9_shadow: 0,
      $lptr_q6: 0, $gptr_q6_shadow: 0, $gptr_q19: 0, $gptr_q13_shadow: 0, $gptr_q22: 0, $lptr_q1: 0, $lptr_q19: 0, $gptr_q0: 0, $lptr_q42: 0, $gptr_q19_shadow: 0}
  - varinst: [$gptr_q71, set, $gptr_q71_shadow]
  - varinst: [$gptr_q68, set, $gptr_q68_shadow]
  - varinst: [$gptr_q65, set, $gptr_q65_shadow]
  - varinst: [$gptr_q61, set, $gptr_q61_shadow]
  - varinst: [$gptr_q58, set, $gptr_q58_shadow]
  - varinst: [$gptr_q55, set, $gptr_q55_shadow]
  - varinst: [$gptr_q52, set, $gptr_q52_shadow]
  - varinst: [$gptr_q48, set, $gptr_q48_shadow]
  - varinst: [$gptr_q45, set, $gptr_q45_shadow]
  - varinst: [$gptr_q42, set, $gptr_q42_shadow]
  - varinst: [$gptr_q16, set, $gptr_q16_shadow]
  - varinst: [$gptr_q13, set, $gptr_q13_shadow]
  - varinst: [$gptr_q9, set, $gptr_q9_shadow]
  - varinst: [$gptr_q6, set, $gptr_q6_shadow]
  - varinst: [$gptr_q3, set, $gptr_q3_shadow]
  - varinst: [$gptr_q1, set, $gptr_q1_shadow]
  - varinst: [$gptr_q19, set, $gptr_q19_shadow]
  - varinst: [$gptr_q22, set, $gptr_q22_shadow]
  - varinst: [$gptr_q26, set, $gptr_q26_shadow]
  - varinst: [$gptr_q29, set, $gptr_q29_shadow]
  - varinst: [$gptr_q32, set, $gptr_q32_shadow]
  - varinst: [$gptr_q35, set, $gptr_q35_shadow]
  - varinst: [$gptr_q39, set, $gptr_q39_shadow]
  - loop: $p_loop_count
  - allocate_queue: [e2e_layernorm_134.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_0, queue_settings: {hidden_states: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, attention_mask: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_112_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_114.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_134.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_134.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_134.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_134.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
  - allocate_queue: [e2e_layernorm_148.dc.multiply.8_0, e2e_layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_1, queue_settings: {e2e_layernorm_134.dc.multiply.9_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_148.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_148.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_148.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_148.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_134.dc.multiply.9_0, e2e_layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_187.dc.reciprocal.7_0, e2e_buffer_0_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8_0]
  - execute: {graph_name: fwd_2, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_layernorm_148.dc.multiply.8_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4,
          rd_ptr_global: $gptr_q4}, lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, input_1_multiply_165_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_167.dc.reduce_sum.1.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_187.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_187.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_187.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_148.dc.multiply.8_0, e2e_layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_201.dc.add.5_0, e2e_buffer_0_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8_0]
  - execute: {graph_name: fwd_3, queue_settings: {e2e_layernorm_187.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, e2e_buffer_0_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, lc.input_tensor.layernorm_187.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.intermediate.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_201.dc.reduce_avg.0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_201.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        dc.input_tensor.layernorm_201.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_187.dc.reciprocal.7_0, e2e_buffer_0_layernorm_187.dc.subtract.1_layernorm_187.dc.multiply.8_0]
  - varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_240.dc.subtract.1_0, e2e_buffer_0_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8_0]
  - execute: {graph_name: fwd_4, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_layernorm_201.dc.add.5_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_buffer_0_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, lc.input_tensor.layernorm_201.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_218_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_220.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_240.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_201.dc.add.5_0, e2e_buffer_0_layernorm_201.dc.subtract.1_layernorm_201.dc.multiply.8_0]
  - varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_240.dc.add.10_0, e2e_gelu_246_0]
  - execute: {graph_name: fwd_5, queue_settings: {e2e_layernorm_240.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_buffer_0_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, lc.input_tensor.layernorm_240.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_240.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_240.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_240.dc.subtract.1_0, e2e_buffer_0_layernorm_240.dc.subtract.1_layernorm_240.dc.multiply.8_0]
  - varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_254.dc.add.10_0, e2e_softmax_273.dc.exp.0_0, e2e_softmax_273.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_6, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, e2e_layernorm_240.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10}, e2e_gelu_246_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
        layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_254.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_254.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_254.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_254.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_271_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_273.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_240.dc.add.10_0, e2e_gelu_246_0]
  - varinst: [$gptr_q10, incwrap, $c_microbatch_size, 4]
  - varinst: [$gptr_q9_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q10, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q9, incwrap, $c_microbatch_size, 8]
  - allocate_queue: [e2e_layernorm_293.dc.add.10_0]
  - execute: {graph_name: fwd_7, queue_settings: {e2e_layernorm_254.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_softmax_273.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_softmax_273.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11,
          rd_ptr_global: $gptr_q11}, layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_293.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_293.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_293.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_293.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_254.dc.add.10_0, e2e_softmax_273.dc.exp.0_0, e2e_softmax_273.dc.reciprocal.2_0]
  - varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_307.dc.multiply.9_0, e2e_layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_8, queue_settings: {e2e_layernorm_293.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}, layer.3.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_307.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_307.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_307.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_307.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_293.dc.add.10_0]
  - varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_346.dc.multiply.8_0]
  - execute: {graph_name: fwd_9, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_layernorm_307.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_324_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_326.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_346.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_346.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_346.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_346.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_307.dc.multiply.9_0, e2e_layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q13_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q14, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q13, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_360.dc.reciprocal.7_0, e2e_buffer_0_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8_0]
  - execute: {graph_name: fwd_10, queue_settings: {e2e_layernorm_346.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15}, lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_360.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_360.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_360.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_346.dc.multiply.8_0]
  - varinst: [$gptr_q15, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q15, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_399.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8_0]
  - execute: {graph_name: fwd_11, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16}, e2e_layernorm_360.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, e2e_buffer_0_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, lc.input_tensor.layernorm_360.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_377_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_379.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_399.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_399.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_360.dc.reciprocal.7_0, e2e_buffer_0_layernorm_360.dc.subtract.1_layernorm_360.dc.multiply.8_0]
  - varinst: [$gptr_q16_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q17, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q16, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q17, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_413.dc.subtract.1_0, e2e_buffer_0_layernorm_413.dc.subtract.1_buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8_0]
  - execute: {graph_name: fwd_12, queue_settings: {e2e_layernorm_399.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, e2e_buffer_0_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, dc.input_tensor.layernorm_399.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_399.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_413.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_399.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_399.dc.subtract.1_layernorm_399.dc.multiply.8_0]
  - varinst: [$gptr_q18, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q18, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_451_0]
  - execute: {graph_name: fwd_13, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19}, e2e_layernorm_413.dc.subtract.1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, e2e_buffer_0_layernorm_413.dc.subtract.1_buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, lc.input_tensor.layernorm_413.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_413.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_413.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_430_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_432.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_413.dc.subtract.1_0, e2e_buffer_0_layernorm_413.dc.subtract.1_buffer_1_layernorm_413.dc.subtract.1_layernorm_413.dc.multiply.8_0]
  - varinst: [$gptr_q19_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q20, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q19, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q20, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_452.dc.add.10_0, e2e_gelu_458_0]
  - execute: {graph_name: fwd_14, queue_settings: {e2e_add_451_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21}, lc.input_tensor.layernorm_452.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_452.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_452.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_452.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_451_0]
  - varinst: [$gptr_q21, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q21, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_466.dc.add.10_0, e2e_softmax_485.dc.exp.0_0, e2e_softmax_485.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_15, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22}, e2e_layernorm_452.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23}, e2e_gelu_458_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
        layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_466.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_466.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_466.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_466.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_483_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_485.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_452.dc.add.10_0, e2e_gelu_458_0]
  - varinst: [$gptr_q22_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q23, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q22, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q23, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_505.dc.add.10_0]
  - execute: {graph_name: fwd_16, queue_settings: {e2e_layernorm_466.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24}, e2e_softmax_485.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24}, e2e_softmax_485.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24,
          rd_ptr_global: $gptr_q24}, layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_505.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_505.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_505.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_505.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_466.dc.add.10_0, e2e_softmax_485.dc.exp.0_0, e2e_softmax_485.dc.reciprocal.2_0]
  - varinst: [$gptr_q24, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q24, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_519.dc.multiply.9_0, e2e_layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_17, queue_settings: {e2e_layernorm_505.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25}, layer.7.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_519.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_519.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_519.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_519.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_505.dc.add.10_0]
  - varinst: [$gptr_q25, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q25, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_558.dc.multiply.8_0]
  - execute: {graph_name: fwd_18, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26}, e2e_layernorm_519.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27}, e2e_layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27}, layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_536_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_538.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_558.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_558.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_558.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_558.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_519.dc.multiply.9_0, e2e_layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q26_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q27, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q26, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q27, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_572.dc.reciprocal.7_0, e2e_buffer_0_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8_0]
  - execute: {graph_name: fwd_19, queue_settings: {e2e_layernorm_558.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28}, lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_572.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_572.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_572.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_558.dc.multiply.8_0]
  - varinst: [$gptr_q28, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q28, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_611.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8_0]
  - execute: {graph_name: fwd_20, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29}, e2e_layernorm_572.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30}, e2e_buffer_0_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30}, lc.input_tensor.layernorm_572.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_589_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_591.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_611.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_611.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_572.dc.reciprocal.7_0, e2e_buffer_0_layernorm_572.dc.subtract.1_layernorm_572.dc.multiply.8_0]
  - varinst: [$gptr_q29_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q30, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q29, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q30, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_625.dc.subtract.1_0, e2e_buffer_0_layernorm_625.dc.subtract.1_buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8_0]
  - execute: {graph_name: fwd_21, queue_settings: {e2e_layernorm_611.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31}, e2e_buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31}, dc.input_tensor.layernorm_611.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_611.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_625.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_611.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_611.dc.subtract.1_layernorm_611.dc.multiply.8_0]
  - varinst: [$gptr_q31, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q31, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_663_0]
  - execute: {graph_name: fwd_22, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32}, e2e_layernorm_625.dc.subtract.1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33}, e2e_buffer_0_layernorm_625.dc.subtract.1_buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33}, lc.input_tensor.layernorm_625.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_625.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_625.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_642_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_644.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_625.dc.subtract.1_0, e2e_buffer_0_layernorm_625.dc.subtract.1_buffer_1_layernorm_625.dc.subtract.1_layernorm_625.dc.multiply.8_0]
  - varinst: [$gptr_q32_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q33, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q32, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q33, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_664.dc.add.10_0, e2e_gelu_670_0]
  - execute: {graph_name: fwd_23, queue_settings: {e2e_add_663_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34}, lc.input_tensor.layernorm_664.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_664.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_664.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_664.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_663_0]
  - varinst: [$gptr_q34, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q34, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_678.dc.add.10_0, e2e_softmax_697.dc.exp.0_0, e2e_softmax_697.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_24, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35}, e2e_layernorm_664.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36}, e2e_gelu_670_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
        layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_678.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_678.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_678.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_678.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_695_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_697.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_664.dc.add.10_0, e2e_gelu_670_0]
  - varinst: [$gptr_q35_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q36, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q35, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q36, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_717.dc.add.10_0]
  - execute: {graph_name: fwd_25, queue_settings: {e2e_layernorm_678.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37}, e2e_softmax_697.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37}, e2e_softmax_697.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q37,
          rd_ptr_global: $gptr_q37}, layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_717.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_717.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_717.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_717.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_678.dc.add.10_0, e2e_softmax_697.dc.exp.0_0, e2e_softmax_697.dc.reciprocal.2_0]
  - varinst: [$gptr_q37, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q37, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_731.dc.multiply.9_0, e2e_layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_26, queue_settings: {e2e_layernorm_717.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38}, layer.11.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_731.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_731.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_731.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_731.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_717.dc.add.10_0]
  - varinst: [$gptr_q38, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q38, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_770.dc.multiply.8_0]
  - execute: {graph_name: fwd_27, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39}, e2e_layernorm_731.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40}, e2e_layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40}, layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_748_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_750.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_770.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_770.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_770.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_770.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_731.dc.multiply.9_0, e2e_layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q39_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q40, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q39, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q40, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_784.dc.reciprocal.7_0, e2e_buffer_0_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8_0]
  - execute: {graph_name: fwd_28, queue_settings: {e2e_layernorm_770.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41}, lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_784.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_784.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_784.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_770.dc.multiply.8_0]
  - varinst: [$gptr_q41, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q41, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_823.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8_0]
  - execute: {graph_name: fwd_29, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42}, e2e_layernorm_784.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43}, e2e_buffer_0_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43}, lc.input_tensor.layernorm_784.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.12.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_801_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_803.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.13.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_823.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_823.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_784.dc.reciprocal.7_0, e2e_buffer_0_layernorm_784.dc.subtract.1_layernorm_784.dc.multiply.8_0]
  - varinst: [$gptr_q42_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q43, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q42, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q43, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_837.dc.subtract.1_0, e2e_buffer_0_layernorm_837.dc.subtract.1_buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8_0]
  - execute: {graph_name: fwd_30, queue_settings: {e2e_layernorm_823.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44}, e2e_buffer_0_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44}, dc.input_tensor.layernorm_823.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_823.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_837.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_823.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_823.dc.subtract.1_layernorm_823.dc.multiply.8_0]
  - varinst: [$gptr_q44, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q44, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_875_0]
  - execute: {graph_name: fwd_31, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45}, e2e_layernorm_837.dc.subtract.1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46}, e2e_buffer_0_layernorm_837.dc.subtract.1_buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46}, lc.input_tensor.layernorm_837.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_837.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_837.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_854_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_856.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_837.dc.subtract.1_0, e2e_buffer_0_layernorm_837.dc.subtract.1_buffer_1_layernorm_837.dc.subtract.1_layernorm_837.dc.multiply.8_0]
  - varinst: [$gptr_q45_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q46, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q45, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q46, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_876.dc.add.10_0, e2e_gelu_882_0]
  - execute: {graph_name: fwd_32, queue_settings: {e2e_add_875_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47}, lc.input_tensor.layernorm_876.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_876.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_876.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_876.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_875_0]
  - varinst: [$gptr_q47, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q47, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_890.dc.add.10_0, e2e_softmax_909.dc.exp.0_0, e2e_softmax_909.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_33, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48}, e2e_layernorm_876.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49}, e2e_gelu_882_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
        layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_890.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_890.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_890.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_890.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_907_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_909.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_876.dc.add.10_0, e2e_gelu_882_0]
  - varinst: [$gptr_q48_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q49, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q48, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q49, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_929.dc.add.10_0]
  - execute: {graph_name: fwd_34, queue_settings: {e2e_layernorm_890.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50}, e2e_softmax_909.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50}, e2e_softmax_909.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q50,
          rd_ptr_global: $gptr_q50}, layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_929.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_929.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_929.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_929.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.15.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_890.dc.add.10_0, e2e_softmax_909.dc.exp.0_0, e2e_softmax_909.dc.reciprocal.2_0]
  - varinst: [$gptr_q50, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q50, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_943.dc.multiply.9_0, e2e_layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_35, queue_settings: {e2e_layernorm_929.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51}, layer.15.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_943.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_943.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_943.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_943.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_929.dc.add.10_0]
  - varinst: [$gptr_q51, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q51, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_982.dc.multiply.8_0]
  - execute: {graph_name: fwd_36, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52}, e2e_layernorm_943.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53}, e2e_layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53}, layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_960_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_962.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_982.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_982.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_982.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_982.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_943.dc.multiply.9_0, e2e_layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q52_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q53, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q52, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q53, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_996.dc.reciprocal.7_0, e2e_buffer_0_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8_0]
  - execute: {graph_name: fwd_37, queue_settings: {e2e_layernorm_982.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54}, lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_996.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_996.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_996.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_982.dc.multiply.8_0]
  - varinst: [$gptr_q54, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q54, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1035.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8_0]
  - execute: {graph_name: fwd_38, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55}, e2e_layernorm_996.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56}, e2e_buffer_0_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56}, lc.input_tensor.layernorm_996.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_1013_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_1015.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1035.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1035.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_996.dc.reciprocal.7_0, e2e_buffer_0_layernorm_996.dc.subtract.1_layernorm_996.dc.multiply.8_0]
  - varinst: [$gptr_q55_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q56, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q55, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q56, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1049.dc.subtract.1_0, e2e_buffer_0_layernorm_1049.dc.subtract.1_buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8_0]
  - execute: {graph_name: fwd_39, queue_settings: {e2e_layernorm_1035.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57}, e2e_buffer_0_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57}, dc.input_tensor.layernorm_1035.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1035.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1049.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1035.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_1035.dc.subtract.1_layernorm_1035.dc.multiply.8_0]
  - varinst: [$gptr_q57, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q57, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_1087_0]
  - execute: {graph_name: fwd_40, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58}, e2e_layernorm_1049.dc.subtract.1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59}, e2e_buffer_0_layernorm_1049.dc.subtract.1_buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59}, lc.input_tensor.layernorm_1049.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1049.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1049.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_1066_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_1068.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1049.dc.subtract.1_0, e2e_buffer_0_layernorm_1049.dc.subtract.1_buffer_1_layernorm_1049.dc.subtract.1_layernorm_1049.dc.multiply.8_0]
  - varinst: [$gptr_q58_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q59, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q58, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q59, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1088.dc.add.10_0, e2e_gelu_1094_0]
  - execute: {graph_name: fwd_41, queue_settings: {e2e_add_1087_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60}, lc.input_tensor.layernorm_1088.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1088.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1088.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1088.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.18.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_1087_0]
  - varinst: [$gptr_q60, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q60, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1102.dc.add.10_0, e2e_softmax_1121.dc.exp.0_0, e2e_softmax_1121.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_42, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61}, e2e_layernorm_1088.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62}, e2e_gelu_1094_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62},
        layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1102.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_1102.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1102.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1102.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.18.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_1119_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_1121.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1088.dc.add.10_0, e2e_gelu_1094_0]
  - varinst: [$gptr_q61_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q62, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q61, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q62, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1141.dc.add.10_0]
  - execute: {graph_name: fwd_43, queue_settings: {e2e_layernorm_1102.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63}, e2e_softmax_1121.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63}, e2e_softmax_1121.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q63,
          rd_ptr_global: $gptr_q63}, layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1141.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1141.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1141.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1141.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.19.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1102.dc.add.10_0, e2e_softmax_1121.dc.exp.0_0, e2e_softmax_1121.dc.reciprocal.2_0]
  - varinst: [$gptr_q63, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q63, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1155.dc.multiply.9_0, e2e_layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_44, queue_settings: {e2e_layernorm_1141.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64}, layer.19.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1155.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1155.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1155.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1155.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1141.dc.add.10_0]
  - varinst: [$gptr_q64, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q64, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1194.dc.multiply.8_0]
  - execute: {graph_name: fwd_45, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q65, rd_ptr_global: $gptr_q65}, e2e_layernorm_1155.dc.multiply.9_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66}, e2e_layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66}, layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_1172_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_1174.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1194.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1194.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1194.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1194.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1155.dc.multiply.9_0, e2e_layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q65_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q66, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q65, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q66, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1208.dc.reciprocal.7_0, e2e_buffer_0_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8_0]
  - execute: {graph_name: fwd_46, queue_settings: {e2e_layernorm_1194.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67}, lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1208.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_1208.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1208.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1194.dc.multiply.8_0]
  - varinst: [$gptr_q67, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q67, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1247.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8_0]
  - execute: {graph_name: fwd_47, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68}, e2e_layernorm_1208.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69}, e2e_buffer_0_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69}, lc.input_tensor.layernorm_1208.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.20.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_1225_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_1227.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1247.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1247.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1208.dc.reciprocal.7_0, e2e_buffer_0_layernorm_1208.dc.subtract.1_layernorm_1208.dc.multiply.8_0]
  - varinst: [$gptr_q68_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q69, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q68, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q69, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1261.dc.subtract.1_0, e2e_buffer_0_layernorm_1261.dc.subtract.1_buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8_0]
  - execute: {graph_name: fwd_48, queue_settings: {e2e_layernorm_1247.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70}, e2e_buffer_0_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70}, dc.input_tensor.layernorm_1247.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1247.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1261.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1247.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_1247.dc.subtract.1_layernorm_1247.dc.multiply.8_0]
  - varinst: [$gptr_q70, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q70, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_1299_0]
  - execute: {graph_name: fwd_49, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71}, e2e_layernorm_1261.dc.subtract.1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q72, rd_ptr_global: $gptr_q72}, e2e_buffer_0_layernorm_1261.dc.subtract.1_buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q72, rd_ptr_global: $gptr_q72}, lc.input_tensor.layernorm_1261.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1261.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1261.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_1278_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_1280.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1261.dc.subtract.1_0, e2e_buffer_0_layernorm_1261.dc.subtract.1_buffer_1_layernorm_1261.dc.subtract.1_layernorm_1261.dc.multiply.8_0]
  - varinst: [$gptr_q71_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q72, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q71, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q72, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1300.dc.add.10_0, e2e_gelu_1306_0]
  - execute: {graph_name: fwd_50, queue_settings: {e2e_add_1299_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q73, rd_ptr_global: $gptr_q73}, lc.input_tensor.layernorm_1300.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1300.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1300.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1300.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_1299_0]
  - varinst: [$gptr_q73, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q73, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1314.dc.add.10_0, e2e_add_1332_0, e2e_matmul_1337_0]
  - execute: {graph_name: fwd_51, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q74, rd_ptr_global: $gptr_q74}, e2e_layernorm_1300.dc.add.10_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q75, rd_ptr_global: $gptr_q75}, e2e_gelu_1306_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q75, rd_ptr_global: $gptr_q75},
        layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1314.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_1314.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1314.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1314.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_1331_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1300.dc.add.10_0, e2e_gelu_1306_0]
  - varinst: [$gptr_q74, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q75, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q74, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q75, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1353.dc.add.10_0]
  - execute: {graph_name: fwd_52, queue_settings: {e2e_layernorm_1314.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q76, rd_ptr_global: $gptr_q76}, e2e_add_1332_0: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q76, rd_ptr_global: $gptr_q76}, e2e_matmul_1337_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q76, rd_ptr_global: $gptr_q76},
        lc.input_tensor.softmax_1333.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.attention.output.dense.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layernorm_1353.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1353.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1353.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1353.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1314.dc.add.10_0, e2e_add_1332_0, e2e_matmul_1337_0]
  - varinst: [$gptr_q76, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q76, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_1367.dc.multiply.9_0, e2e_layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - execute: {graph_name: fwd_53, queue_settings: {e2e_layernorm_1353.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q77, rd_ptr_global: $gptr_q77}, layer.23.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_1367.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1367.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_1367.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_1367.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_1353.dc.add.10_0]
  - varinst: [$gptr_q77, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q77, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: fwd_54, queue_settings: {e2e_layernorm_1367.dc.multiply.9_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78}, e2e_layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78}}}
  - deallocate_queue: [e2e_layernorm_1367.dc.multiply.9_0, e2e_layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
  - varinst: [$gptr_q78, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q78, incwrap, $c_microbatch_size, 4]
  - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 1
