# git checkout 75a2c479e
# pytest pybuda/test/benchmark/benchmark.py -m bert -c large -opt 3 -o perf.json -df Fp16_b --env PYBUDA_EXP_APPROX=1 PYBUDA_DISABLE_DYNAMIC_DRAM=1 TT_BACKEND_PUSH_TIMEOUT=500 PYBUDA_FORK_JOIN_INPUT_BUFFERS=1 --loop_count 1 --layers 1 --microbatch 16 --disable_output 1

devices:
  arch: blackhole

queues:

  # input
  input_1: {input: HOST, type: queue, entries: 32, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [0x0]}
  attention_mask: {input: HOST, type: queue, entries: 32, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [
      0x1980040]}

  # output
  bert_encoders.output_layernorm_52: {input: _fused_op_6_output_nop_0, type: queue, entries: 32, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b,
    target_device: 0, loc: host, host: [0x1a4c080]}

  # parameter
  layer.0.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10000000], [0, 0x10044040], [0, 0x10088080], [0, 0x100cc0c0], [0, 0x10110100], [0, 0x10154140], [0, 0x10198180], [0, 0x101dc1c0]]}
  layer.0.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10000000], [2, 0x10004440], [2, 0x10008880], [2, 0x1000ccc0]]}
  layer.0.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x10000000], [4, 0x10044040], [4, 0x10088080], [4, 0x100cc0c0], [4, 0x10110100], [4, 0x10154140], [4, 0x10198180], [4, 0x101dc1c0]]}
  layer.0.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10011100], [2, 0x10015540], [2, 0x10019980], [2, 0x1001ddc0]]}
  layer.0.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10022200], [2, 0x10066240], [2, 0x100aa280], [2, 0x100ee2c0], [2, 0x10132300], [2, 0x10176340], [2, 0x101ba380], [2, 0x101fe3c0]]}
  layer.0.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10220200], [0, 0x10224640], [0, 0x10228a80], [0, 0x1022cec0]]}
  layer.0.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10242400], [2, 0x10286440], [2, 0x102ca480], [2, 0x1030e4c0], [2, 0x10352500], [2, 0x10396540], [2, 0x103da580], [2, 0x1041e5c0]]}
  layer.0.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x10231300], [0, 0x10235740], [0, 0x10239b80], [0, 0x1023dfc0]]}
  layer.0.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10000000]]}
  layer.0.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10011040]]}
  layer.0.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10462600], [2, 0x104a6640], [2, 0x104ea680], [2, 0x1052e6c0], [2, 0x10572700], [2, 0x105b6740], [2, 0x105fa780], [2, 0x1063e7c0], [2, 0x10682800], [2, 0x106c6840], [2, 0x1070a880], [2,
        0x1074e8c0], [2, 0x10792900], [2, 0x107d6940], [2, 0x1081a980], [2, 0x1085e9c0], [2, 0x108a2a00], [2, 0x108e6a40], [2, 0x1092aa80], [2, 0x1096eac0], [2, 0x109b2b00], [2, 0x109f6b40], [2, 0x10a3ab80],
      [2, 0x10a7ebc0], [2, 0x10ac2c00], [2, 0x10b06c40], [2, 0x10b4ac80], [2, 0x10b8ecc0], [2, 0x10bd2d00], [2, 0x10c16d40], [2, 0x10c5ad80], [2, 0x10c9edc0]]}
  layer.0.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10242400], [0, 0x1024ac40], [0, 0x10253480], [0, 0x1025bcc0], [0, 0x10264500], [0, 0x1026cd40], [0, 0x10275580], [0, 0x1027ddc0]]}
  layer.0.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x10220200], [4, 0x102a8240], [4, 0x10330280], [4, 0x103b82c0], [4, 0x10440300], [4, 0x104c8340], [4, 0x10550380], [4, 0x105d83c0], [4, 0x10660400], [4, 0x106e8440], [4, 0x10770480], [4,
        0x107f84c0], [4, 0x10880500], [4, 0x10908540], [4, 0x10990580], [4, 0x10a185c0]]}
  layer.0.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10ce2e00], [2, 0x10ce5040], [2, 0x10ce7280], [2, 0x10ce94c0], [2, 0x10ceb700], [2, 0x10ced940], [2, 0x10cefb80], [2, 0x10cf1dc0]]}
  layer.0.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x10000000]]}
  layer.0.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x10011040]]}

  # constant
  input_1_multiply_16: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10022080]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10022940]]}
  dc.input_tensor.softmax_18.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x10286600], [0, 0x102b9640]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x10022080]]}
  dc.input_tensor.layernorm_38.1: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x10aa0600], [4, 0x10b06640]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x10b6c680]]}
  dc.input_tensor.layernorm_38.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x102ec680], [0, 0x102ef9c0]]}
  dc.input_tensor.layernorm_38.8: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x102f2d00], [0, 0x102f6040]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x10023200]]}
  dc.input_tensor.layernorm_52.1: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x10023ac0], [1, 0x10089b00]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x10cf4000]]}
  dc.input_tensor.layernorm_52.6: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x10000000], [3, 0x10003340]]}
  dc.input_tensor.layernorm_52.8: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x10b6cf40], [4, 0x10b70280]]}

  # epoch_to_epoch
  e2e_layernorm_38.dc.reduce_sum.0.lc1_0: {input: layernorm_38.dc.reduce_sum.0.lc1, type: queue, entries: 16, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32],
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x100efb40], [1, 0x10122b80]]}
  e2e_add_37_0: {input: add_37, type: queue, entries: 16, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x102f9380], [0, 0x109593c0]]}
  e2e_gelu_44_0: {input: gelu_44, type: queue, entries: 16, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10b735c0], [4, 0x111d3600], [4, 0x11833640], [4, 0x11e93680], [4, 0x124f36c0], [4, 0x12b53700], [4, 0x131b3740], [4, 0x13813780]]}
  e2e__fused_op_4_0: {input: _fused_op_4, type: queue, entries: 16, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x10cf48c0], [2, 0x11354900]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 16
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [
        32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_8: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [
        32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    matmul_14: {type: matmul, grid_loc: [2, 4], grid_size: [2, 2], inputs: [matmul_2, matmul_8], t: 16, mblock: [3, 3], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16], attributes: {m_k: 2, min_buffer_input: 0,
        u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_14, input_1_multiply_16, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
      input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}], attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_2: 24, input_1: 1}}}
    softmax_18.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_0], t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    _fused_op_1: {type: fused_op, grid_loc: [4, 0], grid_size: [2, 3], inputs: [_fused_op_0, softmax_18.dc.reduce_max.0], t: 16, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [
        48, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [broadcast: {c: 12}], attributes: {approximate_mode: true,
        fused_op_id: 1}}
    softmax_18.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [_fused_op_1, lc.input_tensor.softmax_18.dc.reduce_sum.3.0], t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [
        32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {kernel_broadcast: {input_1: 1}, m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_22: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [
        32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    buffer_0__fused_op_1__fused_op_2: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [_fused_op_1], t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [
        128], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_2: {type: fused_op, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_18.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_18.4, buffer_0__fused_op_1__fused_op_2], t: 16, mblock: [3, 3], ublock: [
        2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, attributes: {
        approximate_mode: true, fused_op_id: 2}}
    matmul_29: {type: matmul, grid_loc: [4, 6], grid_size: [2, 2], inputs: [_fused_op_2, matmul_22], t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [hslice: 16], attributes: {m_k: 6, min_buffer_input: 0, u_kt: 2}}
    matmul_33: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_2_tms: [broadcast: {
            r: 12}], input_0_tms: [hstack: 16], attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_33, input_1], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [
        32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [broadcast: {r: 32}], attributes: {
        kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 16
    _fused_op_3: {type: fused_op, grid_loc: [0, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_38.1, e2e_layernorm_38.dc.reduce_sum.0.lc1_0, e2e_add_37_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [broadcast: {
            c: 32}], attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [_fused_op_3, _fused_op_3], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [broadcast: {
            r: 32}], attributes: {kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [_fused_op_3], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_4__fused_op_3__fused_op_4], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_3__fused_op_3__fused_op_4], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_2__fused_op_3__fused_op_4], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_3__fused_op_4: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1__fused_op_3__fused_op_4], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_4: {type: fused_op, grid_loc: [2, 0], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8, buffer_0__fused_op_3__fused_op_4,
        layer.0.attention.output.LayerNorm.weight, layer.0.attention.output.LayerNorm.bias], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b,
        Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {
            r: 12}], attributes: {approximate_mode: true, fused_op_id: 4}}
    matmul_41: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [_fused_op_4, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [1, 4], tile_dim: [
        32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 0, u_kt: 2}}
    gelu_44: {type: gelu, grid_loc: [2, 1], grid_size: [2, 4], inputs: [matmul_41], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, attributes: {approximate_mode: false}}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 16
    matmul_47: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_44_0, layer.0.output.dense.weight, layer.0.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], tile_dim: [32, 32],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_2_tms: [broadcast: {r: 12}], attributes: {
        bias: true, kernel_broadcast: {input_2: 8}, m_k: 16, min_buffer_input: 0, u_kt: 8}}
    add_51: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_47, e2e__fused_op_4_0], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [
        32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [broadcast: {r: 32}], attributes: {
        kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_3_add_51__fused_op_5: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_51], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2_add_51__fused_op_5: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_3_add_51__fused_op_5], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_add_51__fused_op_5: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_add_51__fused_op_5], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_51__fused_op_5: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_add_51__fused_op_5], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_5: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, buffer_0_add_51__fused_op_5], t: 1, mblock: [3, 8], ublock: [
        2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [
        broadcast: {c: 32}], attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [_fused_op_5, _fused_op_5], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0], t: 1, mblock: [3, 1], ublock: [
        2, 1], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_1_tms: [broadcast: {
            r: 32}], attributes: {kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    buffer_4__fused_op_5__fused_op_6: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [_fused_op_5], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_3__fused_op_5__fused_op_6: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [buffer_4__fused_op_5__fused_op_6], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_2__fused_op_5__fused_op_6: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_3__fused_op_5__fused_op_6], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1__fused_op_5__fused_op_6: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2__fused_op_5__fused_op_6], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0__fused_op_5__fused_op_6: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1__fused_op_5__fused_op_6], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_6: {type: fused_op, grid_loc: [4, 6], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8, buffer_0__fused_op_5__fused_op_6,
        layer.0.output.LayerNorm.weight, layer.0.output.LayerNorm.bias], t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b,
        Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi, input_5_tms: [broadcast: {r: 12}], input_4_tms: [broadcast: {r: 12}], attributes: {
        approximate_mode: true, fused_op_id: 4}}
    _fused_op_6_output_nop_0: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [_fused_op_6], untilize_output: true, t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: LoFi}


programs:
- run_fwd_0:
  - param: [$p_loop_count]
  - var: {$c_microbatch_size: 16, $c_one: 1, $c_zero: 0}
  - staticvar: {$lptr_q0: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0}
  - loop: $p_loop_count
  - execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {input_1: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, attention_mask: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_16: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_18.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.softmax_18.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0, incwrap, $c_microbatch_size, 64]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 64]
  - execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {e2e_add_37_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, e2e_layernorm_38.dc.reduce_sum.0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, dc.input_tensor.layernorm_38.1: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_38.6: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.bias: {
          prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q1, incwrap, $c_microbatch_size, 32]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 32]
  - execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {e2e__fused_op_4_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, e2e_gelu_44_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_52.1: {prologue: false, epilogue: false, zero: false, rd_ptr_autoinc: 0,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_52.6: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {prologue: false, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {prologue: false,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - varinst: [$gptr_q2, incwrap, $c_microbatch_size, 32]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 32]
  - endloop


fused_ops:
  0:
    inputs: 3
    intermediates: 0
    schedules:
    - - multiply_16.0: {type: multiply, inputs: [input0, input1], mblock: [3, 3], ublock: [2, 4], output: dest}
      - add_17.0: {type: add, inputs: [dest, input2], mblock: [3, 3], ublock: [2, 4], output: output}
  1:
    inputs: 2
    intermediates: 0
    schedules:
    - - softmax_18.dc.subtract.1.0: {type: subtract, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [3, 1], ublock: [2, 4], output: dest}
      - softmax_18.dc.exp.2.0: {type: exp, inputs: [dest], mblock: [3, 1], ublock: [2, 4], output: output}
  2:
    inputs: 3
    intermediates: 1
    schedules:
    - - softmax_18.dc.add.5.0: {type: add, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 1], output: dest}
      - softmax_18.dc.reciprocal.6.0: {type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: intermed0}
      - softmax_18.dc.multiply.7.0: {type: multiply, inputs: [input2, intermed0], input_1_tms: [broadcast: {c: 12}, tile_broadcast: c], pop: [intermed0], mblock: [3, 3], ublock: [2, 4], output: output}
  3:
    inputs: 3
    intermediates: 0
    schedules:
    - - layernorm_38.dc.multiply.2.0: {type: multiply, inputs: [input0, input1], mblock: [3, 8], ublock: [2, 4], output: dest}
      - layernorm_38.dc.subtract.3.0: {type: subtract, inputs: [input2, dest], mblock: [3, 8], ublock: [2, 4], output: output}
  4:
    inputs: 6
    intermediates: 1
    schedules:
    - - layernorm_38.dc.multiply.7.0: {type: multiply, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 1], output: dest}
      - layernorm_38.dc.add.9.0: {type: add, inputs: [dest, input2], mblock: [3, 1], ublock: [2, 1], output: dest}
      - layernorm_38.dc.sqrt.10.0: {type: sqrt, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: dest}
      - layernorm_38.dc.reciprocal.11.0: {type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: intermed0}
      - layernorm_38.dc.multiply.12.0: {type: multiply, inputs: [input3, intermed0], input_1_tms: [broadcast: {c: 32}, tile_broadcast: c], pop: [intermed0], mblock: [3, 8], ublock: [2, 4], output: dest}
      - layernorm_38.dc.multiply.13.0: {type: multiply, inputs: [dest, input4], mblock: [3, 8], ublock: [2, 4], output: dest}
      - layernorm_38.dc.add.14.0: {type: add, inputs: [dest, input5], mblock: [3, 8], ublock: [2, 4], output: output}
