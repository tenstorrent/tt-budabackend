# git checkout e815fd1f
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole-chip1-enc24-large]

devices:
  arch: blackhole

queues:

  # input
  hidden_states: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10000000]]}
  attention_mask: {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10330040]]}

  # output
  bert_encoder.output_layernorm_3815: {input: layernorm_3815.dc.add.10, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0,
    loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10462080], [2, 0x104a60c0], [2, 0x104ea100], [2, 0x1052e140], [2, 0x10572180], [2, 0x105b61c0], [2, 0x105fa200], [2, 0x1063e240]]}
  layer.0.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10000000], [3, 0x10004440], [3, 0x10008880], [3, 0x1000ccc0]]}
  layer.0.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10000000], [1, 0x10044040], [1, 0x10088080], [1, 0x100cc0c0], [1, 0x10110100], [1, 0x10154140], [1, 0x10198180], [1, 0x101dc1c0]]}
  layer.0.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10011100],
      [3, 0x10015540], [3, 0x10019980], [3, 0x1001ddc0]]}
  layer.0.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10000000], [4, 0x10044040], [4, 0x10088080], [4, 0x100cc0c0], [4, 0x10110100], [4, 0x10154140], [4, 0x10198180], [4, 0x101dc1c0]]}
  layer.0.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10000000], [0, 0x10004440], [0, 0x10008880], [0, 0x1000ccc0]]}
  layer.0.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10220200], [4, 0x10264240], [4, 0x102a8280], [4, 0x102ec2c0], [4, 0x10330300], [4, 0x10374340], [4, 0x103b8380], [4, 0x103fc3c0]]}
  layer.0.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10440400], [4, 0x10444840], [4, 0x10448c80], [4, 0x1044d0c0]]}
  layer.0.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10220200]]}
  layer.0.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10022200]]}
  layer.0.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10451500], [4, 0x10495540], [4, 0x104d9580], [4, 0x1051d5c0], [4, 0x10561600], [4, 0x105a5640], [4, 0x105e9680], [4, 0x1062d6c0], [4, 0x10671700], [4, 0x106b5740], [4, 0x106f9780], [4, 0x1073d7c0],
      [4, 0x10781800], [4, 0x107c5840], [4, 0x10809880], [4, 0x1084d8c0], [4, 0x10891900], [4, 0x108d5940], [4, 0x10919980], [4, 0x1095d9c0], [4, 0x109a1a00], [4, 0x109e5a40], [4, 0x10a29a80], [4, 0x10a6dac0],
      [4, 0x10ab1b00], [4, 0x10af5b40], [4, 0x10b39b80], [4, 0x10b7dbc0], [4, 0x10bc1c00], [4, 0x10c05c40], [4, 0x10c49c80], [4, 0x10c8dcc0]]}
  layer.0.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10011100],
      [0, 0x10019940], [0, 0x10022180], [0, 0x1002a9c0], [0, 0x10033200], [0, 0x1003ba40], [0, 0x10044280], [0, 0x1004cac0]]}
  layer.0.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x10682280],
      [2, 0x1070a2c0], [2, 0x10792300], [2, 0x1081a340], [2, 0x108a2380], [2, 0x1092a3c0], [2, 0x109b2400], [2, 0x10a3a440], [2, 0x10ac2480], [2, 0x10b4a4c0], [2, 0x10bd2500], [2, 0x10c5a540], [2, 0x10ce2580],
      [2, 0x10d6a5c0], [2, 0x10df2600], [2, 0x10e7a640]]}
  layer.0.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10033240],
      [3, 0x10035480], [3, 0x100376c0], [3, 0x10039900], [3, 0x1003bb40], [3, 0x1003dd80], [3, 0x1003ffc0], [3, 0x10042200]]}
  layer.0.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10000000]]}
  layer.0.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10231240]]}
  layer.1.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x10f02680], [2, 0x10f466c0], [2, 0x10f8a700], [2, 0x10fce740], [2, 0x11012780], [2, 0x110567c0], [2, 0x1109a800], [2, 0x110de840]]}
  layer.1.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x10cd1d00], [4, 0x10cd6140], [4, 0x10cda580], [4, 0x10cde9c0]]}
  layer.1.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10242280], [1, 0x102862c0], [1, 0x102ca300], [1, 0x1030e340], [1, 0x10352380], [1, 0x103963c0], [1, 0x103da400], [1, 0x1041e440]]}
  layer.1.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10055300],
      [0, 0x10059740], [0, 0x1005db80], [0, 0x10061fc0]]}
  layer.1.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10066400], [0, 0x100aa440], [0, 0x100ee480], [0, 0x101324c0], [0, 0x10176500], [0, 0x101ba540], [0, 0x101fe580], [0, 0x102425c0]]}
  layer.1.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11122880], [2, 0x11126cc0], [2, 0x1112b100], [2, 0x1112f540]]}
  layer.1.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10286600], [0, 0x102ca640], [0, 0x1030e680], [0, 0x103526c0], [0, 0x10396700], [0, 0x103da740], [0, 0x1041e780], [0, 0x104627c0]]}
  layer.1.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11133980], [2, 0x11137dc0], [2, 0x1113c200], [2, 0x11140640]]}
  layer.1.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11144a80]]}
  layer.1.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x10ce2e00]]}
  layer.1.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10011040], [5, 0x10055080], [5, 0x100990c0], [5, 0x100dd100], [5, 0x10121140], [5, 0x10165180], [5, 0x101a91c0], [5, 0x101ed200], [5, 0x10231240], [5, 0x10275280], [5, 0x102b92c0], [5, 0x102fd300],
      [5, 0x10341340], [5, 0x10385380], [5, 0x103c93c0], [5, 0x1040d400], [5, 0x10451440], [5, 0x10495480], [5, 0x104d94c0], [5, 0x1051d500], [5, 0x10561540], [5, 0x105a5580], [5, 0x105e95c0], [5, 0x1062d600],
      [5, 0x10671640], [5, 0x106b5680], [5, 0x106f96c0], [5, 0x1073d700], [5, 0x10781740], [5, 0x107c5780], [5, 0x108097c0], [5, 0x1084d800]]}
  layer.1.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x10462480],
      [1, 0x1046acc0], [1, 0x10473500], [1, 0x1047bd40], [1, 0x10484580], [1, 0x1048cdc0], [1, 0x10495600], [1, 0x1049de40]]}
  layer.1.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10044440],
      [3, 0x100cc480], [3, 0x101544c0], [3, 0x101dc500], [3, 0x10264540], [3, 0x102ec580], [3, 0x103745c0], [3, 0x103fc600], [3, 0x10484640], [3, 0x1050c680], [3, 0x105946c0], [3, 0x1061c700], [3, 0x106a4740],
      [3, 0x1072c780], [3, 0x107b47c0], [3, 0x1083c800]]}
  layer.1.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x104a6680],
      [1, 0x104a88c0], [1, 0x104aab00], [1, 0x104acd40], [1, 0x104aef80], [1, 0x104b11c0], [1, 0x104b3400], [1, 0x104b5640]]}
  layer.1.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11155ac0]]}
  layer.1.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10cf3e40]]}
  layer.2.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10891840], [5, 0x108d5880], [5, 0x109198c0], [5, 0x1095d900], [5, 0x109a1940], [5, 0x109e5980], [5, 0x10a299c0], [5, 0x10a6da00]]}
  layer.2.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x104b7880], [1, 0x104bbcc0], [1, 0x104c0100], [1, 0x104c4540]]}
  layer.2.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10ab1a40], [5, 0x10af5a80], [5, 0x10b39ac0], [5, 0x10b7db00], [5, 0x10bc1b40], [5, 0x10c05b80], [5, 0x10c49bc0], [5, 0x10c8dc00]]}
  layer.2.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x104c8980],
      [1, 0x104ccdc0], [1, 0x104d1200], [1, 0x104d5640]]}
  layer.2.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x108c4840], [3, 0x10908880], [3, 0x1094c8c0], [3, 0x10990900], [3, 0x109d4940], [3, 0x10a18980], [3, 0x10a5c9c0], [3, 0x10aa0a00]]}
  layer.2.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x10cd1c40], [5, 0x10cd6080], [5, 0x10cda4c0], [5, 0x10cde900]]}
  layer.2.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10ae4a40], [3, 0x10b28a80], [3, 0x10b6cac0], [3, 0x10bb0b00], [3, 0x10bf4b40], [3, 0x10c38b80], [3, 0x10c7cbc0], [3, 0x10cc0c00]]}
  layer.2.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x10ce2d40], [5, 0x10ce7180], [5, 0x10ceb5c0], [5, 0x10cefa00]]}
  layer.2.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10d04c40]]}
  layer.2.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11166b00]]}
  layer.2.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x104a6800], [0, 0x104ea840], [0, 0x1052e880], [0, 0x105728c0], [0, 0x105b6900], [0, 0x105fa940], [0, 0x1063e980], [0, 0x106829c0], [0, 0x106c6a00], [0, 0x1070aa40], [0, 0x1074ea80], [0, 0x10792ac0],
      [0, 0x107d6b00], [0, 0x1081ab40], [0, 0x1085eb80], [0, 0x108a2bc0], [0, 0x108e6c00], [0, 0x1092ac40], [0, 0x1096ec80], [0, 0x109b2cc0], [0, 0x109f6d00], [0, 0x10a3ad40], [0, 0x10a7ed80], [0, 0x10ac2dc0],
      [0, 0x10b06e00], [0, 0x10b4ae40], [0, 0x10b8ee80], [0, 0x10bd2ec0], [0, 0x10c16f00], [0, 0x10c5af40], [0, 0x10c9ef80], [0, 0x10ce2fc0]]}
  layer.2.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11177b40],
      [2, 0x11180380], [2, 0x11188bc0], [2, 0x11191400], [2, 0x11199c40], [2, 0x111a2480], [2, 0x111aacc0], [2, 0x111b3500]]}
  layer.2.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10d04e80],
      [4, 0x10d8cec0], [4, 0x10e14f00], [4, 0x10e9cf40], [4, 0x10f24f80], [4, 0x10facfc0], [4, 0x11035000], [4, 0x110bd040], [4, 0x11145080], [4, 0x111cd0c0], [4, 0x11255100], [4, 0x112dd140], [4, 0x11365180],
      [4, 0x113ed1c0], [4, 0x11475200], [4, 0x114fd240]]}
  layer.2.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x111bbd40],
      [2, 0x111bdf80], [2, 0x111c01c0], [2, 0x111c2400], [2, 0x111c4640], [2, 0x111c6880], [2, 0x111c8ac0], [2, 0x111cad00]]}
  layer.2.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11585280]]}
  layer.2.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x104d9a80]]}
  layer.3.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x111ccf40], [2, 0x11210f80], [2, 0x11254fc0], [2, 0x11299000], [2, 0x112dd040], [2, 0x11321080], [2, 0x113650c0], [2, 0x113a9100]]}
  layer.3.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x115962c0], [4, 0x1159a700], [4, 0x1159eb40], [4, 0x115a2f80]]}
  layer.3.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x113ed140], [2, 0x11431180], [2, 0x114751c0], [2, 0x114b9200], [2, 0x114fd240], [2, 0x11541280], [2, 0x115852c0], [2, 0x115c9300]]}
  layer.3.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x115a73c0],
      [4, 0x115ab800], [4, 0x115afc40], [4, 0x115b4080]]}
  layer.3.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x115b84c0], [4, 0x115fc500], [4, 0x11640540], [4, 0x11684580], [4, 0x116c85c0], [4, 0x1170c600], [4, 0x11750640], [4, 0x11794680]]}
  layer.3.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x10d27000], [0, 0x10d2b440], [0, 0x10d2f880], [0, 0x10d33cc0]]}
  layer.3.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x117d86c0], [4, 0x1181c700], [4, 0x11860740], [4, 0x118a4780], [4, 0x118e87c0], [4, 0x1192c800], [4, 0x11970840], [4, 0x119b4880]]}
  layer.3.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10d38100], [0, 0x10d3c540], [0, 0x10d40980], [0, 0x10d44dc0]]}
  layer.3.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x119f88c0]]}
  layer.3.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10d49200]]}
  layer.3.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x104eaac0], [1, 0x1052eb00], [1, 0x10572b40], [1, 0x105b6b80], [1, 0x105fabc0], [1, 0x1063ec00], [1, 0x10682c40], [1, 0x106c6c80], [1, 0x1070acc0], [1, 0x1074ed00], [1, 0x10792d40], [1, 0x107d6d80],
      [1, 0x1081adc0], [1, 0x1085ee00], [1, 0x108a2e40], [1, 0x108e6e80], [1, 0x1092aec0], [1, 0x1096ef00], [1, 0x109b2f40], [1, 0x109f6f80], [1, 0x10a3afc0], [1, 0x10a7f000], [1, 0x10ac3040], [1, 0x10b07080],
      [1, 0x10b4b0c0], [1, 0x10b8f100], [1, 0x10bd3140], [1, 0x10c17180], [1, 0x10c5b1c0], [1, 0x10c9f200], [1, 0x10ce3240], [1, 0x10d27280]]}
  layer.3.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10d15c80],
      [3, 0x10d1e4c0], [3, 0x10d26d00], [3, 0x10d2f540], [3, 0x10d37d80], [3, 0x10d405c0], [3, 0x10d48e00], [3, 0x10d51640]]}
  layer.3.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10cf3e40],
      [5, 0x10d7be80], [5, 0x10e03ec0], [5, 0x10e8bf00], [5, 0x10f13f40], [5, 0x10f9bf80], [5, 0x11023fc0], [5, 0x110ac000], [5, 0x11134040], [5, 0x111bc080], [5, 0x112440c0], [5, 0x112cc100], [5, 0x11354140],
      [5, 0x113dc180], [5, 0x114641c0], [5, 0x114ec200]]}
  layer.3.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10d59e80],
      [3, 0x10d5c0c0], [3, 0x10d5e300], [3, 0x10d60540], [3, 0x10d62780], [3, 0x10d649c0], [3, 0x10d66c00], [3, 0x10d68e40]]}
  layer.3.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x11a09900]]}
  layer.3.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d5a240]]}
  layer.4.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x10d6b2c0], [1, 0x10daf300], [1, 0x10df3340], [1, 0x10e37380], [1, 0x10e7b3c0], [1, 0x10ebf400], [1, 0x10f03440], [1, 0x10f47480]]}
  layer.4.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10d6b080], [3, 0x10d6f4c0], [3, 0x10d73900], [3, 0x10d77d40]]}
  layer.4.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x10f8b4c0], [1, 0x10fcf500], [1, 0x11013540], [1, 0x11057580], [1, 0x1109b5c0], [1, 0x110df600], [1, 0x11123640], [1, 0x11167680]]}
  layer.4.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d6b280],
      [0, 0x10d6f6c0], [0, 0x10d73b00], [0, 0x10d77f40]]}
  layer.4.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x111ab6c0], [1, 0x111ef700], [1, 0x11233740], [1, 0x11277780], [1, 0x112bb7c0], [1, 0x112ff800], [1, 0x11343840], [1, 0x11387880]]}
  layer.4.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10d7c180], [3, 0x10d805c0], [3, 0x10d84a00], [3, 0x10d88e40]]}
  layer.4.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x113cb8c0], [1, 0x1140f900], [1, 0x11453940], [1, 0x11497980], [1, 0x114db9c0], [1, 0x1151fa00], [1, 0x11563a40], [1, 0x115a7a80]]}
  layer.4.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10d8d280], [3, 0x10d916c0], [3, 0x10d95b00], [3, 0x10d99f40]]}
  layer.4.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x115ebac0]]}
  layer.4.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10d9e380]]}
  layer.4.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x11a1a940], [4, 0x11a5e980], [4, 0x11aa29c0], [4, 0x11ae6a00], [4, 0x11b2aa40], [4, 0x11b6ea80], [4, 0x11bb2ac0], [4, 0x11bf6b00], [4, 0x11c3ab40], [4, 0x11c7eb80], [4, 0x11cc2bc0], [4, 0x11d06c00],
      [4, 0x11d4ac40], [4, 0x11d8ec80], [4, 0x11dd2cc0], [4, 0x11e16d00], [4, 0x11e5ad40], [4, 0x11e9ed80], [4, 0x11ee2dc0], [4, 0x11f26e00], [4, 0x11f6ae40], [4, 0x11faee80], [4, 0x11ff2ec0], [4, 0x12036f00],
      [4, 0x1207af40], [4, 0x120bef80], [4, 0x12102fc0], [4, 0x12147000], [4, 0x1218b040], [4, 0x121cf080], [4, 0x122130c0], [4, 0x12257100]]}
  layer.4.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10d7c380],
      [0, 0x10d84bc0], [0, 0x10d8d400], [0, 0x10d95c40], [0, 0x10d9e480], [0, 0x10da6cc0], [0, 0x10daf500], [0, 0x10db7d40]]}
  layer.4.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1160d340],
      [2, 0x11695380], [2, 0x1171d3c0], [2, 0x117a5400], [2, 0x1182d440], [2, 0x118b5480], [2, 0x1193d4c0], [2, 0x119c5500], [2, 0x11a4d540], [2, 0x11ad5580], [2, 0x11b5d5c0], [2, 0x11be5600], [2, 0x11c6d640],
      [2, 0x11cf5680], [2, 0x11d7d6c0], [2, 0x11e05700]]}
  layer.4.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x10dc0580],
      [0, 0x10dc27c0], [0, 0x10dc4a00], [0, 0x10dc6c40], [0, 0x10dc8e80], [0, 0x10dcb0c0], [0, 0x10dcd300], [0, 0x10dcf540]]}
  layer.4.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11e8d740]]}
  layer.4.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1229b140]]}
  layer.5.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x11574240], [5, 0x115b8280], [5, 0x115fc2c0], [5, 0x11640300], [5, 0x11684340], [5, 0x116c8380], [5, 0x1170c3c0], [5, 0x11750400]]}
  layer.5.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x115fcb00], [1, 0x11600f40], [1, 0x11605380], [1, 0x116097c0]]}
  layer.5.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x11794440], [5, 0x117d8480], [5, 0x1181c4c0], [5, 0x11860500], [5, 0x118a4540], [5, 0x118e8580], [5, 0x1192c5c0], [5, 0x11970600]]}
  layer.5.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1160dc00],
      [1, 0x11612040], [1, 0x11616480], [1, 0x1161a8c0]]}
  layer.5.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10dd1780], [0, 0x10e157c0], [0, 0x10e59800], [0, 0x10e9d840], [0, 0x10ee1880], [0, 0x10f258c0], [0, 0x10f69900], [0, 0x10fad940]]}
  layer.5.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x119b4640], [5, 0x119b8a80], [5, 0x119bcec0], [5, 0x119c1300]]}
  layer.5.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10daf3c0], [3, 0x10df3400], [3, 0x10e37440], [3, 0x10e7b480], [3, 0x10ebf4c0], [3, 0x10f03500], [3, 0x10f47540], [3, 0x10f8b580]]}
  layer.5.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x119c5740], [5, 0x119c9b80], [5, 0x119cdfc0], [5, 0x119d2400]]}
  layer.5.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x10fcf5c0]]}
  layer.5.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x119d6840]]}
  layer.5.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x10fe0600], [3, 0x11024640], [3, 0x11068680], [3, 0x110ac6c0], [3, 0x110f0700], [3, 0x11134740], [3, 0x11178780], [3, 0x111bc7c0], [3, 0x11200800], [3, 0x11244840], [3, 0x11288880], [3, 0x112cc8c0],
      [3, 0x11310900], [3, 0x11354940], [3, 0x11398980], [3, 0x113dc9c0], [3, 0x11420a00], [3, 0x11464a40], [3, 0x114a8a80], [3, 0x114ecac0], [3, 0x11530b00], [3, 0x11574b40], [3, 0x115b8b80], [3, 0x115fcbc0],
      [3, 0x11640c00], [3, 0x11684c40], [3, 0x116c8c80], [3, 0x1170ccc0], [3, 0x11750d00], [3, 0x11794d40], [3, 0x117d8d80], [3, 0x1181cdc0]]}
  layer.5.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11e9e780],
      [2, 0x11ea6fc0], [2, 0x11eaf800], [2, 0x11eb8040], [2, 0x11ec0880], [2, 0x11ec90c0], [2, 0x11ed1900], [2, 0x11eda140]]}
  layer.5.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x122ac180],
      [4, 0x123341c0], [4, 0x123bc200], [4, 0x12444240], [4, 0x124cc280], [4, 0x125542c0], [4, 0x125dc300], [4, 0x12664340], [4, 0x126ec380], [4, 0x127743c0], [4, 0x127fc400], [4, 0x12884440], [4, 0x1290c480],
      [4, 0x129944c0], [4, 0x12a1c500], [4, 0x12aa4540]]}
  layer.5.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11ee2980],
      [2, 0x11ee4bc0], [2, 0x11ee6e00], [2, 0x11ee9040], [2, 0x11eeb280], [2, 0x11eed4c0], [2, 0x11eef700], [2, 0x11ef1940]]}
  layer.5.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12b2c580]]}
  layer.5.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x119e7880]]}
  layer.6.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x10ff1980], [0, 0x110359c0], [0, 0x11079a00], [0, 0x110bda40], [0, 0x11101a80], [0, 0x11145ac0], [0, 0x11189b00], [0, 0x111cdb40]]}
  layer.6.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11211b80], [0, 0x11215fc0], [0, 0x1121a400], [0, 0x1121e840]]}
  layer.6.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x12b3d5c0], [4, 0x12b81600], [4, 0x12bc5640], [4, 0x12c09680], [4, 0x12c4d6c0], [4, 0x12c91700], [4, 0x12cd5740], [4, 0x12d19780]]}
  layer.6.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11222c80],
      [0, 0x112270c0], [0, 0x1122b500], [0, 0x1122f940]]}
  layer.6.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x11233d80], [0, 0x11277dc0], [0, 0x112bbe00], [0, 0x112ffe40], [0, 0x11343e80], [0, 0x11387ec0], [0, 0x113cbf00], [0, 0x1140ff40]]}
  layer.6.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x11ef3b80], [2, 0x11ef7fc0], [2, 0x11efc400], [2, 0x11f00840]]}
  layer.6.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x11453f80], [0, 0x11497fc0], [0, 0x114dc000], [0, 0x11520040], [0, 0x11564080], [0, 0x115a80c0], [0, 0x115ec100], [0, 0x11630140]]}
  layer.6.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x11674180], [0, 0x116785c0], [0, 0x1167ca00], [0, 0x11680e40]]}
  layer.6.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x11860e00]]}
  layer.6.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x119f88c0]]}
  layer.6.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11685280], [0, 0x116c92c0], [0, 0x1170d300], [0, 0x11751340], [0, 0x11795380], [0, 0x117d93c0], [0, 0x1181d400], [0, 0x11861440], [0, 0x118a5480], [0, 0x118e94c0], [0, 0x1192d500], [0, 0x11971540],
      [0, 0x119b5580], [0, 0x119f95c0], [0, 0x11a3d600], [0, 0x11a81640], [0, 0x11ac5680], [0, 0x11b096c0], [0, 0x11b4d700], [0, 0x11b91740], [0, 0x11bd5780], [0, 0x11c197c0], [0, 0x11c5d800], [0, 0x11ca1840],
      [0, 0x11ce5880], [0, 0x11d298c0], [0, 0x11d6d900], [0, 0x11db1940], [0, 0x11df5980], [0, 0x11e399c0], [0, 0x11e7da00], [0, 0x11ec1a40]]}
  layer.6.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11f04c80],
      [2, 0x11f0d4c0], [2, 0x11f15d00], [2, 0x11f1e540], [2, 0x11f26d80], [2, 0x11f2f5c0], [2, 0x11f37e00], [2, 0x11f40640]]}
  layer.6.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x12d5d7c0],
      [4, 0x12de5800], [4, 0x12e6d840], [4, 0x12ef5880], [4, 0x12f7d8c0], [4, 0x13005900], [4, 0x1308d940], [4, 0x13115980], [4, 0x1319d9c0], [4, 0x13225a00], [4, 0x132ada40], [4, 0x13335a80], [4, 0x133bdac0],
      [4, 0x13445b00], [4, 0x134cdb40], [4, 0x13555b80]]}
  layer.6.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x11a09900],
      [5, 0x11a0bb40], [5, 0x11a0dd80], [5, 0x11a0ffc0], [5, 0x11a12200], [5, 0x11a14440], [5, 0x11a16680], [5, 0x11a188c0]]}
  layer.6.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1161ed00]]}
  layer.6.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11871e40]]}
  layer.7.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x135ddbc0], [4, 0x13621c00], [4, 0x13665c40], [4, 0x136a9c80], [4, 0x136edcc0], [4, 0x13731d00], [4, 0x13775d40], [4, 0x137b9d80]]}
  layer.7.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11f05a80], [0, 0x11f09ec0], [0, 0x11f0e300], [0, 0x11f12740]]}
  layer.7.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x11882e80], [3, 0x118c6ec0], [3, 0x1190af00], [3, 0x1194ef40], [3, 0x11992f80], [3, 0x119d6fc0], [3, 0x11a1b000], [3, 0x11a5f040]]}
  layer.7.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x11f48e80],
      [2, 0x11f4d2c0], [2, 0x11f51700], [2, 0x11f55b40]]}
  layer.7.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x11f59f80], [2, 0x11f9dfc0], [2, 0x11fe2000], [2, 0x12026040], [2, 0x1206a080], [2, 0x120ae0c0], [2, 0x120f2100], [2, 0x12136140]]}
  layer.7.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x137fddc0], [4, 0x13802200], [4, 0x13806640], [4, 0x1380aa80]]}
  layer.7.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1217a180], [2, 0x121be1c0], [2, 0x12202200], [2, 0x12246240], [2, 0x1228a280], [2, 0x122ce2c0], [2, 0x12312300], [2, 0x12356340]]}
  layer.7.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x1380eec0], [4, 0x13813300], [4, 0x13817740], [4, 0x1381bb80]]}
  layer.7.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x1162fd40]]}
  layer.7.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x11aa3080]]}
  layer.7.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x1381ffc0], [4, 0x13864000], [4, 0x138a8040], [4, 0x138ec080], [4, 0x139300c0], [4, 0x13974100], [4, 0x139b8140], [4, 0x139fc180], [4, 0x13a401c0], [4, 0x13a84200], [4, 0x13ac8240], [4, 0x13b0c280],
      [4, 0x13b502c0], [4, 0x13b94300], [4, 0x13bd8340], [4, 0x13c1c380], [4, 0x13c603c0], [4, 0x13ca4400], [4, 0x13ce8440], [4, 0x13d2c480], [4, 0x13d704c0], [4, 0x13db4500], [4, 0x13df8540], [4, 0x13e3c580],
      [4, 0x13e805c0], [4, 0x13ec4600], [4, 0x13f08640], [4, 0x13f4c680], [4, 0x13f906c0], [4, 0x13fd4700], [4, 0x14018740], [4, 0x1405c780]]}
  layer.7.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11f16b80],
      [0, 0x11f1f3c0], [0, 0x11f27c00], [0, 0x11f30440], [0, 0x11f38c80], [0, 0x11f414c0], [0, 0x11f49d00], [0, 0x11f52540]]}
  layer.7.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1239a380],
      [2, 0x124223c0], [2, 0x124aa400], [2, 0x12532440], [2, 0x125ba480], [2, 0x126424c0], [2, 0x126ca500], [2, 0x12752540], [2, 0x127da580], [2, 0x128625c0], [2, 0x128ea600], [2, 0x12972640], [2, 0x129fa680],
      [2, 0x12a826c0], [2, 0x12b0a700], [2, 0x12b92740]]}
  layer.7.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11f5ad80],
      [0, 0x11f5cfc0], [0, 0x11f5f200], [0, 0x11f61440], [0, 0x11f63680], [0, 0x11f658c0], [0, 0x11f67b00], [0, 0x11f69d40]]}
  layer.7.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x11f6bf80]]}
  layer.7.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x140a07c0]]}
  layer.8.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x12c1a780], [2, 0x12c5e7c0], [2, 0x12ca2800], [2, 0x12ce6840], [2, 0x12d2a880], [2, 0x12d6e8c0], [2, 0x12db2900], [2, 0x12df6940]]}
  layer.8.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x140b1800], [4, 0x140b5c40], [4, 0x140ba080], [4, 0x140be4c0]]}
  layer.8.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x12e3a980], [2, 0x12e7e9c0], [2, 0x12ec2a00], [2, 0x12f06a40], [2, 0x12f4aa80], [2, 0x12f8eac0], [2, 0x12fd2b00], [2, 0x13016b40]]}
  layer.8.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x11640d80],
      [1, 0x116451c0], [1, 0x11649600], [1, 0x1164da40]]}
  layer.8.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x1305ab80], [2, 0x1309ebc0], [2, 0x130e2c00], [2, 0x13126c40], [2, 0x1316ac80], [2, 0x131aecc0], [2, 0x131f2d00], [2, 0x13236d40]]}
  layer.8.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x11f7cfc0], [0, 0x11f81400], [0, 0x11f85840], [0, 0x11f89c80]]}
  layer.8.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x11f8e0c0], [0, 0x11fd2100], [0, 0x12016140], [0, 0x1205a180], [0, 0x1209e1c0], [0, 0x120e2200], [0, 0x12126240], [0, 0x1216a280]]}
  layer.8.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x11a1ab00], [5, 0x11a1ef40], [5, 0x11a23380], [5, 0x11a277c0]]}
  layer.8.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x11ab40c0]]}
  layer.8.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x11a2bc00]]}
  layer.8.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x140c2900], [4, 0x14106940], [4, 0x1414a980], [4, 0x1418e9c0], [4, 0x141d2a00], [4, 0x14216a40], [4, 0x1425aa80], [4, 0x1429eac0], [4, 0x142e2b00], [4, 0x14326b40], [4, 0x1436ab80], [4, 0x143aebc0],
      [4, 0x143f2c00], [4, 0x14436c40], [4, 0x1447ac80], [4, 0x144becc0], [4, 0x14502d00], [4, 0x14546d40], [4, 0x1458ad80], [4, 0x145cedc0], [4, 0x14612e00], [4, 0x14656e40], [4, 0x1469ae80], [4, 0x146deec0],
      [4, 0x14722f00], [4, 0x14766f40], [4, 0x147aaf80], [4, 0x147eefc0], [4, 0x14833000], [4, 0x14877040], [4, 0x148bb080], [4, 0x148ff0c0]]}
  layer.8.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1327ad80],
      [2, 0x132835c0], [2, 0x1328be00], [2, 0x13294640], [2, 0x1329ce80], [2, 0x132a56c0], [2, 0x132adf00], [2, 0x132b6740]]}
  layer.8.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x14943100],
      [4, 0x149cb140], [4, 0x14a53180], [4, 0x14adb1c0], [4, 0x14b63200], [4, 0x14beb240], [4, 0x14c73280], [4, 0x14cfb2c0], [4, 0x14d83300], [4, 0x14e0b340], [4, 0x14e93380], [4, 0x14f1b3c0], [4, 0x14fa3400],
      [4, 0x1502b440], [4, 0x150b3480], [4, 0x1513b4c0]]}
  layer.8.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x132bef80],
      [2, 0x132c11c0], [2, 0x132c3400], [2, 0x132c5640], [2, 0x132c7880], [2, 0x132c9ac0], [2, 0x132cbd00], [2, 0x132cdf40]]}
  layer.8.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x151c3500]]}
  layer.8.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x151d4540]]}
  layer.9.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x121ae2c0], [0, 0x121f2300], [0, 0x12236340], [0, 0x1227a380], [0, 0x122be3c0], [0, 0x12302400], [0, 0x12346440], [0, 0x1238a480]]}
  layer.9.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x132d0180], [2, 0x132d45c0], [2, 0x132d8a00], [2, 0x132dce40]]}
  layer.9.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x123ce4c0], [0, 0x12412500], [0, 0x12456540], [0, 0x1249a580], [0, 0x124de5c0], [0, 0x12522600], [0, 0x12566640], [0, 0x125aa680]]}
  layer.9.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x132e1280],
      [2, 0x132e56c0], [2, 0x132e9b00], [2, 0x132edf40]]}
  layer.9.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x132f2380], [2, 0x133363c0], [2, 0x1337a400], [2, 0x133be440], [2, 0x13402480], [2, 0x134464c0], [2, 0x1348a500], [2, 0x134ce540]]}
  layer.9.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x151e5580], [4, 0x151e99c0], [4, 0x151ede00], [4, 0x151f2240]]}
  layer.9.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x13512580], [2, 0x135565c0], [2, 0x1359a600], [2, 0x135de640], [2, 0x13622680], [2, 0x136666c0], [2, 0x136aa700], [2, 0x136ee740]]}
  layer.9.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x151f6680], [4, 0x151faac0], [4, 0x151fef00], [4, 0x15203340]]}
  layer.9.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x13732780]]}
  layer.9.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x15207780]]}
  layer.9.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x11a3cc40], [5, 0x11a80c80], [5, 0x11ac4cc0], [5, 0x11b08d00], [5, 0x11b4cd40], [5, 0x11b90d80], [5, 0x11bd4dc0], [5, 0x11c18e00], [5, 0x11c5ce40], [5, 0x11ca0e80], [5, 0x11ce4ec0], [5, 0x11d28f00],
      [5, 0x11d6cf40], [5, 0x11db0f80], [5, 0x11df4fc0], [5, 0x11e39000], [5, 0x11e7d040], [5, 0x11ec1080], [5, 0x11f050c0], [5, 0x11f49100], [5, 0x11f8d140], [5, 0x11fd1180], [5, 0x120151c0], [5, 0x12059200],
      [5, 0x1209d240], [5, 0x120e1280], [5, 0x121252c0], [5, 0x12169300], [5, 0x121ad340], [5, 0x121f1380], [5, 0x122353c0], [5, 0x12279400]]}
  layer.9.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x11651e80],
      [1, 0x1165a6c0], [1, 0x11662f00], [1, 0x1166b740], [1, 0x11673f80], [1, 0x1167c7c0], [1, 0x11685000], [1, 0x1168d840]]}
  layer.9.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x11ac5100],
      [3, 0x11b4d140], [3, 0x11bd5180], [3, 0x11c5d1c0], [3, 0x11ce5200], [3, 0x11d6d240], [3, 0x11df5280], [3, 0x11e7d2c0], [3, 0x11f05300], [3, 0x11f8d340], [3, 0x12015380], [3, 0x1209d3c0], [3, 0x12125400],
      [3, 0x121ad440], [3, 0x12235480], [3, 0x122bd4c0]]}
  layer.9.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x11696080],
      [1, 0x116982c0], [1, 0x1169a500], [1, 0x1169c740], [1, 0x1169e980], [1, 0x116a0bc0], [1, 0x116a2e00], [1, 0x116a5040]]}
  layer.9.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x125ee6c0]]}
  layer.9.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x137437c0]]}
  layer.10.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x12345500], [3, 0x12389540], [3, 0x123cd580], [3, 0x124115c0], [3, 0x12455600], [3, 0x12499640], [3, 0x124dd680], [3, 0x125216c0]]}
  layer.10.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x122bd440], [5, 0x122c1880], [5, 0x122c5cc0], [5, 0x122ca100]]}
  layer.10.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x12565700], [3, 0x125a9740], [3, 0x125ed780], [3, 0x126317c0], [3, 0x12675800], [3, 0x126b9840], [3, 0x126fd880], [3, 0x127418c0]]}
  layer.10.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x122ce540], [5, 0x122d2980], [5, 0x122d6dc0], [5, 0x122db200]]}
  layer.10.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x122df640], [5, 0x12323680], [5, 0x123676c0], [5, 0x123ab700], [5, 0x123ef740], [5, 0x12433780], [5, 0x124777c0], [5, 0x124bb800]]}
  layer.10.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x116a7280], [1, 0x116ab6c0], [1, 0x116afb00], [1, 0x116b3f40]]}
  layer.10.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x124ff840], [5, 0x12543880], [5, 0x125878c0], [5, 0x125cb900], [5, 0x1260f940], [5, 0x12653980], [5, 0x126979c0], [5, 0x126dba00]]}
  layer.10.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x116b8380], [1, 0x116bc7c0], [1, 0x116c0c00], [1, 0x116c5040]]}
  layer.10.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x12785900]]}
  layer.10.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x13754800]]}
  layer.10.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x12796940], [3, 0x127da980], [3, 0x1281e9c0], [3, 0x12862a00], [3, 0x128a6a40], [3, 0x128eaa80], [3, 0x1292eac0], [3, 0x12972b00], [3, 0x129b6b40], [3, 0x129fab80], [3, 0x12a3ebc0], [3, 0x12a82c00],
      [3, 0x12ac6c40], [3, 0x12b0ac80], [3, 0x12b4ecc0], [3, 0x12b92d00], [3, 0x12bd6d40], [3, 0x12c1ad80], [3, 0x12c5edc0], [3, 0x12ca2e00], [3, 0x12ce6e40], [3, 0x12d2ae80], [3, 0x12d6eec0], [3, 0x12db2f00],
      [3, 0x12df6f40], [3, 0x12e3af80], [3, 0x12e7efc0], [3, 0x12ec3000], [3, 0x12f07040], [3, 0x12f4b080], [3, 0x12f8f0c0], [3, 0x12fd3100]]}
  layer.10.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1271fa40], [5, 0x12728280], [5, 0x12730ac0], [5, 0x12739300], [5, 0x12741b40], [5, 0x1274a380], [5, 0x12752bc0], [5, 0x1275b400]]}
  layer.10.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x116c9480],
      [1, 0x117514c0], [1, 0x117d9500], [1, 0x11861540], [1, 0x118e9580], [1, 0x119715c0], [1, 0x119f9600], [1, 0x11a81640], [1, 0x11b09680], [1, 0x11b916c0], [1, 0x11c19700], [1, 0x11ca1740], [1, 0x11d29780],
      [1, 0x11db17c0], [1, 0x11e39800], [1, 0x11ec1840]]}
  layer.10.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x12763c40],
      [5, 0x12765e80], [5, 0x127680c0], [5, 0x1276a300], [5, 0x1276c540], [5, 0x1276e780], [5, 0x127709c0], [5, 0x12772c00]]}
  layer.10.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x13765840]]}
  layer.10.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x152187c0]]}
  layer.11.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x12774e40], [5, 0x127b8e80], [5, 0x127fcec0], [5, 0x12840f00], [5, 0x12884f40], [5, 0x128c8f80], [5, 0x1290cfc0], [5, 0x12951000]]}
  layer.11.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x11f49880], [1, 0x11f4dcc0], [1, 0x11f52100], [1, 0x11f56540]]}
  layer.11.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x12995040], [5, 0x129d9080], [5, 0x12a1d0c0], [5, 0x12a61100], [5, 0x12aa5140], [5, 0x12ae9180], [5, 0x12b2d1c0], [5, 0x12b71200]]}
  layer.11.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x11f5a980], [1, 0x11f5edc0], [1, 0x11f63200], [1, 0x11f67640]]}
  layer.11.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x11f6ba80], [1, 0x11fafac0], [1, 0x11ff3b00], [1, 0x12037b40], [1, 0x1207bb80], [1, 0x120bfbc0], [1, 0x12103c00], [1, 0x12147c40]]}
  layer.11.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x15229800], [4, 0x1522dc40], [4, 0x15232080], [4, 0x152364c0]]}
  layer.11.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x12bb5240], [5, 0x12bf9280], [5, 0x12c3d2c0], [5, 0x12c81300], [5, 0x12cc5340], [5, 0x12d09380], [5, 0x12d4d3c0], [5, 0x12d91400]]}
  layer.11.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x1218bc80], [1, 0x121900c0], [1, 0x12194500], [1, 0x12198940]]}
  layer.11.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x12dd5440]]}
  layer.11.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x1219cd80]]}
  layer.11.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x13776880], [2, 0x137ba8c0], [2, 0x137fe900], [2, 0x13842940], [2, 0x13886980], [2, 0x138ca9c0], [2, 0x1390ea00], [2, 0x13952a40], [2, 0x13996a80], [2, 0x139daac0], [2, 0x13a1eb00], [2, 0x13a62b40],
      [2, 0x13aa6b80], [2, 0x13aeabc0], [2, 0x13b2ec00], [2, 0x13b72c40], [2, 0x13bb6c80], [2, 0x13bfacc0], [2, 0x13c3ed00], [2, 0x13c82d40], [2, 0x13cc6d80], [2, 0x13d0adc0], [2, 0x13d4ee00], [2, 0x13d92e40],
      [2, 0x13dd6e80], [2, 0x13e1aec0], [2, 0x13e5ef00], [2, 0x13ea2f40], [2, 0x13ee6f80], [2, 0x13f2afc0], [2, 0x13f6f000], [2, 0x13fb3040]]}
  layer.11.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3,
        0x13017140], [3, 0x1301f980], [3, 0x130281c0], [3, 0x13030a00], [3, 0x13039240], [3, 0x13041a80], [3, 0x1304a2c0], [3, 0x13052b00]]}
  layer.11.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x13ff7080],
      [2, 0x1407f0c0], [2, 0x14107100], [2, 0x1418f140], [2, 0x14217180], [2, 0x1429f1c0], [2, 0x14327200], [2, 0x143af240], [2, 0x14437280], [2, 0x144bf2c0], [2, 0x14547300], [2, 0x145cf340], [2, 0x14657380],
      [2, 0x146df3c0], [2, 0x14767400], [2, 0x147ef440]]}
  layer.11.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x125ff700],
      [0, 0x12601940], [0, 0x12603b80], [0, 0x12605dc0], [0, 0x12608000], [0, 0x1260a240], [0, 0x1260c480], [0, 0x1260e6c0]]}
  layer.11.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x14877480]]}
  layer.11.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x12de6480]]}
  layer.12.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x1523a900], [4, 0x1527e940], [4, 0x152c2980], [4, 0x153069c0], [4, 0x1534aa00], [4, 0x1538ea40], [4, 0x153d2a80], [4, 0x15416ac0]]}
  layer.12.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x148884c0], [2, 0x1488c900], [2, 0x14890d40], [2, 0x14895180]]}
  layer.12.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x1545ab00], [4, 0x1549eb40], [4, 0x154e2b80], [4, 0x15526bc0], [4, 0x1556ac00], [4, 0x155aec40], [4, 0x155f2c80], [4, 0x15636cc0]]}
  layer.12.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x12610900], [0, 0x12614d40], [0, 0x12619180], [0, 0x1261d5c0]]}
  layer.12.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x12621a00], [0, 0x12665a40], [0, 0x126a9a80], [0, 0x126edac0], [0, 0x12731b00], [0, 0x12775b40], [0, 0x127b9b80], [0, 0x127fdbc0]]}
  layer.12.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x148995c0], [2, 0x1489da00], [2, 0x148a1e40], [2, 0x148a6280]]}
  layer.12.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x12841c00], [0, 0x12885c40], [0, 0x128c9c80], [0, 0x1290dcc0], [0, 0x12951d00], [0, 0x12995d40], [0, 0x129d9d80], [0, 0x12a1ddc0]]}
  layer.12.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x148aa6c0], [2, 0x148aeb00], [2, 0x148b2f40], [2, 0x148b7380]]}
  layer.12.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x12a61e00]]}
  layer.12.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x148bb7c0]]}
  layer.12.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1305b340], [3, 0x1309f380], [3, 0x130e33c0], [3, 0x13127400], [3, 0x1316b440], [3, 0x131af480], [3, 0x131f34c0], [3, 0x13237500], [3, 0x1327b540], [3, 0x132bf580], [3, 0x133035c0], [3, 0x13347600],
      [3, 0x1338b640], [3, 0x133cf680], [3, 0x134136c0], [3, 0x13457700], [3, 0x1349b740], [3, 0x134df780], [3, 0x135237c0], [3, 0x13567800], [3, 0x135ab840], [3, 0x135ef880], [3, 0x136338c0], [3, 0x13677900],
      [3, 0x136bb940], [3, 0x136ff980], [3, 0x137439c0], [3, 0x13787a00], [3, 0x137cba40], [3, 0x1380fa80], [3, 0x13853ac0], [3, 0x13897b00]]}
  layer.12.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x12df74c0], [5, 0x12dffd00], [5, 0x12e08540], [5, 0x12e10d80], [5, 0x12e195c0], [5, 0x12e21e00], [5, 0x12e2a640], [5, 0x12e32e80]]}
  layer.12.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x121addc0],
      [1, 0x12235e00], [1, 0x122bde40], [1, 0x12345e80], [1, 0x123cdec0], [1, 0x12455f00], [1, 0x124ddf40], [1, 0x12565f80], [1, 0x125edfc0], [1, 0x12676000], [1, 0x126fe040], [1, 0x12786080], [1, 0x1280e0c0],
      [1, 0x12896100], [1, 0x1291e140], [1, 0x129a6180]]}
  layer.12.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x12e3b6c0],
      [5, 0x12e3d900], [5, 0x12e3fb40], [5, 0x12e41d80], [5, 0x12e43fc0], [5, 0x12e46200], [5, 0x12e48440], [5, 0x12e4a680]]}
  layer.12.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x148cc800]]}
  layer.12.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1567ad00]]}
  layer.13.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x12e4c8c0], [5, 0x12e90900], [5, 0x12ed4940], [5, 0x12f18980], [5, 0x12f5c9c0], [5, 0x12fa0a00], [5, 0x12fe4a40], [5, 0x13028a80]]}
  layer.13.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x12a2e1c0], [1, 0x12a32600], [1, 0x12a36a40], [1, 0x12a3ae80]]}
  layer.13.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1306cac0], [5, 0x130b0b00], [5, 0x130f4b40], [5, 0x13138b80], [5, 0x1317cbc0], [5, 0x131c0c00], [5, 0x13204c40], [5, 0x13248c80]]}
  layer.13.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x12a3f2c0], [1, 0x12a43700], [1, 0x12a47b40], [1, 0x12a4bf80]]}
  layer.13.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12a503c0], [1, 0x12a94400], [1, 0x12ad8440], [1, 0x12b1c480], [1, 0x12b604c0], [1, 0x12ba4500], [1, 0x12be8540], [1, 0x12c2c580]]}
  layer.13.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x138dbb40], [3, 0x138dff80], [3, 0x138e43c0], [3, 0x138e8800]]}
  layer.13.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12c705c0], [1, 0x12cb4600], [1, 0x12cf8640], [1, 0x12d3c680], [1, 0x12d806c0], [1, 0x12dc4700], [1, 0x12e08740], [1, 0x12e4c780]]}
  layer.13.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x138ecc40], [3, 0x138f1080], [3, 0x138f54c0], [3, 0x138f9900]]}
  layer.13.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x12e907c0]]}
  layer.13.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x1328ccc0]]}
  layer.13.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x12a72e40], [0, 0x12ab6e80], [0, 0x12afaec0], [0, 0x12b3ef00], [0, 0x12b82f40], [0, 0x12bc6f80], [0, 0x12c0afc0], [0, 0x12c4f000], [0, 0x12c93040], [0, 0x12cd7080], [0, 0x12d1b0c0], [0, 0x12d5f100],
      [0, 0x12da3140], [0, 0x12de7180], [0, 0x12e2b1c0], [0, 0x12e6f200], [0, 0x12eb3240], [0, 0x12ef7280], [0, 0x12f3b2c0], [0, 0x12f7f300], [0, 0x12fc3340], [0, 0x13007380], [0, 0x1304b3c0], [0, 0x1308f400],
      [0, 0x130d3440], [0, 0x13117480], [0, 0x1315b4c0], [0, 0x1319f500], [0, 0x131e3540], [0, 0x13227580], [0, 0x1326b5c0], [0, 0x132af600]]}
  layer.13.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x148dd840], [2, 0x148e6080], [2, 0x148ee8c0], [2, 0x148f7100], [2, 0x148ff940], [2, 0x14908180], [2, 0x149109c0], [2, 0x14919200]]}
  layer.13.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1568bd40],
      [4, 0x15713d80], [4, 0x1579bdc0], [4, 0x15823e00], [4, 0x158abe40], [4, 0x15933e80], [4, 0x159bbec0], [4, 0x15a43f00], [4, 0x15acbf40], [4, 0x15b53f80], [4, 0x15bdbfc0], [4, 0x15c64000], [4, 0x15cec040],
      [4, 0x15d74080], [4, 0x15dfc0c0], [4, 0x15e84100]]}
  layer.13.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14921a40],
      [2, 0x14923c80], [2, 0x14925ec0], [2, 0x14928100], [2, 0x1492a340], [2, 0x1492c580], [2, 0x1492e7c0], [2, 0x14930a00]]}
  layer.13.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x1329dd00]]}
  layer.13.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12ea1800]]}
  layer.14.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14932c40], [2, 0x14976c80], [2, 0x149bacc0], [2, 0x149fed00], [2, 0x14a42d40], [2, 0x14a86d80], [2, 0x14acadc0], [2, 0x14b0ee00]]}
  layer.14.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x15f0c140], [4, 0x15f10580], [4, 0x15f149c0], [4, 0x15f18e00]]}
  layer.14.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x14b52e40], [2, 0x14b96e80], [2, 0x14bdaec0], [2, 0x14c1ef00], [2, 0x14c62f40], [2, 0x14ca6f80], [2, 0x14ceafc0], [2, 0x14d2f000]]}
  layer.14.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x15f1d240], [4, 0x15f21680], [4, 0x15f25ac0], [4, 0x15f29f00]]}
  layer.14.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x15f2e340], [4, 0x15f72380], [4, 0x15fb63c0], [4, 0x15ffa400], [4, 0x1603e440], [4, 0x16082480], [4, 0x160c64c0], [4, 0x1610a500]]}
  layer.14.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x14d73040], [2, 0x14d77480], [2, 0x14d7b8c0], [2, 0x14d7fd00]]}
  layer.14.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x138fdd40], [3, 0x13941d80], [3, 0x13985dc0], [3, 0x139c9e00], [3, 0x13a0de40], [3, 0x13a51e80], [3, 0x13a95ec0], [3, 0x13ad9f00]]}
  layer.14.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x132aed40], [5, 0x132b3180], [5, 0x132b75c0], [5, 0x132bba00]]}
  layer.14.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x13b1df40]]}
  layer.14.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x132bfe40]]}
  layer.14.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x132f3640], [0, 0x13337680], [0, 0x1337b6c0], [0, 0x133bf700], [0, 0x13403740], [0, 0x13447780], [0, 0x1348b7c0], [0, 0x134cf800], [0, 0x13513840], [0, 0x13557880], [0, 0x1359b8c0], [0, 0x135df900],
      [0, 0x13623940], [0, 0x13667980], [0, 0x136ab9c0], [0, 0x136efa00], [0, 0x13733a40], [0, 0x13777a80], [0, 0x137bbac0], [0, 0x137ffb00], [0, 0x13843b40], [0, 0x13887b80], [0, 0x138cbbc0], [0, 0x1390fc00],
      [0, 0x13953c40], [0, 0x13997c80], [0, 0x139dbcc0], [0, 0x13a1fd00], [0, 0x13a63d40], [0, 0x13aa7d80], [0, 0x13aebdc0], [0, 0x13b2fe00]]}
  layer.14.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x12eb2840], [1, 0x12ebb080], [1, 0x12ec38c0], [1, 0x12ecc100], [1, 0x12ed4940], [1, 0x12edd180], [1, 0x12ee59c0], [1, 0x12eee200]]}
  layer.14.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x13b73e40],
      [0, 0x13bfbe80], [0, 0x13c83ec0], [0, 0x13d0bf00], [0, 0x13d93f40], [0, 0x13e1bf80], [0, 0x13ea3fc0], [0, 0x13f2c000], [0, 0x13fb4040], [0, 0x1403c080], [0, 0x140c40c0], [0, 0x1414c100], [0, 0x141d4140],
      [0, 0x1425c180], [0, 0x142e41c0], [0, 0x1436c200]]}
  layer.14.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1614e540],
      [4, 0x16150780], [4, 0x161529c0], [4, 0x16154c00], [4, 0x16156e40], [4, 0x16159080], [4, 0x1615b2c0], [4, 0x1615d500]]}
  layer.14.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x143f4240]]}
  layer.14.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14d84140]]}
  layer.15.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x13b2ef80], [3, 0x13b72fc0], [3, 0x13bb7000], [3, 0x13bfb040], [3, 0x13c3f080], [3, 0x13c830c0], [3, 0x13cc7100], [3, 0x13d0b140]]}
  layer.15.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x1615f740], [4, 0x16163b80], [4, 0x16167fc0], [4, 0x1616c400]]}
  layer.15.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x16170840], [4, 0x161b4880], [4, 0x161f88c0], [4, 0x1623c900], [4, 0x16280940], [4, 0x162c4980], [4, 0x163089c0], [4, 0x1634ca00]]}
  layer.15.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x14405280], [0, 0x144096c0], [0, 0x1440db00], [0, 0x14411f40]]}
  layer.15.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14416380], [0, 0x1445a3c0], [0, 0x1449e400], [0, 0x144e2440], [0, 0x14526480], [0, 0x1456a4c0], [0, 0x145ae500], [0, 0x145f2540]]}
  layer.15.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x14d95180], [2, 0x14d995c0], [2, 0x14d9da00], [2, 0x14da1e40]]}
  layer.15.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14636580], [0, 0x1467a5c0], [0, 0x146be600], [0, 0x14702640], [0, 0x14746680], [0, 0x1478a6c0], [0, 0x147ce700], [0, 0x14812740]]}
  layer.15.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14da6280], [2, 0x14daa6c0], [2, 0x14daeb00], [2, 0x14db2f40]]}
  layer.15.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14856780]]}
  layer.15.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14db7380]]}
  layer.15.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x13d4f180], [3, 0x13d931c0], [3, 0x13dd7200], [3, 0x13e1b240], [3, 0x13e5f280], [3, 0x13ea32c0], [3, 0x13ee7300], [3, 0x13f2b340], [3, 0x13f6f380], [3, 0x13fb33c0], [3, 0x13ff7400], [3, 0x1403b440],
      [3, 0x1407f480], [3, 0x140c34c0], [3, 0x14107500], [3, 0x1414b540], [3, 0x1418f580], [3, 0x141d35c0], [3, 0x14217600], [3, 0x1425b640], [3, 0x1429f680], [3, 0x142e36c0], [3, 0x14327700], [3, 0x1436b740],
      [3, 0x143af780], [3, 0x143f37c0], [3, 0x14437800], [3, 0x1447b840], [3, 0x144bf880], [3, 0x145038c0], [3, 0x14547900], [3, 0x1458b940]]}
  layer.15.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x132d0e80], [5, 0x132d96c0], [5, 0x132e1f00], [5, 0x132ea740], [5, 0x132f2f80], [5, 0x132fb7c0], [5, 0x13304000], [5, 0x1330c840]]}
  layer.15.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x12ef6a40],
      [1, 0x12f7ea80], [1, 0x13006ac0], [1, 0x1308eb00], [1, 0x13116b40], [1, 0x1319eb80], [1, 0x13226bc0], [1, 0x132aec00], [1, 0x13336c40], [1, 0x133bec80], [1, 0x13446cc0], [1, 0x134ced00], [1, 0x13556d40],
      [1, 0x135ded80], [1, 0x13666dc0], [1, 0x136eee00]]}
  layer.15.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13315080],
      [5, 0x133172c0], [5, 0x13319500], [5, 0x1331b740], [5, 0x1331d980], [5, 0x1331fbc0], [5, 0x13321e00], [5, 0x13324040]]}
  layer.15.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x148677c0]]}
  layer.15.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14dc83c0]]}
  layer.16.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x145cf980], [3, 0x146139c0], [3, 0x14657a00], [3, 0x1469ba40], [3, 0x146dfa80], [3, 0x14723ac0], [3, 0x14767b00], [3, 0x147abb40]]}
  layer.16.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x13326280], [5, 0x1332a6c0], [5, 0x1332eb00], [5, 0x13332f40]]}
  layer.16.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x147efb80], [3, 0x14833bc0], [3, 0x14877c00], [3, 0x148bbc40], [3, 0x148ffc80], [3, 0x14943cc0], [3, 0x14987d00], [3, 0x149cbd40]]}
  layer.16.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x13337380], [5, 0x1333b7c0], [5, 0x1333fc00], [5, 0x13344040]]}
  layer.16.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13348480], [5, 0x1338c4c0], [5, 0x133d0500], [5, 0x13414540], [5, 0x13458580], [5, 0x1349c5c0], [5, 0x134e0600], [5, 0x13524640]]}
  layer.16.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x13776e40], [1, 0x1377b280], [1, 0x1377f6c0], [1, 0x13783b00]]}
  layer.16.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13568680], [5, 0x135ac6c0], [5, 0x135f0700], [5, 0x13634740], [5, 0x13678780], [5, 0x136bc7c0], [5, 0x13700800], [5, 0x13744840]]}
  layer.16.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x13787f40], [1, 0x1378c380], [1, 0x137907c0], [1, 0x13794c00]]}
  layer.16.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x14878800]]}
  layer.16.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14dd9400]]}
  layer.16.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x14a0fd80], [3, 0x14a53dc0], [3, 0x14a97e00], [3, 0x14adbe40], [3, 0x14b1fe80], [3, 0x14b63ec0], [3, 0x14ba7f00], [3, 0x14bebf40], [3, 0x14c2ff80], [3, 0x14c73fc0], [3, 0x14cb8000], [3, 0x14cfc040],
      [3, 0x14d40080], [3, 0x14d840c0], [3, 0x14dc8100], [3, 0x14e0c140], [3, 0x14e50180], [3, 0x14e941c0], [3, 0x14ed8200], [3, 0x14f1c240], [3, 0x14f60280], [3, 0x14fa42c0], [3, 0x14fe8300], [3, 0x1502c340],
      [3, 0x15070380], [3, 0x150b43c0], [3, 0x150f8400], [3, 0x1513c440], [3, 0x15180480], [3, 0x151c44c0], [3, 0x15208500], [3, 0x1524c540]]}
  layer.16.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x13788880], [5, 0x137910c0], [5, 0x13799900], [5, 0x137a2140], [5, 0x137aa980], [5, 0x137b31c0], [5, 0x137bba00], [5, 0x137c4240]]}
  layer.16.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x13799040],
      [1, 0x13821080], [1, 0x138a90c0], [1, 0x13931100], [1, 0x139b9140], [1, 0x13a41180], [1, 0x13ac91c0], [1, 0x13b51200], [1, 0x13bd9240], [1, 0x13c61280], [1, 0x13ce92c0], [1, 0x13d71300], [1, 0x13df9340],
      [1, 0x13e81380], [1, 0x13f093c0], [1, 0x13f91400]]}
  layer.16.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x137cca80],
      [5, 0x137cecc0], [5, 0x137d0f00], [5, 0x137d3140], [5, 0x137d5380], [5, 0x137d75c0], [5, 0x137d9800], [5, 0x137dba40]]}
  layer.16.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x14019440]]}
  layer.16.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x15290580]]}
  layer.17.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x16390a40], [4, 0x163d4a80], [4, 0x16418ac0], [4, 0x1645cb00], [4, 0x164a0b40], [4, 0x164e4b80], [4, 0x16528bc0], [4, 0x1656cc00]]}
  layer.17.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x14889840], [0, 0x1488dc80], [0, 0x148920c0], [0, 0x14896500]]}
  layer.17.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x165b0c40], [4, 0x165f4c80], [4, 0x16638cc0], [4, 0x1667cd00], [4, 0x166c0d40], [4, 0x16704d80], [4, 0x16748dc0], [4, 0x1678ce00]]}
  layer.17.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x1489a940], [0, 0x1489ed80], [0, 0x148a31c0], [0, 0x148a7600]]}
  layer.17.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x148aba40], [0, 0x148efa80], [0, 0x14933ac0], [0, 0x14977b00], [0, 0x149bbb40], [0, 0x149ffb80], [0, 0x14a43bc0], [0, 0x14a87c00]]}
  layer.17.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x14dea440], [2, 0x14dee880], [2, 0x14df2cc0], [2, 0x14df7100]]}
  layer.17.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x152a15c0], [3, 0x152e5600], [3, 0x15329640], [3, 0x1536d680], [3, 0x153b16c0], [3, 0x153f5700], [3, 0x15439740], [3, 0x1547d780]]}
  layer.17.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x137ddc80], [5, 0x137e20c0], [5, 0x137e6500], [5, 0x137ea940]]}
  layer.17.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x154c17c0]]}
  layer.17.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x137eed80]]}
  layer.17.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x14acbc40], [0, 0x14b0fc80], [0, 0x14b53cc0], [0, 0x14b97d00], [0, 0x14bdbd40], [0, 0x14c1fd80], [0, 0x14c63dc0], [0, 0x14ca7e00], [0, 0x14cebe40], [0, 0x14d2fe80], [0, 0x14d73ec0], [0, 0x14db7f00],
      [0, 0x14dfbf40], [0, 0x14e3ff80], [0, 0x14e83fc0], [0, 0x14ec8000], [0, 0x14f0c040], [0, 0x14f50080], [0, 0x14f940c0], [0, 0x14fd8100], [0, 0x1501c140], [0, 0x15060180], [0, 0x150a41c0], [0, 0x150e8200],
      [0, 0x1512c240], [0, 0x15170280], [0, 0x151b42c0], [0, 0x151f8300], [0, 0x1523c340], [0, 0x15280380], [0, 0x152c43c0], [0, 0x15308400]]}
  layer.17.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1402a480], [1, 0x14032cc0], [1, 0x1403b500], [1, 0x14043d40], [1, 0x1404c580], [1, 0x14054dc0], [1, 0x1405d600], [1, 0x14065e40]]}
  layer.17.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1534c440],
      [0, 0x153d4480], [0, 0x1545c4c0], [0, 0x154e4500], [0, 0x1556c540], [0, 0x155f4580], [0, 0x1567c5c0], [0, 0x15704600], [0, 0x1578c640], [0, 0x15814680], [0, 0x1589c6c0], [0, 0x15924700], [0, 0x159ac740],
      [0, 0x15a34780], [0, 0x15abc7c0], [0, 0x15b44800]]}
  layer.17.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x167d0e40],
      [4, 0x167d3080], [4, 0x167d52c0], [4, 0x167d7500], [4, 0x167d9740], [4, 0x167db980], [4, 0x167ddbc0], [4, 0x167dfe00]]}
  layer.17.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x15bcc840]]}
  layer.17.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x154d2800]]}
  layer.18.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x154e3840], [3, 0x15527880], [3, 0x1556b8c0], [3, 0x155af900], [3, 0x155f3940], [3, 0x15637980], [3, 0x1567b9c0], [3, 0x156bfa00]]}
  layer.18.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x15bdd880], [0, 0x15be1cc0], [0, 0x15be6100], [0, 0x15bea540]]}
  layer.18.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x167e2040], [4, 0x16826080], [4, 0x1686a0c0], [4, 0x168ae100], [4, 0x168f2140], [4, 0x16936180], [4, 0x1697a1c0], [4, 0x169be200]]}
  layer.18.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x15bee980], [0, 0x15bf2dc0], [0, 0x15bf7200], [0, 0x15bfb640]]}
  layer.18.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x15bffa80], [0, 0x15c43ac0], [0, 0x15c87b00], [0, 0x15ccbb40], [0, 0x15d0fb80], [0, 0x15d53bc0], [0, 0x15d97c00], [0, 0x15ddbc40]]}
  layer.18.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x14dfb540], [2, 0x14dff980], [2, 0x14e03dc0], [2, 0x14e08200]]}
  layer.18.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x15e1fc80], [0, 0x15e63cc0], [0, 0x15ea7d00], [0, 0x15eebd40], [0, 0x15f2fd80], [0, 0x15f73dc0], [0, 0x15fb7e00], [0, 0x15ffbe40]]}
  layer.18.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14e0c640], [2, 0x14e10a80], [2, 0x14e14ec0], [2, 0x14e19300]]}
  layer.18.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x1603fe80]]}
  layer.18.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14e1d740]]}
  layer.18.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x15703a40], [3, 0x15747a80], [3, 0x1578bac0], [3, 0x157cfb00], [3, 0x15813b40], [3, 0x15857b80], [3, 0x1589bbc0], [3, 0x158dfc00], [3, 0x15923c40], [3, 0x15967c80], [3, 0x159abcc0], [3, 0x159efd00],
      [3, 0x15a33d40], [3, 0x15a77d80], [3, 0x15abbdc0], [3, 0x15affe00], [3, 0x15b43e40], [3, 0x15b87e80], [3, 0x15bcbec0], [3, 0x15c0ff00], [3, 0x15c53f40], [3, 0x15c97f80], [3, 0x15cdbfc0], [3, 0x15d20000],
      [3, 0x15d64040], [3, 0x15da8080], [3, 0x15dec0c0], [3, 0x15e30100], [3, 0x15e74140], [3, 0x15eb8180], [3, 0x15efc1c0], [3, 0x15f40200]]}
  layer.18.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x137ffdc0], [5, 0x13808600], [5, 0x13810e40], [5, 0x13819680], [5, 0x13821ec0], [5, 0x1382a700], [5, 0x13832f40], [5, 0x1383b780]]}
  layer.18.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1406e680],
      [1, 0x140f66c0], [1, 0x1417e700], [1, 0x14206740], [1, 0x1428e780], [1, 0x143167c0], [1, 0x1439e800], [1, 0x14426840], [1, 0x144ae880], [1, 0x145368c0], [1, 0x145be900], [1, 0x14646940], [1, 0x146ce980],
      [1, 0x147569c0], [1, 0x147dea00], [1, 0x14866a40]]}
  layer.18.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13843fc0],
      [5, 0x13846200], [5, 0x13848440], [5, 0x1384a680], [5, 0x1384c8c0], [5, 0x1384eb00], [5, 0x13850d40], [5, 0x13852f80]]}
  layer.18.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x16050ec0]]}
  layer.18.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14e2e780]]}
  layer.19.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x15f84240], [3, 0x15fc8280], [3, 0x1600c2c0], [3, 0x16050300], [3, 0x16094340], [3, 0x160d8380], [3, 0x1611c3c0], [3, 0x16160400]]}
  layer.19.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x138551c0], [5, 0x13859600], [5, 0x1385da40], [5, 0x13861e80]]}
  layer.19.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x161a4440], [3, 0x161e8480], [3, 0x1622c4c0], [3, 0x16270500], [3, 0x162b4540], [3, 0x162f8580], [3, 0x1633c5c0], [3, 0x16380600]]}
  layer.19.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x138662c0], [5, 0x1386a700], [5, 0x1386eb40], [5, 0x13872f80]]}
  layer.19.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x138773c0], [5, 0x138bb400], [5, 0x138ff440], [5, 0x13943480], [5, 0x139874c0], [5, 0x139cb500], [5, 0x13a0f540], [5, 0x13a53580]]}
  layer.19.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x148eea80], [1, 0x148f2ec0], [1, 0x148f7300], [1, 0x148fb740]]}
  layer.19.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13a975c0], [5, 0x13adb600], [5, 0x13b1f640], [5, 0x13b63680], [5, 0x13ba76c0], [5, 0x13beb700], [5, 0x13c2f740], [5, 0x13c73780]]}
  layer.19.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x148ffb80], [1, 0x14903fc0], [1, 0x14908400], [1, 0x1490c840]]}
  layer.19.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x14910c80]]}
  layer.19.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13cb77c0]]}
  layer.19.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x16061f00], [0, 0x160a5f40], [0, 0x160e9f80], [0, 0x1612dfc0], [0, 0x16172000], [0, 0x161b6040], [0, 0x161fa080], [0, 0x1623e0c0], [0, 0x16282100], [0, 0x162c6140], [0, 0x1630a180], [0, 0x1634e1c0],
      [0, 0x16392200], [0, 0x163d6240], [0, 0x1641a280], [0, 0x1645e2c0], [0, 0x164a2300], [0, 0x164e6340], [0, 0x1652a380], [0, 0x1656e3c0], [0, 0x165b2400], [0, 0x165f6440], [0, 0x1663a480], [0, 0x1667e4c0],
      [0, 0x166c2500], [0, 0x16706540], [0, 0x1674a580], [0, 0x1678e5c0], [0, 0x167d2600], [0, 0x16816640], [0, 0x1685a680], [0, 0x1689e6c0]]}
  layer.19.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x14e3f7c0], [2, 0x14e48000], [2, 0x14e50840], [2, 0x14e59080], [2, 0x14e618c0], [2, 0x14e6a100], [2, 0x14e72940], [2, 0x14e7b180]]}
  layer.19.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x16a02240],
      [4, 0x16a8a280], [4, 0x16b122c0], [4, 0x16b9a300], [4, 0x16c22340], [4, 0x16caa380], [4, 0x16d323c0], [4, 0x16dba400], [4, 0x16e42440], [4, 0x16eca480], [4, 0x16f524c0], [4, 0x16fda500], [4, 0x17062540],
      [4, 0x170ea580], [4, 0x171725c0], [4, 0x171fa600]]}
  layer.19.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x14e839c0],
      [2, 0x14e85c00], [2, 0x14e87e40], [2, 0x14e8a080], [2, 0x14e8c2c0], [2, 0x14e8e500], [2, 0x14e90740], [2, 0x14e92980]]}
  layer.19.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x13cc8800]]}
  layer.19.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x14921cc0]]}
  layer.20.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x14e94bc0], [2, 0x14ed8c00], [2, 0x14f1cc40], [2, 0x14f60c80], [2, 0x14fa4cc0], [2, 0x14fe8d00], [2, 0x1502cd40], [2, 0x15070d80]]}
  layer.20.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x17282640], [4, 0x17286a80], [4, 0x1728aec0], [4, 0x1728f300]]}
  layer.20.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x150b4dc0], [2, 0x150f8e00], [2, 0x1513ce40], [2, 0x15180e80], [2, 0x151c4ec0], [2, 0x15208f00], [2, 0x1524cf40], [2, 0x15290f80]]}
  layer.20.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x17293740], [4, 0x17297b80], [4, 0x1729bfc0], [4, 0x172a0400]]}
  layer.20.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x172a4840], [4, 0x172e8880], [4, 0x1732c8c0], [4, 0x17370900], [4, 0x173b4940], [4, 0x173f8980], [4, 0x1743c9c0], [4, 0x17480a00]]}
  layer.20.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x152d4fc0], [2, 0x152d9400], [2, 0x152dd840], [2, 0x152e1c80]]}
  layer.20.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x163c4640], [3, 0x16408680], [3, 0x1644c6c0], [3, 0x16490700], [3, 0x164d4740], [3, 0x16518780], [3, 0x1655c7c0], [3, 0x165a0800]]}
  layer.20.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13cd9840], [5, 0x13cddc80], [5, 0x13ce20c0], [5, 0x13ce6500]]}
  layer.20.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x165e4840]]}
  layer.20.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13cea940]]}
  layer.20.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x168e2700], [0, 0x16926740], [0, 0x1696a780], [0, 0x169ae7c0], [0, 0x169f2800], [0, 0x16a36840], [0, 0x16a7a880], [0, 0x16abe8c0], [0, 0x16b02900], [0, 0x16b46940], [0, 0x16b8a980], [0, 0x16bce9c0],
      [0, 0x16c12a00], [0, 0x16c56a40], [0, 0x16c9aa80], [0, 0x16cdeac0], [0, 0x16d22b00], [0, 0x16d66b40], [0, 0x16daab80], [0, 0x16deebc0], [0, 0x16e32c00], [0, 0x16e76c40], [0, 0x16ebac80], [0, 0x16efecc0],
      [0, 0x16f42d00], [0, 0x16f86d40], [0, 0x16fcad80], [0, 0x1700edc0], [0, 0x17052e00], [0, 0x17096e40], [0, 0x170dae80], [0, 0x1711eec0]]}
  layer.20.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x14932d00], [1, 0x1493b540], [1, 0x14943d80], [1, 0x1494c5c0], [1, 0x14954e00], [1, 0x1495d640], [1, 0x14965e80], [1, 0x1496e6c0]]}
  layer.20.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x17162f00],
      [0, 0x171eaf40], [0, 0x17272f80], [0, 0x172fafc0], [0, 0x17383000], [0, 0x1740b040], [0, 0x17493080], [0, 0x1751b0c0], [0, 0x175a3100], [0, 0x1762b140], [0, 0x176b3180], [0, 0x1773b1c0], [0, 0x177c3200],
      [0, 0x1784b240], [0, 0x178d3280], [0, 0x1795b2c0]]}
  layer.20.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x174c4a40],
      [4, 0x174c6c80], [4, 0x174c8ec0], [4, 0x174cb100], [4, 0x174cd340], [4, 0x174cf580], [4, 0x174d17c0], [4, 0x174d3a00]]}
  layer.20.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x179e3300]]}
  layer.20.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x152e60c0]]}
  layer.21.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x165f5880], [3, 0x166398c0], [3, 0x1667d900], [3, 0x166c1940], [3, 0x16705980], [3, 0x167499c0], [3, 0x1678da00], [3, 0x167d1a40]]}
  layer.21.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x179f4340], [0, 0x179f8780], [0, 0x179fcbc0], [0, 0x17a01000]]}
  layer.21.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x174d5c40], [4, 0x17519c80], [4, 0x1755dcc0], [4, 0x175a1d00], [4, 0x175e5d40], [4, 0x17629d80], [4, 0x1766ddc0], [4, 0x176b1e00]]}
  layer.21.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x17a05440], [0, 0x17a09880], [0, 0x17a0dcc0], [0, 0x17a12100]]}
  layer.21.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x17a16540], [0, 0x17a5a580], [0, 0x17a9e5c0], [0, 0x17ae2600], [0, 0x17b26640], [0, 0x17b6a680], [0, 0x17bae6c0], [0, 0x17bf2700]]}
  layer.21.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x152f7100], [2, 0x152fb540], [2, 0x152ff980], [2, 0x15303dc0]]}
  layer.21.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x17c36740], [0, 0x17c7a780], [0, 0x17cbe7c0], [0, 0x17d02800], [0, 0x17d46840], [0, 0x17d8a880], [0, 0x17dce8c0], [0, 0x17e12900]]}
  layer.21.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x15308200], [2, 0x1530c640], [2, 0x15310a80], [2, 0x15314ec0]]}
  layer.21.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [0, 0x17e56940]]}
  layer.21.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x15319300]]}
  layer.21.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x16815a80], [3, 0x16859ac0], [3, 0x1689db00], [3, 0x168e1b40], [3, 0x16925b80], [3, 0x16969bc0], [3, 0x169adc00], [3, 0x169f1c40], [3, 0x16a35c80], [3, 0x16a79cc0], [3, 0x16abdd00], [3, 0x16b01d40],
      [3, 0x16b45d80], [3, 0x16b89dc0], [3, 0x16bcde00], [3, 0x16c11e40], [3, 0x16c55e80], [3, 0x16c99ec0], [3, 0x16cddf00], [3, 0x16d21f40], [3, 0x16d65f80], [3, 0x16da9fc0], [3, 0x16dee000], [3, 0x16e32040],
      [3, 0x16e76080], [3, 0x16eba0c0], [3, 0x16efe100], [3, 0x16f42140], [3, 0x16f86180], [3, 0x16fca1c0], [3, 0x1700e200], [3, 0x17052240]]}
  layer.21.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x13cfb980], [5, 0x13d041c0], [5, 0x13d0ca00], [5, 0x13d15240], [5, 0x13d1da80], [5, 0x13d262c0], [5, 0x13d2eb00], [5, 0x13d37340]]}
  layer.21.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x14976f00],
      [1, 0x149fef40], [1, 0x14a86f80], [1, 0x14b0efc0], [1, 0x14b97000], [1, 0x14c1f040], [1, 0x14ca7080], [1, 0x14d2f0c0], [1, 0x14db7100], [1, 0x14e3f140], [1, 0x14ec7180], [1, 0x14f4f1c0], [1, 0x14fd7200],
      [1, 0x1505f240], [1, 0x150e7280], [1, 0x1516f2c0]]}
  layer.21.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x13d3fb80],
      [5, 0x13d41dc0], [5, 0x13d44000], [5, 0x13d46240], [5, 0x13d48480], [5, 0x13d4a6c0], [5, 0x13d4c900], [5, 0x13d4eb40]]}
  layer.21.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x17e67980]]}
  layer.21.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1532a340]]}
  layer.22.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x17096280], [3, 0x170da2c0], [3, 0x1711e300], [3, 0x17162340], [3, 0x171a6380], [3, 0x171ea3c0], [3, 0x1722e400], [3, 0x17272440]]}
  layer.22.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x13d50d80], [5, 0x13d551c0], [5, 0x13d59600], [5, 0x13d5da40]]}
  layer.22.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x172b6480], [3, 0x172fa4c0], [3, 0x1733e500], [3, 0x17382540], [3, 0x173c6580], [3, 0x1740a5c0], [3, 0x1744e600], [3, 0x17492640]]}
  layer.22.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x13d61e80], [5, 0x13d662c0], [5, 0x13d6a700], [5, 0x13d6eb40]]}
  layer.22.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13d72f80], [5, 0x13db6fc0], [5, 0x13dfb000], [5, 0x13e3f040], [5, 0x13e83080], [5, 0x13ec70c0], [5, 0x13f0b100], [5, 0x13f4f140]]}
  layer.22.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x151f7300], [1, 0x151fb740], [1, 0x151ffb80], [1, 0x15203fc0]]}
  layer.22.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x13f93180], [5, 0x13fd71c0], [5, 0x1401b200], [5, 0x1405f240], [5, 0x140a3280], [5, 0x140e72c0], [5, 0x1412b300], [5, 0x1416f340]]}
  layer.22.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x15208400], [1, 0x1520c840], [1, 0x15210c80], [1, 0x152150c0]]}
  layer.22.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [1, 0x15219500]]}
  layer.22.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x141b3380]]}
  layer.22.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x17e789c0], [0, 0x17ebca00], [0, 0x17f00a40], [0, 0x17f44a80], [0, 0x17f88ac0], [0, 0x17fccb00], [0, 0x18010b40], [0, 0x18054b80], [0, 0x18098bc0], [0, 0x180dcc00], [0, 0x18120c40], [0, 0x18164c80],
      [0, 0x181a8cc0], [0, 0x181ecd00], [0, 0x18230d40], [0, 0x18274d80], [0, 0x182b8dc0], [0, 0x182fce00], [0, 0x18340e40], [0, 0x18384e80], [0, 0x183c8ec0], [0, 0x1840cf00], [0, 0x18450f40], [0, 0x18494f80],
      [0, 0x184d8fc0], [0, 0x1851d000], [0, 0x18561040], [0, 0x185a5080], [0, 0x185e90c0], [0, 0x1862d100], [0, 0x18671140], [0, 0x186b5180]]}
  layer.22.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2,
        0x1533b380], [2, 0x15343bc0], [2, 0x1534c400], [2, 0x15354c40], [2, 0x1535d480], [2, 0x15365cc0], [2, 0x1536e500], [2, 0x15376d40]]}
  layer.22.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x176f5e40],
      [4, 0x1777de80], [4, 0x17805ec0], [4, 0x1788df00], [4, 0x17915f40], [4, 0x1799df80], [4, 0x17a25fc0], [4, 0x17aae000], [4, 0x17b36040], [4, 0x17bbe080], [4, 0x17c460c0], [4, 0x17cce100], [4, 0x17d56140],
      [4, 0x17dde180], [4, 0x17e661c0], [4, 0x17eee200]]}
  layer.22.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1537f580],
      [2, 0x153817c0], [2, 0x15383a00], [2, 0x15385c40], [2, 0x15387e80], [2, 0x1538a0c0], [2, 0x1538c300], [2, 0x1538e540]]}
  layer.22.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5,
        0x141c43c0]]}
  layer.22.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x1522a540]]}
  layer.23.attention.self.query.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [2, 0x15390780], [2, 0x153d47c0], [2, 0x15418800], [2, 0x1545c840], [2, 0x154a0880], [2, 0x154e48c0], [2, 0x15528900], [2, 0x1556c940]]}
  layer.23.attention.self.query.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x17f76240], [4, 0x17f7a680], [4, 0x17f7eac0], [4, 0x17f82f00]]}
  layer.23.attention.self.key.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x155b0980], [2, 0x155f49c0], [2, 0x15638a00], [2, 0x1567ca40], [2, 0x156c0a80], [2, 0x15704ac0], [2, 0x15748b00], [2, 0x1578cb40]]}
  layer.23.attention.self.key.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4,
        0x17f87340], [4, 0x17f8b780], [4, 0x17f8fbc0], [4, 0x17f94000]]}
  layer.23.attention.self.value.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [4, 0x17f98440], [4, 0x17fdc480], [4, 0x180204c0], [4, 0x18064500], [4, 0x180a8540], [4, 0x180ec580], [4, 0x181305c0], [4, 0x18174600]]}
  layer.23.attention.self.value.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x157d0b80], [2, 0x157d4fc0], [2, 0x157d9400], [2, 0x157dd840]]}
  layer.23.attention.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x174d6680], [3, 0x1751a6c0], [3, 0x1755e700], [3, 0x175a2740], [3, 0x175e6780], [3, 0x1762a7c0], [3, 0x1766e800], [3, 0x176b2840]]}
  layer.23.attention.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x141d5400], [5, 0x141d9840], [5, 0x141ddc80], [5, 0x141e20c0]]}
  layer.23.attention.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [3, 0x176f6880]]}
  layer.23.attention.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [
      [5, 0x141e6500]]}
  layer.23.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [8, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x186f91c0], [0, 0x1873d200], [0, 0x18781240], [0, 0x187c5280], [0, 0x188092c0], [0, 0x1884d300], [0, 0x18891340], [0, 0x188d5380], [0, 0x189193c0], [0, 0x1895d400], [0, 0x189a1440], [0, 0x189e5480],
      [0, 0x18a294c0], [0, 0x18a6d500], [0, 0x18ab1540], [0, 0x18af5580], [0, 0x18b395c0], [0, 0x18b7d600], [0, 0x18bc1640], [0, 0x18c05680], [0, 0x18c496c0], [0, 0x18c8d700], [0, 0x18cd1740], [0, 0x18d15780],
      [0, 0x18d597c0], [0, 0x18d9d800], [0, 0x18de1840], [0, 0x18e25880], [0, 0x18e698c0], [0, 0x18ead900], [0, 0x18ef1940], [0, 0x18f35980]]}
  layer.23.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1,
        0x1523b580], [1, 0x15243dc0], [1, 0x1524c600], [1, 0x15254e40], [1, 0x1525d680], [1, 0x15265ec0], [1, 0x1526e700], [1, 0x15276f40]]}
  layer.23.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 1], ublock: [4, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x18f799c0],
      [0, 0x19001a00], [0, 0x19089a40], [0, 0x19111a80], [0, 0x19199ac0], [0, 0x19221b00], [0, 0x192a9b40], [0, 0x19331b80], [0, 0x193b9bc0], [0, 0x19441c00], [0, 0x194c9c40], [0, 0x19551c80], [0, 0x195d9cc0],
      [0, 0x19661d00], [0, 0x196e9d40], [0, 0x19771d80]]}
  layer.23.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x181b8640],
      [4, 0x181ba880], [4, 0x181bcac0], [4, 0x181bed00], [4, 0x181c0f40], [4, 0x181c3180], [4, 0x181c53c0], [4, 0x181c7600]]}
  layer.23.output.LayerNorm.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0,
        0x197f9dc0]]}
  layer.23.output.LayerNorm.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x157e1c80]]}

  # constant
  input_1_multiply_2560_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1527f780]]}
  lc.input_tensor.softmax_2562.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x177078c0]]}
  lc.input_tensor.layernorm_2582.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15280040]]}
  lc.input_tensor.layernorm_2582.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x157f2cc0]]}
  dc.input_tensor.layernorm_2582.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x17708180], [3, 0x1770b4c0]]}
  lc.input_tensor.layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x141f7540]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x1980ae00]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x157f3580]]}
  lc.input_tensor.layernorm_2596.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x141f7e00]]}
  lc.input_tensor.layernorm_2596.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1980b6c0]]}
  dc.input_tensor.layernorm_2596.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x15280900], [1, 0x15283c40]]}
  lc.input_tensor.layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1770e800]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181c9840]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1980bf80]]}
  input_1_multiply_2613_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181ca100]]}
  lc.input_tensor.softmax_2615.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x141f86c0]]}
  lc.input_tensor.layernorm_2635.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1980c840]]}
  lc.input_tensor.layernorm_2635.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15286f80]]}
  dc.input_tensor.layernorm_2635.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x157f3e40], [2, 0x157f7180]]}
  lc.input_tensor.layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1980d100]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x15287840]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x1770f0c0]]}
  lc.input_tensor.layernorm_2649.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1770f980]]}
  lc.input_tensor.layernorm_2649.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181ca9c0]]}
  dc.input_tensor.layernorm_2649.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x141f8f80], [5, 0x141fc2c0]]}
  lc.input_tensor.layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1980d9c0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15288100]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17710240]]}
  input_1_multiply_2666_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x141ff600]]}
  lc.input_tensor.softmax_2668.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x141ffec0]]}
  lc.input_tensor.layernorm_2688.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17710b00]]}
  lc.input_tensor.layernorm_2688.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181cb280]]}
  dc.input_tensor.layernorm_2688.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x14200780], [5, 0x14203ac0]]}
  lc.input_tensor.layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152889c0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x157fa4c0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181cbb40]]}
  lc.input_tensor.layernorm_2702.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181cc400]]}
  lc.input_tensor.layernorm_2702.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14206e00]]}
  dc.input_tensor.layernorm_2702.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x1980e280], [0, 0x198115c0]]}
  lc.input_tensor.layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x157fad80]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x177113c0]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x19814900]]}
  input_1_multiply_2719_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x157fb640]]}
  lc.input_tensor.softmax_2721.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17711c80]]}
  lc.input_tensor.layernorm_2741.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x198151c0]]}
  lc.input_tensor.layernorm_2741.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x142076c0]]}
  dc.input_tensor.layernorm_2741.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x19815a80], [0, 0x19818dc0]]}
  lc.input_tensor.layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x157fbf00]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x17712540]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x14207f80]]}
  lc.input_tensor.layernorm_2755.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181cccc0]]}
  lc.input_tensor.layernorm_2755.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14208840]]}
  dc.input_tensor.layernorm_2755.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x1981c100], [0, 0x1981f440]]}
  lc.input_tensor.layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x157fc7c0]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17712e00]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14209100]]}
  input_1_multiply_2772_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x142099c0]]}
  lc.input_tensor.softmax_2774.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19822780]]}
  lc.input_tensor.layernorm_2794.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15289280]]}
  lc.input_tensor.layernorm_2794.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x157fd080]]}
  dc.input_tensor.layernorm_2794.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x177136c0], [3, 0x17716a00]]}
  lc.input_tensor.layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1420a280]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181cd580]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x157fd940]]}
  lc.input_tensor.layernorm_2808.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x157fe200]]}
  lc.input_tensor.layernorm_2808.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17719d40]]}
  dc.input_tensor.layernorm_2808.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x181cde40], [4, 0x181d1180]]}
  lc.input_tensor.layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x19823040]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15289b40]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1771a600]]}
  input_1_multiply_2825_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1420ab40]]}
  lc.input_tensor.softmax_2827.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19823900]]}
  lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1771aec0]]}
  lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181d44c0]]}
  dc.input_tensor.layernorm_2847.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1420b400], [5, 0x1420e740]]}
  lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1528a400]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x157feac0]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181d4d80]]}
  lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181d5640]]}
  lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14211a80]]}
  dc.input_tensor.layernorm_2861.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x198241c0], [0, 0x19827500]]}
  lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x157ff380]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1771b780]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14212340]]}
  input_1_multiply_2878_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181d5f00]]}
  lc.input_tensor.softmax_2880.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14212c00]]}
  lc.input_tensor.layernorm_2900.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1771c040]]}
  lc.input_tensor.layernorm_2900.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181d67c0]]}
  dc.input_tensor.layernorm_2900.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x142134c0], [5, 0x14216800]]}
  lc.input_tensor.layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1528acc0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x157ffc40]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181d7080]]}
  lc.input_tensor.layernorm_2914.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1528b580]]}
  lc.input_tensor.layernorm_2914.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x15800500]]}
  dc.input_tensor.layernorm_2914.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1771c900], [3, 0x1771fc40]]}
  lc.input_tensor.layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14219b40]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1982a840]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x15800dc0]]}
  input_1_multiply_2931_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1982b100]]}
  lc.input_tensor.softmax_2933.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1528be40]]}
  lc.input_tensor.layernorm_2953.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x15801680]]}
  lc.input_tensor.layernorm_2953.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x15801f40]]}
  dc.input_tensor.layernorm_2953.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x17722f80], [3, 0x177262c0]]}
  lc.input_tensor.layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1421a400]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181d7940]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x15802800]]}
  lc.input_tensor.layernorm_2967.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1982b9c0]]}
  lc.input_tensor.layernorm_2967.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181d8200]]}
  dc.input_tensor.layernorm_2967.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1421acc0], [5, 0x1421e000]]}
  lc.input_tensor.layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1528c700]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1528cfc0]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14221340]]}
  input_1_multiply_2984_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1982c280]]}
  lc.input_tensor.softmax_2986.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14221c00]]}
  lc.input_tensor.layernorm_3006.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17729600]]}
  lc.input_tensor.layernorm_3006.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181d8ac0]]}
  dc.input_tensor.layernorm_3006.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x142224c0], [5, 0x14225800]]}
  lc.input_tensor.layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1528d880]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x158030c0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181d9380]]}
  lc.input_tensor.layernorm_3020.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181d9c40]]}
  lc.input_tensor.layernorm_3020.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14228b40]]}
  dc.input_tensor.layernorm_3020.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x1982cb40], [0, 0x1982fe80]]}
  lc.input_tensor.layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x15803980]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17729ec0]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14229400]]}
  input_1_multiply_3037_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x198331c0]]}
  lc.input_tensor.softmax_3039.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1528e140]]}
  lc.input_tensor.layernorm_3059.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x15804240]]}
  lc.input_tensor.layernorm_3059.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14229cc0]]}
  dc.input_tensor.layernorm_3059.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x181da500], [4, 0x181dd840]]}
  lc.input_tensor.layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x19833a80]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x1528ea00]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[3, 0x1772a780]]}
  lc.input_tensor.layernorm_3073.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1772b040]]}
  lc.input_tensor.layernorm_3073.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1528f2c0]]}
  dc.input_tensor.layernorm_3073.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x15804b00], [2, 0x15807e40]]}
  lc.input_tensor.layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181e0b80]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1422a580]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1528fb80]]}
  input_1_multiply_3090_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19834340]]}
  lc.input_tensor.softmax_3092.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181e1440]]}
  lc.input_tensor.layernorm_3112.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1422ae40]]}
  lc.input_tensor.layernorm_3112.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19834c00]]}
  dc.input_tensor.layernorm_3112.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x15290440], [1, 0x15293780]]}
  lc.input_tensor.layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1772b900]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181e1d00]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x15296ac0]]}
  lc.input_tensor.layernorm_3126.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15297380]]}
  lc.input_tensor.layernorm_3126.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1580b180]]}
  dc.input_tensor.layernorm_3126.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1772c1c0], [3, 0x1772f500]]}
  lc.input_tensor.layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1422b700]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x198354c0]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17732840]]}
  input_1_multiply_3143_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1422bfc0]]}
  lc.input_tensor.softmax_3145.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19835d80]]}
  lc.input_tensor.layernorm_3165.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1422c880]]}
  lc.input_tensor.layernorm_3165.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19836640]]}
  dc.input_tensor.layernorm_3165.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x15297c40], [1, 0x1529af80]]}
  lc.input_tensor.layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17733100]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181e25c0]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x19836f00]]}
  lc.input_tensor.layernorm_3179.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1580ba40]]}
  lc.input_tensor.layernorm_3179.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x177339c0]]}
  dc.input_tensor.layernorm_3179.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x181e2e80], [4, 0x181e61c0]]}
  lc.input_tensor.layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x198377c0]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1529e2c0]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17734280]]}
  input_1_multiply_3196_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181e9500]]}
  lc.input_tensor.softmax_3198.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1422d140]]}
  lc.input_tensor.layernorm_3218.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19838080]]}
  lc.input_tensor.layernorm_3218.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x1529eb80]]}
  dc.input_tensor.layernorm_3218.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x1580c300], [2, 0x1580f640]]}
  lc.input_tensor.layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181e9dc0]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x1422da00]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x1529f440]]}
  lc.input_tensor.layernorm_3232.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1422e2c0]]}
  lc.input_tensor.layernorm_3232.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17734b40]]}
  dc.input_tensor.layernorm_3232.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        4, 0x181ea680], [4, 0x181ed9c0]]}
  lc.input_tensor.layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x19838940]]}
  lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1529fd00]]}
  lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17735400]]}
  input_1_multiply_3249_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x15812980]]}
  lc.input_tensor.softmax_3251.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19839200]]}
  lc.input_tensor.layernorm_3271.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152a05c0]]}
  lc.input_tensor.layernorm_3271.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x15813240]]}
  dc.input_tensor.layernorm_3271.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x17735cc0], [3, 0x17739000]]}
  lc.input_tensor.layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1422eb80]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[0, 0x19839ac0]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181f0d00]]}
  lc.input_tensor.layernorm_3285.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f15c0]]}
  lc.input_tensor.layernorm_3285.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1422f440]]}
  dc.input_tensor.layernorm_3285.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x1983a380], [0, 0x1983d6c0]]}
  lc.input_tensor.layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x15813b00]]}
  lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1773c340]]}
  lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x19840a00]]}
  input_1_multiply_3302_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x158143c0]]}
  lc.input_tensor.softmax_3304.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1773cc00]]}
  lc.input_tensor.layernorm_3324.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1773d4c0]]}
  lc.input_tensor.layernorm_3324.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f1e80]]}
  dc.input_tensor.layernorm_3324.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1422fd00], [5, 0x14233040]]}
  lc.input_tensor.layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152a0e80]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x15814c80]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181f2740]]}
  lc.input_tensor.layernorm_3338.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x198412c0]]}
  lc.input_tensor.layernorm_3338.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152a1740]]}
  dc.input_tensor.layernorm_3338.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x15815540], [2, 0x15818880]]}
  lc.input_tensor.layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181f3000]]}
  lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14236380]]}
  lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152a2000]]}
  input_1_multiply_3355_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f38c0]]}
  lc.input_tensor.softmax_3357.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14236c40]]}
  lc.input_tensor.layernorm_3377.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19841b80]]}
  lc.input_tensor.layernorm_3377.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1581bbc0]]}
  dc.input_tensor.layernorm_3377.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x1581c480], [2, 0x1581f7c0]]}
  lc.input_tensor.layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181f4180]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x14237500]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x152a28c0]]}
  lc.input_tensor.layernorm_3391.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152a3180]]}
  lc.input_tensor.layernorm_3391.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19842440]]}
  dc.input_tensor.layernorm_3391.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x15822b00], [2, 0x15825e40]]}
  lc.input_tensor.layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181f4a40]]}
  lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14237dc0]]}
  lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152a3a40]]}
  input_1_multiply_3408_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1773dd80]]}
  lc.input_tensor.softmax_3410.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f5300]]}
  lc.input_tensor.layernorm_3430.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14238680]]}
  lc.input_tensor.layernorm_3430.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19842d00]]}
  dc.input_tensor.layernorm_3430.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x152a4300], [1, 0x152a7640]]}
  lc.input_tensor.layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x1773e640]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x152aa980]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x152ab240]]}
  lc.input_tensor.layernorm_3444.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152abb00]]}
  lc.input_tensor.layernorm_3444.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x15829180]]}
  dc.input_tensor.layernorm_3444.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        3, 0x1773ef00], [3, 0x17742240]]}
  lc.input_tensor.layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14238f40]]}
  lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x198435c0]]}
  lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x15829a40]]}
  input_1_multiply_3461_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f5bc0]]}
  lc.input_tensor.softmax_3463.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14239800]]}
  lc.input_tensor.layernorm_3483.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17745580]]}
  lc.input_tensor.layernorm_3483.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f6480]]}
  dc.input_tensor.layernorm_3483.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1423a0c0], [5, 0x1423d400]]}
  lc.input_tensor.layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152ac3c0]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x1582a300]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181f6d40]]}
  lc.input_tensor.layernorm_3497.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19843e80]]}
  lc.input_tensor.layernorm_3497.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152acc80]]}
  dc.input_tensor.layernorm_3497.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x1582abc0], [2, 0x1582df00]]}
  lc.input_tensor.layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181f7600]]}
  lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14240740]]}
  lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152ad540]]}
  input_1_multiply_3514_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f7ec0]]}
  lc.input_tensor.softmax_3516.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14241000]]}
  lc.input_tensor.layernorm_3536.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19844740]]}
  lc.input_tensor.layernorm_3536.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x142418c0]]}
  dc.input_tensor.layernorm_3536.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x15831240], [2, 0x15834580]]}
  lc.input_tensor.layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181f8780]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x14242180]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x152ade00]]}
  lc.input_tensor.layernorm_3550.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17745e40]]}
  lc.input_tensor.layernorm_3550.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152ae6c0]]}
  dc.input_tensor.layernorm_3550.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x158378c0], [2, 0x1583ac00]]}
  lc.input_tensor.layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181f9040]]}
  lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14242a40]]}
  lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152aef80]]}
  input_1_multiply_3567_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19845000]]}
  lc.input_tensor.softmax_3569.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181f9900]]}
  lc.input_tensor.layernorm_3589.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14243300]]}
  lc.input_tensor.layernorm_3589.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x198458c0]]}
  dc.input_tensor.layernorm_3589.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x152af840], [1, 0x152b2b80]]}
  lc.input_tensor.layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17746700]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181fa1c0]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181faa80]]}
  lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181fb340]]}
  lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14243bc0]]}
  dc.input_tensor.layernorm_3603.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x19846180], [0, 0x198494c0]]}
  lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x1583df40]]}
  lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17746fc0]]}
  lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1984c800]]}
  input_1_multiply_3620_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1583e800]]}
  lc.input_tensor.softmax_3622.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17747880]]}
  lc.input_tensor.layernorm_3642.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17748140]]}
  lc.input_tensor.layernorm_3642.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181fbc00]]}
  dc.input_tensor.layernorm_3642.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x14244480], [5, 0x142477c0]]}
  lc.input_tensor.layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152b5ec0]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x1583f0c0]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181fc4c0]]}
  lc.input_tensor.layernorm_3656.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1984d0c0]]}
  lc.input_tensor.layernorm_3656.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152b6780]]}
  dc.input_tensor.layernorm_3656.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x1583f980], [2, 0x15842cc0]]}
  lc.input_tensor.layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181fcd80]]}
  lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1424ab00]]}
  lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152b7040]]}
  input_1_multiply_3673_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181fd640]]}
  lc.input_tensor.softmax_3675.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1424b3c0]]}
  lc.input_tensor.layernorm_3695.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1984d980]]}
  lc.input_tensor.layernorm_3695.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1424bc80]]}
  dc.input_tensor.layernorm_3695.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x15846000], [2, 0x15849340]]}
  lc.input_tensor.layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181fdf00]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[5, 0x1424c540]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[1, 0x152b7900]]}
  lc.input_tensor.layernorm_3709.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17748a00]]}
  lc.input_tensor.layernorm_3709.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152b81c0]]}
  dc.input_tensor.layernorm_3709.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x1584c680], [2, 0x1584f9c0]]}
  lc.input_tensor.layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x181fe7c0]]}
  lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x1424ce00]]}
  lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152b8a80]]}
  input_1_multiply_3726_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1984e240]]}
  lc.input_tensor.softmax_3728.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x181ff080]]}
  lc.input_tensor.layernorm_3748.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1424d6c0]]}
  lc.input_tensor.layernorm_3748.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1984eb00]]}
  dc.input_tensor.layernorm_3748.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        1, 0x152b9340], [1, 0x152bc680]]}
  lc.input_tensor.layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x177492c0]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x181ff940]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x18200200]]}
  lc.input_tensor.layernorm_3762.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x18200ac0]]}
  lc.input_tensor.layernorm_3762.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1424df80]]}
  dc.input_tensor.layernorm_3762.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        0, 0x1984f3c0], [0, 0x19852700]]}
  lc.input_tensor.layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x15852d00]]}
  lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17749b80]]}
  lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x19855a40]]}
  input_1_multiply_3779_tile_bcast_tile_bcast: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x158535c0]]}
  lc.input_tensor.softmax_3781.dc.reduce_sum.1.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1774a440]]}
  lc.input_tensor.layernorm_3801.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x1774ad00]]}
  lc.input_tensor.layernorm_3801.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x18201380]]}
  dc.input_tensor.layernorm_3801.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        5, 0x1424e840], [5, 0x14251b80]]}
  lc.input_tensor.layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152bf9c0]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[2, 0x15853e80]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b,
    target_device: 0, loc: dram, dram: [[4, 0x18201c40]]}
  lc.input_tensor.layernorm_3815.dc.reduce_avg.0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x19856300]]}
  lc.input_tensor.layernorm_3815.dc.reduce_avg.3.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152c0280]]}
  dc.input_tensor.layernorm_3815.4: {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[
        2, 0x15854740], [2, 0x15857a80]]}
  lc.input_tensor.layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x18202500]]}
  lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x14254ec0]]}
  lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x152c0b40]]}

  # epoch_to_epoch
  e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0: {input: buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x1585adc0], [2, 0x15926e00]]}
  e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {input: layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1774b5c0], [3, 0x17751c00]]}
  e2e_layernorm_2596.dc.subtract.1_0: {input: layernorm_2596.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x18202dc0], [4, 0x182cee00]]}
  e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0: {input: buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x14255780], [5, 0x143217c0]]}
  e2e_add_2634_0: {input: add_2634, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x19856bc0],
      [0, 0x19922c00]]}
  e2e_gelu_2641_0: {input: gelu_2641, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x159f2e40],
      [2, 0x15abee80], [2, 0x15b8aec0], [2, 0x15c56f00], [2, 0x15d22f40], [2, 0x15deef80], [2, 0x15ebafc0], [2, 0x15f87000]]}
  e2e_layernorm_2635.dc.add.10_0: {input: layernorm_2635.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x152c1400], [1, 0x1538d440]]}
  e2e_softmax_2668.dc.exp.0_0: {input: softmax_2668.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x143ed800], [5, 0x14651840], [5, 0x148b5880], [5, 0x14b198c0]]}
  e2e_softmax_2668.dc.reciprocal.2_0: {input: softmax_2668.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17758240], [3, 0x177be280]]}
  e2e_layernorm_2649.dc.add.10_0: {input: layernorm_2649.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1839ae40], [4, 0x18466e80]]}
  e2e_layernorm_2688.dc.add.10_0: {input: layernorm_2688.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x199eec40], [0, 0x19abac80]]}
  e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0: {input: buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16053040], [2, 0x1611f080]]}
  e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {input: layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x15459480], [1, 0x1545fac0]]}
  e2e_layernorm_2741.dc.subtract.1_0: {input: layernorm_2741.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x178242c0], [3, 0x178f0300]]}
  e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0: {input: buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x18532ec0], [4, 0x185fef00]]}
  e2e_gelu_2747_0: {input: gelu_2747, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x19b86cc0],
      [0, 0x19c52d00], [0, 0x19d1ed40], [0, 0x19dead80], [0, 0x19eb6dc0], [0, 0x19f82e00], [0, 0x1a04ee40], [0, 0x1a11ae80]]}
  e2e_layernorm_2741.dc.add.10_0: {input: layernorm_2741.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x14d7d900], [5, 0x14e49940]]}
  e2e_softmax_2774.dc.exp.0_0: {input: softmax_2774.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x161eb0c0], [2, 0x1644f100], [2, 0x166b3140], [2, 0x16917180]]}
  e2e_layernorm_2755.dc.add.10_0: {input: layernorm_2755.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x15466100], [1, 0x15532140]]}
  e2e_layernorm_2794.dc.add.10_0: {input: layernorm_2794.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x179bc340], [3, 0x17a88380]]}
  e2e_layernorm_2808.dc.reciprocal.7_0: {input: layernorm_2808.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x186caf40], [4, 0x186d1580]]}
  e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0: {input: buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x14f15980], [5, 0x14fe19c0]]}
  e2e_layernorm_2847.dc.subtract.1_0: {input: layernorm_2847.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1a1e6ec0], [0, 0x1a2b2f00]]}
  e2e_matmul_2856_0: {input: matmul_2856, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x155fe180],
      [1, 0x156179c0], [1, 0x15631200], [1, 0x1564aa40], [1, 0x15664280], [1, 0x1567dac0], [1, 0x15697300], [1, 0x156b0b40], [1, 0x156ca380], [1, 0x156e3bc0], [1, 0x156fd400], [1, 0x15716c40], [1, 0x15730480],
      [1, 0x15749cc0], [1, 0x15763500], [1, 0x1577cd40]]}
  e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0: {input: buffer_0_layernorm_2847.dc.add.10_add_2860, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16b7b1c0], [2, 0x16c47200]]}
  e2e_softmax_2880.dc.multiply.3_0: {input: softmax_2880.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x186d7bc0], [4, 0x18b9fc00]]}
  e2e_matmul_2884_0: {input: matmul_2884, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x150ada00],
      [5, 0x150e0a40], [5, 0x15113a80], [5, 0x15146ac0], [5, 0x15179b00], [5, 0x151acb40], [5, 0x151dfb80], [5, 0x15212bc0]]}
  e2e_layernorm_2861.dc.add.10_0: {input: layernorm_2861.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x17b543c0], [3, 0x17c20400]]}
  e2e_layernorm_2900.dc.add.10_0: {input: layernorm_2900.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1a37ef40], [0, 0x1a44af80]]}
  e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0: {input: buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x16d13240], [2, 0x16ddf280]]}
  e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {input: layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x15796580], [1, 0x1579cbc0]]}
  e2e_layernorm_2953.dc.subtract.1_0: {input: layernorm_2953.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17cec440], [3, 0x17db8480]]}
  e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0: {input: buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x19067c40], [4, 0x19133c80]]}
  e2e_gelu_2959_0: {input: gelu_2959, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1a516fc0],
      [0, 0x1a5e3000], [0, 0x1a6af040], [0, 0x1a77b080], [0, 0x1a8470c0], [0, 0x1a913100], [0, 0x1a9df140], [0, 0x1aaab180]]}
  e2e_layernorm_2953.dc.add.10_0: {input: layernorm_2953.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x15245c00], [5, 0x15311c40]]}
  e2e_softmax_2986.dc.exp.0_0: {input: softmax_2986.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x16eab2c0], [2, 0x1710f300], [2, 0x17373340], [2, 0x175d7380]]}
  e2e_softmax_2986.dc.reciprocal.2_0: {input: softmax_2986.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x17e844c0], [3, 0x17eea500]]}
  e2e_layernorm_2967.dc.add.10_0: {input: layernorm_2967.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x157a3200], [1, 0x1586f240]]}
  e2e_layernorm_3006.dc.add.10_0: {input: layernorm_3006.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x191ffcc0], [4, 0x192cbd00]]}
  e2e_layernorm_3020.dc.reciprocal.7_0: {input: layernorm_3020.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x153ddc80], [5, 0x153e42c0]]}
  e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0: {input: buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1ab771c0], [0, 0x1ac43200]]}
  e2e_layernorm_3059.dc.subtract.1_0: {input: layernorm_3059.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x1593b280], [1, 0x15a072c0]]}
  e2e_gelu_3065_0: {input: gelu_3065, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x17f50540],
      [3, 0x1801c580], [3, 0x180e85c0], [3, 0x181b4600], [3, 0x18280640], [3, 0x1834c680], [3, 0x184186c0], [3, 0x184e4700]]}
  e2e_layernorm_3059.dc.add.10_0: {input: layernorm_3059.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1783b3c0], [2, 0x17907400]]}
  e2e_softmax_3092.dc.exp.0_0: {input: softmax_3092.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x19397d40], [4, 0x195fbd80], [4, 0x1985fdc0], [4, 0x19ac3e00]]}
  e2e_layernorm_3073.dc.add.10_0: {input: layernorm_3073.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x153ea900], [5, 0x154b6940]]}
  e2e_layernorm_3112.dc.add.10_0: {input: layernorm_3112.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1ad0f240], [0, 0x1addb280]]}
  e2e_layernorm_3126.dc.reciprocal.7_0: {input: layernorm_3126.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15ad3300], [1, 0x15ad9940]]}
  e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0: {input: buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x179d3440], [2, 0x17a9f480]]}
  e2e_layernorm_3165.dc.subtract.1_0: {input: layernorm_3165.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x185b0740], [3, 0x1867c780]]}
  e2e_matmul_3174_0: {input: matmul_3174, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x19d27e40],
      [4, 0x19d41680], [4, 0x19d5aec0], [4, 0x19d74700], [4, 0x19d8df40], [4, 0x19da7780], [4, 0x19dc0fc0], [4, 0x19dda800], [4, 0x19df4040], [4, 0x19e0d880], [4, 0x19e270c0], [4, 0x19e40900], [4, 0x19e5a140],
      [4, 0x19e73980], [4, 0x19e8d1c0], [4, 0x19ea6a00]]}
  e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0: {input: buffer_0_layernorm_3165.dc.add.10_add_3178, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x15582980], [5, 0x1564e9c0]]}
  e2e_softmax_3198.dc.multiply.3_0: {input: softmax_3198.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x15adff80], [1, 0x15fa7fc0]]}
  e2e_matmul_3202_0: {input: matmul_3202, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x17b6b4c0],
      [2, 0x17b9e500], [2, 0x17bd1540], [2, 0x17c04580], [2, 0x17c375c0], [2, 0x17c6a600], [2, 0x17c9d640], [2, 0x17cd0680]]}
  e2e_layernorm_3179.dc.add.10_0: {input: layernorm_3179.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1aea72c0], [0, 0x1af73300]]}
  e2e_layernorm_3218.dc.add.10_0: {input: layernorm_3218.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x187487c0], [3, 0x18814800]]}
  e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0: {input: buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x19ec0240], [4, 0x19f8c280]]}
  e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {input: layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1571aa00], [5, 0x15721040]]}
  e2e_layernorm_3271.dc.subtract.1_0: {input: layernorm_3271.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1b03f340], [0, 0x1b10b380]]}
  e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0: {input: buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x16470000], [1, 0x1653c040]]}
  e2e_gelu_3277_0: {input: gelu_3277, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x188e0840],
      [3, 0x189ac880], [3, 0x18a788c0], [3, 0x18b44900], [3, 0x18c10940], [3, 0x18cdc980], [3, 0x18da89c0], [3, 0x18e74a00]]}
  e2e_layernorm_3271.dc.add.10_0: {input: layernorm_3271.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x17d036c0], [2, 0x17dcf700]]}
  e2e_softmax_3304.dc.exp.0_0: {input: softmax_3304.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x15727680], [5, 0x1598b6c0], [5, 0x15bef700], [5, 0x15e53740]]}
  e2e_softmax_3304.dc.reciprocal.2_0: {input: softmax_3304.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1b1d73c0], [0, 0x1b23d400]]}
  e2e_layernorm_3285.dc.add.10_0: {input: layernorm_3285.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1a0582c0], [4, 0x1a124300]]}
  e2e_layernorm_3324.dc.add.10_0: {input: layernorm_3324.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x16608080], [1, 0x166d40c0]]}
  e2e_layernorm_3338.dc.reciprocal.7_0: {input: layernorm_3338.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[2, 0x17e9b740], [2, 0x17ea1d80]]}
  e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0: {input: buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x18f40a40], [3, 0x1900ca80]]}
  e2e_layernorm_3377.dc.subtract.1_0: {input: layernorm_3377.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x1a1f0340], [4, 0x1a2bc380]]}
  e2e_gelu_3383_0: {input: gelu_3383, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1b2a3440],
      [0, 0x1b36f480], [0, 0x1b43b4c0], [0, 0x1b507500], [0, 0x1b5d3540], [0, 0x1b69f580], [0, 0x1b76b5c0], [0, 0x1b837600]]}
  e2e_layernorm_3377.dc.add.10_0: {input: layernorm_3377.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x160b7780], [5, 0x161837c0]]}
  e2e_softmax_3410.dc.exp.0_0: {input: softmax_3410.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x17ea83c0], [2, 0x1810c400], [2, 0x18370440], [2, 0x185d4480]]}
  e2e_layernorm_3391.dc.add.10_0: {input: layernorm_3391.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x167a0100], [1, 0x1686c140]]}
  e2e_layernorm_3430.dc.add.10_0: {input: layernorm_3430.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x190d8ac0], [3, 0x191a4b00]]}
  e2e_layernorm_3444.dc.reciprocal.7_0: {input: layernorm_3444.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x1a3883c0], [4, 0x1a38ea00]]}
  e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0: {input: buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x1624f800], [5, 0x1631b840]]}
  e2e_layernorm_3483.dc.subtract.1_0: {input: layernorm_3483.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[0, 0x1b903640], [0, 0x1b9cf680]]}
  e2e_matmul_3492_0: {input: matmul_3492, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x16938180],
      [1, 0x169519c0], [1, 0x1696b200], [1, 0x16984a40], [1, 0x1699e280], [1, 0x169b7ac0], [1, 0x169d1300], [1, 0x169eab40], [1, 0x16a04380], [1, 0x16a1dbc0], [1, 0x16a37400], [1, 0x16a50c40], [1, 0x16a6a480],
      [1, 0x16a83cc0], [1, 0x16a9d500], [1, 0x16ab6d40]]}
  e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0: {input: buffer_0_layernorm_3483.dc.add.10_add_3496, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x188384c0], [2, 0x18904500]]}
  e2e_softmax_3516.dc.multiply.3_0: {input: softmax_3516.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[4, 0x1a395040], [4, 0x1a85d080]]}
  e2e_matmul_3520_0: {input: matmul_3520, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x163e7880],
      [5, 0x1641a8c0], [5, 0x1644d900], [5, 0x16480940], [5, 0x164b3980], [5, 0x164e69c0], [5, 0x16519a00], [5, 0x1654ca40]]}
  e2e_layernorm_3497.dc.add.10_0: {input: layernorm_3497.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[3, 0x19270b40], [3, 0x1933cb80]]}
  e2e_layernorm_3536.dc.add.10_0: {input: layernorm_3536.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1ba9b6c0], [0, 0x1bb67700]]}
  e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0: {input: buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x16ad0580], [1, 0x16b9c5c0]]}
  e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {input: layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r,
    df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x189d0540], [2, 0x189d6b80]]}
  e2e_layernorm_3589.dc.subtract.1_0: {input: layernorm_3589.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x19408bc0], [3, 0x194d4c00]]}
  e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0: {input: buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1ad250c0], [4, 0x1adf1100]]}
  e2e_gelu_3595_0: {input: gelu_3595, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1bc33740],
      [0, 0x1bcff780], [0, 0x1bdcb7c0], [0, 0x1be97800], [0, 0x1bf63840], [0, 0x1c02f880], [0, 0x1c0fb8c0], [0, 0x1c1c7900]]}
  e2e_layernorm_3589.dc.add.10_0: {input: layernorm_3589.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x1657fa80], [5, 0x1664bac0]]}
  e2e_softmax_3622.dc.exp.0_0: {input: softmax_3622.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x189dd1c0], [2, 0x18c41200], [2, 0x18ea5240], [2, 0x19109280]]}
  e2e_softmax_3622.dc.reciprocal.2_0: {input: softmax_3622.dc.reciprocal.2, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[3, 0x195a0c40], [3, 0x19606c80]]}
  e2e_layernorm_3603.dc.add.10_0: {input: layernorm_3603.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[1, 0x16c68600], [1, 0x16d34640]]}
  e2e_layernorm_3642.dc.add.10_0: {input: layernorm_3642.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1aebd140], [4, 0x1af89180]]}
  e2e_layernorm_3656.dc.reciprocal.7_0: {input: layernorm_3656.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[5, 0x16717b00], [5, 0x1671e140]]}
  e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0: {input: buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1c293940], [0, 0x1c35f980]]}
  e2e_layernorm_3695.dc.subtract.1_0: {input: layernorm_3695.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x16e00680], [1, 0x16ecc6c0]]}
  e2e_gelu_3701_0: {input: gelu_3701, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x1966ccc0],
      [3, 0x19738d00], [3, 0x19804d40], [3, 0x198d0d80], [3, 0x1999cdc0], [3, 0x19a68e00], [3, 0x19b34e40], [3, 0x19c00e80]]}
  e2e_layernorm_3695.dc.add.10_0: {input: layernorm_3695.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[2, 0x1936d2c0], [2, 0x19439300]]}
  e2e_softmax_3728.dc.exp.0_0: {input: softmax_3728.dc.exp.0, type: queue, entries: 2, grid_size: [2, 2], t: 16, mblock: [3, 3], ublock: [2, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[5, 0x16724780], [5, 0x169887c0], [5, 0x16bec800], [5, 0x16e50840]]}
  e2e_layernorm_3709.dc.add.10_0: {input: layernorm_3709.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[4, 0x1b0551c0], [4, 0x1b121200]]}
  e2e_layernorm_3748.dc.add.10_0: {input: layernorm_3748.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram,
    dram: [[0, 0x1c42b9c0], [0, 0x1c4f7a00]]}
  e2e_layernorm_3762.dc.reciprocal.7_0: {input: layernorm_3762.dc.reciprocal.7, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0,
    loc: dram, dram: [[1, 0x16f98700], [1, 0x16f9ed40]]}
  e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0: {input: buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1,
    mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x19505340], [2, 0x195d1380]]}
  e2e_matmul_3792_0: {input: matmul_3792, type: queue, entries: 2, grid_size: [3, 2], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x19cccec0],
      [3, 0x19d10f00], [3, 0x19d54f40], [3, 0x19d98f80], [3, 0x19ddcfc0], [3, 0x19e21000]]}
  e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0: {input: buffer_0_layernorm_3762.dc.add.10_add_3800, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c,
    df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x1b1ed240], [4, 0x1b2b9280]]}
  e2e_matmul_3804_0: {input: matmul_3804, type: queue, entries: 2, grid_size: [4, 8], t: 1, mblock: [3, 4], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x170b4880],
      [5, 0x170e78c0], [5, 0x1711a900], [5, 0x1714d940], [5, 0x17180980], [5, 0x171b39c0], [5, 0x171e6a00], [5, 0x17219a40], [5, 0x1724ca80], [5, 0x1727fac0], [5, 0x172b2b00], [5, 0x172e5b40], [5, 0x17318b80],
      [5, 0x1734bbc0], [5, 0x1737ec00], [5, 0x173b1c40], [5, 0x173e4c80], [5, 0x17417cc0], [5, 0x1744ad00], [5, 0x1747dd40], [5, 0x174b0d80], [5, 0x174e3dc0], [5, 0x17516e00], [5, 0x17549e40], [5, 0x1757ce80],
      [5, 0x175afec0], [5, 0x175e2f00], [5, 0x17615f40], [5, 0x17648f80], [5, 0x1767bfc0], [5, 0x176af000], [5, 0x176e2040]]}
  e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0: {input: buffer_0_layernorm_3801.dc.add.10_add_3814, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c,
    df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x1c5c3a40], [0, 0x1c68fa80]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 0
    input_count: 2
    matmul_2546: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {
        bias: true, m_k: 16, u_kt: 2}}
    matmul_2552: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 16, u_kt: 2}}
    matmul_2558: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_2546, matmul_2552], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2560: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_2558, input_1_multiply_2560_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2561: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [multiply_2560, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2562.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2561], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2562.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_2562.dc.exp.0, lc.input_tensor.softmax_2562.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2562.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2562.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2562.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2562.dc.exp.0, softmax_2562.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2566: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {
        bias: true, m_k: 16, u_kt: 2}}
    matmul_2573: {type: matmul, grid_loc: [4, 4], grid_size: [3, 2], inputs: [softmax_2562.dc.multiply.3, matmul_2566], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2577: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_2573, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_2581: {type: add, grid_loc: [4, 6], grid_size: [2, 1], inputs: [matmul_2577, hidden_states], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_2581, lc.input_tensor.layernorm_2582.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2582.dc.subtract.1: {type: subtract, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_2581, layernorm_2582.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2582.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2582.dc.subtract.1, layernorm_2582.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_2582.dc.multiply.2, lc.input_tensor.layernorm_2582.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2582.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2582.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2582.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2582.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2582.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2582.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2582.dc.reciprocal.7, lc.input_tensor.layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_2582.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8: {type: nop, grid_loc: [7, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_1_temporal_epoch_1:
    target_device: 0
    input_count: 2
    layernorm_2582.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0, e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.0.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2582.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2582.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2582.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2582.dc.multiply.9, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2585: {type: matmul, grid_loc: [2, 0], grid_size: [4, 8], inputs: [layernorm_2582.dc.add.10, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2588: {type: gelu, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_2585], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_2591: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_2588, layer.0.output.dense.weight, layer.0.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2582.dc.add.10_add_2595: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2582.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        126], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2595: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2591, buffer_0_layernorm_2582.dc.add.10_add_2595], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        0, 98], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_2595, lc.input_tensor.layernorm_2596.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2596.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_2595, layernorm_2596.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_2596.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_2_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2_temporal_epoch_2:
    target_device: 0
    input_count: 2
    layernorm_2596.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2596.dc.subtract.1_0, e2e_layernorm_2596.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_2596.dc.multiply.2, lc.input_tensor.layernorm_2596.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2596.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2596.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2596.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2596.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2596.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2596.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2596.dc.reciprocal.7, lc.input_tensor.layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2596.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0, layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2596.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_2596.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2596.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2596.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2599: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [layernorm_2596.dc.add.10, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2605: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_2596.dc.add.10, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2611: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_2599, matmul_2605], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2613: {type: multiply, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_2611, input_1_multiply_2613_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2614: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [multiply_2613, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2615.dc.exp.0: {type: exp, grid_loc: [4, 5], grid_size: [2, 2], inputs: [add_2614], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2615.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [softmax_2615.dc.exp.0, lc.input_tensor.softmax_2615.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2615.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_2615.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2615.dc.multiply.3: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [softmax_2615.dc.exp.0, softmax_2615.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2619: {type: matmul, grid_loc: [6, 2], grid_size: [2, 4], inputs: [layernorm_2596.dc.add.10, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2626: {type: matmul, grid_loc: [7, 6], grid_size: [3, 2], inputs: [softmax_2615.dc.multiply.3, matmul_2619], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2630: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2626, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2596.dc.add.10_add_2634: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2596.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2634: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [matmul_2630, buffer_0_layernorm_2596.dc.add.10_add_2634], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3_temporal_epoch_3:
    target_device: 0
    input_count: 2
    layernorm_2635.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_2634_0, lc.input_tensor.layernorm_2635.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2,
        1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2635.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_2634_0, layernorm_2635.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2635.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2635.dc.subtract.1, layernorm_2635.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2635.dc.multiply.2, lc.input_tensor.layernorm_2635.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2635.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2635.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2635.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_2635.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2635.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2635.dc.reciprocal.7, lc.input_tensor.layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2635.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_2_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2635.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_layernorm_2635.dc.subtract.1_layernorm_2635.dc.multiply.8, layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.1.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2635.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_2635.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2635.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2635.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2638: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2635.dc.add.10, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2641: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2638], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_4_temporal_epoch_4:
    target_device: 0
    input_count: 2
    matmul_2644: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_2641_0, layer.1.output.dense.weight, layer.1.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2635.dc.add.10_add_2648: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2635.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2648: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_2644, buffer_0_layernorm_2635.dc.add.10_add_2648], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2648, lc.input_tensor.layernorm_2649.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2649.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_2648, layernorm_2649.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2649.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2649.dc.subtract.1, layernorm_2649.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2649.dc.multiply.2, lc.input_tensor.layernorm_2649.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2649.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2649.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2649.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2649.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2649.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2649.dc.reciprocal.7, lc.input_tensor.layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2649.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2649.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2649.dc.subtract.1_layernorm_2649.dc.multiply.8, layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2649.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2649.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2649.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2649.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2652: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_2649.dc.add.10, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2658: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2649.dc.add.10, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2664: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_2652, matmul_2658], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2666: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_2664, input_1_multiply_2666_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2667: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_2666, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2668.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_2667], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2668.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_2668.dc.exp.0, lc.input_tensor.softmax_2668.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2668.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_2668.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_5_temporal_epoch_5:
    target_device: 0
    input_count: 2
    softmax_2668.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_2668.dc.exp.0_0, e2e_softmax_2668.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2672: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_2649.dc.add.10_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias], t: 1, mblock: [3, 2],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2679: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_2668.dc.multiply.3, matmul_2672], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2683: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_2679, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2649.dc.add.10_add_2687: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_2649.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2687: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_2683, buffer_0_layernorm_2649.dc.add.10_add_2687], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_2687, lc.input_tensor.layernorm_2688.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2688.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_2687, layernorm_2688.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2688.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2688.dc.subtract.1, layernorm_2688.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2688.dc.multiply.2, lc.input_tensor.layernorm_2688.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2688.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2688.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2688.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2688.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_2688.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_2688.dc.reciprocal.7, lc.input_tensor.layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_2688.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2688.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_2688.dc.subtract.1_layernorm_2688.dc.multiply.8, layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.2.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2688.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_2688.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2688.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_2688.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_6_temporal_epoch_6:
    target_device: 0
    input_count: 2
    matmul_2691: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_2688.dc.add.10_0, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2694: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_2691], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_2697: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2694, layer.2.output.dense.weight, layer.2.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_2701: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_2697, e2e_layernorm_2688.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_2701, lc.input_tensor.layernorm_2702.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2702.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_2701, layernorm_2702.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2702.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2702.dc.subtract.1, layernorm_2702.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2702.dc.multiply.2, lc.input_tensor.layernorm_2702.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2702.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2702.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2702.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2702.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2702.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2702.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2702.dc.reciprocal.7, lc.input_tensor.layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2702.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_7_temporal_epoch_7:
    target_device: 0
    input_count: 2
    layernorm_2702.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0, e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2702.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2702.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2702.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2702.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2705: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2702.dc.add.10, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2711: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_2702.dc.add.10, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2717: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2705, matmul_2711], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2719: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2717, input_1_multiply_2719_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2720: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_2719, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2721.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2720], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2721.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2721.dc.exp.0, lc.input_tensor.softmax_2721.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2721.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2721.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2721.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_2721.dc.exp.0, softmax_2721.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2725: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2702.dc.add.10, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2732: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_2721.dc.multiply.3, matmul_2725], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2736: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_2732, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2702.dc.add.10_add_2740: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2702.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2740: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_2736, buffer_0_layernorm_2702.dc.add.10_add_2740], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_2740, lc.input_tensor.layernorm_2741.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2741.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_2740, layernorm_2741.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2741.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_8_temporal_epoch_8:
    target_device: 0
    input_count: 2
    layernorm_2741.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_2741.dc.subtract.1_0, e2e_layernorm_2741.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2741.dc.multiply.2, lc.input_tensor.layernorm_2741.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2741.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2741.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2741.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2741.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2741.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2741.dc.reciprocal.7, lc.input_tensor.layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2741.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8, layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.3.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2741.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2741.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2741.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2741.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2744: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2741.dc.add.10, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2747: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2744], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_9_temporal_epoch_9:
    target_device: 0
    input_count: 2
    matmul_2750: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_2747_0, layer.3.output.dense.weight, layer.3.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    add_2754: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_2750, e2e_layernorm_2741.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_2754, lc.input_tensor.layernorm_2755.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2755.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2754, layernorm_2755.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2755.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2755.dc.subtract.1, layernorm_2755.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2755.dc.multiply.2, lc.input_tensor.layernorm_2755.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2755.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2755.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2755.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2755.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2755.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2755.dc.reciprocal.7, lc.input_tensor.layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_2755.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2755.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2755.dc.subtract.1_layernorm_2755.dc.multiply.8, layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2755.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_2755.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2755.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_2755.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2758: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2755.dc.add.10, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2764: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_2755.dc.add.10, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2770: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_2758, matmul_2764], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2772: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_2770, input_1_multiply_2772_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2773: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_2772, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2774.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_2773], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_10_temporal_epoch_10:
    target_device: 0
    input_count: 2
    softmax_2774.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_2774.dc.exp.0_0, lc.input_tensor.softmax_2774.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2774.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_2774.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2774.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_2774.dc.exp.0_0, softmax_2774.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2778: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_2755.dc.add.10_0, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias], t: 1, mblock: [3, 2],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2785: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_2774.dc.multiply.3, matmul_2778], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2789: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_2785, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2755.dc.add.10_add_2793: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_2755.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2793: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_2789, buffer_0_layernorm_2755.dc.add.10_add_2793], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_2793, lc.input_tensor.layernorm_2794.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2794.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_2793, layernorm_2794.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2794.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_2794.dc.subtract.1, layernorm_2794.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2794.dc.multiply.2, lc.input_tensor.layernorm_2794.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2794.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_2794.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2794.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_2794.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_2794.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_2794.dc.reciprocal.7, lc.input_tensor.layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2794.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2794.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2794.dc.subtract.1_layernorm_2794.dc.multiply.8, layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.4.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2794.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_2794.dc.multiply.8, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2794.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_2794.dc.multiply.9, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_11_temporal_epoch_11:
    target_device: 0
    input_count: 2
    matmul_2797: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_2794.dc.add.10_0, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2800: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_2797], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_2803: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2800, layer.4.output.dense.weight, layer.4.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2794.dc.add.10_add_2807: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_2794.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2807: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_2803, buffer_0_layernorm_2794.dc.add.10_add_2807], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_2807, lc.input_tensor.layernorm_2808.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2808.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_2807, layernorm_2808.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2808.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2808.dc.subtract.1, layernorm_2808.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2808.dc.multiply.2, lc.input_tensor.layernorm_2808.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2808.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2808.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2808.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2808.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2808.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2808.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_2_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2808.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_12_temporal_epoch_12:
    target_device: 0
    input_count: 2
    layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2808.dc.reciprocal.7_0, lc.input_tensor.layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2808.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0, layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2808.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2808.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2808.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2808.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2811: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2808.dc.add.10, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2817: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_2808.dc.add.10, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2823: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2811, matmul_2817], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2825: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_2823, input_1_multiply_2825_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2826: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_2825, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2827.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_2826], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2827.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2827.dc.exp.0, lc.input_tensor.softmax_2827.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2827.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_2827.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2827.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_2827.dc.exp.0, softmax_2827.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2831: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2808.dc.add.10, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2838: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_2827.dc.multiply.3, matmul_2831], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2842: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_2838, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2808.dc.add.10_add_2846: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2808.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2846: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_2842, buffer_0_layernorm_2808.dc.add.10_add_2846], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_2846, lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2847.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_2846, layernorm_2847.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}

  fwd_13_temporal_epoch_13:
    target_device: 0
    input_count: 2
    layernorm_2847.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2847.dc.subtract.1_0, e2e_layernorm_2847.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.2, lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2847.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2847.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2847.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2847.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2847.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2847.dc.reciprocal.7, lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2847.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_2847.dc.subtract.1_0, layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.lc1], t: 1, mblock: [3,
        8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.5.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2847.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2847.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.9, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2850: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2847.dc.add.10, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2853: {type: gelu, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_2850], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_2856: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_2853, layer.5.output.dense.weight, layer.5.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2847.dc.add.10_add_2860: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2847.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_14_temporal_epoch_14:
    target_device: 0
    input_count: 2
    add_2860: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_2856_0, e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_2860, lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2861.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_2860, layernorm_2861.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2861.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2861.dc.subtract.1, layernorm_2861.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.2, lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2861.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_2861.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2861.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2861.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2861.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_2861.dc.reciprocal.7, lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2861.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8, layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2861.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2861.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2864: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2870: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2876: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_2864, matmul_2870], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2878: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_2876, input_1_multiply_2878_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2879: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_2878, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2880.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_2879], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2880.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_2880.dc.exp.0, lc.input_tensor.softmax_2880.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2880.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_2880.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2880.dc.multiply.3: {type: multiply, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_2880.dc.exp.0, softmax_2880.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2884: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}

  fwd_15_temporal_epoch_15:
    target_device: 0
    input_count: 2
    matmul_2891: {type: matmul, grid_loc: [0, 0], grid_size: [3, 2], inputs: [e2e_softmax_2880.dc.multiply.3_0, e2e_matmul_2884_0], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2895: {type: matmul, grid_loc: [0, 2], grid_size: [2, 4], inputs: [matmul_2891, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2861.dc.add.10_add_2899: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_2861.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2899: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_2895, buffer_0_layernorm_2861.dc.add.10_add_2899], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2899, lc.input_tensor.layernorm_2900.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2900.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_2899, layernorm_2900.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2900.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2900.dc.subtract.1, layernorm_2900.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [layernorm_2900.dc.multiply.2, lc.input_tensor.layernorm_2900.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2900.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [2, 1], inputs: [layernorm_2900.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2900.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2900.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2900.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2900.dc.reciprocal.7, lc.input_tensor.layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2900.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2900.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2900.dc.subtract.1_layernorm_2900.dc.multiply.8, layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.6.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2900.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2900.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2900.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_2900.dc.multiply.9, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_16_temporal_epoch_16:
    target_device: 0
    input_count: 2
    matmul_2903: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_2900.dc.add.10_0, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2906: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_2903], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_2909: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2906, layer.6.output.dense.weight, layer.6.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 32, u_kt: 4}}
    add_2913: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_2909, e2e_layernorm_2900.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_2913, lc.input_tensor.layernorm_2914.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2914.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_2913, layernorm_2914.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2914.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2914.dc.subtract.1, layernorm_2914.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2914.dc.multiply.2, lc.input_tensor.layernorm_2914.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2914.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2914.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2914.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2914.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2914.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2914.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2914.dc.reciprocal.7, lc.input_tensor.layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2914.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_17_temporal_epoch_17:
    target_device: 0
    input_count: 2
    layernorm_2914.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0, e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2914.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2914.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2914.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2914.dc.multiply.9, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2917: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2914.dc.add.10, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2923: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_2914.dc.add.10, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2929: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2917, matmul_2923], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2931: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2929, input_1_multiply_2931_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2932: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_2931, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2933.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2932], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2933.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2933.dc.exp.0, lc.input_tensor.softmax_2933.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2933.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2933.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2933.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_2933.dc.exp.0, softmax_2933.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2937: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2914.dc.add.10, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2944: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_2933.dc.multiply.3, matmul_2937], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_2948: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_2944, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2914.dc.add.10_add_2952: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2914.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2952: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_2948, buffer_0_layernorm_2914.dc.add.10_add_2952], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_2952, lc.input_tensor.layernorm_2953.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2953.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_2952, layernorm_2953.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2953.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_18_temporal_epoch_18:
    target_device: 0
    input_count: 2
    layernorm_2953.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_2953.dc.subtract.1_0, e2e_layernorm_2953.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2953.dc.multiply.2, lc.input_tensor.layernorm_2953.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2953.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2953.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2953.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2953.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2953.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2953.dc.reciprocal.7, lc.input_tensor.layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2953.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8, layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.7.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_2953.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2953.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2953.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2953.dc.multiply.9, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2956: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_2953.dc.add.10, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_2959: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2956], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_19_temporal_epoch_19:
    target_device: 0
    input_count: 2
    matmul_2962: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_2959_0, layer.7.output.dense.weight, layer.7.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_2953.dc.add.10_add_2966: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2953.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2966: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_2962, buffer_0_layernorm_2953.dc.add.10_add_2966], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2966, lc.input_tensor.layernorm_2967.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_2967.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_2966, layernorm_2967.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_2967.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2967.dc.subtract.1, layernorm_2967.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2967.dc.multiply.2, lc.input_tensor.layernorm_2967.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_2967.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2967.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2967.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2967.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2967.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2967.dc.reciprocal.7, lc.input_tensor.layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2967.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2967.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2967.dc.subtract.1_layernorm_2967.dc.multiply.8, layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2967.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2967.dc.multiply.8, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_2967.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2967.dc.multiply.9, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_2970: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_2967.dc.add.10, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2976: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2967.dc.add.10, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2982: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_2970, matmul_2976], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_2984: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_2982, input_1_multiply_2984_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2985: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_2984, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_2986.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_2985], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_2986.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_2986.dc.exp.0, lc.input_tensor.softmax_2986.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_2986.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_2986.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_20_temporal_epoch_20:
    target_device: 0
    input_count: 2
    softmax_2986.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_2986.dc.exp.0_0, e2e_softmax_2986.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_2990: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_2967.dc.add.10_0, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias], t: 1, mblock: [3, 2],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_2997: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_2986.dc.multiply.3, matmul_2990], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3001: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_2997, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_2967.dc.add.10_add_3005: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_2967.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3005: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3001, buffer_0_layernorm_2967.dc.add.10_add_3005], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3005, lc.input_tensor.layernorm_3006.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3006.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_3005, layernorm_3006.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3006.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3006.dc.subtract.1, layernorm_3006.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3006.dc.multiply.2, lc.input_tensor.layernorm_3006.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3006.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3006.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3006.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3006.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3006.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_3006.dc.reciprocal.7, lc.input_tensor.layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_3006.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3006.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3006.dc.subtract.1_layernorm_3006.dc.multiply.8, layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.8.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3006.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3006.dc.multiply.8, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3006.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3006.dc.multiply.9, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_21_temporal_epoch_21:
    target_device: 0
    input_count: 2
    matmul_3009: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3006.dc.add.10_0, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3012: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3009], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3015: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3012, layer.8.output.dense.weight, layer.8.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r,
      in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true, m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3006.dc.add.10_add_3019: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3006.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3019: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3015, buffer_0_layernorm_3006.dc.add.10_add_3019], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3019, lc.input_tensor.layernorm_3020.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3020.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3019, layernorm_3020.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3020.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3020.dc.subtract.1, layernorm_3020.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3020.dc.multiply.2, lc.input_tensor.layernorm_3020.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3020.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3020.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3020.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3020.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3020.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3020.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_2_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3020.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_22_temporal_epoch_22:
    target_device: 0
    input_count: 2
    layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3020.dc.reciprocal.7_0, lc.input_tensor.layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3020.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0, layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3020.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3020.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3020.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3020.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3023: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3020.dc.add.10, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3029: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3020.dc.add.10, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3035: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3023, matmul_3029], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3037: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3035, input_1_multiply_3037_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3038: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3037, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3039.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3038], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3039.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3039.dc.exp.0, lc.input_tensor.softmax_3039.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3039.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3039.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3039.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3039.dc.exp.0, softmax_3039.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3043: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3020.dc.add.10, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3050: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3039.dc.multiply.3, matmul_3043], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3054: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3050, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [2,
        4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3020.dc.add.10_add_3058: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3020.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3058: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3054, buffer_0_layernorm_3020.dc.add.10_add_3058], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3058, lc.input_tensor.layernorm_3059.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3059.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3058, layernorm_3059.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}

  fwd_23_temporal_epoch_23:
    target_device: 0
    input_count: 2
    layernorm_3059.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_layernorm_3059.dc.subtract.1_0, e2e_layernorm_3059.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3059.dc.multiply.2, lc.input_tensor.layernorm_3059.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3059.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3059.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3059.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3059.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3059.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3059.dc.reciprocal.7, lc.input_tensor.layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3059.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3059.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_3059.dc.subtract.1_layernorm_3059.dc.multiply.8, layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.9.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3059.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3059.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3059.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3059.dc.multiply.9, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3062: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3059.dc.add.10, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3065: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3062], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_24_temporal_epoch_24:
    target_device: 0
    input_count: 2
    matmul_3068: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3065_0, layer.9.output.dense.weight, layer.9.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    add_3072: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3068, e2e_layernorm_3059.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3072, lc.input_tensor.layernorm_3073.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3073.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3072, layernorm_3073.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3073.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3073.dc.subtract.1, layernorm_3073.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3073.dc.multiply.2, lc.input_tensor.layernorm_3073.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3073.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3073.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3073.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3073.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3073.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3073.dc.reciprocal.7, lc.input_tensor.layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3073.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3073.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3073.dc.subtract.1_layernorm_3073.dc.multiply.8, layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3073.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3073.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3073.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3073.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [2,
        4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3076: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3073.dc.add.10, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3082: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_3073.dc.add.10, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3088: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_3076, matmul_3082], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3090: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3088, input_1_multiply_3090_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3091: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_3090, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3092.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_3091], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_25_temporal_epoch_25:
    target_device: 0
    input_count: 2
    softmax_3092.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3092.dc.exp.0_0, lc.input_tensor.softmax_3092.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3092.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_3092.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3092.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3092.dc.exp.0_0, softmax_3092.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3096: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_3073.dc.add.10_0, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias], t: 1, mblock: [3,
        2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3103: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_3092.dc.multiply.3, matmul_3096], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3107: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3103, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3073.dc.add.10_add_3111: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3073.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3111: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_3107, buffer_0_layernorm_3073.dc.add.10_add_3111], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3111, lc.input_tensor.layernorm_3112.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3112.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3111, layernorm_3112.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3112.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3112.dc.subtract.1, layernorm_3112.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3112.dc.multiply.2, lc.input_tensor.layernorm_3112.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3112.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_3112.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3112.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3112.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3112.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3112.dc.reciprocal.7, lc.input_tensor.layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3112.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3112.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3112.dc.subtract.1_layernorm_3112.dc.multiply.8, layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.10.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3112.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_3112.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3112.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_3112.dc.multiply.9, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_26_temporal_epoch_26:
    target_device: 0
    input_count: 2
    matmul_3115: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3112.dc.add.10_0, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias], t: 1, mblock: [3, 4],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3118: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3115], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3121: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3118, layer.10.output.dense.weight, layer.10.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3112.dc.add.10_add_3125: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3112.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3125: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3121, buffer_0_layernorm_3112.dc.add.10_add_3125], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3125, lc.input_tensor.layernorm_3126.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3126.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3125, layernorm_3126.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3126.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3126.dc.subtract.1, layernorm_3126.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3126.dc.multiply.2, lc.input_tensor.layernorm_3126.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3126.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3126.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3126.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3126.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3126.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3126.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_2_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3126.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_27_temporal_epoch_27:
    target_device: 0
    input_count: 2
    layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3126.dc.reciprocal.7_0, lc.input_tensor.layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3126.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0, layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3126.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3126.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3126.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3126.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3129: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3126.dc.add.10, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3135: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3126.dc.add.10, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3141: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3129, matmul_3135], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3143: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3141, input_1_multiply_3143_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3144: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3143, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3145.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3144], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3145.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3145.dc.exp.0, lc.input_tensor.softmax_3145.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3145.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3145.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3145.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3145.dc.exp.0, softmax_3145.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3149: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3126.dc.add.10, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3156: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3145.dc.multiply.3, matmul_3149], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3160: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3156, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3126.dc.add.10_add_3164: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3126.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3164: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3160, buffer_0_layernorm_3126.dc.add.10_add_3164], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3164, lc.input_tensor.layernorm_3165.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3165.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3164, layernorm_3165.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}

  fwd_28_temporal_epoch_28:
    target_device: 0
    input_count: 2
    layernorm_3165.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3165.dc.subtract.1_0, e2e_layernorm_3165.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3165.dc.multiply.2, lc.input_tensor.layernorm_3165.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3165.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3165.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3165.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3165.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3165.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3165.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3165.dc.reciprocal.7, lc.input_tensor.layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3165.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3165.dc.subtract.1_0, layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.lc1], t: 1, mblock: [3,
        8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.11.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3165.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3165.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3165.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3165.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3168: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3165.dc.add.10, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3171: {type: gelu, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3168], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3174: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_3171, layer.11.output.dense.weight, layer.11.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3165.dc.add.10_add_3178: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3165.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_29_temporal_epoch_29:
    target_device: 0
    input_count: 2
    add_3178: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3174_0, e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3178, lc.input_tensor.layernorm_3179.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3179.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3178, layernorm_3179.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3179.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3179.dc.subtract.1, layernorm_3179.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3179.dc.multiply.2, lc.input_tensor.layernorm_3179.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3179.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3179.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3179.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3179.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3179.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3179.dc.reciprocal.7, lc.input_tensor.layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3179.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3179.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3179.dc.subtract.1_layernorm_3179.dc.multiply.8, layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3179.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3179.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3179.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3179.dc.multiply.9, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3182: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3179.dc.add.10, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3188: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3179.dc.add.10, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3194: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3182, matmul_3188], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3196: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3194, input_1_multiply_3196_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3197: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3196, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3198.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3197], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3198.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3198.dc.exp.0, lc.input_tensor.softmax_3198.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3198.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3198.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3198.dc.multiply.3: {type: multiply, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3198.dc.exp.0, softmax_3198.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3202: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3179.dc.add.10, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}

  fwd_30_temporal_epoch_30:
    target_device: 0
    input_count: 2
    matmul_3209: {type: matmul, grid_loc: [0, 0], grid_size: [3, 2], inputs: [e2e_softmax_3198.dc.multiply.3_0, e2e_matmul_3202_0], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3213: {type: matmul, grid_loc: [0, 2], grid_size: [2, 4], inputs: [matmul_3209, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3179.dc.add.10_add_3217: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3179.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3217: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3213, buffer_0_layernorm_3179.dc.add.10_add_3217], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3217, lc.input_tensor.layernorm_3218.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3218.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3217, layernorm_3218.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3218.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3218.dc.subtract.1, layernorm_3218.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [layernorm_3218.dc.multiply.2, lc.input_tensor.layernorm_3218.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3218.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [2, 1], inputs: [layernorm_3218.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3218.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3218.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3218.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3218.dc.reciprocal.7, lc.input_tensor.layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3218.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3218.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3218.dc.subtract.1_layernorm_3218.dc.multiply.8, layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.12.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3218.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3218.dc.multiply.8, layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3218.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3218.dc.multiply.9, layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_31_temporal_epoch_31:
    target_device: 0
    input_count: 2
    matmul_3221: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3218.dc.add.10_0, layer.12.intermediate.dense.weight, layer.12.intermediate.dense.bias], t: 1, mblock: [3, 4],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3224: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3221], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3227: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3224, layer.12.output.dense.weight, layer.12.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    add_3231: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_3227, e2e_layernorm_3218.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_3231, lc.input_tensor.layernorm_3232.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3232.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3231, layernorm_3232.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3232.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3232.dc.subtract.1, layernorm_3232.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3232.dc.multiply.2, lc.input_tensor.layernorm_3232.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3232.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3232.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3232.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3232.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3232.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3232.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3232.dc.reciprocal.7, lc.input_tensor.layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3232.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_32_temporal_epoch_32:
    target_device: 0
    input_count: 2
    layernorm_3232.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0, e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3232.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3232.dc.multiply.8, layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3232.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3232.dc.multiply.9, layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3235: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3232.dc.add.10, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3241: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3232.dc.add.10, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3247: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3235, matmul_3241], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3249: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3247, input_1_multiply_3249_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3250: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_3249, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3251.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_3250], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3251.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3251.dc.exp.0, lc.input_tensor.softmax_3251.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3251.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3251.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3251.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3251.dc.exp.0, softmax_3251.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3255: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3232.dc.add.10, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3262: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_3251.dc.multiply.3, matmul_3255], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3266: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3262, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3232.dc.add.10_add_3270: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3232.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3270: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3266, buffer_0_layernorm_3232.dc.add.10_add_3270], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3270, lc.input_tensor.layernorm_3271.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3271.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3270, layernorm_3271.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3271.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_33_temporal_epoch_33:
    target_device: 0
    input_count: 2
    layernorm_3271.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_3271.dc.subtract.1_0, e2e_layernorm_3271.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3271.dc.multiply.2, lc.input_tensor.layernorm_3271.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3271.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3271.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3271.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3271.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3271.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3271.dc.reciprocal.7, lc.input_tensor.layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3271.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8, layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.13.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3271.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3271.dc.multiply.8, layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3271.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3271.dc.multiply.9, layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3274: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3271.dc.add.10, layer.13.intermediate.dense.weight, layer.13.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3277: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3274], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_34_temporal_epoch_34:
    target_device: 0
    input_count: 2
    matmul_3280: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3277_0, layer.13.output.dense.weight, layer.13.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3271.dc.add.10_add_3284: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3271.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3284: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_3280, buffer_0_layernorm_3271.dc.add.10_add_3284], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3284, lc.input_tensor.layernorm_3285.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3285.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3284, layernorm_3285.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3285.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3285.dc.subtract.1, layernorm_3285.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3285.dc.multiply.2, lc.input_tensor.layernorm_3285.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3285.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3285.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3285.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3285.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3285.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3285.dc.reciprocal.7, lc.input_tensor.layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3285.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3285.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3285.dc.subtract.1_layernorm_3285.dc.multiply.8, layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3285.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3285.dc.multiply.8, layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3285.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3285.dc.multiply.9, layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3288: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_3285.dc.add.10, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3294: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3285.dc.add.10, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3300: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_3288, matmul_3294], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3302: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_3300, input_1_multiply_3302_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3303: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_3302, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3304.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_3303], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3304.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_3304.dc.exp.0, lc.input_tensor.softmax_3304.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3304.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_3304.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_35_temporal_epoch_35:
    target_device: 0
    input_count: 2
    softmax_3304.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3304.dc.exp.0_0, e2e_softmax_3304.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3308: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_3285.dc.add.10_0, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias], t: 1, mblock: [3,
        2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3315: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_3304.dc.multiply.3, matmul_3308], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3319: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_3315, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3285.dc.add.10_add_3323: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3285.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3323: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3319, buffer_0_layernorm_3285.dc.add.10_add_3323], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3323, lc.input_tensor.layernorm_3324.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3324.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_3323, layernorm_3324.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3324.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3324.dc.subtract.1, layernorm_3324.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3324.dc.multiply.2, lc.input_tensor.layernorm_3324.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3324.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3324.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3324.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3324.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3324.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_3324.dc.reciprocal.7, lc.input_tensor.layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_3324.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3324.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3324.dc.subtract.1_layernorm_3324.dc.multiply.8, layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.14.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3324.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3324.dc.multiply.8, layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3324.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3324.dc.multiply.9, layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_36_temporal_epoch_36:
    target_device: 0
    input_count: 2
    matmul_3327: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3324.dc.add.10_0, layer.14.intermediate.dense.weight, layer.14.intermediate.dense.bias], t: 1, mblock: [3, 4],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3330: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3327], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3333: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3330, layer.14.output.dense.weight, layer.14.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3324.dc.add.10_add_3337: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3324.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3337: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3333, buffer_0_layernorm_3324.dc.add.10_add_3337], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3337, lc.input_tensor.layernorm_3338.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3338.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3337, layernorm_3338.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3338.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3338.dc.subtract.1, layernorm_3338.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3338.dc.multiply.2, lc.input_tensor.layernorm_3338.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3338.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3338.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3338.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3338.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3338.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3338.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_2_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3338.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_37_temporal_epoch_37:
    target_device: 0
    input_count: 2
    layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3338.dc.reciprocal.7_0, lc.input_tensor.layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3338.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0, layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3338.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3338.dc.multiply.8, layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3338.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3338.dc.multiply.9, layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3341: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3338.dc.add.10, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3347: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3338.dc.add.10, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3353: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3341, matmul_3347], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3355: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3353, input_1_multiply_3355_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3356: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3355, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3357.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3356], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3357.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3357.dc.exp.0, lc.input_tensor.softmax_3357.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3357.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3357.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3357.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3357.dc.exp.0, softmax_3357.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3361: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3338.dc.add.10, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3368: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3357.dc.multiply.3, matmul_3361], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3372: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3368, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3338.dc.add.10_add_3376: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3338.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3376: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3372, buffer_0_layernorm_3338.dc.add.10_add_3376], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3376, lc.input_tensor.layernorm_3377.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3377.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3376, layernorm_3377.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}

  fwd_38_temporal_epoch_38:
    target_device: 0
    input_count: 2
    layernorm_3377.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_layernorm_3377.dc.subtract.1_0, e2e_layernorm_3377.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3377.dc.multiply.2, lc.input_tensor.layernorm_3377.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3377.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3377.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3377.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3377.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3377.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3377.dc.reciprocal.7, lc.input_tensor.layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3377.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3377.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_3377.dc.subtract.1_layernorm_3377.dc.multiply.8, layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.15.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3377.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3377.dc.multiply.8, layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3377.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3377.dc.multiply.9, layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3380: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3377.dc.add.10, layer.15.intermediate.dense.weight, layer.15.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3383: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3380], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_39_temporal_epoch_39:
    target_device: 0
    input_count: 2
    matmul_3386: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3383_0, layer.15.output.dense.weight, layer.15.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    add_3390: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3386, e2e_layernorm_3377.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3390, lc.input_tensor.layernorm_3391.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3391.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3390, layernorm_3391.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3391.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3391.dc.subtract.1, layernorm_3391.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3391.dc.multiply.2, lc.input_tensor.layernorm_3391.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3391.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3391.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3391.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3391.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3391.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3391.dc.reciprocal.7, lc.input_tensor.layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3391.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3391.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3391.dc.subtract.1_layernorm_3391.dc.multiply.8, layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3391.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3391.dc.multiply.8, layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3391.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3391.dc.multiply.9, layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3394: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3391.dc.add.10, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3400: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_3391.dc.add.10, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3406: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_3394, matmul_3400], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3408: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3406, input_1_multiply_3408_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3409: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_3408, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3410.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_3409], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_40_temporal_epoch_40:
    target_device: 0
    input_count: 2
    softmax_3410.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3410.dc.exp.0_0, lc.input_tensor.softmax_3410.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3410.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_3410.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3410.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3410.dc.exp.0_0, softmax_3410.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3414: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_3391.dc.add.10_0, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias], t: 1, mblock: [3,
        2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3421: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_3410.dc.multiply.3, matmul_3414], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3425: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3421, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3391.dc.add.10_add_3429: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3391.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3429: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_3425, buffer_0_layernorm_3391.dc.add.10_add_3429], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3429, lc.input_tensor.layernorm_3430.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3430.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3429, layernorm_3430.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3430.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3430.dc.subtract.1, layernorm_3430.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3430.dc.multiply.2, lc.input_tensor.layernorm_3430.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3430.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_3430.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3430.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3430.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3430.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3430.dc.reciprocal.7, lc.input_tensor.layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3430.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3430.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3430.dc.subtract.1_layernorm_3430.dc.multiply.8, layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.16.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3430.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_3430.dc.multiply.8, layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3430.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_3430.dc.multiply.9, layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_41_temporal_epoch_41:
    target_device: 0
    input_count: 2
    matmul_3433: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3430.dc.add.10_0, layer.16.intermediate.dense.weight, layer.16.intermediate.dense.bias], t: 1, mblock: [3, 4],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3436: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3433], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3439: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3436, layer.16.output.dense.weight, layer.16.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3430.dc.add.10_add_3443: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3430.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3443: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3439, buffer_0_layernorm_3430.dc.add.10_add_3443], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3443, lc.input_tensor.layernorm_3444.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3444.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3443, layernorm_3444.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3444.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3444.dc.subtract.1, layernorm_3444.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3444.dc.multiply.2, lc.input_tensor.layernorm_3444.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3444.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3444.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3444.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3444.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3444.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3444.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_2_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3444.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_42_temporal_epoch_42:
    target_device: 0
    input_count: 2
    layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3444.dc.reciprocal.7_0, lc.input_tensor.layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3444.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0, layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3444.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3444.dc.multiply.8, layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3444.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3444.dc.multiply.9, layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3447: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3444.dc.add.10, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3453: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3444.dc.add.10, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3459: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3447, matmul_3453], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3461: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3459, input_1_multiply_3461_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3462: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3461, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3463.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3462], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3463.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3463.dc.exp.0, lc.input_tensor.softmax_3463.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3463.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3463.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3463.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3463.dc.exp.0, softmax_3463.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3467: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3444.dc.add.10, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3474: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3463.dc.multiply.3, matmul_3467], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3478: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3474, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3444.dc.add.10_add_3482: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3444.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3482: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3478, buffer_0_layernorm_3444.dc.add.10_add_3482], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3482, lc.input_tensor.layernorm_3483.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3483.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3482, layernorm_3483.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}

  fwd_43_temporal_epoch_43:
    target_device: 0
    input_count: 2
    layernorm_3483.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3483.dc.subtract.1_0, e2e_layernorm_3483.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3483.dc.multiply.2, lc.input_tensor.layernorm_3483.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3483.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3483.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3483.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3483.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3483.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3483.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3483.dc.reciprocal.7, lc.input_tensor.layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3483.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3483.dc.subtract.1_0, layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.lc1], t: 1, mblock: [3,
        8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            c: 32}]}
    layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.17.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3483.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3483.dc.multiply.8, layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3483.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3483.dc.multiply.9, layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3486: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3483.dc.add.10, layer.17.intermediate.dense.weight, layer.17.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3489: {type: gelu, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3486], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3492: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_3489, layer.17.output.dense.weight, layer.17.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3483.dc.add.10_add_3496: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3483.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_44_temporal_epoch_44:
    target_device: 0
    input_count: 2
    add_3496: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3492_0, e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3496, lc.input_tensor.layernorm_3497.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3497.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3496, layernorm_3497.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3497.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3497.dc.subtract.1, layernorm_3497.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3497.dc.multiply.2, lc.input_tensor.layernorm_3497.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3497.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3497.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3497.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3497.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3497.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3497.dc.reciprocal.7, lc.input_tensor.layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3497.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3497.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3497.dc.subtract.1_layernorm_3497.dc.multiply.8, layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3497.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3497.dc.multiply.8, layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3497.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3497.dc.multiply.9, layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3500: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3497.dc.add.10, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3506: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3497.dc.add.10, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3512: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3500, matmul_3506], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3514: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3512, input_1_multiply_3514_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3515: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3514, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3516.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3515], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3516.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3516.dc.exp.0, lc.input_tensor.softmax_3516.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3516.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3516.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3516.dc.multiply.3: {type: multiply, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3516.dc.exp.0, softmax_3516.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3520: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3497.dc.add.10, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}

  fwd_45_temporal_epoch_45:
    target_device: 0
    input_count: 2
    matmul_3527: {type: matmul, grid_loc: [0, 0], grid_size: [3, 2], inputs: [e2e_softmax_3516.dc.multiply.3_0, e2e_matmul_3520_0], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3531: {type: matmul, grid_loc: [0, 2], grid_size: [2, 4], inputs: [matmul_3527, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3497.dc.add.10_add_3535: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [e2e_layernorm_3497.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3535: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3531, buffer_0_layernorm_3497.dc.add.10_add_3535], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3535, lc.input_tensor.layernorm_3536.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3536.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3535, layernorm_3536.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3536.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3536.dc.subtract.1, layernorm_3536.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [2, 1], inputs: [layernorm_3536.dc.multiply.2, lc.input_tensor.layernorm_3536.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3536.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [2, 1], inputs: [layernorm_3536.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3536.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3536.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3536.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3536.dc.reciprocal.7, lc.input_tensor.layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3536.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3536.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3536.dc.subtract.1_layernorm_3536.dc.multiply.8, layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.18.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3536.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3536.dc.multiply.8, layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3536.dc.add.10: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3536.dc.multiply.9, layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_46_temporal_epoch_46:
    target_device: 0
    input_count: 2
    matmul_3539: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3536.dc.add.10_0, layer.18.intermediate.dense.weight, layer.18.intermediate.dense.bias], t: 1, mblock: [3, 4],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3542: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3539], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3545: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3542, layer.18.output.dense.weight, layer.18.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    add_3549: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_3545, e2e_layernorm_3536.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_3549, lc.input_tensor.layernorm_3550.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3550.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3549, layernorm_3550.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3550.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3550.dc.subtract.1, layernorm_3550.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3550.dc.multiply.2, lc.input_tensor.layernorm_3550.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3550.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3550.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3550.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3550.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3550.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3550.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3550.dc.reciprocal.7, lc.input_tensor.layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3550.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_47_temporal_epoch_47:
    target_device: 0
    input_count: 2
    layernorm_3550.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0, e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3550.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3550.dc.multiply.8, layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3550.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3550.dc.multiply.9, layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3553: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3550.dc.add.10, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3559: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3550.dc.add.10, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3565: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3553, matmul_3559], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3567: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3565, input_1_multiply_3567_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3568: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_3567, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3569.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_3568], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3569.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3569.dc.exp.0, lc.input_tensor.softmax_3569.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3569.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3569.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3569.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3569.dc.exp.0, softmax_3569.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3573: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3550.dc.add.10, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3580: {type: matmul, grid_loc: [4, 5], grid_size: [3, 2], inputs: [softmax_3569.dc.multiply.3, matmul_3573], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3584: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3580, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3550.dc.add.10_add_3588: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3550.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3588: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3584, buffer_0_layernorm_3550.dc.add.10_add_3588], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3588, lc.input_tensor.layernorm_3589.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3589.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3588, layernorm_3589.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8: {type: nop, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3589.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_48_temporal_epoch_48:
    target_device: 0
    input_count: 2
    layernorm_3589.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_3589.dc.subtract.1_0, e2e_layernorm_3589.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3589.dc.multiply.2, lc.input_tensor.layernorm_3589.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3589.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3589.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3589.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3589.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3589.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3589.dc.reciprocal.7, lc.input_tensor.layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3589.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8, layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.19.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3589.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3589.dc.multiply.8, layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3589.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3589.dc.multiply.9, layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3592: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3589.dc.add.10, layer.19.intermediate.dense.weight, layer.19.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3595: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3592], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_49_temporal_epoch_49:
    target_device: 0
    input_count: 2
    matmul_3598: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3595_0, layer.19.output.dense.weight, layer.19.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3589.dc.add.10_add_3602: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3589.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3602: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_3598, buffer_0_layernorm_3589.dc.add.10_add_3602], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3602, lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3603.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_3602, layernorm_3603.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3603.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3603.dc.subtract.1, layernorm_3603.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.2, lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3603.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3603.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3603.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3603.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3603.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3603.dc.reciprocal.7, lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3603.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_2_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8, layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3603.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.8, layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3603.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.9, layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3606: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_3603.dc.add.10, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3612: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3603.dc.add.10, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3618: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_3606, matmul_3612], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3620: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_3618, input_1_multiply_3620_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3621: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_3620, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3622.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_3621], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3622.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_3622.dc.exp.0, lc.input_tensor.softmax_3622.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3622.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_3622.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_50_temporal_epoch_50:
    target_device: 0
    input_count: 2
    softmax_3622.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3622.dc.exp.0_0, e2e_softmax_3622.dc.reciprocal.2_0], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3626: {type: matmul, grid_loc: [0, 1], grid_size: [2, 4], inputs: [e2e_layernorm_3603.dc.add.10_0, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias], t: 1, mblock: [3,
        2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3633: {type: matmul, grid_loc: [0, 5], grid_size: [3, 2], inputs: [softmax_3622.dc.multiply.3, matmul_3626], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3637: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [matmul_3633, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3603.dc.add.10_add_3641: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3603.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3641: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3637, buffer_0_layernorm_3603.dc.add.10_add_3641], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3641, lc.input_tensor.layernorm_3642.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3642.dc.subtract.1: {type: subtract, grid_loc: [3, 5], grid_size: [2, 1], inputs: [add_3641, layernorm_3642.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3642.dc.multiply.2: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3642.dc.subtract.1, layernorm_3642.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3642.dc.multiply.2, lc.input_tensor.layernorm_3642.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3642.dc.add.5: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3642.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3642.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3642.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3642.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [layernorm_3642.dc.reciprocal.7, lc.input_tensor.layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8: {type: nop, grid_loc: [3, 6], grid_size: [2, 1], inputs: [layernorm_3642.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_2_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3642.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3642.dc.subtract.1_layernorm_3642.dc.multiply.8, layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.20.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3642.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3642.dc.multiply.8, layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3642.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3642.dc.multiply.9, layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_51_temporal_epoch_51:
    target_device: 0
    input_count: 2
    matmul_3645: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3642.dc.add.10_0, layer.20.intermediate.dense.weight, layer.20.intermediate.dense.bias], t: 1, mblock: [3, 4],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3648: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3645], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3651: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3648, layer.20.output.dense.weight, layer.20.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3642.dc.add.10_add_3655: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3642.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3655: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3651, buffer_0_layernorm_3642.dc.add.10_add_3655], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3655, lc.input_tensor.layernorm_3656.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3656.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3655, layernorm_3656.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3656.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3656.dc.subtract.1, layernorm_3656.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3656.dc.multiply.2, lc.input_tensor.layernorm_3656.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3656.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3656.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3656.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3656.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3656.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3656.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_2_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3656.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_52_temporal_epoch_52:
    target_device: 0
    input_count: 2
    layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3656.dc.reciprocal.7_0, lc.input_tensor.layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3656.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0, layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3656.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3656.dc.multiply.8, layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3656.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3656.dc.multiply.9, layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3659: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3656.dc.add.10, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3665: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3656.dc.add.10, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3671: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3659, matmul_3665], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3673: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3671, input_1_multiply_3673_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3674: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3673, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3675.dc.exp.0: {type: exp, grid_loc: [4, 1], grid_size: [2, 2], inputs: [add_3674], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3675.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_3675.dc.exp.0, lc.input_tensor.softmax_3675.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3675.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_3675.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3675.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_3675.dc.exp.0, softmax_3675.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3679: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3656.dc.add.10, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3686: {type: matmul, grid_loc: [4, 6], grid_size: [3, 2], inputs: [softmax_3675.dc.multiply.3, matmul_3679], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3690: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [matmul_3686, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3656.dc.add.10_add_3694: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3656.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3694: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [matmul_3690, buffer_0_layernorm_3656.dc.add.10_add_3694], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3694, lc.input_tensor.layernorm_3695.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3695.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [add_3694, layernorm_3695.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}

  fwd_53_temporal_epoch_53:
    target_device: 0
    input_count: 2
    layernorm_3695.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [e2e_layernorm_3695.dc.subtract.1_0, e2e_layernorm_3695.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3695.dc.multiply.2, lc.input_tensor.layernorm_3695.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3695.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3695.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3695.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3695.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3695.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3695.dc.reciprocal.7, lc.input_tensor.layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3695.dc.subtract.1_0], t: 1, mblock: [3, 8], ublock: [2, 4],
      buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3695.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_3695.dc.subtract.1_layernorm_3695.dc.multiply.8, layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.21.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3695.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3695.dc.multiply.8, layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3695.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3695.dc.multiply.9, layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3698: {type: matmul, grid_loc: [4, 0], grid_size: [4, 8], inputs: [layernorm_3695.dc.add.10, layer.21.intermediate.dense.weight, layer.21.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3701: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_3698], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_54_temporal_epoch_54:
    target_device: 0
    input_count: 2
    matmul_3704: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_3701_0, layer.21.output.dense.weight, layer.21.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    add_3708: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3704, e2e_layernorm_3695.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b,
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3708, lc.input_tensor.layernorm_3709.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3709.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3708, layernorm_3709.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3709.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3709.dc.subtract.1, layernorm_3709.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3709.dc.multiply.2, lc.input_tensor.layernorm_3709.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3709.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3709.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3709.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3709.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3709.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3709.dc.reciprocal.7, lc.input_tensor.layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8: {type: nop, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3709.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3709.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3709.dc.subtract.1_layernorm_3709.dc.multiply.8, layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3709.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3709.dc.multiply.8, layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3709.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3709.dc.multiply.9, layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3712: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_3709.dc.add.10, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3718: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [layernorm_3709.dc.add.10, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3724: {type: matmul, grid_loc: [5, 7], grid_size: [2, 1], inputs: [matmul_3712, matmul_3718], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3726: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3724, input_1_multiply_3726_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3727: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [multiply_3726, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3728.dc.exp.0: {type: exp, grid_loc: [8, 2], grid_size: [2, 2], inputs: [add_3727], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}

  fwd_55_temporal_epoch_55:
    target_device: 0
    input_count: 2
    softmax_3728.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_softmax_3728.dc.exp.0_0, lc.input_tensor.softmax_3728.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3728.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [2, 1], inputs: [softmax_3728.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3728.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3728.dc.exp.0_0, softmax_3728.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3732: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [e2e_layernorm_3709.dc.add.10_0, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias], t: 1, mblock: [3,
        2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3739: {type: matmul, grid_loc: [2, 0], grid_size: [3, 2], inputs: [softmax_3728.dc.multiply.3, matmul_3732], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    matmul_3743: {type: matmul, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_3739, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_layernorm_3709.dc.add.10_add_3747: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3709.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3747: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_3743, buffer_0_layernorm_3709.dc.add.10_add_3747], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_3747, lc.input_tensor.layernorm_3748.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3748.dc.subtract.1: {type: subtract, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3747, layernorm_3748.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3748.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3748.dc.subtract.1, layernorm_3748.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3748.dc.multiply.2, lc.input_tensor.layernorm_3748.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3748.dc.add.5: {type: add, grid_loc: [5, 0], grid_size: [2, 1], inputs: [layernorm_3748.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3748.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.sqrt.6: {type: sqrt, grid_loc: [5, 1], grid_size: [2, 1], inputs: [layernorm_3748.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3748.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3748.dc.reciprocal.7, lc.input_tensor.layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3748.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_2_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3748.dc.multiply.8: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3748.dc.subtract.1_layernorm_3748.dc.multiply.8, layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.22.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3748.dc.multiply.9: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_3748.dc.multiply.8, layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3748.dc.add.10: {type: add, grid_loc: [7, 0], grid_size: [2, 1], inputs: [layernorm_3748.dc.multiply.9, layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}

  fwd_56_temporal_epoch_56:
    target_device: 0
    input_count: 2
    matmul_3751: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [e2e_layernorm_3748.dc.add.10_0, layer.22.intermediate.dense.weight, layer.22.intermediate.dense.bias], t: 1, mblock: [3, 4],
      ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {
            r: 12}], attributes: {bias: true, m_k: 32, u_kt: 1}}
    gelu_3754: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [matmul_3751], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b,
      acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3757: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3754, layer.22.output.dense.weight, layer.22.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    buffer_0_layernorm_3748.dc.add.10_add_3761: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [e2e_layernorm_3748.dc.add.10_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3761: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [matmul_3757, buffer_0_layernorm_3748.dc.add.10_add_3761], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_3761, lc.input_tensor.layernorm_3762.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3762.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_3761, layernorm_3762.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3762.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3762.dc.subtract.1, layernorm_3762.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3762.dc.multiply.2, lc.input_tensor.layernorm_3762.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3762.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3762.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3762.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3762.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3762.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3762.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    buffer_2_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3762.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_2_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_57_temporal_epoch_57:
    target_device: 0
    input_count: 2
    layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3762.dc.reciprocal.7_0, lc.input_tensor.layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3762.dc.multiply.8: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0, layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3762.dc.multiply.9: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3762.dc.multiply.8, layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3762.dc.add.10: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3762.dc.multiply.9, layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3765: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3762.dc.add.10, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3771: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_3762.dc.add.10, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3777: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3765, matmul_3771], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 2}}
    multiply_3779: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3777, input_1_multiply_3779_tile_bcast_tile_bcast], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3780: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [multiply_3779, attention_mask], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {z: 16}]}
    softmax_3781.dc.exp.0: {type: exp, grid_loc: [4, 6], grid_size: [2, 2], inputs: [add_3780], t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3781.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_3781.dc.exp.0, lc.input_tensor.softmax_3781.dc.reduce_sum.1.0], t: 16, mblock: [3, 1], ublock: [
        2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}, broadcast: {
            z: 16}], attributes: {m_k: 1, u_kt: 12}}
    softmax_3781.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 1], grid_size: [2, 1], inputs: [softmax_3781.dc.reduce_sum.1.lc1], t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    softmax_3781.dc.multiply.3: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_3781.dc.exp.0, softmax_3781.dc.reciprocal.2], t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2,
      input_buf_min_size_tiles: [232, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 12}]}
    matmul_3785: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [layernorm_3762.dc.add.10, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 16, u_kt: 2}}
    matmul_3792: {type: matmul, grid_loc: [6, 3], grid_size: [3, 2], inputs: [softmax_3781.dc.multiply.3, matmul_3785], t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [
        Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [hslice: 16], attributes: {m_k: 1, u_kt: 12}}
    buffer_0_layernorm_3762.dc.add.10_add_3800: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3762.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_58_temporal_epoch_58:
    target_device: 0
    input_count: 2
    matmul_3796: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_3792_0, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias], t: 1, mblock: [3, 2], ublock: [
        2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      input_0_tms: [hstack: 16], attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_3800: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_3796, e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_3800, lc.input_tensor.layernorm_3801.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3801.dc.subtract.1: {type: subtract, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_3800, layernorm_3801.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3801.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3801.dc.subtract.1, layernorm_3801.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3801.dc.multiply.2, lc.input_tensor.layernorm_3801.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3801.dc.add.5: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3801.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3801.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.sqrt.6: {type: sqrt, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_3801.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3801.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3801.dc.reciprocal.7, lc.input_tensor.layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3801.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [buffer_2_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3801.dc.multiply.8: {type: multiply, grid_loc: [4, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3801.dc.subtract.1_layernorm_3801.dc.multiply.8, layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0,
        layer.23.attention.output.LayerNorm.weight], t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32,
      math_fidelity: HiFi3, attributes: {m_k: 1, u_kt: 1}}
    layernorm_3801.dc.multiply.9: {type: multiply, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3801.dc.multiply.8, layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}
    layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3801.dc.add.10: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_3801.dc.multiply.9, layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    matmul_3804: {type: matmul, grid_loc: [6, 0], grid_size: [4, 8], inputs: [layernorm_3801.dc.add.10, layer.23.intermediate.dense.weight, layer.23.intermediate.dense.bias], t: 1, mblock: [3, 4], ublock: [
        1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}],
      attributes: {bias: true, m_k: 32, u_kt: 1}}
    buffer_0_layernorm_3801.dc.add.10_add_3814: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3801.dc.add.10], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_59_temporal_epoch_59:
    target_device: 0
    input_count: 2
    gelu_3807: {type: gelu, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_3804_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b,
      intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    matmul_3810: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_3807, layer.23.output.dense.weight, layer.23.output.dense.bias], t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_2_tms: [broadcast: {r: 12}], attributes: {bias: true,
        m_k: 32, u_kt: 4}}
    add_3814: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_3810, e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c,
      in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [add_3814, lc.input_tensor.layernorm_3815.dc.reduce_avg.0.0], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {m_k: 1, u_kt: 32}}
    layernorm_3815.dc.subtract.1: {type: subtract, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_3814, layernorm_3815.dc.reduce_avg.0.lc1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [
        110, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {c: 32}]}
    layernorm_3815.dc.multiply.2: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3815.dc.subtract.1, layernorm_3815.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3815.dc.multiply.2, lc.input_tensor.layernorm_3815.dc.reduce_avg.3.0], t: 1, mblock: [3, 1],
      ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 32}], attributes: {
        m_k: 1, u_kt: 32}}
    layernorm_3815.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3815.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3815.4], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2,
      ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.sqrt.6: {type: sqrt, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3815.dc.add.5], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b],
      out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3815.dc.sqrt.6], t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [
        Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, attributes: {approximate_mode: false}}
    layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3815.dc.reciprocal.7, lc.input_tensor.layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.0],
      t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    buffer_2_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3815.dc.subtract.1], t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2,
      ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [buffer_2_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_1_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8], t: 1,
      mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3815.dc.multiply.8: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_3815.dc.subtract.1_layernorm_3815.dc.multiply.8, layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
      t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [
        broadcast: {c: 32}]}
    layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.weight],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3815.dc.multiply.9: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3815.dc.multiply.8, layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1], t: 1, mblock: [3, 8],
      ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 12}]}
    layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.bias],
      t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}}
    layernorm_3815.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3815.dc.multiply.9, layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true, t: 1, mblock: [
        3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {
            r: 12}]}


programs:
- run_fwd:
  - param: [$p_loop_count]
  - var: {$c_microbatch_size: 2, $c_zero: 0, $lptr_q82: 0, $gptr_q81: 0, $lptr_q81: 0, $lptr_q79: 0, $gptr_q78: 0, $gptr_q75: 0, $gptr_q77: 0, $gptr_q70: 0, $lptr_q68: 0, $lptr_q67: 0, $gptr_q65: 0, $gptr_q67: 0,
      $gptr_q64: 0, $lptr_q64: 0, $lptr_q71: 0, $gptr_q63: 0, $gptr_q61: 0, $gptr_q60: 0, $lptr_q60: 0, $gptr_q58: 0, $gptr_q57: 0, $lptr_q57: 0, $gptr_q56: 0, $lptr_q54: 0, $lptr_q53: 0, $lptr_q50: 0,
      $gptr_q49: 0, $lptr_q49: 0, $gptr_q79: 0, $gptr_q54: 0, $gptr_q53: 0, $gptr_q47: 0, $lptr_q47: 0, $gptr_q44: 0, $gptr_q72: 0, $gptr_q71: 0, $gptr_q68: 0, $lptr_q30: 0, $gptr_q32: 0, $gptr_q35: 0,
      $lptr_q25: 0, $gptr_q30: 0, $gptr_q43: 0, $lptr_q32: 0, $gptr_q33: 0, $gptr_q82: 0, $lptr_q65: 0, $gptr_q29: 0, $lptr_q72: 0, $lptr_q51: 0, $lptr_q26: 0, $lptr_q16: 0, $gptr_q39: 0, $lptr_q58: 0,
      $gptr_q83: 0, $lptr_q23: 0, $gptr_q51: 0, $lptr_q39: 0, $gptr_q25: 0, $gptr_q26: 0, $lptr_q9: 0, $gptr_q40: 0, $lptr_q2: 0, $gptr_q37: 0, $gptr_q42: 0, $lptr_q14: 0, $gptr_q18: 0, $lptr_q74: 0, $lptr_q21: 0,
      $lptr_q29: 0, $lptr_q36: 0, $gptr_q5: 0, $gptr_q28: 0, $gptr_q2: 0, $gptr_q36: 0, $lptr_q83: 0, $lptr_q4: 0, $lptr_q56: 0, $lptr_q35: 0, $lptr_q70: 0, $lptr_q44: 0, $lptr_q43: 0, $gptr_q4: 0, $lptr_q33: 0,
      $lptr_q61: 0, $lptr_q7: 0, $gptr_q22: 0, $c_one: 1, $gptr_q9: 0, $lptr_q78: 0, $gptr_q50: 0, $lptr_q46: 0, $gptr_q21: 0, $lptr_q5: 0, $lptr_q19: 0, $gptr_q19: 0, $lptr_q18: 0, $lptr_q8: 0, $lptr_q40: 0,
      $gptr_q8: 0, $gptr_q14: 0, $lptr_q28: 0, $lptr_q11: 0, $gptr_q23: 0, $lptr_q15: 0, $gptr_q74: 0, $gptr_q11: 0, $gptr_q46: 0, $lptr_q22: 0, $lptr_q12: 0, $lptr_q75: 0, $lptr_q37: 0, $gptr_q15: 0, $lptr_q63: 0,
      $gptr_q12: 0, $lptr_q77: 0, $gptr_q16: 0, $gptr_q7: 0, $lptr_q42: 0}
  - staticvar: {$gptr_q80: 0, $lptr_q80: 0, $gptr_q76_shadow: 0, $gptr_q76: 0, $gptr_q69_shadow: 0, $gptr_q69: 0, $lptr_q69: 0, $gptr_q66_shadow: 0, $gptr_q73: 0, $gptr_q66: 0, $gptr_q34: 0, $lptr_q3: 0,
      $lptr_q0: 0, $lptr_q10: 0, $gptr_q48_shadow: 0, $gptr_q1: 0, $gptr_q31_shadow: 0, $lptr_q59: 0, $gptr_q20: 0, $gptr_q6: 0, $gptr_q17: 0, $gptr_q45: 0, $gptr_q73_shadow: 0, $gptr_q62_shadow: 0, $gptr_q17_shadow: 0,
      $gptr_q0: 0, $gptr_q20_shadow: 0, $lptr_q66: 0, $gptr_q10_shadow: 0, $gptr_q41: 0, $gptr_q24: 0, $lptr_q20: 0, $gptr_q27_shadow: 0, $gptr_q52_shadow: 0, $gptr_q27: 0, $lptr_q6: 0, $gptr_q38_shadow: 0,
      $lptr_q34: 0, $lptr_q45: 0, $gptr_q6_shadow: 0, $gptr_q45_shadow: 0, $gptr_q34_shadow: 0, $lptr_q31: 0, $lptr_q27: 0, $gptr_q3_shadow: 0, $gptr_q31: 0, $lptr_q24: 0, $gptr_q52: 0, $gptr_q55_shadow: 0,
      $gptr_q24_shadow: 0, $gptr_q10: 0, $lptr_q13: 0, $lptr_q1: 0, $gptr_q13_shadow: 0, $gptr_q13: 0, $lptr_q17: 0, $lptr_q41: 0, $gptr_q1_shadow: 0, $gptr_q41_shadow: 0, $lptr_q48: 0, $lptr_q73: 0, $lptr_q52: 0,
      $lptr_q38: 0, $gptr_q48: 0, $lptr_q55: 0, $gptr_q55: 0, $gptr_q3: 0, $gptr_q59: 0, $gptr_q38: 0, $gptr_q59_shadow: 0, $lptr_q76: 0, $lptr_q62: 0, $gptr_q62: 0}
  - varinst: [$gptr_q38, set, $gptr_q38_shadow]
  - varinst: [$gptr_q34, set, $gptr_q34_shadow]
  - varinst: [$gptr_q31, set, $gptr_q31_shadow]
  - varinst: [$gptr_q27, set, $gptr_q27_shadow]
  - varinst: [$gptr_q24, set, $gptr_q24_shadow]
  - varinst: [$gptr_q20, set, $gptr_q20_shadow]
  - varinst: [$gptr_q1, set, $gptr_q1_shadow]
  - varinst: [$gptr_q3, set, $gptr_q3_shadow]
  - varinst: [$gptr_q6, set, $gptr_q6_shadow]
  - varinst: [$gptr_q10, set, $gptr_q10_shadow]
  - varinst: [$gptr_q13, set, $gptr_q13_shadow]
  - varinst: [$gptr_q17, set, $gptr_q17_shadow]
  - varinst: [$gptr_q41, set, $gptr_q41_shadow]
  - varinst: [$gptr_q45, set, $gptr_q45_shadow]
  - varinst: [$gptr_q48, set, $gptr_q48_shadow]
  - varinst: [$gptr_q52, set, $gptr_q52_shadow]
  - varinst: [$gptr_q55, set, $gptr_q55_shadow]
  - varinst: [$gptr_q59, set, $gptr_q59_shadow]
  - varinst: [$gptr_q62, set, $gptr_q62_shadow]
  - varinst: [$gptr_q66, set, $gptr_q66_shadow]
  - varinst: [$gptr_q69, set, $gptr_q69_shadow]
  - varinst: [$gptr_q73, set, $gptr_q73_shadow]
  - varinst: [$gptr_q76, set, $gptr_q76_shadow]
  - loop: $p_loop_count
  - allocate_queue: [e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0]
  - execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {hidden_states: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}, attention_mask: {prologue: false,
          epilogue: false, zero: false, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}, layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2560_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2562.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2582.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2582.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2582.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
  - allocate_queue: [e2e_layernorm_2596.dc.subtract.1_0, e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0]
  - execute: {graph_name: fwd_1_temporal_epoch_1, queue_settings: {e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
        e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}, lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2596.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2582.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2582.dc.subtract.1_layernorm_2582.dc.multiply.8_0]
  - varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_add_2634_0]
  - execute: {graph_name: fwd_2_temporal_epoch_2, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}, e2e_layernorm_2596.dc.subtract.1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4}, lc.input_tensor.layernorm_2596.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        dc.input_tensor.layernorm_2596.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2596.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2613_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2615.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2596.dc.subtract.1_0, e2e_buffer_0_layernorm_2596.dc.subtract.1_layernorm_2596.dc.multiply.8_0]
  - varinst: [$gptr_q3_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q4, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q3, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q4, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2635.dc.add.10_0, e2e_gelu_2641_0]
  - execute: {graph_name: fwd_3_temporal_epoch_3, queue_settings: {e2e_add_2634_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}, lc.input_tensor.layernorm_2635.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2635.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2635.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2635.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_add_2634_0]
  - varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2649.dc.add.10_0, e2e_softmax_2668.dc.exp.0_0, e2e_softmax_2668.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_4_temporal_epoch_4, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}, e2e_layernorm_2635.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}, e2e_gelu_2641_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
        layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2649.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_2649.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2649.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2649.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2666_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2668.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2635.dc.add.10_0, e2e_gelu_2641_0]
  - varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2688.dc.add.10_0]
  - execute: {graph_name: fwd_5_temporal_epoch_5, queue_settings: {e2e_layernorm_2649.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_softmax_2668.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}, e2e_softmax_2668.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q8,
          rd_ptr_global: $gptr_q8}, layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2688.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2688.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2688.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2688.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2649.dc.add.10_0, e2e_softmax_2668.dc.exp.0_0, e2e_softmax_2668.dc.reciprocal.2_0]
  - varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0]
  - execute: {graph_name: fwd_6_temporal_epoch_6, queue_settings: {e2e_layernorm_2688.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9}, layer.2.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.2.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2702.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2702.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2702.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2688.dc.add.10_0]
  - varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2741.dc.subtract.1_0, e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0]
  - execute: {graph_name: fwd_7_temporal_epoch_7, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10}, e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11}, lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2719_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2721.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2741.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2702.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2702.dc.subtract.1_layernorm_2702.dc.multiply.8_0]
  - varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q11, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q10, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q11, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2741.dc.add.10_0, e2e_gelu_2747_0]
  - execute: {graph_name: fwd_8_temporal_epoch_8, queue_settings: {e2e_layernorm_2741.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
        e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}, lc.input_tensor.layernorm_2741.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2741.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2741.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2741.dc.subtract.1_0, e2e_buffer_2_layernorm_2741.dc.subtract.1_layernorm_2741.dc.multiply.8_0]
  - varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2755.dc.add.10_0, e2e_softmax_2774.dc.exp.0_0]
  - execute: {graph_name: fwd_9_temporal_epoch_9, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13}, e2e_layernorm_2741.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}, e2e_gelu_2747_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
        layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2755.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_2755.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2755.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2755.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2772_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2741.dc.add.10_0, e2e_gelu_2747_0]
  - varinst: [$gptr_q13_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q14, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q13, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2794.dc.add.10_0]
  - execute: {graph_name: fwd_10_temporal_epoch_10, queue_settings: {e2e_layernorm_2755.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15}, e2e_softmax_2774.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15}, lc.input_tensor.softmax_2774.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2794.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2794.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2794.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2794.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2755.dc.add.10_0, e2e_softmax_2774.dc.exp.0_0]
  - varinst: [$gptr_q15, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q15, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2808.dc.reciprocal.7_0, e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0]
  - execute: {graph_name: fwd_11_temporal_epoch_11, queue_settings: {e2e_layernorm_2794.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16}, layer.4.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.4.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2808.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2808.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2808.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2794.dc.add.10_0]
  - varinst: [$gptr_q16, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q16, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2847.dc.subtract.1_0]
  - execute: {graph_name: fwd_12_temporal_epoch_12, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17}, e2e_layernorm_2808.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18}, lc.input_tensor.layernorm_2808.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2825_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2827.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2808.dc.reciprocal.7_0, e2e_buffer_0_layernorm_2808.dc.subtract.1_layernorm_2808.dc.multiply.8_0]
  - varinst: [$gptr_q17_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q18, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q17, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q18, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_matmul_2856_0, e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0]
  - execute: {graph_name: fwd_13_temporal_epoch_13, queue_settings: {e2e_layernorm_2847.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
        lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2847.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.intermediate.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2847.dc.subtract.1_0]
  - varinst: [$gptr_q19, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q19, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2861.dc.add.10_0, e2e_softmax_2880.dc.multiply.3_0, e2e_matmul_2884_0]
  - execute: {graph_name: fwd_14_temporal_epoch_14, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20}, e2e_matmul_2856_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21}, e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21}, lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2861.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2878_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2880.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_matmul_2856_0, e2e_buffer_0_layernorm_2847.dc.add.10_add_2860_0]
  - varinst: [$gptr_q20_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q21, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q20, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q21, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2900.dc.add.10_0]
  - execute: {graph_name: fwd_15_temporal_epoch_15, queue_settings: {e2e_layernorm_2861.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22}, e2e_softmax_2880.dc.multiply.3_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22}, e2e_matmul_2884_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
        layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.attention.output.dense.bias: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2900.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_2900.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2900.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2900.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2861.dc.add.10_0, e2e_softmax_2880.dc.multiply.3_0, e2e_matmul_2884_0]
  - varinst: [$gptr_q22, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q22, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0]
  - execute: {graph_name: fwd_16_temporal_epoch_16, queue_settings: {e2e_layernorm_2900.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23}, layer.6.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.6.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2914.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2914.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2914.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2900.dc.add.10_0]
  - varinst: [$gptr_q23, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q23, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2953.dc.subtract.1_0, e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0]
  - execute: {graph_name: fwd_17_temporal_epoch_17, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24}, e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25}, e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25}, lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2931_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2933.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2953.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2914.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2914.dc.subtract.1_layernorm_2914.dc.multiply.8_0]
  - varinst: [$gptr_q24_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q25, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q24, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q25, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2953.dc.add.10_0, e2e_gelu_2959_0]
  - execute: {graph_name: fwd_18_temporal_epoch_18, queue_settings: {e2e_layernorm_2953.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
        e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26}, lc.input_tensor.layernorm_2953.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2953.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2953.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2953.dc.subtract.1_0, e2e_buffer_2_layernorm_2953.dc.subtract.1_layernorm_2953.dc.multiply.8_0]
  - varinst: [$gptr_q26, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q26, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_2967.dc.add.10_0, e2e_softmax_2986.dc.exp.0_0, e2e_softmax_2986.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_19_temporal_epoch_19, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27}, e2e_layernorm_2953.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28}, e2e_gelu_2959_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
        layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_2967.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_2967.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_2967.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_2967.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_2984_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_2986.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2953.dc.add.10_0, e2e_gelu_2959_0]
  - varinst: [$gptr_q27_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q28, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q27, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q28, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3006.dc.add.10_0]
  - execute: {graph_name: fwd_20_temporal_epoch_20, queue_settings: {e2e_layernorm_2967.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29}, e2e_softmax_2986.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29}, e2e_softmax_2986.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q29,
          rd_ptr_global: $gptr_q29}, layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3006.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3006.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3006.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3006.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_2967.dc.add.10_0, e2e_softmax_2986.dc.exp.0_0, e2e_softmax_2986.dc.reciprocal.2_0]
  - varinst: [$gptr_q29, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q29, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3020.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0]
  - execute: {graph_name: fwd_21_temporal_epoch_21, queue_settings: {e2e_layernorm_3006.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30}, layer.8.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.8.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3020.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3020.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3020.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3006.dc.add.10_0]
  - varinst: [$gptr_q30, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q30, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3059.dc.subtract.1_0]
  - execute: {graph_name: fwd_22_temporal_epoch_22, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31}, e2e_layernorm_3020.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32}, e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32}, lc.input_tensor.layernorm_3020.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.key.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3037_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3039.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3059.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3020.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3020.dc.subtract.1_layernorm_3020.dc.multiply.8_0]
  - varinst: [$gptr_q31_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q32, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q31, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q32, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3059.dc.add.10_0, e2e_gelu_3065_0]
  - execute: {graph_name: fwd_23_temporal_epoch_23, queue_settings: {e2e_layernorm_3059.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
        lc.input_tensor.layernorm_3059.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3059.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3059.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.intermediate.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3059.dc.subtract.1_0]
  - varinst: [$gptr_q33, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q33, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3073.dc.add.10_0, e2e_softmax_3092.dc.exp.0_0]
  - execute: {graph_name: fwd_24_temporal_epoch_24, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34}, e2e_layernorm_3059.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35}, e2e_gelu_3065_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
        layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3073.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3073.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3073.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3073.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3090_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3059.dc.add.10_0, e2e_gelu_3065_0]
  - varinst: [$gptr_q34_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q35, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q34, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q35, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3112.dc.add.10_0]
  - execute: {graph_name: fwd_25_temporal_epoch_25, queue_settings: {e2e_layernorm_3073.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36}, e2e_softmax_3092.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36}, lc.input_tensor.softmax_3092.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3112.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3112.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3112.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3112.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3073.dc.add.10_0, e2e_softmax_3092.dc.exp.0_0]
  - varinst: [$gptr_q36, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q36, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3126.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0]
  - execute: {graph_name: fwd_26_temporal_epoch_26, queue_settings: {e2e_layernorm_3112.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37}, layer.10.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.10.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3126.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3126.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3126.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3112.dc.add.10_0]
  - varinst: [$gptr_q37, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q37, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3165.dc.subtract.1_0]
  - execute: {graph_name: fwd_27_temporal_epoch_27, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38}, e2e_layernorm_3126.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39}, e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39}, lc.input_tensor.layernorm_3126.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3143_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3145.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3165.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3126.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3126.dc.subtract.1_layernorm_3126.dc.multiply.8_0]
  - varinst: [$gptr_q38_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q39, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q38, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q39, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_matmul_3174_0, e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0]
  - execute: {graph_name: fwd_28_temporal_epoch_28, queue_settings: {e2e_layernorm_3165.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
        lc.input_tensor.layernorm_3165.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3165.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3165.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.intermediate.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3165.dc.subtract.1_0]
  - varinst: [$gptr_q40, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q40, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3179.dc.add.10_0, e2e_softmax_3198.dc.multiply.3_0, e2e_matmul_3202_0]
  - execute: {graph_name: fwd_29_temporal_epoch_29, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41}, e2e_matmul_3174_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42}, e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42}, lc.input_tensor.layernorm_3179.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3179.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3179.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3179.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3196_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3198.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.12.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_matmul_3174_0, e2e_buffer_0_layernorm_3165.dc.add.10_add_3178_0]
  - varinst: [$gptr_q41_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q42, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q41, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q42, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3218.dc.add.10_0]
  - execute: {graph_name: fwd_30_temporal_epoch_30, queue_settings: {e2e_layernorm_3179.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43}, e2e_softmax_3198.dc.multiply.3_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43}, e2e_matmul_3202_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
        layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.attention.output.dense.bias: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3218.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3218.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3218.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3218.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3179.dc.add.10_0, e2e_softmax_3198.dc.multiply.3_0, e2e_matmul_3202_0]
  - varinst: [$gptr_q43, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q43, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0]
  - execute: {graph_name: fwd_31_temporal_epoch_31, queue_settings: {e2e_layernorm_3218.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44}, layer.12.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.12.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3232.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3232.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3232.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3218.dc.add.10_0]
  - varinst: [$gptr_q44, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q44, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3271.dc.subtract.1_0, e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0]
  - execute: {graph_name: fwd_32_temporal_epoch_32, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45}, e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46}, e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46}, lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3249_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3251.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3271.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3232.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3232.dc.subtract.1_layernorm_3232.dc.multiply.8_0]
  - varinst: [$gptr_q45_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q46, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q45, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q46, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3271.dc.add.10_0, e2e_gelu_3277_0]
  - execute: {graph_name: fwd_33_temporal_epoch_33, queue_settings: {e2e_layernorm_3271.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
        e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47}, lc.input_tensor.layernorm_3271.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3271.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3271.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3271.dc.subtract.1_0, e2e_buffer_2_layernorm_3271.dc.subtract.1_layernorm_3271.dc.multiply.8_0]
  - varinst: [$gptr_q47, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q47, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3285.dc.add.10_0, e2e_softmax_3304.dc.exp.0_0, e2e_softmax_3304.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_34_temporal_epoch_34, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48}, e2e_layernorm_3271.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49}, e2e_gelu_3277_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
        layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3285.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3285.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3285.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3285.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.13.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3302_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3304.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3271.dc.add.10_0, e2e_gelu_3277_0]
  - varinst: [$gptr_q48_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q49, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q48, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q49, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3324.dc.add.10_0]
  - execute: {graph_name: fwd_35_temporal_epoch_35, queue_settings: {e2e_layernorm_3285.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50}, e2e_softmax_3304.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50}, e2e_softmax_3304.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q50,
          rd_ptr_global: $gptr_q50}, layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3324.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3324.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3324.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3324.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3285.dc.add.10_0, e2e_softmax_3304.dc.exp.0_0, e2e_softmax_3304.dc.reciprocal.2_0]
  - varinst: [$gptr_q50, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q50, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3338.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0]
  - execute: {graph_name: fwd_36_temporal_epoch_36, queue_settings: {e2e_layernorm_3324.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51}, layer.14.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.14.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3338.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3338.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3338.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3324.dc.add.10_0]
  - varinst: [$gptr_q51, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q51, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3377.dc.subtract.1_0]
  - execute: {graph_name: fwd_37_temporal_epoch_37, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q52, rd_ptr_global: $gptr_q52}, e2e_layernorm_3338.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53}, e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q53, rd_ptr_global: $gptr_q53}, lc.input_tensor.layernorm_3338.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3355_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3357.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.15.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3377.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3338.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3338.dc.subtract.1_layernorm_3338.dc.multiply.8_0]
  - varinst: [$gptr_q52_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q53, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q52, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q53, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3377.dc.add.10_0, e2e_gelu_3383_0]
  - execute: {graph_name: fwd_38_temporal_epoch_38, queue_settings: {e2e_layernorm_3377.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q54, rd_ptr_global: $gptr_q54},
        lc.input_tensor.layernorm_3377.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3377.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3377.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.intermediate.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3377.dc.subtract.1_0]
  - varinst: [$gptr_q54, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q54, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3391.dc.add.10_0, e2e_softmax_3410.dc.exp.0_0]
  - execute: {graph_name: fwd_39_temporal_epoch_39, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q55, rd_ptr_global: $gptr_q55}, e2e_layernorm_3377.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56}, e2e_gelu_3383_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q56, rd_ptr_global: $gptr_q56},
        layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3391.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3391.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3391.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3391.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.15.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3408_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3377.dc.add.10_0, e2e_gelu_3383_0]
  - varinst: [$gptr_q55_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q56, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q55, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q56, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3430.dc.add.10_0]
  - execute: {graph_name: fwd_40_temporal_epoch_40, queue_settings: {e2e_layernorm_3391.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57}, e2e_softmax_3410.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q57, rd_ptr_global: $gptr_q57}, lc.input_tensor.softmax_3410.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3430.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3430.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3430.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3430.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3391.dc.add.10_0, e2e_softmax_3410.dc.exp.0_0]
  - varinst: [$gptr_q57, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q57, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3444.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0]
  - execute: {graph_name: fwd_41_temporal_epoch_41, queue_settings: {e2e_layernorm_3430.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q58, rd_ptr_global: $gptr_q58}, layer.16.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.16.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3444.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3444.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3444.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3430.dc.add.10_0]
  - varinst: [$gptr_q58, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q58, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3483.dc.subtract.1_0]
  - execute: {graph_name: fwd_42_temporal_epoch_42, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q59, rd_ptr_global: $gptr_q59}, e2e_layernorm_3444.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60}, e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q60, rd_ptr_global: $gptr_q60}, lc.input_tensor.layernorm_3444.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3461_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3463.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3483.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3444.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3444.dc.subtract.1_layernorm_3444.dc.multiply.8_0]
  - varinst: [$gptr_q59_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q60, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q59, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q60, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_matmul_3492_0, e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0]
  - execute: {graph_name: fwd_43_temporal_epoch_43, queue_settings: {e2e_layernorm_3483.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q61, rd_ptr_global: $gptr_q61},
        lc.input_tensor.layernorm_3483.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3483.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3483.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.intermediate.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3483.dc.subtract.1_0]
  - varinst: [$gptr_q61, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q61, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3497.dc.add.10_0, e2e_softmax_3516.dc.multiply.3_0, e2e_matmul_3520_0]
  - execute: {graph_name: fwd_44_temporal_epoch_44, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q62, rd_ptr_global: $gptr_q62}, e2e_matmul_3492_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63}, e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr_q63, rd_ptr_global: $gptr_q63}, lc.input_tensor.layernorm_3497.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3497.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3497.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3497.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3514_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3516.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.18.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_matmul_3492_0, e2e_buffer_0_layernorm_3483.dc.add.10_add_3496_0]
  - varinst: [$gptr_q62_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q63, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q62, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q63, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3536.dc.add.10_0]
  - execute: {graph_name: fwd_45_temporal_epoch_45, queue_settings: {e2e_layernorm_3497.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64}, e2e_softmax_3516.dc.multiply.3_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64}, e2e_matmul_3520_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q64, rd_ptr_global: $gptr_q64},
        layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.attention.output.dense.bias: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3536.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3536.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3536.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3536.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3497.dc.add.10_0, e2e_softmax_3516.dc.multiply.3_0, e2e_matmul_3520_0]
  - varinst: [$gptr_q64, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q64, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0]
  - execute: {graph_name: fwd_46_temporal_epoch_46, queue_settings: {e2e_layernorm_3536.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q65, rd_ptr_global: $gptr_q65}, layer.18.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.18.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3550.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3550.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3550.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3536.dc.add.10_0]
  - varinst: [$gptr_q65, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q65, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3589.dc.subtract.1_0, e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0]
  - execute: {graph_name: fwd_47_temporal_epoch_47, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q66, rd_ptr_global: $gptr_q66}, e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67}, e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q67, rd_ptr_global: $gptr_q67}, lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.self.query.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3567_tile_bcast_tile_bcast: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3569.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3589.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3550.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_3550.dc.subtract.1_layernorm_3550.dc.multiply.8_0]
  - varinst: [$gptr_q66_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q67, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q66, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q67, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3589.dc.add.10_0, e2e_gelu_3595_0]
  - execute: {graph_name: fwd_48_temporal_epoch_48, queue_settings: {e2e_layernorm_3589.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68},
        e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q68, rd_ptr_global: $gptr_q68}, lc.input_tensor.layernorm_3589.dc.reduce_avg.3.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3589.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3589.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.intermediate.dense.weight: {prologue: true, epilogue: false,
          zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3589.dc.subtract.1_0, e2e_buffer_2_layernorm_3589.dc.subtract.1_layernorm_3589.dc.multiply.8_0]
  - varinst: [$gptr_q68, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q68, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3603.dc.add.10_0, e2e_softmax_3622.dc.exp.0_0, e2e_softmax_3622.dc.reciprocal.2_0]
  - execute: {graph_name: fwd_49_temporal_epoch_49, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q69, rd_ptr_global: $gptr_q69}, e2e_layernorm_3589.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70}, e2e_gelu_3595_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q70, rd_ptr_global: $gptr_q70},
        layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3603.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.19.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3620_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3622.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3589.dc.add.10_0, e2e_gelu_3595_0]
  - varinst: [$gptr_q69_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q70, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q69, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q70, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3642.dc.add.10_0]
  - execute: {graph_name: fwd_50_temporal_epoch_50, queue_settings: {e2e_layernorm_3603.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71}, e2e_softmax_3622.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q71, rd_ptr_global: $gptr_q71}, e2e_softmax_3622.dc.reciprocal.2_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q71,
          rd_ptr_global: $gptr_q71}, layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3642.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3642.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3642.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3642.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.20.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3603.dc.add.10_0, e2e_softmax_3622.dc.exp.0_0, e2e_softmax_3622.dc.reciprocal.2_0]
  - varinst: [$gptr_q71, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q71, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3656.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0]
  - execute: {graph_name: fwd_51_temporal_epoch_51, queue_settings: {e2e_layernorm_3642.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q72, rd_ptr_global: $gptr_q72}, layer.20.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.20.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3656.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3656.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3656.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3642.dc.add.10_0]
  - varinst: [$gptr_q72, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q72, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3695.dc.subtract.1_0]
  - execute: {graph_name: fwd_52_temporal_epoch_52, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q73, rd_ptr_global: $gptr_q73}, e2e_layernorm_3656.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q74, rd_ptr_global: $gptr_q74}, e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q74, rd_ptr_global: $gptr_q74}, lc.input_tensor.layernorm_3656.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.20.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3673_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3675.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.attention.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3695.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3656.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3656.dc.subtract.1_layernorm_3656.dc.multiply.8_0]
  - varinst: [$gptr_q73_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q74, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q73, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q74, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3695.dc.add.10_0, e2e_gelu_3701_0]
  - execute: {graph_name: fwd_53_temporal_epoch_53, queue_settings: {e2e_layernorm_3695.dc.subtract.1_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q75, rd_ptr_global: $gptr_q75},
        lc.input_tensor.layernorm_3695.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3695.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3695.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.intermediate.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3695.dc.subtract.1_0]
  - varinst: [$gptr_q75, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q75, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3709.dc.add.10_0, e2e_softmax_3728.dc.exp.0_0]
  - execute: {graph_name: fwd_54_temporal_epoch_54, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q76, rd_ptr_global: $gptr_q76}, e2e_layernorm_3695.dc.add.10_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q77, rd_ptr_global: $gptr_q77}, e2e_gelu_3701_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q77, rd_ptr_global: $gptr_q77},
        layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: false,
          rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3709.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
        lc.input_tensor.layernorm_3709.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3709.4: {prologue: true,
          epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3709.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3726_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3695.dc.add.10_0, e2e_gelu_3701_0]
  - varinst: [$gptr_q76_shadow, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q77, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q76, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q77, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3748.dc.add.10_0]
  - execute: {graph_name: fwd_55_temporal_epoch_55, queue_settings: {e2e_layernorm_3709.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78}, e2e_softmax_3728.dc.exp.0_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q78, rd_ptr_global: $gptr_q78}, lc.input_tensor.softmax_3728.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.self.value.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3748.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3748.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3748.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3748.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3709.dc.add.10_0, e2e_softmax_3728.dc.exp.0_0]
  - varinst: [$gptr_q78, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q78, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_layernorm_3762.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0]
  - execute: {graph_name: fwd_56_temporal_epoch_56, queue_settings: {e2e_layernorm_3748.dc.add.10_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q79, rd_ptr_global: $gptr_q79}, layer.22.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.22.output.dense.bias: {prologue: true,
          epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3762.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3762.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3762.4: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3748.dc.add.10_0]
  - varinst: [$gptr_q79, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q79, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_matmul_3792_0, e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0]
  - execute: {graph_name: fwd_57_temporal_epoch_57, queue_settings: {attention_mask: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q80, rd_ptr_global: $gptr_q80}, e2e_layernorm_3762.dc.reciprocal.7_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q81, rd_ptr_global: $gptr_q81}, e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0: {prologue: false, epilogue: false,
          zero: false, rd_ptr_local: $lptr_q81, rd_ptr_global: $gptr_q81}, lc.input_tensor.layernorm_3762.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.output.LayerNorm.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.query.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.key.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, input_1_multiply_3779_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, lc.input_tensor.softmax_3781.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.attention.self.value.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_layernorm_3762.dc.reciprocal.7_0, e2e_buffer_0_layernorm_3762.dc.subtract.1_layernorm_3762.dc.multiply.8_0]
  - varinst: [$gptr_q80, incwrap, $c_microbatch_size, 8]
  - varinst: [$gptr_q81, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q80, incwrap, $c_microbatch_size, 8]
  - varinst: [$lptr_q81, incwrap, $c_microbatch_size, 4]
  - allocate_queue: [e2e_matmul_3804_0, e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0]
  - execute: {graph_name: fwd_58_temporal_epoch_58, queue_settings: {e2e_matmul_3792_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q82, rd_ptr_global: $gptr_q82}, e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q82, rd_ptr_global: $gptr_q82}, layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3801.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3801.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3801.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3801.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.attention.output.LayerNorm.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_matmul_3792_0, e2e_buffer_0_layernorm_3762.dc.add.10_add_3800_0]
  - varinst: [$gptr_q82, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q82, incwrap, $c_microbatch_size, 4]
  - execute: {graph_name: fwd_59_temporal_epoch_59, queue_settings: {e2e_matmul_3804_0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q83, rd_ptr_global: $gptr_q83}, e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0: {
          prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q83, rd_ptr_global: $gptr_q83}, layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}, layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layernorm_3815.dc.reduce_avg.0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3815.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero,
          rd_ptr_global: $c_zero}, dc.input_tensor.layernorm_3815.4: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layernorm_3815.dc.reciprocal.7_s_brcst_m1_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false,
          zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {
          prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero,
          wr_ptr_global: $c_zero}}}
  - deallocate_queue: [e2e_matmul_3804_0, e2e_buffer_0_layernorm_3801.dc.add.10_add_3814_0]
  - varinst: [$gptr_q83, incwrap, $c_microbatch_size, 4]
  - varinst: [$lptr_q83, incwrap, $c_microbatch_size, 4]
  - endloop


