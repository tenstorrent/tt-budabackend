devices:
  arch: [wormhole, wormhole_b0, blackhole]

### This variant for wha0 differs only in the dram channels that are selected for each queue. This is because of a constraint imposed
### for wha0 where there is a HW bug for DRAM reads that can cause a hang, for which the work-around involves occassionally performing
### those DRAM reads through relay buffers that forward the results to the reader. To implement that work around, we require that all
### pipes reading from DRAM, where the consumer is on a different noc row must either:
###   1) have all queue allocations on the same dram channel if any of the pipe inputs are scatter buffers
###   2) queue allocations can be to any channel if none are scatter buffers
queues:
  input0: {type: queue, input: HOST, entries: 8, grid_size: [5, 5], t: 2, mblock: [1, 2], ublock: [1, 4], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x10000000], [3, 0x10044040], [3, 0x10088080],
      [3, 0x100cc0c0], [3, 0x10110100], [3, 0x10154140], [3, 0x10198180], [3, 0x101dc1c0], [3, 0x10220200], [3, 0x10264240], [3, 0x102a8280], [3, 0x102ec2c0], [3, 0x10330300], [3, 0x10374340], [3, 0x103b8380],
      [3, 0x103fc3c0], [3, 0x10440400], [3, 0x10484440], [3, 0x104c8480], [3, 0x1050c4c0], [3, 0x10550500], [3, 0x10594540], [3, 0x105d8580], [3, 0x1061c5c0], [3, 0x10660600]]}
  input1: {type: queue, input: HOST, entries: 8, grid_size: [5, 5], t: 2, mblock: [2, 1], ublock: [4, 8], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x10000000], [4, 0x10220040], [4, 0x10440080],
      [4, 0x106600c0], [4, 0x10880100], [4, 0x10aa0140], [4, 0x10cc0180], [4, 0x10ee01c0], [4, 0x11100200], [4, 0x11320240], [4, 0x11540280], [4, 0x117602c0], [4, 0x11980300], [4, 0x11ba0340], [4, 0x11dc0380],
      [4, 0x11fe03c0], [4, 0x12200400], [4, 0x12420440], [4, 0x12640480], [4, 0x128604c0], [4, 0x12a80500], [4, 0x12ca0540], [4, 0x12ec0580], [4, 0x130e05c0], [4, 0x13300600]]}
  output: {type: ram, input: op0, entries: 1, grid_size: [5, 5], t: 2, mblock: [1, 1], ublock: [1, 8], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x10000000], [5, 0x10008840], [5, 0x10011080],
      [5, 0x100198c0], [5, 0x10022100], [5, 0x1002a940], [5, 0x10033180], [5, 0x1003b9c0], [5, 0x10044200], [5, 0x1004ca40], [5, 0x10055280], [5, 0x1005dac0], [5, 0x10066300], [5, 0x1006eb40], [5, 0x10077380],
      [5, 0x1007fbc0], [5, 0x10088400], [5, 0x10090c40], [5, 0x10099480], [5, 0x100a1cc0], [5, 0x100aa500], [5, 0x100b2d40], [5, 0x100bb580], [5, 0x100c3dc0], [5, 0x100cc600]]}

graphs:
  test_op:
    target_device: 0
    input_count: 4
    op0:
      type: matmul
      grid_loc: [2, 0]   # [r,c]
      grid_size: [5, 5] # [r,c]
      inputs: [input0, input1]
      in_df: [Float16_b, Float16_b]
      acc_df: Float16
      out_df: Float16_b
      intermed_df: Float16_b
      ublock_order: r
      buf_size_mb: 2
      math_fidelity: HiFi3
      attributes: {m_k: 10, u_kt: 4}
      untilize_output: false
      gradient_op: true
      t: 2
      mblock: [1, 1]
      ublock: [1, 8]
      input_0_tms:
      input_1_tms: None

programs:
- program0:
  - staticvar: [$lptr, $gptr, $c_loop_count, $c_input_count]
  - var: [$ramrdptr, $ramwrptr]
  - varinst: [$c_loop_count, set, 1]      # load loop count
  - varinst: [$c_input_count, set, 4]      # load loop count
  - loop: $c_loop_count
  - execute: {graph_name: test_op, queue_settings: {input0: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr, rd_ptr_global: $gptr}, input1: {prologue: false, epilogue: false, zero: false,
          rd_ptr_local: $lptr, rd_ptr_global: $gptr}, output: {prologue: true, epilogue: true, zero: true, rd_ptr_global: $ramrdptr, wr_ptr_global: $ramwrptr}}}
  - varinst: [$lptr, incwrap, $c_input_count, 16]
  - varinst: [$gptr, incwrap, $c_input_count, 16]
  - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.9
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: -2.0
    uniform_upper_bound: 2.0

