# git checkout 4d373f6ae
# pytest pybuda/test/model_demos/high_prio/cnn/pytorch/test_efficientnet_lite.py::test_efficientnet_lite_0_pytorch[Wormhole_B0]

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                                                   {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 392], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: host, host: [[0, 0x20]]}

  # output
  pt_effnet_lite0.output_add_700:                                                            {input: add_700, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: host, host: [[0, 0x316260]]}

  # parameter
  stem.0.weight:                                                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4043b0a0], [2, 0x2413f40]]}
  stem.0.weight_fork_clone1856:                                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x40303ba0], [5, 0x33e8cc0]]}
  blocks.0.0._depthwise_conv.weight_fork_clone1204:                                          {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x2416f00], [2, 0x403b5c80], [3, 0x6a5cea0]]}
  blocks.0.0._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6a51cc0], [4, 0x40306b60], [5, 0x33ebc80]]}
  blocks.0.0._depthwise_conv.weight_fork_clone1202:                                          {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40453c40], [2, 0x242cae0], [2, 0x403cb860]]}
  blocks.0.0._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6a53d40]]}
  blocks.1.0._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x33edd00]]}
  blocks.1.0._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [3, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x401cfd60], [0, 0x225ff80], [0, 0x40bbe940]]}
  blocks.1.0._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x23e5e20], [2, 0x403854e0], [3, 0x6a2c700]]}
  blocks.1.1._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6a272a0]]}
  blocks.1.1._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [3, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x23e8fc0], [2, 0x40388680], [3, 0x6a2f8a0]]}
  blocks.1.1._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x31ee5a0], [1, 0x40437fe0], [2, 0x23f8b80], [2, 0x40398240], [3, 0x6a3f460]]}
  blocks.2.0._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6a2f840]]}
  blocks.2.0._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x23f9bc0], [2, 0x40399280], [3, 0x6a404a0], [3, 0x402f7f40], [4, 0x6a34900]]}
  blocks.2.0._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x33f49e0], [5, 0x401ebd00], [0, 0x229f280], [0, 0x40c02c80], [1, 0x3221bc0]]}
  blocks.2.1._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3233e60], [1, 0x4047f280]]}
  blocks.2.1._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x4022b720], [0, 0x22d1b00], [0, 0x40c16f80], [1, 0x323bf80], [1, 0x404873a0]]}
  blocks.2.1._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [2, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x244a120], [2, 0x403e6e60], [3, 0x6a88b80], [3, 0x403403c0]]}
  blocks.3.0._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40454c80], [2, 0x242db20]]}
  blocks.3.0._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [3, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x2286f60], [0, 0x40bea960], [1, 0x32098a0]]}
  blocks.3.0._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [2, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x403d5a00], [3, 0x6a77720], [3, 0x4032ef60], [4, 0x6a589e0]]}
  blocks.3.1._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x2275140], [0, 0x40bd8b40], [1, 0x31f7a80]]}
  blocks.3.1._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [3, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x33f7c40], [5, 0x401eef60], [0, 0x22a24e0]]}
  blocks.3.1._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [3, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40476140], [2, 0x2440fe0], [2, 0x403ddd20], [3, 0x6a7fa40], [3, 0x40337280]]}
  blocks.3.2._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x40312b40], [5, 0x3425200], [5, 0x4021c520]]}
  blocks.3.2._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [3, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x67c1340], [4, 0x40089f40], [5, 0x3157800]]}
  blocks.3.2._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [3, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x400916a0], [1, 0x3104de0], [1, 0x40256080], [2, 0x2130fa0], [2, 0x4003f460]]}
  blocks.4.0._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [3, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x40036340], [4, 0x67ee900], [4, 0x400b7500]]}
  blocks.4.0._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x4009cbe0], [1, 0x3110320], [1, 0x402615c0], [2, 0x213c4e0], [2, 0x4004a9a0]]}
  blocks.4.0._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [3, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6868a40], [4, 0x40131640], [5, 0x31fef00], [5, 0x400fa780], [0, 0x2200d60]]}
  blocks.4.1._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x315bca0], [1, 0x402acf40], [2, 0x2187e60], [2, 0x40096320]]}
  blocks.4.1._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x67fed00], [4, 0x400c7900], [5, 0x31951c0], [5, 0x40090a40], [0, 0x2197020]]}
  blocks.4.1._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [7, 1], t: 1, mblock: [3, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x40000000], [4, 0x67a0100], [4, 0x40000000], [5, 0x30d0100], [5, 0x40000000], [0, 0x20f53c0], [0, 0x40001040]]}
  blocks.4.2._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40001040], [2, 0x20e1140], [2, 0x40001040], [3, 0x67b53c0]]}
  blocks.4.2._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [5, 1], t: 1, mblock: [5, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x4000e5a0], [5, 0x30de6a0], [5, 0x4000e5a0], [0, 0x2103960], [0, 0x4000f5e0]]}
  blocks.4.2._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [7, 1], t: 1, mblock: [3, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x20f8640], [2, 0x40018540], [3, 0x67cc8c0], [3, 0x400236a0], [4, 0x67af6e0], [4, 0x400782e0], [5, 0x31483e0]]}
  blocks.5.0._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x216d6a0], [0, 0x40079320], [1, 0x30ec8a0], [1, 0x4002d9c0]]}
  blocks.5.0._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40042c80]]}
  blocks.5.0._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [21, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x67dc2e0]]}
  blocks.5.1._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [3, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x402c3400], [2, 0x219e320], [2, 0x400ac7e0], [3, 0x6888040]]}
  blocks.5.1._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 9], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x404a2c00]]}
  blocks.5.1._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [18, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x230a760], [2, 0x402a9e20]]}
  blocks.5.2._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [3, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x402840a0], [4, 0x69b7ec0], [4, 0x40280ac0], [5, 0x3360040]]}
  blocks.5.2._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 9], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x4082e7e0]]}
  blocks.5.2._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [18, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x2378040], [2, 0x40317700]]}
  blocks.5.3._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [3, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x402bb2c0], [4, 0x69efb80], [4, 0x402b8780], [5, 0x3397d00]]}
  blocks.5.3._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [25, 9], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x400eea00]]}
  blocks.5.3._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [18, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x21d4a00], [2, 0x400e2ec0]]}
  blocks.6.0._expand_conv.weight:                                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [3, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x4019f7e0], [4, 0x6878020], [4, 0x40140c20], [5, 0x320e4e0]]}
  blocks.6.0._depthwise_conv.weight:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [9, 9], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x40058f40]]}
  blocks.6.0._project_conv.weight:                                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [18, 10], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x401517e0], [3, 0x68c6380]]}
  head.0.weight:                                                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [5, 5], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x68af7e0], [4, 0x401783e0], [5, 0x3246740], [5, 0x4011db40]]}
  fc.weight:                                                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [40, 1], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40343fa0], [2, 0x22677c0], [2, 0x40206e80], [3, 0x697ba20], [3, 0x401e1100], [4, 0x6914480], [4, 0x401dd080], [5, 0x32ab3e0]]}
  fc.bias:                                                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x401827e0]]}

  # constant
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x40311280], [4, 0x6a4dc40], [4, 0x40302ae0], [5, 0x33e7c00], [5, 0x401df440], [0, 0x226f660], [0, 0x40bd2720], [1, 0x31f1660]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x319b980], [1, 0x403e53c0], [2, 0x2308be0], [2, 0x402a82a0], [3, 0x6a1ce40], [3, 0x40282520], [4, 0x69b58a0], [4, 0x4027e4a0]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0:  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x403b3600], [3, 0x6a5a820], [3, 0x40311c40], [4, 0x6a4e600], [4, 0x403034a0], [5, 0x33e85c0], [5, 0x401dfe00], [0, 0x2270020]]}
  lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1:  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x40bd30e0], [1, 0x31f2020], [1, 0x4043c0e0], [2, 0x2414f80], [2, 0x403b3d00], [3, 0x6a5af20], [3, 0x40312340], [4, 0x6a4ed00]]}
  input_1_add_1_fork_clone1082:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x401e0500]]}
  lc.input_tensor.conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x2270720], [0, 0x40bd4120], [1, 0x31f3060], [1, 0x4043d120], [2, 0x2415fc0], [2, 0x403b4d40], [3, 0x6a5bf60], [3, 0x40313380]]}
  lc.input_tensor.conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x6a4fd40], [4, 0x40304be0], [5, 0x33e9d00], [5, 0x401e1540], [0, 0x2271660], [0, 0x40bd5060], [1, 0x31f3fa0], [1, 0x4043e060]]}
  lc.input_tensor.conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x403142c0], [4, 0x6a50d80], [4, 0x40305c20], [5, 0x33ead40], [5, 0x401e2580], [0, 0x22726a0], [0, 0x40bd60a0], [1, 0x31f4fe0]]}
  lc.input_tensor.conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x401de400], [0, 0x226e620], [0, 0x40bd16e0], [1, 0x31f0620], [1, 0x4043a060], [2, 0x2412f00], [2, 0x403b25c0], [3, 0x6a597e0]]}
  lc.input_tensor.conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0:             {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x401e34c0], [0, 0x22735e0], [0, 0x40bd6fe0], [1, 0x31f5f20], [1, 0x40453120], [2, 0x242bfc0], [2, 0x403cad40], [3, 0x6a71f60]]}
  lc.input_tensor.conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1:             {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x40329280], [4, 0x6a52d00], [4, 0x40307ba0], [5, 0x33eccc0], [5, 0x401e3fe0], [0, 0x2274100], [0, 0x40bd7b00], [1, 0x31f6a40]]}
  input_1_add_16:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x6a72a80]]}
  input_1_add_16_fork_clone1059:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x4032a2c0]]}
  input_1_add_30_fork_clone1033:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x40308be0]]}
  input_1_add_43_fork_clone1003:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x401e5020]]}
  lc.input_tensor.conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [9, 2], t: 1, mblock: [1, 7], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x402f5b80], [4, 0x6a2e4c0], [4, 0x402f75e0], [5, 0x33d25c0], [5, 0x401dc040], [0, 0x226c260], [0, 0x40bcac20], [1, 0x31edbe0], [1, 0x40437620], [2, 0x23f81c0], [2, 0x40397880], [3, 0x6a3eaa0], [3, 0x402f6540], [4, 0x6a2ee80], [4, 0x402f7fa0], [5, 0x33d2f80], [5, 0x401dca00], [0, 0x226cc20]]}
  lc.input_tensor.conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x31e6980], [1, 0x404303c0], [2, 0x23e4de0], [2, 0x403844a0], [3, 0x6a2b6c0], [3, 0x402f19a0], [4, 0x6a26260], [4, 0x402eee60], [5, 0x33ce3e0]]}
  input_1_add_58:                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x31e79c0]]}
  input_1_add_58_fork_clone958:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40431400]]}
  input_1_add_72_fork_clone914:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x402f29e0]]}
  input_1_add_85_fork_clone997:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x402efea0]]}
  lc.input_tensor.conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                         {input: HOST, type: queue, entries: 1, grid_size: [7, 5], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x33cf420], [5, 0x401d8ea0], [0, 0x22690c0], [0, 0x40bc7a80], [1, 0x31eaa40], [1, 0x40434480], [2, 0x23e6e60], [2, 0x40386520], [3, 0x6a2d740], [3, 0x402f3a20], [4, 0x6a2c360], [4, 0x402f4f60], [5, 0x33cff40], [5, 0x401d99c0], [0, 0x2269be0], [0, 0x40bc85a0], [1, 0x31eb560], [1, 0x40434fa0], [2, 0x23e7980], [2, 0x40387040], [3, 0x6a2e260], [3, 0x402f4540], [4, 0x6a2ce80], [4, 0x402f5a80], [5, 0x33d0a60], [5, 0x401da4e0], [0, 0x226a700], [0, 0x40bc90c0], [1, 0x31ec080], [1, 0x40435ac0], [2, 0x23e84a0], [2, 0x40387b60], [3, 0x6a2ed80], [3, 0x402f5060], [4, 0x6a2d9a0]]}
  lc.input_tensor.conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                         {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x402f65a0], [5, 0x33d1580], [5, 0x401db000], [0, 0x226b220], [0, 0x40bc9be0], [1, 0x31ecba0], [1, 0x404365e0]]}
  input_1_add_100:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x40bb9880]]}
  input_1_add_100_fork_clone952:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x40bcb5e0]]}
  input_1_add_114_fork_clone909:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x402f6f00]]}
  input_1_add_128_fork_clone870:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x402f8960]]}
  lc.input_tensor.conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 233], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x4043f0a0], [2, 0x2417f40], [2, 0x403b6cc0], [3, 0x6a5dee0], [3, 0x40315200]]}
  lc.input_tensor.conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x401dd3c0], [0, 0x226d5e0], [0, 0x40bd06a0], [1, 0x31ef5e0], [1, 0x40439020]]}
  input_1_add_143:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x402fda20]]}
  input_1_add_143_fork_clone825:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x33e2b40]]}
  input_1_add_157_fork_clone781:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x40c14f20]]}
  input_1_add_170_fork_clone864:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6a63d80]]}
  lc.input_tensor.conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 2], t: 1, mblock: [1, 72], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x244e1c0], [2, 0x403eaf00], [3, 0x6a8cc20], [3, 0x40344460], [4, 0x6a6bea0], [4, 0x40329e60], [5, 0x343c520], [5, 0x40253c40], [0, 0x22fa020], [0, 0x40c3f4a0]]}
  lc.input_tensor.conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x32644a0], [1, 0x404af8c0], [2, 0x24544e0], [2, 0x403f1220], [3, 0x6a92f40]]}
  input_1_add_185:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x3434400]]}
  input_1_add_185_fork_clone819:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x40321d40]]}
  input_1_add_199_fork_clone776:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x22cfaa0]]}
  input_1_add_213_fork_clone729:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x403cc8a0]]}
  lc.input_tensor.conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x6a73ac0], [3, 0x4032b300], [4, 0x6a54d80], [4, 0x40309c20], [5, 0x33f0d80], [5, 0x401e80a0], [0, 0x2284340], [0, 0x40be7d40], [1, 0x3206c80]]}
  lc.input_tensor.conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[1, 0x4045cda0], [2, 0x2435c40], [2, 0x403d49c0], [3, 0x6a766e0], [3, 0x4032df20], [4, 0x6a579a0], [4, 0x4030c840], [5, 0x33f39a0], [5, 0x401eacc0]]}
  input_1_add_228:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4045dde0]]}
  input_1_add_228_fork_clone664:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x2436c80]]}
  input_1_add_242_fork_clone600:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x4030d880]]}
  input_1_add_255_fork_clone723:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40465f00]]}
  lc.input_tensor.conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 13], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x243eda0], [2, 0x403dbae0], [3, 0x6a7d800], [3, 0x40335040], [4, 0x6a5eac0], [4, 0x40310900], [5, 0x33f6a40], [5, 0x401edd60], [0, 0x22a12e0]]}
  lc.input_tensor.conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x40c04ce0], [1, 0x3223c20], [1, 0x40475100], [2, 0x243ffa0], [2, 0x403dcce0], [3, 0x6a7ea00], [3, 0x40336240], [4, 0x6a5fcc0], [4, 0x40311b00]]}
  input_1_add_270:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x40c05d20]]}
  input_1_add_270_fork_clone658:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3224c60]]}
  input_1_add_284_fork_clone595:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6a60d00]]}
  input_1_add_298_fork_clone705:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x33d3940]]}
  lc.input_tensor.conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 13], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[1, 0x3102ba0], [1, 0x40253e40], [2, 0x212ed60], [2, 0x4003d220], [3, 0x685b2c0], [3, 0x40034100], [4, 0x67c0140], [4, 0x40088d40], [5, 0x3156600]]}
  lc.input_tensor.conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x4007e400], [0, 0x21849e0], [0, 0x40090660], [1, 0x3103da0], [1, 0x40255040], [2, 0x212ff60], [2, 0x4003e420], [3, 0x685c4c0], [3, 0x40035300]]}
  input_1_add_313:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x4007f440]]}
  input_1_add_313_fork_clone637:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x2185a20]]}
  input_1_add_327_fork_clone577:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x685d500]]}
  input_1_add_341_fork_clone529:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x3184dc0]]}
  lc.input_tensor.conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [7, 3], t: 1, mblock: [1, 13], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[5, 0x4008e640], [0, 0x2194c20], [0, 0x4009a7e0], [1, 0x310df20], [1, 0x4025f1c0], [2, 0x213a0e0], [2, 0x400485a0], [3, 0x6860580], [3, 0x40045540], [4, 0x67fdb00], [4, 0x400c6700], [5, 0x3193fc0], [5, 0x4008f840], [0, 0x2195e20], [0, 0x4009b9e0], [1, 0x310f120], [1, 0x402603c0], [2, 0x213b2e0], [2, 0x400497a0], [3, 0x6861780], [3, 0x40046740]]}
  lc.input_tensor.conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[3, 0x400330c0], [4, 0x67bf100], [4, 0x40087d00], [5, 0x31555c0], [5, 0x4007d3c0], [0, 0x21839a0], [0, 0x4008f620]]}
  input_1_add_356:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x6862980]]}
  input_1_add_356_fork_clone464:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x40047940]]}
  input_1_add_370_fork_clone400:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x400e8560]]}
  input_1_add_383_fork_clone523:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x6871b80]]}
  lc.input_tensor.conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 4], t: 1, mblock: [1, 13], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x40056b40], [4, 0x6874be0], [4, 0x4013d7e0], [5, 0x320b0a0], [5, 0x40106920], [0, 0x220cf00], [0, 0x400ec600], [1, 0x3170f60], [1, 0x402c2200], [2, 0x219d120], [2, 0x400ab5e0], [3, 0x6886e40], [3, 0x40057d40], [4, 0x6875de0], [4, 0x4013e9e0], [5, 0x320c2a0], [5, 0x40107b20], [0, 0x220e100], [0, 0x400ed800], [1, 0x3172160]]}
  lc.input_tensor.conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[0, 0x40000000], [1, 0x30d0100], [1, 0x40000000], [2, 0x20e0100], [2, 0x40000000]]}
  input_1_add_398:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x20e0100]]}
  input_1_add_398_fork_clone458:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x67a0100]]}
  input_1_add_412_fork_clone395:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x30d1140]]}
  input_1_add_426_fork_clone505:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x4000c1a0]]}
  lc.input_tensor.conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 4], t: 1, mblock: [1, 13], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x67ac2a0], [4, 0x4000c1a0], [5, 0x30dc2a0], [5, 0x4000c1a0], [0, 0x2101560], [0, 0x4000d1e0], [1, 0x30d51e0], [1, 0x40016300], [2, 0x20f6400], [2, 0x40016300], [3, 0x67ca680], [3, 0x40021460], [4, 0x67ad4a0], [4, 0x4000d3a0], [5, 0x30dd4a0], [5, 0x4000d3a0], [0, 0x2102760], [0, 0x4000e3e0], [1, 0x30d63e0], [1, 0x40017500]]}
  lc.input_tensor.conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[2, 0x20f7600], [2, 0x40017500], [3, 0x67cb880], [3, 0x40022660], [4, 0x67ae6a0]]}
  input_1_add_441:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x30d75e0]]}
  input_1_add_441_fork_clone437:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x40018700]]}
  input_1_add_455_fork_clone377:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x400782e0]]}
  input_1_add_469_fork_clone325:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x21047e0]]}
  lc.input_tensor.conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 41], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[2, 0x400246e0], [3, 0x67d8a60], [3, 0x4002f840], [4, 0x67bb880], [4, 0x40084480]]}
  lc.input_tensor.conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3154580], [5, 0x4007c380], [0, 0x2182960], [0, 0x4008e5e0], [1, 0x3101b60]]}
  input_1_add_484:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x2119aa0]]}
  input_1_add_484_fork_clone242:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x40027f60]]}
  input_1_add_498_fork_clone157:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x401b6440]]}
  input_1_add_511_fork_clone319:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x334c800], [5, 0x401a2c00]]}
  lc.input_tensor.conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 4], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x224a620], [0, 0x404a20c0], [1, 0x319c9c0], [1, 0x403e6400], [2, 0x2309c20], [2, 0x402a92e0], [3, 0x6a1de80], [3, 0x40283560], [4, 0x69b68e0], [4, 0x4027f4e0], [5, 0x335ea60], [5, 0x401b4e60], [0, 0x224abc0], [0, 0x404a2660], [1, 0x319cf60], [1, 0x403e69a0], [2, 0x230a1c0], [2, 0x402a9880], [3, 0x6a1e420], [3, 0x40283b00]]}
  lc.input_tensor.conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x69b6e80], [4, 0x4027fa80], [5, 0x335f000], [5, 0x401b5400], [0, 0x224b160]]}
  input_1_add_526:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x319d500]]}
  input_1_add_526_fork_clone236:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x403e6f40]]}
  input_1_add_540_fork_clone152:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x6a1e9c0]]}
  input_1_add_554_fork_clone297:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x22383c0], [0, 0x4048fe60]]}
  lc.input_tensor.conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 4], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x224c1a0], [0, 0x4082dca0], [1, 0x31c19a0], [1, 0x4040b3e0], [2, 0x2377500], [2, 0x40316bc0], [3, 0x6a24aa0], [3, 0x402ba780], [4, 0x69ee5a0], [4, 0x402b71a0], [5, 0x3396720], [5, 0x401bc520], [0, 0x224c740], [0, 0x4082e240], [1, 0x31c1f40], [1, 0x4040b980], [2, 0x2377aa0], [2, 0x40317160], [3, 0x6a25040], [3, 0x402bad20]]}
  lc.input_tensor.conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x69eeb40], [4, 0x402b7740], [5, 0x3396cc0], [5, 0x401bcac0], [0, 0x224cce0]]}
  input_1_add_569:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x31c24e0]]}
  input_1_add_569_fork_clone209:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4040bf20]]}
  input_1_add_583_fork_clone129:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x6a255e0]]}
  input_1_add_597_fork_clone271:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x401bdb00], [0, 0x224dd20]]}
  lc.input_tensor.conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 4], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[3, 0x401d6460], [4, 0x68aeca0], [4, 0x401778a0], [5, 0x3245c00], [5, 0x4011d000], [0, 0x22235e0], [0, 0x4047b080], [1, 0x3198de0], [1, 0x4031f560], [2, 0x2242d80], [2, 0x40151240], [3, 0x68c5de0], [3, 0x401d6a00], [4, 0x68af240], [4, 0x40177e40], [5, 0x32461a0], [5, 0x4011d5a0], [0, 0x2223b80], [0, 0x4047b620], [1, 0x3199380]]}
  lc.input_tensor.conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [5, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[4, 0x6876fe0], [4, 0x4013fbe0], [5, 0x320d4a0], [5, 0x40108d20], [0, 0x220f300]]}
  input_1_add_612:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3173360]]}
  input_1_add_612_fork_clone180:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x402f9ae0]]}
  input_1_add_626_fork_clone112:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x68be720]]}
  input_1_add_640_fork_clone83:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x40109d60], [0, 0x2210340]]}
  lc.input_tensor.conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp2_b, target_device: 0, loc: dram, dram: [[0, 0x40479aa0], [1, 0x3197800], [1, 0x4031df80], [2, 0x22417a0], [2, 0x4014fc60], [3, 0x68c4800], [3, 0x401d5ec0], [4, 0x68ae700], [4, 0x40177300]]}
  lc.input_tensor.conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.1:                        {input: HOST, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x3244bc0], [5, 0x4011bfc0], [0, 0x22225a0], [0, 0x4047a040], [1, 0x3197da0], [1, 0x4031e520], [2, 0x2241d40], [2, 0x40150200], [3, 0x68c4da0]]}
  input_1_add_655:                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4031fb00]]}
  input_1_add_655_fork_clone60:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x2243320]]}
  input_1_add_669_fork_clone37:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 10], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x401d6fa0]]}
  input_1_add_682_fork_clone20:                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 5], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x2224120], [0, 0x4047bbc0]]}
  lc.input_tensor.avg_pool2d_695.dc.reduce_avg.2.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 2], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3199920]]}

  # epoch_to_epoch
  e2e_conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0:                     {input: conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, type: queue, entries: 1, grid_size: [8, 1], t: 49, mblock: [1, 1], ublock: [3, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x4028b5a0], [0, 0x2331980], [0, 0x40c76e00], [1, 0x3296b20], [1, 0x404e1f40], [2, 0x2486b60], [2, 0x404238a0], [3, 0x6ac55c0]]}
  e2e__fused_op_0_0:                                                                         {input: _fused_op_0, type: queue, entries: 1, grid_size: [8, 1], t: 49, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x404b0900], [2, 0x2455520], [2, 0x403f2260], [3, 0x6a93f80], [3, 0x4037bdc0], [4, 0x6aa3800], [4, 0x403617c0], [5, 0x3473e80]]}
  e2e_conv2d_15.dc.conv2d.5.dc.depthwise.10_0:                                               {input: conv2d_15.dc.conv2d.5.dc.depthwise.10, type: queue, entries: 1, grid_size: [8, 1], t: 49, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x4034a780], [4, 0x6a721c0], [4, 0x40330180], [5, 0x3442840], [5, 0x40259f60], [0, 0x2300340], [0, 0x40c457c0], [1, 0x32654e0]]}
  e2e_conv2d_42.dc.matmul.8_0:                                                               {input: conv2d_42.dc.matmul.8, type: queue, entries: 1, grid_size: [8, 1], t: 49, mblock: [1, 1], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x403ad400], [4, 0x6ad4e40], [4, 0x40392e00], [5, 0x34a54c0], [5, 0x4031f820], [0, 0x23c5c00], [0, 0x40d0b080], [1, 0x332ada0]]}
  e2e_conv2d_84.dc.matmul.8_0:                                                               {input: conv2d_84.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 1], t: 2, mblock: [7, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x405761c0], [2, 0x251ade0], [2, 0x404b7b20], [3, 0x6b59840], [3, 0x40441680], [4, 0x6b690c0], [4, 0x40427080]]}
  e2e_conv2d_113.dc.matmul.8_0:                                                              {input: conv2d_113.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 1], t: 2, mblock: [7, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x403f2260], [3, 0x6a93f80], [3, 0x4034a780], [4, 0x6a721c0], [4, 0x40330180], [5, 0x3450a20], [5, 0x40268140]]}
  e2e_conv2d_71.dc.matmul.8_0:                                                               {input: conv2d_71.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 1], t: 2, mblock: [7, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x3442840], [5, 0x40259f60], [0, 0x2300340], [0, 0x40c457c0], [1, 0x32654e0], [1, 0x404b0900], [2, 0x2455520]]}
  e2e_conv2d_142.dc.depthwise.10_0:                                                          {input: conv2d_142.dc.depthwise.10, type: queue, entries: 1, grid_size: [5, 1], t: 5, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x230e520], [0, 0x40c539a0], [1, 0x32736c0], [1, 0x404beae0], [2, 0x2463700]]}
  e2e_conv2d_198.dc.matmul.8_0:                                                              {input: conv2d_198.dc.matmul.8, type: queue, entries: 1, grid_size: [5, 1], t: 5, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x40400440], [3, 0x6aa2160], [3, 0x40358960], [4, 0x6a803a0], [4, 0x4033e360]]}
  e2e_conv2d_156.dc.matmul.8_0:                                                              {input: conv2d_156.dc.matmul.8, type: queue, entries: 1, grid_size: [5, 1], t: 5, mblock: [1, 1], ublock: [1, 2], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x3442840], [5, 0x40259f60], [0, 0x2300340], [0, 0x40c457c0], [1, 0x32654e0]]}
  e2e_conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0:                                {input: conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, type: queue, entries: 1, grid_size: [9, 1], t: 1, mblock: [1, 15], ublock: [7, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x344c9a0], [5, 0x402640c0], [0, 0x230a4a0], [0, 0x40c4f920], [1, 0x326f640], [1, 0x404b3980], [2, 0x24585a0], [2, 0x4040a5a0], [3, 0x6aac2c0]]}
  e2e_conv2d_241.dc.matmul.8_0:                                                              {input: conv2d_241.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 1], t: 3, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x404b0900], [2, 0x2455520], [2, 0x403f2260], [3, 0x6a93f80], [3, 0x4034a780], [4, 0x6a721c0], [4, 0x40330180]]}
  e2e__fused_op_8_0:                                                                         {input: _fused_op_8, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 5], ublock: [1, 3], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x40362ac0], [4, 0x6a8a500], [4, 0x403484c0], [5, 0x34b66e0], [5, 0x402cde00], [0, 0x23741e0], [0, 0x40cb9660]]}
  e2e_add_296_0:                                                                             {input: add_296, type: queue, entries: 1, grid_size: [7, 1], t: 3, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x32654e0], [1, 0x4051d6c0], [2, 0x24c22e0], [2, 0x403f52e0], [3, 0x6a97000], [3, 0x4034d800], [4, 0x6a75240]]}
  e2e_conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0:                                {input: conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, type: queue, entries: 1, grid_size: [7, 5], t: 1, mblock: [5, 3], ublock: [5, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x403576c0], [5, 0x34c58e0], [5, 0x402dd000], [0, 0x23833e0], [0, 0x40cc8860], [1, 0x32d9380], [1, 0x40520740], [2, 0x24c5360], [2, 0x404742e0], [3, 0x6b16000], [3, 0x40371cc0], [4, 0x6a99700], [4, 0x403a3040], [5, 0x3511260], [5, 0x40328980], [0, 0x23ced60], [0, 0x40d141e0], [1, 0x3324d00], [1, 0x4056c0c0], [2, 0x2510ce0], [2, 0x404bfc60], [3, 0x6b61980], [3, 0x403bd640], [4, 0x6ae5080], [4, 0x403ee9c0], [5, 0x355cbe0], [5, 0x40374300], [0, 0x241a6e0], [0, 0x40d5fb60], [1, 0x3370680], [1, 0x405b7a40], [2, 0x255c660], [2, 0x4050b5e0], [3, 0x6bad300], [3, 0x40408fc0]]}
  e2e_conv2d_382.dc.matmul.8_0:                                                              {input: conv2d_382.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6b30a00], [4, 0x40330180], [5, 0x3442840], [5, 0x40259f60], [0, 0x2300340], [0, 0x40c457c0], [1, 0x3268560]]}
  e2e_conv2d_397.dc.depthwise.10_0:                                                          {input: conv2d_397.dc.depthwise.10, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[5, 0x3457b00], [5, 0x4026f220], [0, 0x2315600], [0, 0x40c5aa80], [1, 0x327d820], [1, 0x404b49a0], [2, 0x24595c0]]}
  e2e_conv2d_369.dc.matmul.8_0:                                                              {input: conv2d_369.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 1], t: 4, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x404b0900], [2, 0x2455520], [2, 0x403f2260], [3, 0x6a93f80], [3, 0x4034a780], [4, 0x6a721c0], [4, 0x40345440]]}
  e2e_conv2d_425.dc.matmul.8_0:                                                              {input: conv2d_425.dc.matmul.8, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x403f6300], [3, 0x6a98020], [3, 0x4034e820], [4, 0x6a76260], [4, 0x403494e0], [5, 0x346cdc0], [5, 0x402844e0]]}
  e2e_conv2d_440.dc.depthwise.10_0:                                                          {input: conv2d_440.dc.depthwise.10, type: queue, entries: 1, grid_size: [7, 1], t: 1, mblock: [1, 7], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x40363ae0], [4, 0x6a8b520], [4, 0x40330180], [5, 0x3442840], [5, 0x40259f60], [0, 0x232a8c0], [0, 0x40c6fd40]]}
  e2e_add_424_0:                                                                             {input: add_424, type: queue, entries: 1, grid_size: [7, 1], t: 4, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x2300340], [0, 0x40c457c0], [1, 0x32654e0], [1, 0x404c9c60], [2, 0x246e880], [2, 0x4040b5c0], [3, 0x6aad2e0]]}
  e2e_conv2d_510.dc.matmul.8_0:                                                              {input: conv2d_510.dc.matmul.8, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[2, 0x2455520], [2, 0x403f2260], [3, 0x6a93f80], [3, 0x4034a780]]}
  e2e_conv2d_497.dc.matmul.8_0:                                                              {input: conv2d_497.dc.matmul.8, type: queue, entries: 1, grid_size: [2, 1], t: 6, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3269580], [1, 0x404b0900]]}
  e2e_conv2d_596.dc.matmul.8_0:                                                              {input: conv2d_596.dc.matmul.8, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[4, 0x6a721c0], [4, 0x40345440], [5, 0x3457b00], [5, 0x4026f220]]}
  e2e_add_595_0:                                                                             {input: add_595, type: queue, entries: 1, grid_size: [2, 1], t: 6, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[0, 0x2300340], [0, 0x40c457c0]]}
  e2e_conv2d_639.dc.matmul.8_0:                                                              {input: conv2d_639.dc.matmul.8, type: queue, entries: 1, grid_size: [2, 2], t: 1, mblock: [1, 6], ublock: [1, 3], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[1, 0x326f660], [1, 0x404b69e0], [2, 0x2467780], [2, 0x404044c0]]}
  e2e_avg_pool2d_695.dc.reduce_avg.2.lc1_0:                                                  {input: avg_pool2d_695.dc.reduce_avg.2.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 8, mblock: [1, 5], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float32, target_device: 0, loc: dram, dram: [[3, 0x6a93f80]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 1
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [8, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0, input_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 49, mblock: [1, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 392, num_index_tiles: 1, num_sparse_tiles: 7, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11: {type: matmul, grid_loc: [0, 1], grid_size: [8, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, stem.0.weight],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 16, hstack: 2, vstack: 8],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [8, 1], inputs: [lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0, input_1, lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1],
         t: 49, mblock: [1, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [transpose],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 392, num_index_tiles: 1, num_sparse_tiles: 5, sparse_tile_ptr_bits: 3, sparse_ublock_idx_bits: 3, u_kt: 1}}
    conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11: {type: matmul, grid_loc: [0, 3], grid_size: [8, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.lc2, stem.0.weight_fork_clone1856],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 16, hstack: 2, vstack: 8],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 4], grid_size: [8, 1], inputs: [conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.matmul.11, conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.matmul.11, input_1_add_1_fork_clone1082],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 392}, vslice: 49],
         attributes: {fused_op_id: 0, kernel_broadcast: {input_2: 1}}}
    conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [8, 1], inputs: [lc.input_tensor.conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 49, mblock: [1, 1], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 49],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 392, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_15.dc.conv2d.5.dc.depthwise.10: {type: depthwise, grid_loc: [0, 6], grid_size: [8, 1], inputs: [conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.0.0._depthwise_conv.weight_fork_clone1204],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 24, hstack: 3, vstack: 8],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 7], grid_size: [8, 1], inputs: [lc.input_tensor.conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0, _fused_op_0, lc.input_tensor.conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 49, mblock: [1, 1], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 49],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 392, num_index_tiles: 1, num_sparse_tiles: 11, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 1
    conv2d_15.dc.conv2d.1.dc.depthwise.10: {type: depthwise, grid_loc: [0, 0], grid_size: [8, 1], inputs: [e2e_conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0, blocks.0.0._depthwise_conv.weight],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 24, hstack: 3, vstack: 8],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 1], grid_size: [8, 1], inputs: [lc.input_tensor.conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e__fused_op_0_0, lc.input_tensor.conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 49, mblock: [1, 1], ublock: [3, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 49],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 392, num_index_tiles: 1, num_sparse_tiles: 8, sparse_tile_ptr_bits: 4, sparse_ublock_idx_bits: 4, u_kt: 1}}
    conv2d_15.dc.conv2d.3.dc.depthwise.10: {type: depthwise, grid_loc: [0, 2], grid_size: [8, 1], inputs: [conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.0.0._depthwise_conv.weight_fork_clone1202],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 49}, hslice: 49], input_0_tms: [vslice: 24, hstack: 3, vstack: 8],
         attributes: {l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    _fused_op_1: {type: fused_op, grid_loc: [0, 3], grid_size: [8, 1], inputs: [conv2d_15.dc.conv2d.1.dc.depthwise.10, conv2d_15.dc.conv2d.3.dc.depthwise.10, e2e_conv2d_15.dc.conv2d.5.dc.depthwise.10_0, input_1_add_16, input_1_add_16_fork_clone1059],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 24, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_4_tms: [broadcast: {r: 392}, vslice: 49], input_3_tms: [broadcast: {r: 392}, vslice: 49],
         attributes: {fused_op_id: 1, kernel_broadcast: {input_4: 1, input_3: 1}}}
    conv2d_29.dc.matmul.8: {type: matmul, grid_loc: [0, 4], grid_size: [8, 1], inputs: [_fused_op_1, blocks.0.0._project_conv.weight, input_1_add_30_fork_clone1033],
         t: 49, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 392}, vslice: 49], input_1_tms: [broadcast: {c: 49}, hslice: 49],
         attributes: {bias: true, kernel_broadcast: {input_2: 1, input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    conv2d_42.dc.matmul.8: {type: matmul, grid_loc: [0, 5], grid_size: [8, 1], inputs: [conv2d_29.dc.matmul.8, blocks.1.0._expand_conv.weight, input_1_add_43_fork_clone1003],
         t: 49, mblock: [1, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 392}, vslice: 49], input_1_tms: [broadcast: {c: 49}, hslice: 49],
         attributes: {bias: true, kernel_broadcast: {input_2: 3, input_1: 147}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 1
    conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [9, 3], inputs: [lc.input_tensor.conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_42.dc.matmul.8_0, lc.input_tensor.conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [14, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_1_tms: [vstack: 49], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 392, num_index_tiles: 1, num_sparse_tiles: 14, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_57.dc.depthwise.10: {type: depthwise, grid_loc: [0, 3], grid_size: [7, 1], inputs: [conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.0._depthwise_conv.weight],
         t: 1, mblock: [14, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 9, hstack: 9],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [0, 4], grid_size: [7, 1], inputs: [conv2d_57.dc.depthwise.10, input_1_add_58, input_1_add_58_fork_clone958],
         t: 1, mblock: [14, 1], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}], input_1_tms: [broadcast: {r: 98}],
         attributes: {fused_op_id: 2}}
    conv2d_71.dc.matmul.8: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [_fused_op_2, blocks.1.0._project_conv.weight, input_1_add_72_fork_clone914],
         t: 2, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 3, min_buffer_input: 0, u_kt: 1}}
    conv2d_84.dc.matmul.8: {type: matmul, grid_loc: [0, 6], grid_size: [7, 1], inputs: [conv2d_71.dc.matmul.8, blocks.1.1._expand_conv.weight, input_1_add_85_fork_clone997],
         t: 2, mblock: [7, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 5, input_1: 10}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 1
    conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [7, 5], inputs: [lc.input_tensor.conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_84.dc.matmul.8_0, lc.input_tensor.conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 2, mblock: [9, 1], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 5}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 5}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 98, num_index_tiles: 1, num_sparse_tiles: 40, sparse_tile_ptr_bits: 6, sparse_ublock_idx_bits: 6, u_kt: 1}}
    conv2d_99.dc.depthwise.10: {type: depthwise, grid_loc: [0, 5], grid_size: [7, 1], inputs: [conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.1.1._depthwise_conv.weight],
         t: 2, mblock: [7, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 2}, hslice: 2], input_0_tms: [vslice: 441, hstack: 9, vstack: 49],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 1}}
    _fused_op_3: {type: fused_op, grid_loc: [0, 6], grid_size: [7, 1], inputs: [conv2d_99.dc.depthwise.10, input_1_add_100, input_1_add_100_fork_clone952],
         t: 2, mblock: [7, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {r: 98}, vslice: 2],
         attributes: {fused_op_id: 3}}
    conv2d_113.dc.matmul.8: {type: matmul, grid_loc: [0, 7], grid_size: [7, 1], inputs: [_fused_op_3, blocks.1.1._project_conv.weight, input_1_add_114_fork_clone909],
         t: 2, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 1
    add_126: {type: add, grid_loc: [0, 0], grid_size: [7, 1], inputs: [e2e_conv2d_113.dc.matmul.8_0, e2e_conv2d_71.dc.matmul.8_0],
         t: 2, mblock: [7, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [12, 12], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_127.dc.matmul.8: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [add_126, blocks.2.0._expand_conv.weight, input_1_add_128_fork_clone870],
         t: 2, mblock: [7, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 98}, vslice: 2], input_1_tms: [broadcast: {c: 2}, hslice: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 5, input_1: 10}, m_k: 1, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [5, 5], inputs: [lc.input_tensor.conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_127.dc.matmul.8, lc.input_tensor.conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 1], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 5}], input_1_tms: [vstack: 2], input_0_tms: [broadcast: {c: 5}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 98, num_index_tiles: 1, num_sparse_tiles: 233, sparse_tile_ptr_bits: 9, sparse_ublock_idx_bits: 9, u_kt: 1}}
    conv2d_142.dc.depthwise.10: {type: depthwise, grid_loc: [0, 7], grid_size: [5, 1], inputs: [conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.0._depthwise_conv.weight],
         t: 5, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 125, hstack: 25, vstack: 5],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 1
    _fused_op_4: {type: fused_op, grid_loc: [0, 0], grid_size: [5, 1], inputs: [e2e_conv2d_142.dc.depthwise.10_0, input_1_add_143, input_1_add_143_fork_clone825],
         t: 5, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {r: 25}, vslice: 5],
         attributes: {fused_op_id: 4}}
    conv2d_156.dc.matmul.8: {type: matmul, grid_loc: [0, 1], grid_size: [5, 1], inputs: [_fused_op_4, blocks.2.0._project_conv.weight, input_1_add_157_fork_clone781],
         t: 5, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 5, min_buffer_input: 0, u_kt: 1}}
    conv2d_169.dc.matmul.8: {type: matmul, grid_loc: [0, 2], grid_size: [5, 1], inputs: [conv2d_156.dc.matmul.8, blocks.2.1._expand_conv.weight, input_1_add_170_fork_clone864],
         t: 5, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [5, 2], inputs: [lc.input_tensor.conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_169.dc.matmul.8, lc.input_tensor.conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 5, mblock: [5, 4], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 5], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 144, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}
    conv2d_184.dc.depthwise.10: {type: depthwise, grid_loc: [0, 5], grid_size: [5, 1], inputs: [conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.2.1._depthwise_conv.weight],
         t: 5, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 5}, hslice: 5], input_0_tms: [vslice: 125, hstack: 25, vstack: 5],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}
    _fused_op_5: {type: fused_op, grid_loc: [0, 6], grid_size: [5, 1], inputs: [conv2d_184.dc.depthwise.10, input_1_add_185, input_1_add_185_fork_clone819],
         t: 5, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {r: 25}, vslice: 5],
         attributes: {fused_op_id: 5}}
    conv2d_198.dc.matmul.8: {type: matmul, grid_loc: [0, 7], grid_size: [5, 1], inputs: [_fused_op_5, blocks.2.1._project_conv.weight, input_1_add_199_fork_clone776],
         t: 5, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 2}, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 1}}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 1
    add_211: {type: add, grid_loc: [0, 0], grid_size: [5, 1], inputs: [e2e_conv2d_198.dc.matmul.8_0, e2e_conv2d_156.dc.matmul.8_0],
         t: 5, mblock: [1, 1], ublock: [1, 2], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [12, 12], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_212.dc.matmul.8: {type: matmul, grid_loc: [0, 1], grid_size: [5, 1], inputs: [add_211, blocks.3.0._expand_conv.weight, input_1_add_213_fork_clone729],
         t: 5, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 25}, vslice: 5], input_1_tms: [broadcast: {c: 5}, hslice: 5],
         attributes: {bias: true, kernel_broadcast: {input_2: 8}, m_k: 2, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 2], grid_size: [9, 1], inputs: [lc.input_tensor.conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_212.dc.matmul.8, lc.input_tensor.conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [7, 1], ublock: [1, 8], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_1_tms: [vstack: 5],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 25, num_index_tiles: 1, num_sparse_tiles: 32, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}
    conv2d_227.dc.depthwise.10: {type: depthwise, grid_loc: [0, 3], grid_size: [7, 1], inputs: [conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.0._depthwise_conv.weight],
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 9, hstack: 9],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 1}}
    _fused_op_6: {type: fused_op, grid_loc: [0, 4], grid_size: [7, 1], inputs: [conv2d_227.dc.depthwise.10, input_1_add_228, input_1_add_228_fork_clone664],
         t: 1, mblock: [1, 2], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_1_tms: [broadcast: {r: 7}],
         attributes: {fused_op_id: 5}}
    conv2d_241.dc.matmul.8: {type: matmul, grid_loc: [0, 5], grid_size: [7, 1], inputs: [_fused_op_6, blocks.3.0._project_conv.weight, input_1_add_242_fork_clone600],
         t: 3, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 3], input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {bias: true, l1_acc: true, m_k: 8, min_buffer_input: 0, u_kt: 1}}
    conv2d_254.dc.matmul.8: {type: matmul, grid_loc: [0, 6], grid_size: [7, 1], inputs: [conv2d_241.dc.matmul.8, blocks.3.1._expand_conv.weight, input_1_add_255_fork_clone723],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 3],
         attributes: {bias: true, kernel_broadcast: {input_2: 15}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 7], grid_size: [9, 1], inputs: [lc.input_tensor.conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_254.dc.matmul.8, lc.input_tensor.conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 15], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 13, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}

  fwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 1
    conv2d_269.dc.depthwise.10: {type: depthwise, grid_loc: [0, 0], grid_size: [7, 1], inputs: [e2e_conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0, blocks.3.1._depthwise_conv.weight],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 9, hstack: 9],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [0, 1], grid_size: [7, 1], inputs: [conv2d_269.dc.depthwise.10, input_1_add_270, input_1_add_270_fork_clone658],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_1_tms: [broadcast: {r: 7}],
         attributes: {fused_op_id: 7}}
    conv2d_283.dc.matmul.8: {type: matmul, grid_loc: [0, 2], grid_size: [7, 1], inputs: [_fused_op_7, blocks.3.1._project_conv.weight, input_1_add_284_fork_clone595],
         t: 3, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 3], input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {bias: true, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 1}}
    add_296: {type: add, grid_loc: [0, 3], grid_size: [7, 1], inputs: [conv2d_283.dc.matmul.8, e2e_conv2d_241.dc.matmul.8_0],
         t: 3, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_297.dc.matmul.8: {type: matmul, grid_loc: [0, 4], grid_size: [7, 1], inputs: [add_296, blocks.3.2._expand_conv.weight, input_1_add_298_fork_clone705],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 3],
         attributes: {bias: true, kernel_broadcast: {input_2: 15}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 5], grid_size: [9, 1], inputs: [lc.input_tensor.conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_297.dc.matmul.8, lc.input_tensor.conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [1, 15], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 13, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_312.dc.depthwise.10: {type: depthwise, grid_loc: [0, 6], grid_size: [7, 1], inputs: [conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.3.2._depthwise_conv.weight],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 9, hstack: 9],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 1}}
    _fused_op_8: {type: fused_op, grid_loc: [0, 7], grid_size: [7, 1], inputs: [conv2d_312.dc.depthwise.10, input_1_add_313, input_1_add_313_fork_clone637],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_1_tms: [broadcast: {r: 7}],
         attributes: {fused_op_id: 7}}

  fwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 1
    conv2d_326.dc.matmul.8: {type: matmul, grid_loc: [0, 0], grid_size: [7, 1], inputs: [e2e__fused_op_8_0, blocks.3.2._project_conv.weight, input_1_add_327_fork_clone577],
         t: 3, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 3], input_1_tms: [hslice: 3], input_0_tms: [broadcast: {r: 3}, vslice: 3],
         attributes: {bias: true, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 1}}
    add_339: {type: add, grid_loc: [0, 1], grid_size: [7, 1], inputs: [conv2d_326.dc.matmul.8, e2e_add_296_0],
         t: 3, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_340.dc.matmul.8: {type: matmul, grid_loc: [0, 2], grid_size: [7, 1], inputs: [add_339, blocks.4.0._expand_conv.weight, input_1_add_341_fork_clone529],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 3],
         attributes: {bias: true, kernel_broadcast: {input_2: 15}, m_k: 3, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 3], grid_size: [7, 5], inputs: [lc.input_tensor.conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_340.dc.matmul.8, lc.input_tensor.conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [5, 3], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 5}], input_0_tms: [broadcast: {c: 5}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 39, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}

  fwd_0_9_temporal_epoch_9:
    target_device: 0
    input_count: 1
    conv2d_355.dc.depthwise.10: {type: depthwise, grid_loc: [0, 0], grid_size: [7, 1], inputs: [e2e_conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0, blocks.4.0._depthwise_conv.weight],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25, hstack: 25],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}
    _fused_op_9: {type: fused_op, grid_loc: [0, 1], grid_size: [7, 1], inputs: [conv2d_355.dc.depthwise.10, input_1_add_356, input_1_add_356_fork_clone464],
         t: 1, mblock: [1, 5], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_1_tms: [broadcast: {r: 7}],
         attributes: {fused_op_id: 7}}
    conv2d_369.dc.matmul.8: {type: matmul, grid_loc: [0, 2], grid_size: [7, 1], inputs: [_fused_op_9, blocks.4.0._project_conv.weight, input_1_add_370_fork_clone400],
         t: 4, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 15, min_buffer_input: 0, u_kt: 1}}
    conv2d_382.dc.matmul.8: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [conv2d_369.dc.matmul.8, blocks.4.1._expand_conv.weight, input_1_add_383_fork_clone523],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 21}, m_k: 4, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}

  fwd_0_10_temporal_epoch_10:
    target_device: 0
    input_count: 1
    conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [5, 7], inputs: [lc.input_tensor.conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_382.dc.matmul.8_0, lc.input_tensor.conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [5, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 7}], input_0_tms: [broadcast: {c: 7}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 52, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}
    conv2d_397.dc.depthwise.10: {type: depthwise, grid_loc: [0, 7], grid_size: [7, 1], inputs: [conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.1._depthwise_conv.weight],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25, hstack: 25],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}

  fwd_0_11_temporal_epoch_11:
    target_device: 0
    input_count: 1
    _fused_op_10: {type: fused_op, grid_loc: [0, 0], grid_size: [7, 1], inputs: [e2e_conv2d_397.dc.depthwise.10_0, input_1_add_398, input_1_add_398_fork_clone458],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_1_tms: [broadcast: {r: 7}],
         attributes: {fused_op_id: 10}}
    conv2d_411.dc.matmul.8: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [_fused_op_10, blocks.4.1._project_conv.weight, input_1_add_412_fork_clone395],
         t: 4, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 21, min_buffer_input: 0, u_kt: 1}}
    add_424: {type: add, grid_loc: [0, 2], grid_size: [7, 1], inputs: [conv2d_411.dc.matmul.8, e2e_conv2d_369.dc.matmul.8_0],
         t: 4, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_425.dc.matmul.8: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [add_424, blocks.4.2._expand_conv.weight, input_1_add_426_fork_clone505],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 21}, m_k: 4, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}

  fwd_0_12_temporal_epoch_12:
    target_device: 0
    input_count: 1
    conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [5, 7], inputs: [lc.input_tensor.conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_425.dc.matmul.8_0, lc.input_tensor.conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [5, 3], ublock: [7, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 7}], input_0_tms: [broadcast: {c: 7}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 52, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}
    conv2d_440.dc.depthwise.10: {type: depthwise, grid_loc: [0, 7], grid_size: [7, 1], inputs: [conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.4.2._depthwise_conv.weight],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25, hstack: 25],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}

  fwd_0_13_temporal_epoch_13:
    target_device: 0
    input_count: 1
    _fused_op_11: {type: fused_op, grid_loc: [0, 0], grid_size: [7, 1], inputs: [e2e_conv2d_440.dc.depthwise.10_0, input_1_add_441, input_1_add_441_fork_clone437],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_1_tms: [broadcast: {r: 7}],
         attributes: {fused_op_id: 10}}
    conv2d_454.dc.matmul.8: {type: matmul, grid_loc: [0, 1], grid_size: [7, 1], inputs: [_fused_op_11, blocks.4.2._project_conv.weight, input_1_add_455_fork_clone377],
         t: 4, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}, hslice: 4], input_1_tms: [hslice: 4], input_0_tms: [broadcast: {r: 4}, vslice: 4],
         attributes: {bias: true, l1_acc: true, m_k: 21, min_buffer_input: 0, u_kt: 1}}
    add_467: {type: add, grid_loc: [0, 2], grid_size: [7, 1], inputs: [conv2d_454.dc.matmul.8, e2e_add_424_0],
         t: 4, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_468.dc.matmul.8: {type: matmul, grid_loc: [0, 3], grid_size: [7, 1], inputs: [add_467, blocks.5.0._expand_conv.weight, input_1_add_469_fork_clone325],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 7}], input_0_tms: [hstack: 4],
         attributes: {bias: true, kernel_broadcast: {input_2: 21}, m_k: 4, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 4], grid_size: [5, 3], inputs: [lc.input_tensor.conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_468.dc.matmul.8, lc.input_tensor.conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [10, 1], ublock: [1, 7], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 3}], input_0_tms: [broadcast: {c: 3}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 7, num_index_tiles: 1, num_sparse_tiles: 41, sparse_tile_ptr_bits: 8, sparse_ublock_idx_bits: 8, u_kt: 1}}
    conv2d_483.dc.depthwise.10: {type: depthwise, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.0._depthwise_conv.weight],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 21], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25, hstack: 25],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}
    _fused_op_12: {type: fused_op, grid_loc: [2, 7], grid_size: [2, 1], inputs: [conv2d_483.dc.depthwise.10, input_1_add_484, input_1_add_484_fork_clone242],
         t: 1, mblock: [1, 7], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_1_tms: [broadcast: {r: 2}],
         attributes: {fused_op_id: 10}}
    conv2d_497.dc.matmul.8: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [_fused_op_12, blocks.5.0._project_conv.weight, input_1_add_498_fork_clone157],
         t: 6, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, hslice: 6], input_1_tms: [hslice: 6], input_0_tms: [broadcast: {r: 6}, vslice: 6],
         attributes: {bias: true, l1_acc: true, m_k: 21, min_buffer_input: 0, u_kt: 1}}
    conv2d_510.dc.matmul.8: {type: matmul, grid_loc: [5, 4], grid_size: [2, 2], inputs: [conv2d_497.dc.matmul.8, blocks.5.1._expand_conv.weight, input_1_add_511_fork_clone319],
         t: 1, mblock: [1, 6], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 6],
         attributes: {bias: true, kernel_broadcast: {input_2: 18}, m_k: 6, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}

  fwd_0_14_temporal_epoch_14:
    target_device: 0
    input_count: 1
    conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [5, 4], inputs: [lc.input_tensor.conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_510.dc.matmul.8_0, lc.input_tensor.conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [2, 9], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 18, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 4}], input_0_tms: [broadcast: {c: 4}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_525.dc.depthwise.10: {type: depthwise, grid_loc: [0, 4], grid_size: [2, 1], inputs: [conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.1._depthwise_conv.weight],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25, hstack: 25],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}
    _fused_op_13: {type: fused_op, grid_loc: [0, 5], grid_size: [2, 1], inputs: [conv2d_525.dc.depthwise.10, input_1_add_526, input_1_add_526_fork_clone236],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_1_tms: [broadcast: {r: 2}],
         attributes: {fused_op_id: 13}}
    conv2d_539.dc.matmul.8: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_13, blocks.5.1._project_conv.weight, input_1_add_540_fork_clone152],
         t: 6, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, hslice: 6], input_1_tms: [hslice: 6], input_0_tms: [broadcast: {r: 6}, vslice: 6],
         attributes: {bias: true, l1_acc: true, m_k: 36, min_buffer_input: 0, u_kt: 1}}
    add_552: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_539.dc.matmul.8, e2e_conv2d_497.dc.matmul.8_0],
         t: 6, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 12, input_dram_io_buf_size_tiles: [0, 24], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_553.dc.matmul.8: {type: matmul, grid_loc: [2, 4], grid_size: [2, 2], inputs: [add_552, blocks.5.2._expand_conv.weight, input_1_add_554_fork_clone297],
         t: 1, mblock: [1, 6], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 6],
         attributes: {bias: true, kernel_broadcast: {input_2: 18}, m_k: 6, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [4, 4], grid_size: [5, 4], inputs: [lc.input_tensor.conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.0, conv2d_553.dc.matmul.8, lc.input_tensor.conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [2, 9], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 4}], input_0_tms: [broadcast: {c: 4}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_568.dc.depthwise.10: {type: depthwise, grid_loc: [2, 6], grid_size: [2, 1], inputs: [conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.2._depthwise_conv.weight],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25, hstack: 25],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}
    _fused_op_14: {type: fused_op, grid_loc: [2, 7], grid_size: [2, 1], inputs: [conv2d_568.dc.depthwise.10, input_1_add_569, input_1_add_569_fork_clone209],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_1_tms: [broadcast: {r: 2}],
         attributes: {fused_op_id: 13}}
    conv2d_582.dc.matmul.8: {type: matmul, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_14, blocks.5.2._project_conv.weight, input_1_add_583_fork_clone129],
         t: 6, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, hslice: 6], input_1_tms: [hslice: 6], input_0_tms: [broadcast: {r: 6}, vslice: 6],
         attributes: {bias: true, l1_acc: true, m_k: 36, min_buffer_input: 0, u_kt: 1}}
    add_595: {type: add, grid_loc: [5, 1], grid_size: [2, 1], inputs: [conv2d_582.dc.matmul.8, add_552],
         t: 6, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 232], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_596.dc.matmul.8: {type: matmul, grid_loc: [5, 2], grid_size: [2, 2], inputs: [add_595, blocks.5.3._expand_conv.weight, input_1_add_597_fork_clone271],
         t: 1, mblock: [1, 6], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 6],
         attributes: {bias: true, kernel_broadcast: {input_2: 18}, m_k: 6, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}

  fwd_0_15_temporal_epoch_15:
    target_device: 0
    input_count: 1
    conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [5, 4], inputs: [lc.input_tensor.conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_596.dc.matmul.8_0, lc.input_tensor.conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [2, 9], ublock: [5, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 18, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         input_2_tms: [broadcast: {c: 4}], input_0_tms: [broadcast: {c: 4}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 16, sparse_tile_ptr_bits: 7, sparse_ublock_idx_bits: 7, u_kt: 1}}
    conv2d_611.dc.depthwise.10: {type: depthwise, grid_loc: [0, 4], grid_size: [2, 1], inputs: [conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.5.3._depthwise_conv.weight],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 25, hstack: 25],
         attributes: {l1_acc: true, m_k: 25, min_buffer_input: 0, u_kt: 1}}
    _fused_op_15: {type: fused_op, grid_loc: [0, 5], grid_size: [2, 1], inputs: [conv2d_611.dc.depthwise.10, input_1_add_612, input_1_add_612_fork_clone180],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_1_tms: [broadcast: {r: 2}],
         attributes: {fused_op_id: 13}}
    conv2d_625.dc.matmul.8: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_15, blocks.5.3._project_conv.weight, input_1_add_626_fork_clone112],
         t: 6, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, hslice: 6], input_1_tms: [hslice: 6], input_0_tms: [broadcast: {r: 6}, vslice: 6],
         attributes: {bias: true, l1_acc: true, m_k: 36, min_buffer_input: 0, u_kt: 1}}
    add_638: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [conv2d_625.dc.matmul.8, e2e_add_595_0],
         t: 6, mblock: [1, 1], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    conv2d_639.dc.matmul.8: {type: matmul, grid_loc: [2, 4], grid_size: [2, 2], inputs: [add_638, blocks.6.0._expand_conv.weight, input_1_add_640_fork_clone83],
         t: 1, mblock: [1, 6], ublock: [1, 3], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 6],
         attributes: {bias: true, kernel_broadcast: {input_2: 18}, m_k: 6, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}

  fwd_0_16_temporal_epoch_16:
    target_device: 0
    input_count: 1
    conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [9, 1], inputs: [lc.input_tensor.conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.0, e2e_conv2d_639.dc.matmul.8_0, lc.input_tensor.conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.1],
         t: 1, mblock: [2, 6], ublock: [1, 6], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24, 0], ublock_order: r, in_df: [Bfp2_b, Float32, RawUInt32], out_df: Float32, intermed_df: Float32, acc_df: Float16_b, math_fidelity: HiFi2,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, l1_acc: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 4, sparse_tile_ptr_bits: 5, sparse_ublock_idx_bits: 5, u_kt: 1}}
    conv2d_654.dc.depthwise.10: {type: depthwise, grid_loc: [0, 1], grid_size: [2, 1], inputs: [conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2, blocks.6.0._depthwise_conv.weight],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 24], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 9, hstack: 9],
         attributes: {l1_acc: true, m_k: 9, min_buffer_input: 0, u_kt: 1}}
    _fused_op_16: {type: fused_op, grid_loc: [0, 2], grid_size: [2, 1], inputs: [conv2d_654.dc.depthwise.10, input_1_add_655, input_1_add_655_fork_clone60],
         t: 1, mblock: [1, 9], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_1_tms: [broadcast: {r: 2}],
         attributes: {fused_op_id: 13}}
    conv2d_668.dc.matmul.8: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [_fused_op_16, blocks.6.0._project_conv.weight, input_1_add_669_fork_clone37],
         t: 2, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}, hslice: 2], input_1_tms: [hslice: 2], input_0_tms: [broadcast: {r: 2}, vslice: 2],
         attributes: {bias: true, l1_acc: true, m_k: 36, min_buffer_input: 0, u_kt: 1}}
    conv2d_681.dc.matmul.8: {type: matmul, grid_loc: [0, 4], grid_size: [2, 2], inputs: [conv2d_668.dc.matmul.8, head.0.weight, input_1_add_682_fork_clone20],
         t: 1, mblock: [1, 5], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 2}], input_0_tms: [hstack: 2],
         attributes: {bias: true, kernel_broadcast: {input_2: 20}, m_k: 10, min_buffer_input: 0, relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00, u_kt: 1}}
    avg_pool2d_695.dc.reduce_avg.2.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.avg_pool2d_695.dc.reduce_avg.2.0, conv2d_681.dc.matmul.8],
         t: 8, mblock: [1, 5], ublock: [1, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 8], input_0_tms: [broadcast: {r: 8}, vslice: 8],
         attributes: {l1_acc: true, m_k: 2, min_buffer_input: 0, u_kt: 1}}

  fwd_0_17_temporal_epoch_17:
    target_device: 0
    input_count: 1
    matmul_699: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_avg_pool2d_695.dc.reduce_avg.2.lc1_0, fc.weight],
         t: 1, mblock: [1, 1], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [24, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8],
         attributes: {l1_acc: true, m_k: 40, min_buffer_input: 0, u_kt: 1}}
    add_700: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_699, fc.bias], untilize_output: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$gptr_q12: 0, $gptr_q11: 0, $gptr_q9: 0, $lptr_q2: 0, $gptr_q13: 0, $gptr_q2: 0, $gptr_q3: 0, $lptr_q14: 0, $lptr_q11: 0, $lptr_q15: 0, $lptr_q16: 0, $gptr_q14: 0, $gptr_q8: 0, $lptr_q12: 0, $gptr_q15: 0, $c_microbatch_size: 1, $c_one: 1, $c_zero: 0, $lptr_q1: 0, $lptr_q13: 0, $gptr_q16: 0, $lptr_q9: 0, $gptr_q1: 0, $lptr_q8: 0, $lptr_q10: 0, $lptr_q3: 0, $lptr_q4: 0, $lptr_q5: 0, $gptr_q10: 0, $gptr_q5: 0, $lptr_q6: 0, $gptr_q17: 0, $gptr_q4: 0, $gptr_q6: 0, $lptr_q17: 0, $gptr_q7: 0, $lptr_q7: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e__fused_op_0_0, e2e_conv2d_15.dc.conv2d.5.dc.depthwise.10_0, e2e_conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.1.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               stem.0.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_0.dc.conv2d.3.dc.conv2d.3.dc.sparse_matmul.9.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               stem.0.weight_fork_clone1856: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_1_fork_clone1082: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_15.dc.conv2d.5.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.0.0._depthwise_conv.weight_fork_clone1204: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_conv2d_42.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e__fused_op_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_15.dc.conv2d.5.dc.depthwise.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               blocks.0.0._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_15.dc.conv2d.3.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.0.0._depthwise_conv.weight_fork_clone1202: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_16_fork_clone1059: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.0.0._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_30_fork_clone1033: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_43_fork_clone1003: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_0_0, e2e_conv2d_15.dc.conv2d.5.dc.depthwise.10_0, e2e_conv2d_15.dc.conv2d.1.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_84.dc.matmul.8_0, e2e_conv2d_71.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               e2e_conv2d_42.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_57.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_58: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_58_fork_clone958: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.0._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_72_fork_clone914: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_85_fork_clone997: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_42.dc.matmul.8_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_113.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               e2e_conv2d_84.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               lc.input_tensor.conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_99.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_100: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_100_fork_clone952: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.1.1._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_114_fork_clone909: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_84.dc.matmul.8_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_142.dc.depthwise.10_0]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               e2e_conv2d_71.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_conv2d_113.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               blocks.2.0._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_128_fork_clone870: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_142.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_71.dc.matmul.8_0, e2e_conv2d_113.dc.matmul.8_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_156.dc.matmul.8_0, e2e_conv2d_198.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               e2e_conv2d_142.dc.depthwise.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               input_1_add_143: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_143_fork_clone825: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.0._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_157_fork_clone781: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_170_fork_clone864: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_184.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_185: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_185_fork_clone819: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.2.1._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_199_fork_clone776: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_142.dc.depthwise.10_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_241.dc.matmul.8_0, e2e_conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               e2e_conv2d_156.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_conv2d_198.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               blocks.3.0._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_213_fork_clone729: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_227.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.0._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_228_fork_clone664: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.0._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_242_fork_clone600: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.1._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_255_fork_clone723: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_156.dc.matmul.8_0, e2e_conv2d_198.dc.matmul.8_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_add_296_0, e2e__fused_op_8_0]
    -   execute: {graph_name: fwd_0_7_temporal_epoch_7, queue_settings: {
               e2e_conv2d_241.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               blocks.3.1._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_270: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_270_fork_clone658: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.1._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_284_fork_clone595: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.2._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_298_fork_clone705: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_312.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.3.2._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_313: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_313_fork_clone637: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_241.dc.matmul.8_0, e2e_conv2d_269.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   execute: {graph_name: fwd_0_8_temporal_epoch_8, queue_settings: {
               e2e_add_296_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               blocks.3.2._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_327_fork_clone577: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_341_fork_clone529: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_296_0, e2e__fused_op_8_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_382.dc.matmul.8_0, e2e_conv2d_369.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_9_temporal_epoch_9, queue_settings: {
               e2e_conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               blocks.4.0._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_356: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_356_fork_clone464: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.0._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_370_fork_clone400: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_383_fork_clone523: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_355.dc.sparse_matmul.8.dc.sparse_matmul.1.lc2_0]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_397.dc.depthwise.10_0]
    -   execute: {graph_name: fwd_0_10_temporal_epoch_10, queue_settings: {
               e2e_conv2d_382.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               lc.input_tensor.conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_397.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_382.dc.matmul.8_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_425.dc.matmul.8_0, e2e_add_424_0]
    -   execute: {graph_name: fwd_0_11_temporal_epoch_11, queue_settings: {
               e2e_conv2d_369.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_conv2d_397.dc.depthwise.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               input_1_add_398: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_398_fork_clone458: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.1._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_412_fork_clone395: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.2._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_426_fork_clone505: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_369.dc.matmul.8_0, e2e_conv2d_397.dc.depthwise.10_0]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_440.dc.depthwise.10_0]
    -   execute: {graph_name: fwd_0_12_temporal_epoch_12, queue_settings: {
               e2e_conv2d_425.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_440.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.2._depthwise_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_425.dc.matmul.8_0]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_497.dc.matmul.8_0, e2e_conv2d_510.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_13_temporal_epoch_13, queue_settings: {
               e2e_add_424_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_conv2d_440.dc.depthwise.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               input_1_add_441: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_441_fork_clone437: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.4.2._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_455_fork_clone377: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_469_fork_clone325: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_483.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0._depthwise_conv.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_484: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_484_fork_clone242: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.0._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_498_fork_clone157: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_511_fork_clone319: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_424_0, e2e_conv2d_440.dc.depthwise.10_0]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_add_595_0, e2e_conv2d_596.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_14_temporal_epoch_14, queue_settings: {
               e2e_conv2d_497.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_conv2d_510.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               lc.input_tensor.conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_525.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1._depthwise_conv.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_526: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_526_fork_clone236: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.1._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_540_fork_clone152: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_554_fork_clone297: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_568.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2._depthwise_conv.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_569: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_569_fork_clone209: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.2._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_583_fork_clone129: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.3._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_597_fork_clone271: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_497.dc.matmul.8_0, e2e_conv2d_510.dc.matmul.8_0]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_conv2d_639.dc.matmul.8_0]
    -   execute: {graph_name: fwd_0_15_temporal_epoch_15, queue_settings: {
               e2e_add_595_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_conv2d_596.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               lc.input_tensor.conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_611.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.3._depthwise_conv.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_612: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_612_fork_clone180: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.5.3._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_626_fork_clone112: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.6.0._expand_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_640_fork_clone83: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_add_595_0, e2e_conv2d_596.dc.matmul.8_0]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_avg_pool2d_695.dc.reduce_avg.2.lc1_0]
    -   execute: {graph_name: fwd_0_16_temporal_epoch_16, queue_settings: {
               e2e_conv2d_639.dc.matmul.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               lc.input_tensor.conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.conv2d_654.dc.sparse_matmul.8.dc.sparse_matmul.1.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.6.0._depthwise_conv.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_655: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_655_fork_clone60: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               blocks.6.0._project_conv.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_669_fork_clone37: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               head.0.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_add_682_fork_clone20: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.avg_pool2d_695.dc.reduce_avg.2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_conv2d_639.dc.matmul.8_0]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_17_temporal_epoch_17, queue_settings: {
               e2e_avg_pool2d_695.dc.reduce_avg.2.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               fc.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               fc.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_avg_pool2d_695.dc.reduce_avg.2.lc1_0]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 2]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - conv2d_0.dc.conv2d.3.dc.add.4.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 1], output: dest}
        - add_12.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 1], output: output}
  1: 
    inputs: 5
    intermediates: 0
    schedules: 
      -
        - conv2d_15.dc.add.6.0: { type: add, inputs: [input0, input1], mblock: [1, 1], ublock: [1, 1], output: dest}
        - conv2d_15.dc.add.7.0: { type: add, inputs: [input2, dest], mblock: [1, 1], ublock: [1, 1], output: dest}
        - multiply_22.0: { type: multiply, inputs: [dest, input3], mblock: [1, 1], ublock: [1, 1], output: dest}
        - add_27.0: { type: add, inputs: [dest, input4], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 1], ublock: [1, 1], output: output}
  2: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_64.0: { type: multiply, inputs: [input0, input1], mblock: [14, 1], ublock: [1, 3], output: dest}
        - add_69.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [14, 1], ublock: [1, 3], output: output}
  3: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_106.0: { type: multiply, inputs: [input0, input1], mblock: [7, 5], ublock: [1, 1], output: dest}
        - add_111.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [7, 5], ublock: [1, 1], output: output}
  4: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_149.0: { type: multiply, inputs: [input0, input1], mblock: [1, 5], ublock: [1, 1], output: dest}
        - add_154.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 5], ublock: [1, 1], output: output}
  5: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_191.0: { type: multiply, inputs: [input0, input1], mblock: [1, 2], ublock: [1, 4], output: dest}
        - add_196.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 2], ublock: [1, 4], output: output}
  7: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_276.0: { type: multiply, inputs: [input0, input1], mblock: [1, 5], ublock: [1, 3], output: dest}
        - add_281.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 5], ublock: [1, 3], output: output}
  10: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_404.0: { type: multiply, inputs: [input0, input1], mblock: [1, 7], ublock: [1, 3], output: dest}
        - add_409.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 7], ublock: [1, 3], output: output}
  13: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_532.0: { type: multiply, inputs: [input0, input1], mblock: [1, 9], ublock: [1, 4], output: dest}
        - add_537.0: { type: add, inputs: [dest, input2], attributes: {relu_en: true, relu_mode: max, relu_threshold: 6.000000e+00}, mblock: [1, 9], ublock: [1, 4], output: output}
