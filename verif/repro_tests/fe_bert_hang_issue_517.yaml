devices:
  arch: grayskull

queues:

  # input
  hidden_states:                                                               {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                              {input: HOST, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x30d20020]]}

  # output
  bert_encoders.output_layernorm_273_bias:                                     {input: layernorm_273_bias, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.3.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x88aa580]]}
  layer.3.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9184b20]]}
  layer.3.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9ee9e20]]}
  layer.3.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8bae5e0]]}
  layer.2.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b51200]]}
  layer.2.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9104bc0]]}
  layer.2.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8bb3920]]}
  layer.2.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b63740]]}
  layer.1.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92b1c40]]}
  layer.1.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e948e0]]}
  layer.1.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e8dfc0]]}
  layer.1.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8b9df00]]}
  layer.0.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b43b40]]}
  layer.0.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x91778e0]]}
  layer.0.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9170fc0]]}
  layer.0.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8ba4820]]}
  layer.0.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b4a460]]}
  layer.0.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x917e200]]}
  layer.0.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e9b200], [2, 0x8b64880]]}
  layer.0.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x93143c0]]}
  layer.0.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x92925a0], [0, 0x917e740]]}
  layer.0.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8cc73c0]]}
  layer.0.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8c6e5e0], [4, 0x8c081a0]]}
  layer.0.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x92e11c0], [0, 0x91cd360]]}
  layer.0.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9327f00]]}
  layer.0.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x921bf80]]}
  layer.0.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 16], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9229580], [0, 0x9115720], [1, 0x9f4ca20], [2, 0x8c5e3a0], [3, 0x8c055c0], [4, 0x8b9f180]]}
  layer.0.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [96, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9fbc7e0], [2, 0x8cd4600], [3, 0x8cbe3a0], [4, 0x8c57240], [5, 0x8ba0c00], [6, 0x932e820]]}
  layer.1.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92b8560]]}
  layer.1.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x910e980]]}
  layer.1.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9ef74e0], [2, 0x8bc0b60]]}
  layer.1.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b58420]]}
  layer.1.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92bee80], [7, 0x91da960]]}
  layer.1.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9f46100]]}
  layer.1.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8c0f780], [3, 0x8bb69a0]]}
  layer.1.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9263020], [7, 0x918b440]]}
  layer.1.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8f963a0]]}
  layer.1.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8948660]]}
  layer.1.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 16], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x892e260], [4, 0x88b0ea0], [5, 0x8935420], [6, 0x9026e00], [7, 0x8f90360], [0, 0x8f9ccc0]]}
  layer.1.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [96, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x8fbd960], [7, 0x8f26ec0], [0, 0x8f2d380], [1, 0x9c93380], [2, 0x88df640], [3, 0x88c5240]]}
  layer.2.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9005ce0]]}
  layer.2.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x89b16a0]]}
  layer.2.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9cfc820], [2, 0x8962a80]]}
  layer.2.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8890120]]}
  layer.2.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8890120], [6, 0x8f20120]]}
  layer.2.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8f20140]]}
  layer.2.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9c435c0], [2, 0x88905a0]]}
  layer.2.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x88ded40], [6, 0x8f6ed40]]}
  layer.2.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x88a3c60]]}
  layer.2.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8af9500]]}
  layer.2.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 16], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8a8b1a0], [3, 0x8a76980], [4, 0x8a274c0], [5, 0x8a6ad60], [6, 0x919d420], [7, 0x90b2a60]]}
  layer.2.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [96, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x90967e0], [1, 0x9e246a0], [2, 0x8af41c0], [3, 0x8adf9a0], [4, 0x8a904e0], [5, 0x8ad3d80]]}
  layer.3.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8b489c0]]}
  layer.3.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b3cda0]]}
  layer.3.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x920cd60], [7, 0x91223a0]]}
  layer.3.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8b5d660]]}
  layer.3.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8b4f2e0], [4, 0x8b13920]]}
  layer.3.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x925b980]]}
  layer.3.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x89ac800], [6, 0x90deec0]]}
  layer.3.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [24, 12], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x90902a0], [7, 0x8ff9800]]}
  layer.3.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 24], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x899dba0]]}
  layer.3.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 96], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9012f20]]}
  layer.3.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [24, 16], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9d521e0], [2, 0x89b9160], [3, 0x89a4940], [4, 0x8955480], [5, 0x89fb420], [6, 0x912dae0]]}
  layer.3.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 6], t: 1, mblock: [96, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x90488a0], [0, 0x902d340], [1, 0x9dbb200], [2, 0x8a22180], [3, 0x8a0d960], [4, 0x89be4a0]]}

  # constant
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b50d80]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9262ba0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9104740]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8bb34a0]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b632c0]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b62e40]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9ef0740]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8bb4f00]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b57b20]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x91da060]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9103e40]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8b63f80]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b62540]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x92622a0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x91042c0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8b64400]]}
  lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b629c0]]}
  lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9262720]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8bab140]]}
  lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8bb6520]]}
  lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9fb5a40]]}
  constant_1_multiply_17:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b6bf60]]}
  lc.input_tensor.softmax_19.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x931ace0]]}
  lc.input_tensor.layernorm_37_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9fb5ec0]]}
  lc.input_tensor.layernorm_37_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8ccdce0]]}
  constant_1_layernorm_37_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8cbd200]]}
  lc.input_tensor.layernorm_37_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8c56dc0]]}
  lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8ba0780]]}
  lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x932fde0]]}
  lc.input_tensor.layernorm_60_mean.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9ef0bc0]]}
  lc.input_tensor.layernorm_60_var.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8bba240]]}
  constant_1_layernorm_60_var_plus_eps:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8bb5380]]}
  lc.input_tensor.layernorm_60_recip_s_brcst_m1_0_0.0:                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b6a060]]}
  lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b57fa0]]}
  lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x91da4e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x910b4e0]]}
  lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b6a4e0]]}
  lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x91152a0]]}
  constant_1_multiply_88:                                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8b6a960]]}
  lc.input_tensor.softmax_90.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b5ed40]]}
  lc.input_tensor.layernorm_108_mean.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x930daa0]]}
  lc.input_tensor.layernorm_108_var.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x892d960]]}
  constant_1_layernorm_108_var_plus_eps:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8934280]]}
  lc.input_tensor.layernorm_108_recip_s_brcst_m1_0_0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9026980]]}
  lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8f8fee0]]}
  lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9cfc3a0]]}
  lc.input_tensor.layernorm_131_mean.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8997280]]}
  lc.input_tensor.layernorm_131_var.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8919ec0]]}
  constant_1_layernorm_131_var_plus_eps:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x899e440]]}
  lc.input_tensor.layernorm_131_recip_s_brcst_m1_0_0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x908fe20]]}
  lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8ff9380]]}
  lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9d4b440]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9c40120]]}
  lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8890120]]}
  lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8f20120]]}
  constant_1_multiply_159:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x88905a0]]}
  lc.input_tensor.softmax_161.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8896a40]]}
  lc.input_tensor.layernorm_179_mean.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8f205a0]]}
  lc.input_tensor.layernorm_179_var.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8f26a60]]}
  constant_1_layernorm_179_var_plus_eps:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9c921e0]]}
  lc.input_tensor.layernorm_179_recip_s_brcst_m1_0_0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x88df1c0]]}
  lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x88c4dc0]]}
  lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8890120]]}
  lc.input_tensor.layernorm_202_mean.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9206440]]}
  lc.input_tensor.layernorm_202_var.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x911ba80]]}
  constant_1_layernorm_202_var_plus_eps:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x90ff800]]}
  lc.input_tensor.layernorm_202_recip_s_brcst_m1_0_0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e8d6c0]]}
  lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8b5d1e0]]}
  lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e24220]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x91009a0]]}
  lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9e8db40]]}
  lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0:          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8b436c0]]}
  constant_1_multiply_230:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x89207e0]]}
  lc.input_tensor.softmax_232.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x899f5e0]]}
  lc.input_tensor.layernorm_250_mean.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x900c600]]}
  lc.input_tensor.layernorm_250_var.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x9d4b8c0]]}
  constant_1_layernorm_250_var_plus_eps:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x89b7fc0]]}
  lc.input_tensor.layernorm_250_recip_s_brcst_m1_0_0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x89a44c0]]}
  lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x8955000]]}
  lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9048420]]}
  lc.input_tensor.layernorm_273_mean.0:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x8a64440]]}
  lc.input_tensor.layernorm_273_var.0:                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [24, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x9196b00]]}
  constant_1_layernorm_273_var_plus_eps:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [4, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x90b18c0]]}
  lc.input_tensor.layernorm_273_recip_s_brcst_m1_0_0.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x9096360]]}

  # epoch_to_epoch
  e2e_layernorm_60_bias_0:                                                     {input: layernorm_60_bias, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x8200100]]}
  e2e_softmax_90.dc.exp.0_0:                                                   {input: softmax_90.dc.exp.0, type: queue, entries: 128, grid_size: [1, 4], t: 12, mblock: [1, 1], ublock: [4, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x8200100], [3, 0x8200100], [4, 0x8200100], [5, 0x8200100]]}
  e2e_buffer_0_softmax_90.dc.exp.0_softmax_90.dc.multiply.3_0:                 {input: buffer_0_softmax_90.dc.exp.0_softmax_90.dc.multiply.3, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [2, 4], ublock: [2, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x8200100]]}
  e2e_layernorm_60_bias_1:                                                     {input: layernorm_60_bias, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[6, 0x8200100]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8200100]]}
  e2e_layernorm_179_var.lc1_0:                                                 {input: layernorm_179_var.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [4, 1], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[7, 0x9330260]]}
  e2e_buffer_0_layernorm_179_sub_layernorm_179_output_0:                       {input: buffer_0_layernorm_179_sub_layernorm_179_output, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x92363a0]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                     {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 128, grid_size: [1, 1], t: 12, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x8890120]]}
  e2e_add_265_0:                                                               {input: add_265, type: queue, entries: 128, grid_size: [1, 2], t: 1, mblock: [4, 6], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0xa025800], [2, 0x8d3d620]]}
  e2e_buffer_0_layernorm_250_bias_add_272_0:                                   {input: buffer_0_layernorm_250_bias_add_272, type: queue, entries: 128, grid_size: [1, 1], t: 1, mblock: [4, 3], ublock: [1, 8], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x8d273c0]]}

graphs:
  fwd_0:
    target_device: 0
    input_count: 128
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.0.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0, layer.0.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_hidden_states_matmul_22: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [hidden_states],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_hidden_states_matmul_22: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [buffer_1_hidden_states_matmul_22],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_22: {type: matmul, grid_loc: [1, 10], grid_size: [1, 2], inputs: [buffer_0_hidden_states_matmul_22, layer.0.attention.self.value.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_24: {type: add, grid_loc: [2, 9], grid_size: [1, 1], inputs: [matmul_22, layer.0.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, attention_mask],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0, layer.0.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_8: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.key.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_10: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_8, layer.0.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0, layer.0.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_2: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_4: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_2, layer.0.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_14: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_4, add_10],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_17: {type: multiply, grid_loc: [1, 8], grid_size: [1, 1], inputs: [matmul_14, constant_1_multiply_17],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_18: {type: add, grid_loc: [1, 9], grid_size: [1, 1], inputs: [multiply_17, attention_mask_s_brcst_m2_3_1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_19.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [1, 4], inputs: [add_18],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_19.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [softmax_19.dc.exp.0, lc.input_tensor.softmax_19.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_19.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_19.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_19.dc.exp.0_softmax_19.dc.multiply.3: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_19.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_19.dc.multiply.3: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [buffer_0_softmax_19.dc.exp.0_softmax_19.dc.multiply.3, softmax_19.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_29: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [softmax_19.dc.multiply.3, add_24],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [matmul_29, layer.0.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_35: {type: add, grid_loc: [3, 3], grid_size: [1, 1], inputs: [matmul_33, layer.0.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_hidden_states_add_36: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [hidden_states],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_hidden_states_add_36: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [buffer_1_hidden_states_add_36],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_36: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [add_35, buffer_0_hidden_states_add_36],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_mean.lc1: {type: matmul, grid_loc: [3, 8], grid_size: [1, 1], inputs: [add_36, lc.input_tensor.layernorm_37_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_36_layernorm_37_sub: {type: nop, grid_loc: [3, 9], grid_size: [1, 1], inputs: [add_36],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_37_sub: {type: subtract, grid_loc: [3, 10], grid_size: [1, 1], inputs: [buffer_0_add_36_layernorm_37_sub, layernorm_37_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_37_sq: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_37_sub, layernorm_37_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_var.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_37_sq, lc.input_tensor.layernorm_37_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_37_var_plus_eps: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_37_var.lc1, constant_1_layernorm_37_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_sqrt: {type: sqrt, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_37_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_recip: {type: reciprocal, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_37_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_37_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_37_recip, lc.input_tensor.layernorm_37_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_37_sub_layernorm_37_output: {type: nop, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_37_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_37_sub_layernorm_37_output: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_1_layernorm_37_sub_layernorm_37_output],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_37_output: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_37_sub_layernorm_37_output, layernorm_37_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_37_weights: {type: multiply, grid_loc: [4, 9], grid_size: [1, 1], inputs: [layernorm_37_output, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_37_bias: {type: add, grid_loc: [4, 11], grid_size: [1, 1], inputs: [layernorm_37_weights, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0, layer.0.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_50: {type: matmul, grid_loc: [5, 0], grid_size: [1, 6], inputs: [layernorm_37_bias, layer.0.intermediate.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 24, u_kt: 1}}
    add_52: {type: add, grid_loc: [5, 7], grid_size: [1, 2], inputs: [matmul_50, layer.0.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_53: {type: gelu, grid_loc: [6, 0], grid_size: [1, 4], inputs: [add_52],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_56: {type: matmul, grid_loc: [6, 4], grid_size: [1, 6], inputs: [gelu_53, layer.0.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 12, u_kt: 8}}
    add_58: {type: add, grid_loc: [6, 11], grid_size: [1, 1], inputs: [matmul_56, layer.0.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_37_bias_add_59: {type: nop, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_37_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_37_bias_add_59: {type: nop, grid_loc: [7, 1], grid_size: [1, 1], inputs: [buffer_1_layernorm_37_bias_add_59],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_59: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [add_58, buffer_0_layernorm_37_bias_add_59],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_mean.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [add_59, lc.input_tensor.layernorm_60_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_59_layernorm_60_sub: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_59],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_60_sub: {type: subtract, grid_loc: [7, 5], grid_size: [1, 1], inputs: [buffer_0_add_59_layernorm_60_sub, layernorm_60_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_60_sq: {type: multiply, grid_loc: [7, 8], grid_size: [1, 1], inputs: [layernorm_60_sub, layernorm_60_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_var.lc1: {type: matmul, grid_loc: [7, 9], grid_size: [1, 1], inputs: [layernorm_60_sq, lc.input_tensor.layernorm_60_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_60_var_plus_eps: {type: add, grid_loc: [7, 10], grid_size: [1, 1], inputs: [layernorm_60_var.lc1, constant_1_layernorm_60_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_sqrt: {type: sqrt, grid_loc: [7, 11], grid_size: [1, 1], inputs: [layernorm_60_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_recip: {type: reciprocal, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_60_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_60_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_60_recip, lc.input_tensor.layernorm_60_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_60_sub_layernorm_60_output: {type: nop, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_60_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_60_sub_layernorm_60_output: {type: nop, grid_loc: [7, 7], grid_size: [1, 1], inputs: [buffer_1_layernorm_60_sub_layernorm_60_output],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_60_output: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_60_sub_layernorm_60_output, layernorm_60_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_60_weights: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_60_output, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_60_bias: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_60_weights, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_2_1.lc1: {type: nop, grid_loc: [0, 8], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_2_1.lc1: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_2_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_2_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0, layer.1.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_79: {type: matmul, grid_loc: [9, 0], grid_size: [1, 2], inputs: [layernorm_60_bias, layer.1.attention.self.key.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_81: {type: add, grid_loc: [9, 3], grid_size: [1, 1], inputs: [matmul_79, layer.1.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0, layer.1.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_73: {type: matmul, grid_loc: [8, 7], grid_size: [1, 2], inputs: [layernorm_60_bias, layer.1.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_75: {type: add, grid_loc: [8, 10], grid_size: [1, 1], inputs: [matmul_73, layer.1.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_85: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [add_75, add_81],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_88: {type: multiply, grid_loc: [9, 5], grid_size: [1, 1], inputs: [matmul_85, constant_1_multiply_88],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_89: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [multiply_88, attention_mask_s_brcst_m2_2_1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_90.dc.exp.0: {type: exp, grid_loc: [9, 7], grid_size: [1, 4], inputs: [add_89],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    buffer_0_softmax_90.dc.exp.0_softmax_90.dc.multiply.3: {type: nop, grid_loc: [9, 11], grid_size: [1, 1], inputs: [softmax_90.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_1_1.lc1: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_1_1.lc1: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_1_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_1_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_attention_mask_attention_mask_s_brcst_m2_0_1.lc1: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_attention_mask_attention_mask_s_brcst_m2_0_1.lc1: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [buffer_1_attention_mask_attention_mask_s_brcst_m2_0_1.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, buffer_0_attention_mask_attention_mask_s_brcst_m2_0_1.lc1],
         t: 12, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 1}}

  fwd_1:
    target_device: 0
    input_count: 128
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.1.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0, layer.1.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_60_bias_matmul_93: {type: nop, grid_loc: [0, 3], grid_size: [1, 1], inputs: [e2e_layernorm_60_bias_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_60_bias_matmul_93: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [buffer_1_layernorm_60_bias_matmul_93],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_93: {type: matmul, grid_loc: [0, 5], grid_size: [1, 2], inputs: [buffer_0_layernorm_60_bias_matmul_93, layer.1.attention.self.value.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_95: {type: add, grid_loc: [0, 8], grid_size: [1, 1], inputs: [matmul_93, layer.1.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_90.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_softmax_90.dc.exp.0_0, lc.input_tensor.softmax_90.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_90.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 1], grid_size: [1, 1], inputs: [softmax_90.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    softmax_90.dc.multiply.3: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_buffer_0_softmax_90.dc.exp.0_softmax_90.dc.multiply.3_0, softmax_90.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_100: {type: matmul, grid_loc: [0, 9], grid_size: [1, 1], inputs: [softmax_90.dc.multiply.3, add_95],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_104: {type: matmul, grid_loc: [0, 10], grid_size: [1, 2], inputs: [matmul_100, layer.1.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_106: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_104, layer.1.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_60_bias_add_107: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_layernorm_60_bias_1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_60_bias_add_107: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [buffer_1_layernorm_60_bias_add_107],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_107: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_106, buffer_0_layernorm_60_bias_add_107],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_mean.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_107, lc.input_tensor.layernorm_108_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_107_layernorm_108_sub: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [add_107],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_108_sub: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_0_add_107_layernorm_108_sub, layernorm_108_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_108_sq: {type: multiply, grid_loc: [1, 10], grid_size: [1, 1], inputs: [layernorm_108_sub, layernorm_108_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_var.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_108_sq, lc.input_tensor.layernorm_108_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_108_var_plus_eps: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_108_var.lc1, constant_1_layernorm_108_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_sqrt: {type: sqrt, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_108_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_recip: {type: reciprocal, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_108_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_108_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_108_recip, lc.input_tensor.layernorm_108_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_108_sub_layernorm_108_output: {type: nop, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_108_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_108_sub_layernorm_108_output: {type: nop, grid_loc: [1, 9], grid_size: [1, 1], inputs: [buffer_1_layernorm_108_sub_layernorm_108_output],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_108_output: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_108_sub_layernorm_108_output, layernorm_108_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_108_weights: {type: multiply, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_108_output, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_108_bias: {type: add, grid_loc: [2, 8], grid_size: [1, 1], inputs: [layernorm_108_weights, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0, layer.1.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_121: {type: matmul, grid_loc: [3, 0], grid_size: [1, 6], inputs: [layernorm_108_bias, layer.1.intermediate.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 24, u_kt: 1}}
    add_123: {type: add, grid_loc: [3, 7], grid_size: [1, 2], inputs: [matmul_121, layer.1.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_124: {type: gelu, grid_loc: [4, 0], grid_size: [1, 4], inputs: [add_123],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_127: {type: matmul, grid_loc: [4, 4], grid_size: [1, 6], inputs: [gelu_124, layer.1.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 12, u_kt: 8}}
    add_129: {type: add, grid_loc: [4, 11], grid_size: [1, 1], inputs: [matmul_127, layer.1.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_108_bias_add_130: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_108_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_108_bias_add_130: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [buffer_1_layernorm_108_bias_add_130],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_130: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [add_129, buffer_0_layernorm_108_bias_add_130],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_mean.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_130, lc.input_tensor.layernorm_131_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_130_layernorm_131_sub: {type: nop, grid_loc: [5, 3], grid_size: [1, 1], inputs: [add_130],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_131_sub: {type: subtract, grid_loc: [5, 5], grid_size: [1, 1], inputs: [buffer_0_add_130_layernorm_131_sub, layernorm_131_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_131_sq: {type: multiply, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_131_sub, layernorm_131_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_var.lc1: {type: matmul, grid_loc: [5, 9], grid_size: [1, 1], inputs: [layernorm_131_sq, lc.input_tensor.layernorm_131_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_131_var_plus_eps: {type: add, grid_loc: [5, 10], grid_size: [1, 1], inputs: [layernorm_131_var.lc1, constant_1_layernorm_131_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_sqrt: {type: sqrt, grid_loc: [5, 11], grid_size: [1, 1], inputs: [layernorm_131_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_recip: {type: reciprocal, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_131_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_131_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_131_recip, lc.input_tensor.layernorm_131_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_131_sub_layernorm_131_output: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_131_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_131_sub_layernorm_131_output: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [buffer_1_layernorm_131_sub_layernorm_131_output],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_131_output: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_131_sub_layernorm_131_output, layernorm_131_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_131_weights: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_131_output, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_131_bias: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_131_weights, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.2.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0, layer.2.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_131_bias_matmul_164: {type: nop, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_131_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_131_bias_matmul_164: {type: nop, grid_loc: [8, 4], grid_size: [1, 1], inputs: [buffer_1_layernorm_131_bias_matmul_164],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_164: {type: matmul, grid_loc: [8, 5], grid_size: [1, 2], inputs: [buffer_0_layernorm_131_bias_matmul_164, layer.2.attention.self.value.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_166: {type: add, grid_loc: [8, 8], grid_size: [1, 1], inputs: [matmul_164, layer.2.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0, layer.2.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_150: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [layernorm_131_bias, layer.2.attention.self.key.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_152: {type: add, grid_loc: [7, 3], grid_size: [1, 1], inputs: [matmul_150, layer.2.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0, layer.2.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_144: {type: matmul, grid_loc: [6, 7], grid_size: [1, 2], inputs: [layernorm_131_bias, layer.2.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_146: {type: add, grid_loc: [6, 10], grid_size: [1, 1], inputs: [matmul_144, layer.2.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_156: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [add_146, add_152],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_159: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [matmul_156, constant_1_multiply_159],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_160: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [multiply_159, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_161.dc.exp.0: {type: exp, grid_loc: [7, 7], grid_size: [1, 4], inputs: [add_160],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_161.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [softmax_161.dc.exp.0, lc.input_tensor.softmax_161.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_161.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 1], grid_size: [1, 1], inputs: [softmax_161.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_161.dc.exp.0_softmax_161.dc.multiply.3: {type: nop, grid_loc: [7, 11], grid_size: [1, 1], inputs: [softmax_161.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_161.dc.multiply.3: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [buffer_0_softmax_161.dc.exp.0_softmax_161.dc.multiply.3, softmax_161.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_171: {type: matmul, grid_loc: [8, 9], grid_size: [1, 1], inputs: [softmax_161.dc.multiply.3, add_166],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_175: {type: matmul, grid_loc: [8, 10], grid_size: [1, 2], inputs: [matmul_171, layer.2.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_177: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [matmul_175, layer.2.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_131_bias_add_178: {type: nop, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_131_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_131_bias_add_178: {type: nop, grid_loc: [9, 3], grid_size: [1, 1], inputs: [buffer_1_layernorm_131_bias_add_178],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_178: {type: add, grid_loc: [9, 4], grid_size: [1, 1], inputs: [add_177, buffer_0_layernorm_131_bias_add_178],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_mean.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_178, lc.input_tensor.layernorm_179_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_178_layernorm_179_sub: {type: nop, grid_loc: [9, 5], grid_size: [1, 1], inputs: [add_178],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_179_sub: {type: subtract, grid_loc: [9, 7], grid_size: [1, 1], inputs: [buffer_0_add_178_layernorm_179_sub, layernorm_179_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_179_sq: {type: multiply, grid_loc: [9, 10], grid_size: [1, 1], inputs: [layernorm_179_sub, layernorm_179_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_var.lc1: {type: matmul, grid_loc: [9, 11], grid_size: [1, 1], inputs: [layernorm_179_sq, lc.input_tensor.layernorm_179_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_1_layernorm_179_sub_layernorm_179_output: {type: nop, grid_loc: [9, 8], grid_size: [1, 1], inputs: [layernorm_179_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_179_sub_layernorm_179_output: {type: nop, grid_loc: [9, 9], grid_size: [1, 1], inputs: [buffer_1_layernorm_179_sub_layernorm_179_output],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2:
    target_device: 0
    input_count: 128
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 8], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_179_var_plus_eps: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_179_var.lc1_0, constant_1_layernorm_179_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_sqrt: {type: sqrt, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_179_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_recip: {type: reciprocal, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_179_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_179_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_179_recip, lc.input_tensor.layernorm_179_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_179_output: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [e2e_buffer_0_layernorm_179_sub_layernorm_179_output_0, layernorm_179_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_179_weights: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_179_output, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_179_bias: {type: add, grid_loc: [0, 8], grid_size: [1, 1], inputs: [layernorm_179_weights, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0, layer.2.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_192: {type: matmul, grid_loc: [1, 0], grid_size: [1, 6], inputs: [layernorm_179_bias, layer.2.intermediate.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 24, u_kt: 1}}
    add_194: {type: add, grid_loc: [1, 7], grid_size: [1, 2], inputs: [matmul_192, layer.2.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    gelu_195: {type: gelu, grid_loc: [2, 0], grid_size: [1, 4], inputs: [add_194],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_198: {type: matmul, grid_loc: [2, 4], grid_size: [1, 6], inputs: [gelu_195, layer.2.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 12, u_kt: 8}}
    add_200: {type: add, grid_loc: [2, 11], grid_size: [1, 1], inputs: [matmul_198, layer.2.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_179_bias_add_201: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_179_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_179_bias_add_201: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [buffer_1_layernorm_179_bias_add_201],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_201: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_200, buffer_0_layernorm_179_bias_add_201],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_mean.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [add_201, lc.input_tensor.layernorm_202_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_201_layernorm_202_sub: {type: nop, grid_loc: [3, 3], grid_size: [1, 1], inputs: [add_201],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_202_sub: {type: subtract, grid_loc: [3, 5], grid_size: [1, 1], inputs: [buffer_0_add_201_layernorm_202_sub, layernorm_202_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_202_sq: {type: multiply, grid_loc: [3, 8], grid_size: [1, 1], inputs: [layernorm_202_sub, layernorm_202_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_var.lc1: {type: matmul, grid_loc: [3, 9], grid_size: [1, 1], inputs: [layernorm_202_sq, lc.input_tensor.layernorm_202_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_202_var_plus_eps: {type: add, grid_loc: [3, 10], grid_size: [1, 1], inputs: [layernorm_202_var.lc1, constant_1_layernorm_202_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_sqrt: {type: sqrt, grid_loc: [3, 11], grid_size: [1, 1], inputs: [layernorm_202_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_recip: {type: reciprocal, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_202_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_202_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_202_recip, lc.input_tensor.layernorm_202_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_202_sub_layernorm_202_output: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_202_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_202_sub_layernorm_202_output: {type: nop, grid_loc: [3, 7], grid_size: [1, 1], inputs: [buffer_1_layernorm_202_sub_layernorm_202_output],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_202_output: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_202_sub_layernorm_202_output, layernorm_202_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_202_weights: {type: multiply, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_202_output, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_202_bias: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_202_weights, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0, layer.3.attention.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0, layer.3.attention.self.value.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_202_bias_matmul_235: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_202_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_202_bias_matmul_235: {type: nop, grid_loc: [6, 4], grid_size: [1, 1], inputs: [buffer_1_layernorm_202_bias_matmul_235],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_235: {type: matmul, grid_loc: [6, 8], grid_size: [1, 2], inputs: [buffer_0_layernorm_202_bias_matmul_235, layer.3.attention.self.value.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_237: {type: add, grid_loc: [7, 1], grid_size: [1, 1], inputs: [matmul_235, layer.3.attention.self.value.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 12], ublock: [2, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0, layer.3.attention.self.key.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_221: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [layernorm_202_bias, layer.3.attention.self.key.weight],
         t: 1, mblock: [4, 6], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_223: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [matmul_221, layer.3.attention.self.key.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 9], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0, layer.3.attention.self.query.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_215: {type: matmul, grid_loc: [4, 7], grid_size: [1, 2], inputs: [layernorm_202_bias, layer.3.attention.self.query.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 6, u_kt: 4}}
    add_217: {type: add, grid_loc: [4, 10], grid_size: [1, 1], inputs: [matmul_215, layer.3.attention.self.query.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_227: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_217, add_223],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_230: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [matmul_227, constant_1_multiply_230],
         t: 12, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    add_231: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [multiply_230, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    softmax_232.dc.exp.0: {type: exp, grid_loc: [6, 0], grid_size: [1, 4], inputs: [add_231],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {approximate_mode: true}}
    softmax_232.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [softmax_232.dc.exp.0, lc.input_tensor.softmax_232.dc.reduce_sum.1.0],
         t: 12, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 4}}
    softmax_232.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 10], grid_size: [1, 1], inputs: [softmax_232.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    buffer_0_softmax_232.dc.exp.0_softmax_232.dc.multiply.3: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_232.dc.exp.0],
         t: 12, mblock: [2, 4], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_232.dc.multiply.3: {type: multiply, grid_loc: [6, 11], grid_size: [1, 1], inputs: [buffer_0_softmax_232.dc.exp.0_softmax_232.dc.multiply.3, softmax_232.dc.reciprocal.2],
         t: 12, mblock: [2, 2], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_242: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_232.dc.multiply.3, add_237],
         t: 12, mblock: [1, 1], ublock: [4, 2], buf_size_mb: 24, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_246: {type: matmul, grid_loc: [7, 3], grid_size: [1, 2], inputs: [matmul_242, layer.3.attention.output.dense.weight],
         t: 1, mblock: [4, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_0_tms: [hstack: 12],
         attributes: {m_k: 6, u_kt: 4}}
    add_248: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_246, layer.3.attention.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_202_bias_add_249: {type: nop, grid_loc: [5, 8], grid_size: [1, 1], inputs: [layernorm_202_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_202_bias_add_249: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [buffer_1_layernorm_202_bias_add_249],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_249: {type: add, grid_loc: [7, 7], grid_size: [1, 1], inputs: [add_248, buffer_0_layernorm_202_bias_add_249],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_mean.lc1: {type: matmul, grid_loc: [7, 8], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_249_layernorm_250_sub: {type: nop, grid_loc: [7, 9], grid_size: [1, 1], inputs: [add_249],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250_sub: {type: subtract, grid_loc: [7, 10], grid_size: [1, 1], inputs: [buffer_0_add_249_layernorm_250_sub, layernorm_250_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250_sq: {type: multiply, grid_loc: [7, 11], grid_size: [1, 1], inputs: [layernorm_250_sub, layernorm_250_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_var.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_250_sq, lc.input_tensor.layernorm_250_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_250_var_plus_eps: {type: add, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_250_var.lc1, constant_1_layernorm_250_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_sqrt: {type: sqrt, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_250_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_recip: {type: reciprocal, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_250_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_250_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_250_recip, lc.input_tensor.layernorm_250_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_250_sub_layernorm_250_output: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_250_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_250_sub_layernorm_250_output: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [buffer_1_layernorm_250_sub_layernorm_250_output],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250_output: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_250_sub_layernorm_250_output, layernorm_250_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250_weights: {type: multiply, grid_loc: [8, 9], grid_size: [1, 1], inputs: [layernorm_250_output, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_250_bias: {type: add, grid_loc: [8, 11], grid_size: [1, 1], inputs: [layernorm_250_weights, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0, layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 12], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    matmul_263: {type: matmul, grid_loc: [9, 0], grid_size: [1, 6], inputs: [layernorm_250_bias, layer.3.intermediate.dense.weight],
         t: 1, mblock: [4, 2], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 24, u_kt: 1}}
    add_265: {type: add, grid_loc: [9, 8], grid_size: [1, 2], inputs: [matmul_263, layer.3.intermediate.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 6], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    buffer_1_layernorm_250_bias_add_272: {type: nop, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_250_bias],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_250_bias_add_272: {type: nop, grid_loc: [9, 10], grid_size: [1, 1], inputs: [buffer_1_layernorm_250_bias_add_272],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3:
    target_device: 0
    input_count: 128
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    layer.3.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 10], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0, layer.3.output.dense.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    gelu_266: {type: gelu, grid_loc: [0, 0], grid_size: [1, 4], inputs: [e2e_add_265_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    matmul_269: {type: matmul, grid_loc: [0, 4], grid_size: [1, 6], inputs: [gelu_266, layer.3.output.dense.weight],
         t: 1, mblock: [4, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 12, u_kt: 8}}
    add_271: {type: add, grid_loc: [0, 11], grid_size: [1, 1], inputs: [matmul_269, layer.3.output.dense.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    add_272: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [add_271, e2e_buffer_0_layernorm_250_bias_add_272_0],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_mean.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_272, lc.input_tensor.layernorm_273_mean.0],
         t: 1, mblock: [4, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    buffer_0_add_272_layernorm_273_sub: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_272],
         t: 1, mblock: [4, 24], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_273_sub: {type: subtract, grid_loc: [1, 3], grid_size: [1, 1], inputs: [buffer_0_add_272_layernorm_273_sub, layernorm_273_mean.lc1],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_273_sq: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_273_sub, layernorm_273_sub],
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_var.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_273_sq, lc.input_tensor.layernorm_273_var.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_273_var_plus_eps: {type: add, grid_loc: [1, 8], grid_size: [1, 1], inputs: [layernorm_273_var.lc1, constant_1_layernorm_273_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_sqrt: {type: sqrt, grid_loc: [1, 9], grid_size: [1, 1], inputs: [layernorm_273_var_plus_eps],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_recip: {type: reciprocal, grid_loc: [1, 10], grid_size: [1, 1], inputs: [layernorm_273_sqrt],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi}
    layernorm_273_recip_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 11], grid_size: [1, 1], inputs: [layernorm_273_recip, lc.input_tensor.layernorm_273_recip_s_brcst_m1_0_0.0],
         t: 1, mblock: [1, 1], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_273_sub_layernorm_273_output: {type: nop, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_273_sub],
         t: 1, mblock: [1, 12], ublock: [4, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_273_sub_layernorm_273_output: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [buffer_1_layernorm_273_sub_layernorm_273_output],
         t: 1, mblock: [1, 24], ublock: [4, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_273_output: {type: multiply, grid_loc: [2, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_273_sub_layernorm_273_output, layernorm_273_recip_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_273_weights: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_273_output, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 12], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_273_bias: {type: add, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_273_weights, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [4, 3], ublock: [1, 8], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 4}]}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 128, $c_one: 1, $c_zero: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $lptr_q3: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e_layernorm_60_bias_0, e2e_layernorm_60_bias_1, e2e_softmax_90.dc.exp.0_0, e2e_buffer_0_softmax_90.dc.exp.0_softmax_90.dc.multiply.3_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   execute: {graph_name: fwd_0, queue_settings: {
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_17: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_19.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_37_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_37_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_60_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_60_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_60_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_60_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_88: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_layernorm_179_var.lc1_0, e2e_buffer_0_layernorm_179_sub_layernorm_179_output_0]
    -   execute: {graph_name: fwd_1, queue_settings: {
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.softmax_90.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_108_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_108_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_108_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_108_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_131_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_131_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_131_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_131_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_159: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_161.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_179_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_179_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               e2e_layernorm_60_bias_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_60_bias_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_softmax_90.dc.exp.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_buffer_0_softmax_90.dc.exp.0_softmax_90.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1}} }
    -   deallocate_queue: [e2e_layernorm_60_bias_0, e2e_layernorm_60_bias_1, e2e_softmax_90.dc.exp.0_0, e2e_buffer_0_softmax_90.dc.exp.0_softmax_90.dc.multiply.3_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 256]
    -   allocate_queue: [e2e_add_265_0, e2e_buffer_0_layernorm_250_bias_add_272_0]
    -   execute: {graph_name: fwd_2, queue_settings: {
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_layernorm_179_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_179_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_202_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_202_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_202_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_202_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.value.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.key.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.self.query.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_230: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_232.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_250_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               e2e_layernorm_179_var.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_buffer_0_layernorm_179_sub_layernorm_179_output_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2}} }
    -   deallocate_queue: [e2e_layernorm_179_var.lc1_0, e2e_buffer_0_layernorm_179_sub_layernorm_179_output_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 256]
    -   execute: {graph_name: fwd_3, queue_settings: {
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_273_mean.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_273_var.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               constant_1_layernorm_273_var_plus_eps: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_273_recip_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               e2e_add_265_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_buffer_0_layernorm_250_bias_add_272_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}} }
    -   deallocate_queue: [e2e_add_265_0, e2e_buffer_0_layernorm_250_bias_add_272_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 256]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 256]
    - endloop


