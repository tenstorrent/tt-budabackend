# git checkout 98fc62f5
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole-large]

devices:
  arch: [wormhole, wormhole_b0]

queues:

  # input
  hidden_states:                                                                {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                               {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [12, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30000000]]}

  # output
  bert_encoder.output_layernorm_3887:                                           {input: layernorm_3887.dc.add.10_output_nop_0, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x85c3740], [4, 0x8707720], [5, 0x4746c40], [0, 0x43dbc00], [1, 0x4697940], [2, 0x45e2480], [3, 0x8604760], [4, 0x8748740]]}
  layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d1b0a0], [0, 0x5ea2140], [1, 0x5e56ec0], [2, 0x604e380]]}
  layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9be97e0], [4, 0xa45cf40], [5, 0x5cda080], [0, 0x5e61120], [1, 0x5e15ea0], [2, 0x600d360], [3, 0x9c2a800], [4, 0xa49df60]]}
  layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5cd5f60], [0, 0x5e5d000], [1, 0x5e11d80], [2, 0x6009240]]}
  layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5d8fd40], [2, 0x5f87200], [3, 0x9ba7f80], [4, 0xa41b6e0], [5, 0x5c94f40], [0, 0x5e1bfe0], [1, 0x5dd0d60], [2, 0x5fc8220]]}
  layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ba3e60], [4, 0xa4175c0], [5, 0x5c90e20], [0, 0x5e17ec0]]}
  layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5d0dd00], [2, 0x5f051c0], [3, 0x9b62e40], [4, 0xa3d65a0], [5, 0x5c4fe00], [0, 0x5dd6ea0], [1, 0x5d4ed20], [2, 0x5f461e0]]}
  layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9b5ed20], [4, 0xa3d2480], [5, 0x5c4bce0], [0, 0x5dd2d80]]}
  layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5c7b8a0]]}
  layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x619e680]]}
  layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9ee3b40], [4, 0xa1e8a60], [5, 0x5cf8fe0], [0, 0x616da80], [1, 0x609a640], [2, 0x598f7e0], [3, 0x9f65b60], [4, 0xa26aa80], [5, 0x5d7b000], [0, 0x61efaa0], [1, 0x611c660], [2, 0x5a11800], [3, 0x9fe7b80], [4, 0xa2ecaa0], [5, 0x5dfd020], [0, 0x6271ac0]]}
  layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5d8e540], [1, 0x5defd60], [1, 0x5e51580], [1, 0x5eb2da0], [1, 0x5f145c0], [1, 0x5f75de0], [1, 0x5fd7600], [1, 0x6038e20]]}
  layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9d5dae0], [4, 0xa062a00], [5, 0x5b72f80], [0, 0x5fe7a20], [1, 0x5c8a500], [2, 0x588b7a0], [3, 0x9ddfb00], [4, 0xa0e4a20], [5, 0x5bf4fa0], [0, 0x6069a40], [1, 0x5d0c520], [2, 0x590d7c0], [3, 0x9e61b20], [4, 0xa166a40], [5, 0x5c76fc0], [0, 0x60eba60]]}
  layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5c863c0], [2, 0x5887660], [3, 0x9d5ba40], [4, 0xa060960], [5, 0x5b70ee0], [0, 0x5fe5980], [1, 0x5c88460], [2, 0x5889700]]}
  layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bf77c0]]}
  layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9b0d0a0]]}
  layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5bf9860], [2, 0x5e00900], [3, 0x9acc080], [4, 0xa34cb20], [5, 0x5bb67a0], [0, 0x5d4fcc0], [1, 0x5c3a880], [2, 0x5e41920]]}
  layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ac7f60], [4, 0xa348a00], [5, 0x5bb2680], [0, 0x5d4bba0]]}
  layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5c8bcc0], [2, 0x5e83180], [3, 0x9b1dd00], [4, 0xa391460], [5, 0x5c0acc0], [0, 0x5d91d60], [1, 0x5cccce0], [2, 0x5ec41a0]]}
  layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9dd4240], [4, 0xa638600], [5, 0x5f40160], [0, 0x60b7620]]}
  layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ebe120], [0, 0x60355e0], [1, 0x62f5c20], [2, 0x61e0fe0], [3, 0x9d93220], [4, 0xa5f75e0], [5, 0x5eff140], [0, 0x6076600]]}
  layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x62f1b00], [2, 0x61dcec0], [3, 0x9d8f100], [4, 0xa5f34c0]]}
  layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e3c0e0], [0, 0x5fb35a0], [1, 0x62b0ae0], [2, 0x619bea0], [3, 0x9d4e0e0], [4, 0xa5b24a0], [5, 0x5e7d100], [0, 0x5ff45c0]]}
  layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x62ac9c0], [2, 0x6197d80], [3, 0x9d49fc0], [4, 0xa5ae380]]}
  layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5876a00]]}
  layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5fd1c40]]}
  layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x56f09a0], [3, 0x9bd20c0], [4, 0x9ed6fe0], [5, 0x59e7560], [0, 0x5ecdc00], [1, 0x5b81300], [2, 0x57729c0], [3, 0x9c540e0], [4, 0x9f59000], [5, 0x5a69580], [0, 0x5f4fc20], [1, 0x5c03320], [2, 0x57f49e0], [3, 0x9cd6100], [4, 0x9fdb020], [5, 0x5aeb5a0]]}
  layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5bc1b00], [0, 0x5c23320], [0, 0x5c84b40], [0, 0x5ce6360], [0, 0x5d47b80], [0, 0x5da93a0], [0, 0x5e0abc0], [0, 0x5e6c3e0]]}
  layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x556a940], [3, 0x9a4c060], [4, 0x9d50f80], [5, 0x5861500], [0, 0x5abdac0], [1, 0x5a7d2c0], [2, 0x55ec960], [3, 0x9ace080], [4, 0x9dd2fa0], [5, 0x58e3520], [0, 0x5b3fae0], [1, 0x5aff2e0], [2, 0x566e980], [3, 0x9b500a0], [4, 0x9e54fc0], [5, 0x5965540]]}
  layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5ab9980], [1, 0x5a79180], [2, 0x55688a0], [3, 0x9a49fc0], [4, 0x9d4eee0], [5, 0x585f460], [0, 0x5abba20], [1, 0x5a7b220]]}
  layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e2b480]]}
  layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9d36ac0]]}
  layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f1e880], [2, 0x6115d40], [3, 0x9cf5aa0], [4, 0xa569a40], [5, 0x5dea460], [0, 0x5f71500], [1, 0x5f5f8a0], [2, 0x6156d60]]}
  layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9cf1980], [4, 0xa565920], [5, 0x5de6340], [0, 0x5f6d3e0]]}
  layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5e9c840], [2, 0x6093d00], [3, 0x9cb0960], [4, 0xa524900], [5, 0x5da5320], [0, 0x5f2c3c0], [1, 0x5edd860], [2, 0x60d4d20]]}
  layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9cac840], [4, 0xa5207e0], [5, 0x5da1200], [0, 0x5f282a0]]}
  layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5d1f1c0], [0, 0x5ea6260], [1, 0x5e5afe0], [2, 0x60524a0], [3, 0x9c6b820], [4, 0xa4df7c0], [5, 0x5d601e0], [0, 0x5ee7280]]}
  layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x59e43c0], [1, 0x58a1c60], [2, 0x5a6be00], [3, 0x9733ca0]]}
  layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5c23be0], [3, 0xa291ce0], [4, 0xa58a900], [5, 0x61123e0], [0, 0x6589720], [1, 0x6660cc0], [2, 0x5c64c00], [3, 0xa2d2d00]]}
  layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa5867e0], [5, 0x610e2c0], [0, 0x6585600], [1, 0x665cba0]]}
  layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa281080]]}
  layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x66496a0]]}
  layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa069ba0], [4, 0xa36eac0], [5, 0x5e7f040], [0, 0x62f3ae0], [1, 0x61aeaa0], [2, 0x5a94060], [3, 0xa0ebbc0], [4, 0xa3f0ae0], [5, 0x5f01060], [0, 0x6375b00], [1, 0x6230ac0], [2, 0x5b16080], [3, 0xa16dbe0], [4, 0xa472b00], [5, 0x5f83080], [0, 0x63f7b20]]}
  layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x633d5a0], [1, 0x639edc0], [1, 0x64005e0], [1, 0x6461e00], [1, 0x64c3620], [1, 0x6524e40], [1, 0x6586660], [1, 0x65e7e80]]}
  layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x584d700], [1, 0x570afa0], [2, 0x58e4d20], [3, 0x95aa320], [4, 0x9cd07a0], [5, 0x5848dc0], [0, 0x58cf720], [1, 0x578cfc0], [2, 0x5966d40], [3, 0x962c340], [4, 0x9d527c0], [5, 0x58cade0], [0, 0x5951740], [1, 0x580efe0], [2, 0x59e8d60], [3, 0x96ae360]]}
  layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ccc660], [5, 0x5844c80], [0, 0x584b660], [1, 0x5708f00], [2, 0x58e2c80], [3, 0x95a8280], [4, 0x9cce700], [5, 0x5846d20]]}
  layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9597620]]}
  layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x56f5a00]]}
  layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97ec900], [5, 0x5671020], [0, 0x55f92c0], [1, 0x54a6f80], [2, 0x56cf040], [3, 0x9384a60], [4, 0x982d920], [5, 0x56b2040]]}
  layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x58ddae0], [3, 0x9593500], [4, 0x9cc7d00], [5, 0x5840320]]}
  layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x57c5d00], [1, 0x56739c0], [2, 0x589cac0], [3, 0x95524e0], [4, 0x9c86ce0], [5, 0x57ff300], [0, 0x5806d20], [1, 0x56b49e0]]}
  layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x58989a0], [3, 0x954e3c0], [4, 0x9c82bc0], [5, 0x57fb1e0]]}
  layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6087920], [0, 0x64fc3c0], [1, 0x62fc580], [2, 0x5be1b40], [3, 0xa23bf40], [4, 0xa540e60], [5, 0x60c8940], [0, 0x653d3e0]]}
  layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x62f8460], [2, 0x5bdda20], [3, 0xa237e20], [4, 0xa53cd40]]}
  layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x60058e0], [0, 0x647a380], [1, 0x62b7440], [2, 0x5b9ca00], [3, 0xa1f6e00], [4, 0xa4fbd20], [5, 0x6046900], [0, 0x64bb3a0]]}
  layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x62b3320], [2, 0x5b988e0], [3, 0xa1f2ce0], [4, 0xa4f7c00]]}
  layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x59da1a0]]}
  layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x56f8260]]}
  layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5722540], [2, 0x5194580], [3, 0x96b0300], [4, 0x9a22d20], [5, 0x55f4220], [0, 0x583f3a0], [1, 0x57a4560], [2, 0x52165a0], [3, 0x9732320], [4, 0x9aa4d40], [5, 0x5676240], [0, 0x58c13c0], [1, 0x5826580], [2, 0x52985c0], [3, 0x97b4340], [4, 0x9b26d60]]}
  layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9976ac0], [4, 0x99d82e0], [4, 0x9a39b00], [4, 0x9a9b320], [4, 0x9afcb40], [4, 0x9b5e360], [4, 0x9bbfb80], [4, 0x9c213a0]]}
  layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x563f460], [1, 0x54ed120], [2, 0x5712940], [3, 0x93c8360], [4, 0x9872a80], [5, 0x56f71a0], [0, 0x56c1480], [1, 0x556f140], [2, 0x5794960], [3, 0x944a380], [4, 0x98f4aa0], [5, 0x57791c0], [0, 0x57434a0], [1, 0x55f1160], [2, 0x5816980], [3, 0x94cc3a0]]}
  layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x986e940], [5, 0x56f3060], [0, 0x563d3c0], [1, 0x54eb080], [2, 0x57108a0], [3, 0x93c62c0], [4, 0x98709e0], [5, 0x56f5100]]}
  layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d3b780]]}
  layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa3385e0]]}
  layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5d7e080], [3, 0x9a45f20], [4, 0xa2f75c0], [5, 0x5b70e20], [0, 0x5cfa760], [1, 0x5bb8000], [2, 0x5dbf0a0], [3, 0x9a86f40]]}
  layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa2f34a0], [5, 0x5b6cd00], [0, 0x5cf6640], [1, 0x5bb3ee0]]}
  layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x55721e0], [0, 0x57bd360], [1, 0x56e1520], [2, 0x5153560], [3, 0x966f2e0], [4, 0x99e1d00], [5, 0x55b3200], [0, 0x57fe380]]}
  layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x56dd400], [2, 0x514f440], [3, 0x966b1c0], [4, 0x99ddbe0]]}
  layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x95e9180], [4, 0x995bba0], [5, 0x5530980], [0, 0x577bb00], [1, 0x569c3e0], [2, 0x510e420], [3, 0x962a1a0], [4, 0x999cbc0]]}
  layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x552c860], [0, 0x57779e0], [1, 0x56982c0], [2, 0x510a300]]}
  layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9567140], [4, 0x98d9b60], [5, 0x54eb840], [0, 0x57369c0], [1, 0x56572a0], [2, 0x50c92e0], [3, 0x95a8160], [4, 0x991ab80]]}
  layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x56dcb00], [1, 0x560a720], [2, 0x506cb80], [3, 0x9520f80]]}
  layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x98c8f00]]}
  layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x50b1cc0]]}
  layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5b705e0], [1, 0x5a2de80], [2, 0x5bf8020], [3, 0x98bfec0], [4, 0xa1ef460], [5, 0x5a68cc0], [0, 0x5bf2600], [1, 0x5aafea0], [2, 0x5c7a040], [3, 0x9941ee0], [4, 0xa271480], [5, 0x5aeace0], [0, 0x5c74620], [1, 0x5b31ec0], [2, 0x5cfc060], [3, 0x99c3f00]]}
  layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ee3360], [4, 0x9f44b80], [4, 0x9fa63a0], [4, 0xa007bc0], [4, 0xa0693e0], [4, 0xa0cac00], [4, 0xa12c420], [4, 0xa18dc40]]}
  layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x59ea580], [1, 0x58a7e20], [2, 0x5a71fc0], [3, 0x9739e60], [4, 0x9ddf320], [5, 0x5964c80], [0, 0x5a6c5a0], [1, 0x5929e40], [2, 0x5af3fe0], [3, 0x97bbe80], [4, 0x9e61340], [5, 0x59e6ca0], [0, 0x5aee5c0], [1, 0x59abe60], [2, 0x5b76000], [3, 0x983dea0]]}
  layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ddb1e0], [5, 0x5960b40], [0, 0x59e84e0], [1, 0x58a5d80], [2, 0x5a6ff20], [3, 0x9737dc0], [4, 0x9ddd280], [5, 0x5962be0]]}
  layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x59d3fa0]]}
  layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5721c40]]}
  layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x9846ec0], [5, 0x5464ea0], [0, 0x56e0c20], [1, 0x560e840], [2, 0x5070ca0], [3, 0x95250a0], [4, 0x9887ee0], [5, 0x54a5ec0]]}
  layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x54e7720], [0, 0x57328a0], [1, 0x5653180], [2, 0x50c51c0]]}
  layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x99c7f80], [4, 0x9cccea0], [5, 0x581dc00], [0, 0x5a78960], [1, 0x5a38160], [2, 0x5527880], [3, 0x9a08fa0], [4, 0x9d0dec0]]}
  layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x5819ae0], [0, 0x5a74840], [1, 0x5a34040], [2, 0x5523760]]}
  layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x59b2000], [2, 0x54a1720], [3, 0x9986720], [4, 0x9c8b640], [5, 0x57d8ac0], [0, 0x5a33820], [1, 0x59f3020], [2, 0x54e2740]]}
  layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9982600], [4, 0x9c87520], [5, 0x57d49a0], [0, 0x5a2f700]]}
  layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x592ffc0], [2, 0x541f6e0], [3, 0x99415e0], [4, 0x9c46500], [5, 0x5793980], [0, 0x59ee6e0], [1, 0x5970fe0], [2, 0x5460700]]}
  layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x993d4c0], [4, 0x9c423e0], [5, 0x578f860], [0, 0x59ea5c0]]}
  layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5891000]]}
  layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x594ce00]]}
  layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6d72620], [1, 0x717b620], [2, 0x7020880], [3, 0xad37280], [4, 0xb3e4140], [5, 0x6cfa860], [0, 0x6df4640], [1, 0x71fd640], [2, 0x70a28a0], [3, 0xadb92a0], [4, 0xb466160], [5, 0x6d7c880], [0, 0x6e76660], [1, 0x727f660], [2, 0x71248c0], [3, 0xae3b2c0]]}
  layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa2fa120], [3, 0xa35b940], [3, 0xa3bd160], [3, 0xa41e980], [3, 0xa4801a0], [3, 0xa4e19c0], [3, 0xa5431e0], [3, 0xa5a4a00]]}
  layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6e9a820], [3, 0xabb1220], [4, 0xb25e0e0], [5, 0x6b74800], [0, 0x6c6e5e0], [1, 0x70775e0], [2, 0x6f1c840], [3, 0xac33240], [4, 0xb2e0100], [5, 0x6bf6820], [0, 0x6cf0600], [1, 0x70f9600], [2, 0x6f9e860], [3, 0xacb5260], [4, 0xb362120], [5, 0x6c78840]]}
  layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c6a4a0], [1, 0x70734a0], [2, 0x6e98780], [3, 0xabaf180], [4, 0xb25c040], [5, 0x6b72760], [0, 0x6c6c540], [1, 0x7075540]]}
  layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x59c9d80]]}
  layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x9c2eee0]]}
  layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x539c620], [3, 0x98b83a0], [4, 0x9bedec0], [5, 0x574d7c0], [0, 0x5988d60], [1, 0x58edf20], [2, 0x53dd640], [3, 0x98f93c0]]}
  layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x9be9da0], [5, 0x57496a0], [0, 0x5984c40], [1, 0x58e9e00]]}
  layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x531a5e0], [3, 0x9836360], [4, 0x9ba8d80], [5, 0x5708680], [0, 0x5943c20], [1, 0x58a8de0], [2, 0x535b600], [3, 0x9877380]]}
  layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa27cf60], [4, 0xa581e80], [5, 0x6109960], [0, 0x657e400]]}
  layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7128940], [1, 0x6fb4d60], [2, 0x66016a0], [3, 0xae86c40], [4, 0xae25420], [5, 0x6a0ff80], [0, 0x7169960], [1, 0x6ff5d80]]}
  layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x65fd580], [3, 0xae82b20], [4, 0xae21300], [5, 0x6a0be60]]}
  layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb1da000], [5, 0x6af0720], [0, 0x6c29480], [1, 0x7032480], [2, 0x6e57760], [3, 0xab6e160], [4, 0xb21b020], [5, 0x6b31740]]}
  layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6c25360], [1, 0x702e360], [2, 0x6e53640], [3, 0xab6a040]]}
  layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb188380]]}
  layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6dff120]]}
  layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb002320], [5, 0x68e7e40], [0, 0x6a1d2c0], [1, 0x6e66aa0], [2, 0x6cfb0e0], [3, 0xaa216c0], [4, 0xb084340], [5, 0x6969e60], [0, 0x6a9f2e0], [1, 0x6ee8ac0], [2, 0x6d7d100], [3, 0xaaa36e0], [4, 0xb106360], [5, 0x69ebe80], [0, 0x6b21300], [1, 0x6f6aae0]]}
  layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x6e1c840], [0, 0x6e7e060], [0, 0x6edf880], [0, 0x6f410a0], [0, 0x6fa28c0], [0, 0x70040e0], [0, 0x7065900], [0, 0x70c7120]]}
  layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x6477520], [3, 0xacfcac0], [4, 0xac9b2a0], [5, 0x6885e00], [0, 0x6d18800], [1, 0x6eb0d20], [2, 0x64f9540], [3, 0xad7eae0], [4, 0xad1d2c0], [5, 0x6907e20], [0, 0x6d9a820], [1, 0x6f32d40], [2, 0x657b560], [3, 0xae00b00], [4, 0xad9f2e0], [5, 0x6989e40]]}
  layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x6d146c0], [1, 0x6eacbe0], [2, 0x6475480], [3, 0xacfaa20], [4, 0xac99200], [5, 0x6883d60], [0, 0x6d16760], [1, 0x6eaec80]]}
  layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6873100]]}
  layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xace7520]]}
  layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x6e2a360], [2, 0x63f0360], [3, 0xaca6500], [4, 0xac57160], [5, 0x68320e0], [0, 0x6cd2e60], [1, 0x6e6b380], [2, 0x6431380]]}
  layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xaca23e0], [4, 0xac53040], [5, 0x682dfc0], [0, 0x6cced40]]}
  layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c790a0], [3, 0xa99f680], [4, 0xafc1300], [5, 0x68a6e20], [0, 0x69dc2a0], [1, 0x6e25a80], [2, 0x6cba0c0], [3, 0xa9e06a0]]}
  layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xafbd1e0], [5, 0x68a2d00], [0, 0x69d8180], [1, 0x6e21960]]}
  layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6956140], [1, 0x6d9f920], [2, 0x6c37840], [3, 0xa95de20], [4, 0xaf7c1c0], [5, 0x6861ce0], [0, 0x6997160], [1, 0x6de0940]]}
  layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c33720], [3, 0xa959d00], [4, 0xaf780a0], [5, 0x685dbc0]]}
  layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6a6dea0], [0, 0x6ba3320], [1, 0x6fecb00], [2, 0x6e0f540], [3, 0xab25f40], [4, 0xb1987a0], [5, 0x6aaeec0], [0, 0x6be4340]]}
  layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaf73f80], [5, 0x6859aa0], [0, 0x6952020], [1, 0x6d9b800]]}
  layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb088d20]]}
  layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x74163c0]]}
  layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x66a7f60], [0, 0x6b48ce0], [1, 0x6ca4300], [2, 0x626a300], [3, 0xab9e3a0], [4, 0xab4f000], [5, 0x6729f80], [0, 0x6bcad00], [1, 0x6d26320], [2, 0x62ec320], [3, 0xac203c0], [4, 0xabd1020], [5, 0x67abfa0], [0, 0x6c4cd20], [1, 0x6da8340], [2, 0x636e340]]}
  layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa8922a0], [3, 0xa8f3ac0], [3, 0xa9552e0], [3, 0xa9b6b00], [3, 0xaa18320], [3, 0xaa79b40], [3, 0xaadb360], [3, 0xab3cb80]]}
  layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6521f00], [0, 0x69c2c80], [1, 0x6b1e2a0], [2, 0x60e42a0], [3, 0xa78e260], [4, 0xaa4afc0], [5, 0x65a3f20], [0, 0x6a44ca0], [1, 0x6ba02c0], [2, 0x61662c0], [3, 0xa810280], [4, 0xaaccfe0], [5, 0x6625f40], [0, 0x6ac6cc0], [1, 0x6c222e0], [2, 0x61e82e0]]}
  layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa78a120], [4, 0xaa46e80], [5, 0x651fe60], [0, 0x69c0be0], [1, 0x6b1c200], [2, 0x60e2200], [3, 0xa78c1c0], [4, 0xaa48f20]]}
  layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6ca3320]]}
  layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f504e0]]}
  layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb006ce0], [4, 0xb5b4ce0], [5, 0x6f0f4c0], [0, 0x700cb80], [1, 0x73d53a0], [2, 0x72f4400], [3, 0xb047d00], [4, 0xb5f5d00]]}
  layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f0b3a0], [0, 0x7008a60], [1, 0x73d1280], [2, 0x72f02e0]]}
  layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6dfe8a0], [0, 0x6ef8680], [1, 0x7301680], [2, 0x71a68e0], [3, 0xaebd2e0], [4, 0xb4e89c0], [5, 0x6e3f8c0], [0, 0x6f396a0]]}
  layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x7004940], [1, 0x73cd160], [2, 0x72ec1c0], [3, 0xb002bc0]]}
  layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x726a180], [3, 0xaf80b80], [4, 0xb572c40], [5, 0x6ec9b40], [0, 0x6fc3920], [1, 0x738c140], [2, 0x72ab1a0], [3, 0xafc1ba0]]}
  layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb56eb20], [5, 0x6ec5a20], [0, 0x6fbf800], [1, 0x7388020]]}
  layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x71e8140], [3, 0xaefeb40], [4, 0xb52db00], [5, 0x6e84a00], [0, 0x6f7e7e0], [1, 0x7347000], [2, 0x7229160], [3, 0xaf3fb60]]}
  layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb5299e0], [5, 0x6e808e0], [0, 0x6f7a6c0], [1, 0x7342ee0]]}
  layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7500d00]]}
  layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xae66440]]}
  layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x737aca0], [1, 0x72070c0], [2, 0x684d880], [3, 0xb0d2e20], [4, 0xb2cfc20], [5, 0x6b9eaa0], [0, 0x73fccc0], [1, 0x72890e0], [2, 0x68cf8a0], [3, 0xb154e40], [4, 0xb351c40], [5, 0x6c20ac0], [0, 0x747ece0], [1, 0x730b100], [2, 0x69518c0], [3, 0xb1d6e60]]}
  layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xafc3b20], [4, 0xb025340], [4, 0xb086b60], [4, 0xb0e8380], [4, 0xb149ba0], [4, 0xb1ab3c0], [4, 0xb20cbe0], [4, 0xb26e400]]}
  layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x71f4c40], [1, 0x7081060], [2, 0x66c7820], [3, 0xaf4cdc0], [4, 0xaebfae0], [5, 0x6a9aa60], [0, 0x7276c60], [1, 0x7103080], [2, 0x6749840], [3, 0xafcede0], [4, 0xaf41b00], [5, 0x6b1ca80], [0, 0x72f8c80], [1, 0x71850a0], [2, 0x67cb860], [3, 0xb050e00]]}
  layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xaebb9a0], [5, 0x6a96920], [0, 0x71f2ba0], [1, 0x707efc0], [2, 0x66c5780], [3, 0xaf4ad20], [4, 0xaebda40], [5, 0x6a989c0]]}
  layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x67435c0]]}
  layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x6313440]]}
  layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa88f2c0], [5, 0x61a6a00], [0, 0x62d2420], [1, 0x654e9a0], [2, 0x67025a0], [3, 0x9fabfc0], [4, 0xa8d02e0], [5, 0x61e7a20]]}
  layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x62ce300], [1, 0x654a880], [2, 0x66fe480], [3, 0x9fa7ea0]]}
  layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa80d280], [5, 0x61249c0], [0, 0x628d2e0], [1, 0x6509860], [2, 0x66bd460], [3, 0x9f66e80], [4, 0xa84e2a0], [5, 0x61659e0]]}
  layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x62891c0], [1, 0x6505740], [2, 0x66b9340], [3, 0x9f62d60]]}
  layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6637300], [3, 0x9ee0d20], [4, 0xa7cba20], [5, 0x60d3580], [0, 0x62481a0], [1, 0x64c4720], [2, 0x6678320], [3, 0x9f21d40]]}
  layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa7c7900], [5, 0x60cf460], [0, 0x6244080], [1, 0x64c0600]]}
  layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x6643740], [3, 0xaec8ce0], [4, 0xae7a980], [5, 0x6a55900], [0, 0x71b1b80], [1, 0x703dfa0], [2, 0x6684760], [3, 0xaf09d00]]}
  layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xae76860], [5, 0x6a517e0], [0, 0x71ada60], [1, 0x7039e80]]}
  layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x693ba40]]}
  layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x63e67c0]]}
  layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x67b59e0], [2, 0x5dc9500], [3, 0xa49ef80], [4, 0xa7a7780], [5, 0x62e2780], [0, 0x6728ec0], [1, 0x6837a00], [2, 0x5e4b520], [3, 0xa520fa0], [4, 0xa8297a0], [5, 0x63647a0], [0, 0x67aaee0], [1, 0x68b9a20], [2, 0x5ecd540], [3, 0xa5a2fc0], [4, 0xa8ab7c0]]}
  layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x632b200], [2, 0x638ca20], [2, 0x63ee240], [2, 0x644fa60], [2, 0x64b1280], [2, 0x6512aa0], [2, 0x65742c0], [2, 0x65d5ae0]]}
  layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa6418a0], [5, 0x5f49400], [0, 0x60be020], [1, 0x633a5a0], [2, 0x62271c0], [3, 0x9ddcce0], [4, 0xa6c38c0], [5, 0x5fcb420], [0, 0x6140040], [1, 0x63bc5c0], [2, 0x62a91e0], [3, 0x9e5ed00], [4, 0xa7458e0], [5, 0x604d440], [0, 0x61c2060], [1, 0x643e5e0]]}
  layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6223080], [3, 0x9dd8ba0], [4, 0xa63f800], [5, 0x5f47360], [0, 0x60bbf80], [1, 0x6338500], [2, 0x6225120], [3, 0x9ddac40]]}
  layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x61145a0]]}
  layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d8b3e0]]}
  layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x67d7a60], [0, 0x68cffe0], [1, 0x6d4a3c0], [2, 0x6bf1ec0], [3, 0xa9184a0], [4, 0xaf32f60], [5, 0x6818a80], [0, 0x6911000]]}
  layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6d462a0], [2, 0x6bedda0], [3, 0xa914380], [4, 0xaf2ee40]]}
  layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6260740], [0, 0x66a6e80], [1, 0x67749c0], [2, 0x5d884e0], [3, 0xa45df60], [4, 0xa766760], [5, 0x62a1760], [0, 0x66e7ea0]]}
  layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x67708a0], [2, 0x5d843c0], [3, 0xa459e40], [4, 0xa762640]]}
  layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa3d7e00], [4, 0xa6e0600], [5, 0x621eee0], [0, 0x6665620], [1, 0x672f880], [2, 0x5d433a0], [3, 0xa418e20], [4, 0xa721620]]}
  layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x621adc0], [0, 0x6661500], [1, 0x672b760], [2, 0x5d3f280]]}
  layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa5cb920], [5, 0x6153400], [0, 0x65ca740], [1, 0x66a1ce0], [2, 0x5ca5c20], [3, 0xa313d20], [4, 0xa60c940], [5, 0x6194420]]}
  layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x61d5c80], [0, 0x661c3c0], [1, 0x66e6620], [2, 0x5cfa140]]}
  layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa64d960]]}
  layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5ce6c40]]}
  layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa78e320], [4, 0xada8de0], [5, 0x6651a00], [0, 0x6749f80], [1, 0x6c42260], [2, 0x6ae9d60], [3, 0xa810340], [4, 0xae2ae00], [5, 0x66d3a20], [0, 0x67cbfa0], [1, 0x6cc4280], [2, 0x6b6bd80], [3, 0xa892360], [4, 0xaeace20], [5, 0x6755a40], [0, 0x684dfc0]]}
  layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6936160], [1, 0x6997980], [1, 0x69f91a0], [1, 0x6a5a9c0], [1, 0x6abc1e0], [1, 0x6b1da00], [1, 0x6b7f220], [1, 0x6be0a40]]}
  layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa6082c0], [4, 0xac22d80], [5, 0x64cb9a0], [0, 0x65c3f20], [1, 0x6832120], [2, 0x69e5d20], [3, 0xa68a2e0], [4, 0xaca4da0], [5, 0x654d9c0], [0, 0x6645f40], [1, 0x68b4140], [2, 0x6a67d40], [3, 0xa70c300], [4, 0xad26dc0], [5, 0x65cf9e0], [0, 0x66c7f60]]}
  layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x682dfe0], [2, 0x69e1be0], [3, 0xa606220], [4, 0xac20ce0], [5, 0x64c9900], [0, 0x65c1e80], [1, 0x6830080], [2, 0x69e3c80]]}
  layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x65b1220]]}
  layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x660b760]]}
  layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa355dc0], [4, 0xa65e5c0], [5, 0x61d9da0], [0, 0x66204e0], [1, 0x66ea740], [2, 0x5cfe260], [3, 0xa396de0], [4, 0xa69f5e0]]}
  layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xaa42d60], [5, 0x651bd40], [0, 0x69bcac0], [1, 0x6b180e0]]}
  layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x605f980], [3, 0xa7080e0], [4, 0xaa01d40], [5, 0x64dad20], [0, 0x697baa0], [1, 0x6ad70c0], [2, 0x60a09a0], [3, 0xa749100]]}
  layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa9fdc20], [5, 0x64d6c00], [0, 0x6977980], [1, 0x6ad2fa0]]}
  layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x68f5940], [1, 0x6a50f60], [2, 0x601e120], [3, 0xa6c6880], [4, 0xa9bcc00], [5, 0x6495be0], [0, 0x6936960], [1, 0x6a91f80]]}
  layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x601a000], [3, 0xa6c2760], [4, 0xa9b8ae0], [5, 0x6491ac0]]}
  layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x6873900], [1, 0x69cef20], [2, 0x5fd8fe0], [3, 0xa681740], [4, 0xa977ac0], [5, 0x6450aa0], [0, 0x68b4920], [1, 0x6a0ff40]]}
  layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5fd4ec0], [3, 0xa67d620], [4, 0xa9739a0], [5, 0x644c980]]}
  layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x681d380]]}
  layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x64b5bc0]]}
  layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6697320], [2, 0x685ab00], [3, 0xa173880], [4, 0xaa97ba0], [5, 0x63b1b80], [0, 0x64ac9a0], [1, 0x6719340], [2, 0x68dcb20], [3, 0xa1f58a0], [4, 0xab19bc0], [5, 0x6433ba0], [0, 0x652e9c0], [1, 0x679b360], [2, 0x695eb40], [3, 0xa2778c0], [4, 0xab9bbe0]]}
  layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5fa08c0], [1, 0x60020e0], [1, 0x6063900], [1, 0x60c5120], [1, 0x6126940], [1, 0x6188160], [1, 0x61e9980], [1, 0x624b1a0]]}
  layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9fed820], [4, 0xa911b40], [5, 0x622bb20], [0, 0x6326940], [1, 0x65932e0], [2, 0x6756ac0], [3, 0xa06f840], [4, 0xa993b60], [5, 0x62adb40], [0, 0x63a8960], [1, 0x6615300], [2, 0x67d8ae0], [3, 0xa0f1860], [4, 0xaa15b80], [5, 0x632fb60], [0, 0x642a980]]}
  layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7dcd600], [4, 0x7f0ed40], [5, 0x3d34d40], [0, 0x3cd86a0], [1, 0x3cb8ee0], [2, 0x3f0fb20], [3, 0x7dcf6a0], [4, 0x7f10de0]]}
  layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x643bd20]]}
  layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa66a120]]}
  layer.12.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x694be60], [2, 0x5f4fda0], [3, 0xa629100], [4, 0xa931900], [5, 0x63fad00], [0, 0x6831860], [1, 0x698ce80], [2, 0x5f90dc0]]}
  layer.12.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa624fe0], [4, 0xa92d7e0], [5, 0x63f6be0], [0, 0x682d740]]}
  layer.12.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x948c240], [4, 0x97c0500], [5, 0x541ecc0], [0, 0x5696120], [1, 0x55c14a0], [2, 0x50261a0], [3, 0x94cd260], [4, 0x9801520]]}
  layer.12.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x40c7140], [3, 0x844caa0], [4, 0x8744ea0], [5, 0x4238c40]]}
  layer.12.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x86c2e60], [5, 0x41b6c00], [0, 0x422a8c0], [1, 0x43e0560], [2, 0x4086120], [3, 0x840ba80], [4, 0x8703e80], [5, 0x41f7c20]]}
  layer.12.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x42267a0], [1, 0x43dc440], [2, 0x4082000], [3, 0x8407960]]}
  layer.12.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4615900], [2, 0x4560440], [3, 0x8582720], [4, 0x86c6700], [5, 0x4705c20], [0, 0x439abe0], [1, 0x4656920], [2, 0x45a1460]]}
  layer.12.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x857e600], [4, 0x86c25e0], [5, 0x4701b00], [0, 0x4396ac0]]}
  layer.12.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x454f7e0]]}
  layer.12.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42e9fa0]]}
  layer.12.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x448e820], [2, 0x43c9780], [3, 0x83f7d60], [4, 0x853bd40], [5, 0x45f7900], [0, 0x428c8c0], [1, 0x4510840], [2, 0x444b7a0], [3, 0x8479d80], [4, 0x85bdd60], [5, 0x4679920], [0, 0x430e8e0], [1, 0x4592860], [2, 0x44cd7c0], [3, 0x84fbda0], [4, 0x863fd80]]}
  layer.12.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x83b6d60], [4, 0x8418580], [4, 0x8479da0], [4, 0x84db5c0], [4, 0x853cde0], [4, 0x859e600], [4, 0x85ffe20], [4, 0x8661640]]}
  layer.12.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40a0740], [1, 0x42563e0], [2, 0x3efbfa0], [3, 0x8281900], [4, 0x82b2d20], [5, 0x40b2bc0], [0, 0x4122760], [1, 0x42d8400], [2, 0x3f7dfc0], [3, 0x8303920], [4, 0x8334d40], [5, 0x4134be0], [0, 0x41a4780], [1, 0x435a420], [2, 0x3ffffe0], [3, 0x8385940]]}
  layer.12.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x82aebe0], [5, 0x40aea80], [0, 0x409e6a0], [1, 0x4254340], [2, 0x3ef9f00], [3, 0x827f860], [4, 0x82b0c80], [5, 0x40b0b20]]}
  layer.12.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x826cb60]]}
  layer.12.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x423fde0]]}
  layer.13.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x402a160], [0, 0x4018520], [1, 0x41fedc0], [2, 0x3eb5dc0], [3, 0x822bb40], [4, 0x826b2e0], [5, 0x406b180], [0, 0x4059540]]}
  layer.13.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x41faca0], [2, 0x3eb1ca0], [3, 0x8227a20], [4, 0x82671c0]]}
  layer.13.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x45758c0], [0, 0x420a880], [1, 0x444d800], [2, 0x4388760], [3, 0x83b6d40], [4, 0x84fad20], [5, 0x45b68e0], [0, 0x424b8a0]]}
  layer.13.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x44496e0], [2, 0x4384640], [3, 0x83b2c20], [4, 0x84f6c00]]}
  layer.13.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8330be0], [4, 0x8474bc0], [5, 0x4534060], [0, 0x41c9020], [1, 0x44086c0], [2, 0x4343620], [3, 0x8371c00], [4, 0x84b5be0]]}
  layer.13.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x452ff40], [0, 0x41c4f00], [1, 0x44045a0], [2, 0x433f500]]}
  layer.13.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x82aeba0], [4, 0x83f2b80], [5, 0x44eef20], [0, 0x4183ee0], [1, 0x43c3580], [2, 0x42fe4e0], [3, 0x82efbc0], [4, 0x8433ba0]]}
  layer.13.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x44eae00], [0, 0x417fdc0], [1, 0x43bf460], [2, 0x42fa3c0]]}
  layer.13.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x49946c0]]}
  layer.13.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8759480]]}
  layer.13.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x80a19c0], [4, 0x80e1160], [5, 0x3ea4100], [0, 0x3e924c0], [1, 0x40f6c60], [2, 0x3dadc60], [3, 0x81239e0], [4, 0x8163180], [5, 0x3f26120], [0, 0x3f144e0], [1, 0x4178c80], [2, 0x3e2fc80], [3, 0x81a5a00], [4, 0x81e51a0], [5, 0x3fa8140], [0, 0x3f96500]]}
  layer.13.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3deab60], [1, 0x3e4c380], [1, 0x3eadba0], [1, 0x3f0f3c0], [1, 0x3f70be0], [1, 0x3fd2400], [1, 0x4033c20], [1, 0x4095440]]}
  layer.13.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f1b960], [4, 0x7f5b100], [5, 0x3d1e0a0], [0, 0x3d0c460], [1, 0x3ce6b20], [2, 0x3ca9c20], [3, 0x7f9d980], [4, 0x7fdd120], [5, 0x3da00c0], [0, 0x3d8e480], [1, 0x3d68b40], [2, 0x3d2bc40], [3, 0x801f9a0], [4, 0x805f140], [5, 0x3e220e0], [0, 0x3e104a0]]}
  layer.13.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x409a560], [1, 0x4250200], [2, 0x3ef7620], [3, 0x827cf80], [4, 0x82acb40], [5, 0x40ac9e0], [0, 0x409c600], [1, 0x42522a0]]}
  layer.13.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x87e56c0]]}
  layer.13.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47ed6a0]]}
  layer.14.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4912680], [0, 0x45a7640], [1, 0x47ac680], [2, 0x46f71c0], [3, 0x8718460], [4, 0x885c440], [5, 0x49536a0], [0, 0x45e8660]]}
  layer.14.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47a8560], [2, 0x46f30a0], [3, 0x8714340], [4, 0x8858320]]}
  layer.14.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4890640], [0, 0x4525600], [1, 0x4767540], [2, 0x46b2080], [3, 0x86d3320], [4, 0x8817300], [5, 0x48d1660], [0, 0x4566620]]}
  layer.14.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4787c60], [0, 0x441cc20], [1, 0x46d8960], [2, 0x46234a0]]}
  layer.14.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x480e600], [0, 0x44a35c0], [1, 0x4725ce0], [2, 0x4670820], [3, 0x868f220], [4, 0x87d3200], [5, 0x484f620], [0, 0x44e45e0]]}
  layer.14.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4721bc0], [2, 0x466c700], [3, 0x868b100], [4, 0x87cf0e0]]}
  layer.14.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x478c5c0], [0, 0x4421580], [1, 0x46e0ba0], [2, 0x462b6e0], [3, 0x864a0e0], [4, 0x878e0c0], [5, 0x47cd5e0], [0, 0x44625a0]]}
  layer.14.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46dca80], [2, 0x46275c0], [3, 0x8645fc0], [4, 0x8789fa0]]}
  layer.14.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8ad99a0]]}
  layer.14.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x479a160]]}
  layer.14.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x45f20e0], [2, 0x429b580], [3, 0x865b540], [4, 0x8953940], [5, 0x4696120], [0, 0x43b9c20], [1, 0x4674100], [2, 0x431d5a0], [3, 0x86dd560], [4, 0x89d5960], [5, 0x4718140], [0, 0x443bc40], [1, 0x46f6120], [2, 0x439f5c0], [3, 0x875f580], [4, 0x8a57980]]}
  layer.14.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x438a020], [5, 0x43eb840], [5, 0x444d060], [5, 0x44ae880], [5, 0x45100a0], [5, 0x45718c0], [5, 0x45d30e0], [5, 0x4634900]]}
  layer.14.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x446c080], [2, 0x4115520], [3, 0x84d54e0], [4, 0x87cd8e0], [5, 0x4285fe0], [0, 0x42b5be0], [1, 0x44ee0a0], [2, 0x4197540], [3, 0x8557500], [4, 0x884f900], [5, 0x4308000], [0, 0x4337c00], [1, 0x45700c0], [2, 0x4219560], [3, 0x85d9520], [4, 0x88d1920]]}
  layer.14.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4281ea0], [0, 0x42b1aa0], [1, 0x4469fe0], [2, 0x4113480], [3, 0x84d3440], [4, 0x87cb840], [5, 0x4283f40], [0, 0x42b3b40]]}
  layer.14.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3dfae80]]}
  layer.14.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3b42220]]}
  layer.15.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d06ca0], [5, 0x3b2cca0], [0, 0x3b01200], [1, 0x3af1620], [2, 0x3db9e60], [3, 0x7c87d60], [4, 0x7d47cc0], [5, 0x3b6dcc0]]}
  layer.15.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3afd0e0], [1, 0x3aed500], [2, 0x3db5d40], [3, 0x7c83c40]]}
  layer.15.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c84c60], [5, 0x3aaac60], [0, 0x3abc0c0], [1, 0x3aac4e0], [2, 0x3d74d20], [3, 0x7c42c20], [4, 0x7cc5c80], [5, 0x3aebc80]]}
  layer.15.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ab7fa0], [1, 0x3aa83c0], [2, 0x3d70c00], [3, 0x7c3eb00]]}
  layer.15.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ceebc0], [3, 0x7bbcac0], [4, 0x7c43400], [5, 0x3a69400], [0, 0x3a76f80], [1, 0x3a673a0], [2, 0x3d2fbe0], [3, 0x7bfdae0]]}
  layer.15.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c3f2e0], [5, 0x3a652e0], [0, 0x3a72e60], [1, 0x3a63280]]}
  layer.15.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8451400], [4, 0x8749800], [5, 0x4240e80], [0, 0x4270a80], [1, 0x4428fc0], [2, 0x40d2460], [3, 0x8492420], [4, 0x878a820]]}
  layer.15.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x423cd60], [0, 0x426c960], [1, 0x4424ea0], [2, 0x40ce340]]}
  layer.15.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3b7d080]]}
  layer.15.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3afa020]]}
  layer.15.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x39f7020], [2, 0x39ba120], [3, 0x7bc4d20], [4, 0x7bc3ce0], [5, 0x39f5fe0], [0, 0x3a24b80], [1, 0x3a79040], [2, 0x3a3c140], [3, 0x7c46d40], [4, 0x7c45d00], [5, 0x3a78000], [0, 0x3aa6ba0], [1, 0x3afb060], [2, 0x3abe160], [3, 0x7cc8d60], [4, 0x7cc7d20]]}
  layer.15.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39e2ac0], [2, 0x3a442e0], [2, 0x3aa5b00], [2, 0x3b07320], [2, 0x3b68b40], [2, 0x3bca360], [2, 0x3c2bb80], [2, 0x3c8d3a0]]}
  layer.15.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab9280], [5, 0x38df280], [0, 0x38ece00], [1, 0x38dd220], [2, 0x38dea80], [3, 0x7ab8a80], [4, 0x7b3b2a0], [5, 0x39612a0], [0, 0x396ee20], [1, 0x395f240], [2, 0x3960aa0], [3, 0x7b3aaa0], [4, 0x7bbd2c0], [5, 0x39e32c0], [0, 0x39f0e40], [1, 0x39e1260]]}
  layer.15.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da940], [3, 0x7ab4940], [4, 0x7ab71e0], [5, 0x38dd1e0], [0, 0x38ead60], [1, 0x38db180], [2, 0x38dc9e0], [3, 0x7ab69e0]]}
  layer.15.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100]]}
  layer.15.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x43af040]]}
  layer.16.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4468dc0], [0, 0x40fdd80], [1, 0x436e020], [2, 0x42a8f80], [3, 0x826d340], [4, 0x83b1320], [5, 0x44a9de0], [0, 0x413eda0]]}
  layer.16.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4369f00], [2, 0x42a4e60], [3, 0x8269220], [4, 0x83ad200]]}
  layer.16.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3973fa0], [0, 0x39a2b40], [1, 0x39b6000], [2, 0x3979100], [3, 0x7b83d00], [4, 0x7b82cc0], [5, 0x39b4fc0], [0, 0x39e3b60]]}
  layer.16.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x39b1ee0], [2, 0x3974fe0], [3, 0x7b7fbe0], [4, 0x7b7eba0]]}
  layer.16.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7afdba0], [4, 0x7afcb60], [5, 0x3932740], [0, 0x39612e0], [1, 0x3970ec0], [2, 0x3933fc0], [3, 0x7b3ebc0], [4, 0x7b3db80]]}
  layer.16.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100]]}
  layer.16.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38df2a0], [1, 0x38eee80], [2, 0x38e2b80], [3, 0x7abcb80], [4, 0x7abbb40], [5, 0x38f1720], [0, 0x39202c0], [1, 0x392fea0]]}
  layer.16.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x38dea60], [3, 0x7ab8a60], [4, 0x7ab7a20], [5, 0x38ed600]]}
  layer.16.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x38de220]]}
  layer.16.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x38da100]]}
  layer.16.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80e31c0], [4, 0x82271a0], [5, 0x42e2d60], [0, 0x3f77d20], [1, 0x4265ec0], [2, 0x41a0e20], [3, 0x81651e0], [4, 0x82a91c0], [5, 0x4364d80], [0, 0x3ff9d40], [1, 0x42e7ee0], [2, 0x4222e40], [3, 0x81e7200], [4, 0x832b1e0], [5, 0x43e6da0], [0, 0x407bd60]]}
  layer.16.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f59dc0], [1, 0x3fbb5e0], [1, 0x401ce00], [1, 0x407e620], [1, 0x40dfe40], [1, 0x4141660], [1, 0x41a2e80], [1, 0x42046a0]]}
  layer.16.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f5d160], [4, 0x80a1140], [5, 0x415cd00], [0, 0x3df1cc0], [1, 0x3e55d80], [2, 0x409cde0], [3, 0x7fdf180], [4, 0x8123160], [5, 0x41ded20], [0, 0x3e73ce0], [1, 0x3ed7da0], [2, 0x411ee00], [3, 0x80611a0], [4, 0x81a5180], [5, 0x4260d40], [0, 0x3ef5d00]]}
  layer.16.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e51c40], [2, 0x4098ca0], [3, 0x7f5b0c0], [4, 0x809f0a0], [5, 0x415ac60], [0, 0x3defc20], [1, 0x3e53ce0], [2, 0x409ad40]]}
  layer.16.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ddefc0]]}
  layer.16.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3923ba0]]}
  layer.17.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3c9c060], [0, 0x3c8a420], [1, 0x3ca2a20], [2, 0x3c65b20], [3, 0x7eda940], [4, 0x7f1a0e0], [5, 0x3cdd080], [0, 0x3ccb440]]}
  layer.17.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c9e900], [2, 0x3c61a00], [3, 0x7ed6820], [4, 0x7f15fc0]]}
  layer.17.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3c1a020], [0, 0x3c083e0], [1, 0x3c5d8e0], [2, 0x3c209e0], [3, 0x7e95800], [4, 0x7ed4fa0], [5, 0x3c5b040], [0, 0x3c49400]]}
  layer.17.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c597c0], [2, 0x3c1c8c0], [3, 0x7e916e0], [4, 0x7ed0e80]]}
  layer.17.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7e0f6a0], [4, 0x7e4ee40], [5, 0x3bd87c0], [0, 0x3bc6b80], [1, 0x3c187a0], [2, 0x3bdb8a0], [3, 0x7e506c0], [4, 0x7e8fe60]]}
  layer.17.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3bd46a0], [0, 0x3bc2a60], [1, 0x3c14680], [2, 0x3bd7780]]}
  layer.17.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d8d660], [4, 0x7dcce00], [5, 0x3b93680], [0, 0x3b81a40], [1, 0x3bd3660], [2, 0x3b96760], [3, 0x7dce680], [4, 0x7e0de20]]}
  layer.17.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b8f560], [0, 0x3b7d920], [1, 0x3bcf540], [2, 0x3b92640]]}
  layer.17.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e40fe0]]}
  layer.17.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4146f20]]}
  layer.17.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3cbaf80], [2, 0x3f11bc0], [3, 0x7dd1740], [4, 0x7f12e80], [5, 0x4042ee0], [0, 0x3cda740], [1, 0x3d3cfa0], [2, 0x3f93be0], [3, 0x7e53760], [4, 0x7f94ea0], [5, 0x40c4f00], [0, 0x3d5c760], [1, 0x3dbefc0], [2, 0x4015c00], [3, 0x7ed5780], [4, 0x8016ec0]]}
  layer.17.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d36de0], [5, 0x3d98600], [5, 0x3df9e20], [5, 0x3e5b640], [5, 0x3ebce60], [5, 0x3f1e680], [5, 0x3f7fea0], [5, 0x3fe16c0]]}
  layer.17.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d88ce0], [5, 0x3baece0], [0, 0x3b52640], [1, 0x3b32e80], [2, 0x3e0b2a0], [3, 0x7cc95c0], [4, 0x7e0ad00], [5, 0x3c30d00], [0, 0x3bd4660], [1, 0x3bb4ea0], [2, 0x3e8d2c0], [3, 0x7d4b5e0], [4, 0x7e8cd20], [5, 0x3cb2d20], [0, 0x3c56680], [1, 0x3c36ec0]]}
  layer.17.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55b6a60], [3, 0x927c060], [4, 0x97501c0], [5, 0x55e44c0], [0, 0x555cb80], [1, 0x541a420], [2, 0x55b8b00], [3, 0x927e100]]}
  layer.17.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3b819e0]]}
  layer.17.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b6a420]]}
  layer.18.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d49d40], [5, 0x3b0a440], [0, 0x3b29400], [1, 0x3b8d4a0], [2, 0x3b409c0], [3, 0x7d4b5c0], [4, 0x7d8ad60], [5, 0x3b4b460]]}
  layer.18.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x44bdc60], [1, 0x4778140], [2, 0x44215e0], [3, 0x87e15a0]]}
  layer.18.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4d52620], [3, 0x90b6f60], [4, 0x941be20], [5, 0x504d2c0], [0, 0x52c4720], [1, 0x5012c60], [2, 0x4d93640], [3, 0x90f7f80]]}
  layer.18.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x9417d00], [5, 0x50491a0], [0, 0x52c0600], [1, 0x500eb40]]}
  layer.18.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x523e5c0], [1, 0x4f8cb00], [2, 0x4d10dc0], [3, 0x9075700], [4, 0x93d6ce0], [5, 0x5008180], [0, 0x527f5e0], [1, 0x4fcdb20]]}
  layer.18.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4d0cca0], [3, 0x90715e0], [4, 0x93d2bc0], [5, 0x5004060]]}
  layer.18.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x96436c0], [5, 0x545dbc0], [0, 0x5416a60], [1, 0x534b860], [2, 0x54e86e0], [3, 0x91b0580], [4, 0x96846e0], [5, 0x549ebe0]]}
  layer.18.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5412940], [1, 0x5347740], [2, 0x54e45c0], [3, 0x91ac460]]}
  layer.18.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x544cf60]]}
  layer.18.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9198f60]]}
  layer.18.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9372be0], [5, 0x517d500], [0, 0x517f5a0], [1, 0x50b43a0], [2, 0x530a7c0], [3, 0x8fc2a80], [4, 0x93f4c00], [5, 0x51ff520], [0, 0x52015c0], [1, 0x51363c0], [2, 0x538c7e0], [3, 0x9044aa0], [4, 0x9476c20], [5, 0x5281540], [0, 0x52835e0], [1, 0x51b83e0]]}
  layer.18.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4f324c0], [0, 0x4f93ce0], [0, 0x4ff5500], [0, 0x5056d20], [0, 0x50b8540], [0, 0x5119d60], [0, 0x517b580], [0, 0x51dcda0]]}
  layer.18.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4b86c40], [3, 0x8eeb580], [4, 0x924cb60], [5, 0x4e7e000], [0, 0x4e2e480], [1, 0x4e88ac0], [2, 0x4c08c60], [3, 0x8f6d5a0], [4, 0x92ceb80], [5, 0x4f00020], [0, 0x4eb04a0], [1, 0x4f0aae0], [2, 0x4c8ac80], [3, 0x8fef5c0], [4, 0x9350ba0], [5, 0x4f82040]]}
  layer.18.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4e2a340], [1, 0x4e84980], [2, 0x4b84ba0], [3, 0x8ee94e0], [4, 0x924aac0], [5, 0x4e7bf60], [0, 0x4e2c3e0], [1, 0x4e86a20]]}
  layer.18.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x49ee720]]}
  layer.18.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4d12e00]]}
  layer.19.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x90421e0], [5, 0x4cf25c0], [0, 0x4cd1de0], [1, 0x4cbd0c0], [2, 0x49ad700], [3, 0x8d21c20], [4, 0x9083200], [5, 0x4d335e0]]}
  layer.19.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4ccdcc0], [1, 0x4cb8fa0], [2, 0x49a95e0], [3, 0x8d1db00]]}
  layer.19.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x53900c0], [1, 0x52c4ec0], [2, 0x549fc80], [3, 0x9157f40], [4, 0x9601620], [5, 0x540bf40], [0, 0x53d10e0], [1, 0x5305ee0]]}
  layer.19.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x549bb60], [3, 0x9153e20], [4, 0x95fd500], [5, 0x5407e20]]}
  layer.19.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x957b4c0], [5, 0x5385de0], [0, 0x534e860], [1, 0x5283660], [2, 0x545ab40], [3, 0x9112e00], [4, 0x95bc4e0], [5, 0x53c6e00]]}
  layer.19.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x534a740], [1, 0x527f540], [2, 0x5456a20], [3, 0x910ece0]]}
  layer.19.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x94f9480], [5, 0x5303da0], [0, 0x5309720], [1, 0x523e520], [2, 0x5415a00], [3, 0x90cdcc0], [4, 0x953a4a0], [5, 0x5344dc0]]}
  layer.19.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5305600], [1, 0x523a400], [2, 0x54118e0], [3, 0x90c9ba0]]}
  layer.19.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56bec20]]}
  layer.19.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x55e8ea0]]}
  layer.19.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4823580], [3, 0x8b97aa0], [4, 0x8ebc180], [5, 0x4b6c560], [0, 0x4bc9c80], [1, 0x4bb4f60], [2, 0x48a55a0], [3, 0x8c19ac0], [4, 0x8f3e1a0], [5, 0x4bee580], [0, 0x4c4bca0], [1, 0x4c36f80], [2, 0x49275c0], [3, 0x8c9bae0], [4, 0x8fc01c0], [5, 0x4c705a0]]}
  layer.19.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x48bdb80], [0, 0x491f3a0], [0, 0x4980bc0], [0, 0x49e23e0], [0, 0x4a43c00], [0, 0x4aa5420], [0, 0x4b06c40], [0, 0x4b68460]]}
  layer.19.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4cfe920], [2, 0x49feb40], [3, 0x8d63480], [4, 0x90c4a60], [5, 0x4d776e0], [0, 0x4d26300], [1, 0x4d80940], [2, 0x4a80b60], [3, 0x8de54a0], [4, 0x9146a80], [5, 0x4df9700], [0, 0x4da8320], [1, 0x4e02960], [2, 0x4b02b80], [3, 0x8e674c0], [4, 0x91c8aa0]]}
  layer.19.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x9842d80], [5, 0x5460d60], [0, 0x56daa60], [1, 0x5608680], [2, 0x506aae0], [3, 0x951eee0], [4, 0x9844e20], [5, 0x5462e00]]}
  layer.19.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x950e280]]}
  layer.19.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97dc4e0]]}
  layer.20.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x563cbe0], [3, 0x93021e0], [4, 0x979b4c0], [5, 0x562f7c0], [0, 0x55a7e80], [1, 0x5465720], [2, 0x567dc00], [3, 0x9343200]]}
  layer.20.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97973a0], [5, 0x562b6a0], [0, 0x55a3d60], [1, 0x5461600]]}
  layer.20.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x55baba0], [3, 0x92801a0], [4, 0x9756380], [5, 0x55ea680], [0, 0x5562d40], [1, 0x54205e0], [2, 0x55fbbc0], [3, 0x92c11c0]]}
  layer.20.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9752260], [5, 0x55e6560], [0, 0x555ec20], [1, 0x541c4c0]]}
  layer.20.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5562480], [0, 0x54dab40], [1, 0x53d8bc0], [2, 0x5575a40], [3, 0x923b040], [4, 0x970f1a0], [5, 0x55a34a0], [0, 0x551bb60]]}
  layer.20.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x53d4aa0], [2, 0x5571920], [3, 0x9236f20], [4, 0x970b080]]}
  layer.20.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x54e0440], [0, 0x5458b00], [1, 0x5393a80], [2, 0x5530900], [3, 0x91f5f00], [4, 0x96ca060], [5, 0x5521460], [0, 0x5499b20]]}
  layer.20.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x538f960], [2, 0x552c7e0], [3, 0x91f1de0], [4, 0x96c5f40]]}
  layer.20.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x945ce40]]}
  layer.20.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x55b1080]]}
  layer.20.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x93061e0], [4, 0x963a4a0], [5, 0x5298c60], [0, 0x55100c0], [1, 0x54ad040], [2, 0x4f21920], [3, 0x9388200], [4, 0x96bc4c0], [5, 0x531ac80], [0, 0x55920e0], [1, 0x552f060], [2, 0x4fa3940], [3, 0x940a220], [4, 0x973e4e0], [5, 0x539cca0], [0, 0x5614100]]}
  layer.20.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x51a0f40], [1, 0x5202760], [1, 0x5263f80], [1, 0x52c57a0], [1, 0x5326fc0], [1, 0x53887e0], [1, 0x53ea000], [1, 0x544b820]]}
  layer.20.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9180180], [4, 0x94b4440], [5, 0x5112c00], [0, 0x538a060], [1, 0x509cf00], [2, 0x4e1d8e0], [3, 0x92021a0], [4, 0x9536460], [5, 0x5194c20], [0, 0x540c080], [1, 0x511ef20], [2, 0x4e9f900], [3, 0x92841c0], [4, 0x95b8480], [5, 0x5216c40], [0, 0x548e0a0]]}
  layer.20.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5098dc0], [2, 0x4e197a0], [3, 0x917e0e0], [4, 0x94b23a0], [5, 0x5110b60], [0, 0x5387fc0], [1, 0x509ae60], [2, 0x4e1b840]]}
  layer.20.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4c0cc00]]}
  layer.20.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8a04e60]]}
  layer.21.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x49ca560], [2, 0x49cc5e0], [3, 0x89c3e40], [4, 0x8ab51c0], [5, 0x4bcbbe0], [0, 0x4ad9020], [1, 0x4a0b580], [2, 0x4a0d600]]}
  layer.21.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x89bfd20], [4, 0x8ab10a0], [5, 0x4bc7ac0], [0, 0x4ad4f00]]}
  layer.21.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4948520], [2, 0x494a5a0], [3, 0x897ed00], [4, 0x8a70080], [5, 0x4b86aa0], [0, 0x4a93ee0], [1, 0x4989540], [2, 0x498b5c0]]}
  layer.21.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x897abe0], [4, 0x8a6bf60], [5, 0x4b82980], [0, 0x4a8fdc0]]}
  layer.21.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4738a20], [3, 0x87698a0], [4, 0x889dca0], [5, 0x49a4ae0], [0, 0x4629ec0], [1, 0x47fe300], [2, 0x4779a40], [3, 0x87aa8c0]]}
  layer.21.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4945c40], [3, 0x8976ac0], [4, 0x8a67e40], [5, 0x4b7e860]]}
  layer.21.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x508eb20], [0, 0x5305f80], [1, 0x5057da0], [2, 0x4dd8780], [3, 0x913d0c0], [4, 0x9471380], [5, 0x50cfb40], [0, 0x5346fa0]]}
  layer.21.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5053c80], [2, 0x4dd4660], [3, 0x9138fa0], [4, 0x946d260]]}
  layer.21.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8d81280]]}
  layer.21.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x46835a0]]}
  layer.21.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8bfb220], [5, 0x48beac0], [0, 0x45a7f60], [1, 0x488be80], [2, 0x457f560], [3, 0x8903660], [4, 0x8c7d240], [5, 0x4940ae0], [0, 0x4629f80], [1, 0x490dea0], [2, 0x4601580], [3, 0x8985680], [4, 0x8cff260], [5, 0x49c2b00], [0, 0x46abfa0], [1, 0x498fec0]]}
  layer.21.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x47738a0], [0, 0x47d50c0], [0, 0x48368e0], [0, 0x4898100], [0, 0x48f9920], [0, 0x495b140], [0, 0x49bc960], [0, 0x4a1e180]]}
  layer.21.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x47bfbe0], [3, 0x87f0a60], [4, 0x88e1de0], [5, 0x49f8800], [0, 0x466f860], [1, 0x4843ca0], [2, 0x4841c00], [3, 0x8872a80], [4, 0x8963e00], [5, 0x4a7a820], [0, 0x46f1880], [1, 0x48c5cc0], [2, 0x48c3c20], [3, 0x88f4aa0], [4, 0x89e5e20], [5, 0x4afc840]]}
  layer.21.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x466b720], [1, 0x483fb60], [2, 0x47bdb40], [3, 0x87ee9c0], [4, 0x88dfd40], [5, 0x49f6760], [0, 0x466d7c0], [1, 0x4841c00]]}
  layer.21.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x49e5b00]]}
  layer.21.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4a7f9a0]]}
  layer.22.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5288780], [3, 0x8f40a40], [4, 0x9331bc0], [5, 0x513c4e0], [0, 0x513e580], [1, 0x5073380], [2, 0x52c97a0], [3, 0x8f81a60]]}
  layer.22.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x932daa0], [5, 0x51383c0], [0, 0x513a460], [1, 0x506f260]]}
  layer.22.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x44fd520], [3, 0x8881620], [4, 0x8bba200], [5, 0x487daa0], [0, 0x4566f40], [1, 0x484ae60], [2, 0x453e540], [3, 0x88c2640]]}
  layer.22.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8bb60e0], [5, 0x4879980], [0, 0x4562e20], [1, 0x4846d40]]}
  layer.22.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8aea600], [5, 0x47ab600], [0, 0x44c56a0], [1, 0x4782420], [2, 0x4429020], [3, 0x87f6320], [4, 0x8b2b620], [5, 0x47ec620]]}
  layer.22.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x887ccc0], [4, 0x8bb1fc0], [5, 0x4875860], [0, 0x455ed00]]}
  layer.22.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x47c44c0], [2, 0x447aca0], [3, 0x883bca0], [4, 0x8b70fa0], [5, 0x4834840], [0, 0x451dce0], [1, 0x48054e0], [2, 0x44bbcc0]]}
  layer.22.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8837b80], [4, 0x8b6ce80], [5, 0x4830720], [0, 0x4519bc0]]}
  layer.22.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x446a040]]}
  layer.22.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x45066c0]]}
  layer.22.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4fb4400], [1, 0x4ee9200], [2, 0x5102720], [3, 0x8dba9e0], [4, 0x9229a60], [5, 0x5034380], [0, 0x5036420], [1, 0x4f6b220], [2, 0x5184740], [3, 0x8e3ca00], [4, 0x92aba80], [5, 0x50b63a0], [0, 0x50b8440], [1, 0x4fed240], [2, 0x5206760], [3, 0x8ebea20]]}
  layer.22.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8f1d960], [4, 0x8f7f180], [4, 0x8fe09a0], [4, 0x90421c0], [4, 0x90a39e0], [4, 0x9105200], [4, 0x9166a20], [4, 0x91c8240]]}
  layer.22.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4e2e3a0], [1, 0x4d631a0], [2, 0x4f7c6c0], [3, 0x8c34980], [4, 0x8e19920], [5, 0x4f30340], [0, 0x4eb03c0], [1, 0x4de51c0], [2, 0x4ffe6e0], [3, 0x8cb69a0], [4, 0x8e9b940], [5, 0x4fb2360], [0, 0x4f323e0], [1, 0x4e671e0], [2, 0x5080700], [3, 0x8d389c0]]}
  layer.22.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8e157e0], [5, 0x4f2c200], [0, 0x4e2c300], [1, 0x4d61100], [2, 0x4f7a620], [3, 0x8c328e0], [4, 0x8e17880], [5, 0x4f2e2a0]]}
  layer.22.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c21c80]]}
  layer.22.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8eabd60]]}
  layer.23.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x47a1540], [3, 0x8b15a60], [4, 0x8e6ad40], [5, 0x4b2ad00], [0, 0x487cb60], [1, 0x4b73f40], [2, 0x47e2560], [3, 0x8b56a80]]}
  layer.23.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8e66c20], [5, 0x4b26be0], [0, 0x4878a40], [1, 0x4b6fe20]]}
  layer.23.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x471f500], [3, 0x8a93a20], [4, 0x8e25c00], [5, 0x4ae5bc0], [0, 0x4837a20], [1, 0x4b2ee00], [2, 0x4760520], [3, 0x8ad4a40]]}
  layer.23.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8e21ae0], [5, 0x4ae1aa0], [0, 0x4833900], [1, 0x4b2ace0]]}
  layer.23.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x47b18c0], [1, 0x4aa8ca0], [2, 0x46ddca0], [3, 0x8a521c0], [4, 0x8de0ac0], [5, 0x4aa0a80], [0, 0x47f28e0], [1, 0x4ae9cc0]]}
  layer.23.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x46d9b80], [3, 0x8a4e0a0], [4, 0x8ddc9a0], [5, 0x4a9c960]]}
  layer.23.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [16, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x472f880], [1, 0x4a26c60], [2, 0x4698b60], [3, 0x8a0d080], [4, 0x8d9b980], [5, 0x4a5b940], [0, 0x47708a0], [1, 0x4a67c80]]}
  layer.23.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4a11ee0], [2, 0x46939c0], [3, 0x8a07ee0], [4, 0x8d916a0]]}
  layer.23.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8e04b80]]}
  layer.23.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4f668e0]]}
  layer.23.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [16, 16], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8c7eb20], [5, 0x4da5120], [0, 0x4ca2980], [1, 0x4bd4ee0], [2, 0x4e628a0], [3, 0x8b1d400], [4, 0x8d00b40], [5, 0x4e27140], [0, 0x4d249a0], [1, 0x4c56f00], [2, 0x4ee48c0], [3, 0x8b9f420], [4, 0x8d82b60], [5, 0x4ea9160], [0, 0x4da69c0], [1, 0x4cd8f20]]}
  layer.23.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [6, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4b567a0], [2, 0x4bb7fc0], [2, 0x4c197e0], [2, 0x4c7b000], [2, 0x4cdc820], [2, 0x4d3e040], [2, 0x4d9f860], [2, 0x4e01080]]}
  layer.23.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [64, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8af8ac0], [5, 0x4c1f0c0], [0, 0x4b1c920], [1, 0x4a4ee80], [2, 0x4a52760], [3, 0x8a193c0], [4, 0x8b7aae0], [5, 0x4ca10e0], [0, 0x4b9e940], [1, 0x4ad0ea0], [2, 0x4ad4780], [3, 0x8a9b3e0], [4, 0x8bfcb00], [5, 0x4d23100], [0, 0x4c20960], [1, 0x4b52ec0]]}
  layer.23.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4a4e620], [3, 0x8a15280], [4, 0x8af6a20], [5, 0x4c1d020], [0, 0x4b1a880], [1, 0x4a4cde0], [2, 0x4a506c0], [3, 0x8a17320]]}
  layer.23.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4a16000]]}
  layer.23.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4a45360]]}

  # constant
  constant_1_multiply_2609:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa45c700]]}
  lc.input_tensor.softmax_2611.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9be8fa0]]}
  lc.input_tensor.layernorm_2631.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5e000c0]]}
  lc.input_tensor.layernorm_2631.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d91520]]}
  dc.input_tensor.layernorm_2631.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa38e380], [5, 0x5c07be0]]}
  lc.input_tensor.layernorm_2631.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9b1d4c0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5e82940]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5a93820]]}
  lc.input_tensor.layernorm_2645.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5fe5140]]}
  lc.input_tensor.layernorm_2645.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x5b706a0]]}
  dc.input_tensor.layernorm_2645.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9d58960], [4, 0xa05d880]]}
  lc.input_tensor.layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5886e20]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5d90ce0]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa38db40]]}
  constant_1_multiply_2663:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6222000]]}
  lc.input_tensor.softmax_2665.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6336c40]]}
  lc.input_tensor.layernorm_2685.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5fb2d60]]}
  lc.input_tensor.layernorm_2685.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x585ec20]]}
  dc.input_tensor.layernorm_2685.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x5b6d5c0], [0, 0x5fe2060]]}
  lc.input_tensor.layernorm_2685.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa05d040]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9d58120]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5c85340]]}
  lc.input_tensor.layernorm_2699.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x5c85b80]]}
  lc.input_tensor.layernorm_2699.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5e3b8a0]]}
  dc.input_tensor.layernorm_2699.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9d46ee0], [4, 0xa5ab2a0]]}
  lc.input_tensor.layernorm_2699.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa4def80]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5fb2520]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa5aaa60]]}
  constant_1_multiply_2717:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x60934c0]]}
  lc.input_tensor.softmax_2719.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5e9c000]]}
  lc.input_tensor.layernorm_2739.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa2914a0]]}
  lc.input_tensor.layernorm_2739.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5c233a0]]}
  dc.input_tensor.layernorm_2739.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x6582520], [1, 0x6659ac0]]}
  lc.input_tensor.layernorm_2739.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x610da80]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa585fa0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5c22b60]]}
  lc.input_tensor.layernorm_2753.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x95a7a40]]}
  lc.input_tensor.layernorm_2753.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x58e2440]]}
  dc.input_tensor.layernorm_2753.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5848580], [1, 0x5705e20]]}
  lc.input_tensor.layernorm_2753.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5844440]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ccbe20]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x58e1c00]]}
  constant_1_multiply_2771:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5673180]]}
  lc.input_tensor.softmax_2773.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x57c54c0]]}
  lc.input_tensor.layernorm_2793.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x6479b40]]}
  lc.input_tensor.layernorm_2793.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x60050a0]]}
  dc.input_tensor.layernorm_2793.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa1efc00], [4, 0xa4f4b20]]}
  lc.input_tensor.layernorm_2793.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5b980a0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x62b2ae0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x59433e0]]}
  lc.input_tensor.layernorm_2807.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x93c5a80]]}
  lc.input_tensor.layernorm_2807.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5710060]]}
  dc.input_tensor.layernorm_2807.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x563a2e0], [1, 0x54e7fa0]]}
  lc.input_tensor.layernorm_2807.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5847d40]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5bf9020]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bb1e40]]}
  constant_1_multiply_2825:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x57bcb20]]}
  lc.input_tensor.softmax_2827.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x55719a0]]}
  lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x98d9320]]}
  lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x9566900]]}
  dc.input_tensor.layernorm_2847.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x56500a0], [2, 0x50c20e0]]}
  lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5732060]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x54e6ee0]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x95660c0]]}
  lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9dd47e0]]}
  lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5960300]]}
  dc.input_tensor.layernorm_2861.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9730bc0], [4, 0x9dd8100]]}
  lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a6b5c0]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x58a1420]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x564f860]]}
  constant_1_multiply_2879:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x9ccc660]]}
  lc.input_tensor.softmax_2881.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x99c7740]]}
  lc.input_tensor.layernorm_2901.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x541eea0]]}
  lc.input_tensor.layernorm_2901.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x592f780]]}
  dc.input_tensor.layernorm_2901.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9dd5020], [5, 0x595d220]]}
  lc.input_tensor.layernorm_2901.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9730380]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a6ad80]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x59d3760]]}
  lc.input_tensor.layernorm_2915.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x58a85a0]]}
  lc.input_tensor.layernorm_2915.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x578f020]]}
  dc.input_tensor.layernorm_2915.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x993a3e0], [4, 0x9c3f300]]}
  lc.input_tensor.layernorm_2915.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x541e660]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x592ef40]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x578e7e0]]}
  constant_1_multiply_2933:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xaec7c60]]}
  lc.input_tensor.softmax_2935.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x66426c0]]}
  lc.input_tensor.layernorm_2955.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6aefee0]]}
  lc.input_tensor.layernorm_2955.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb1d97c0]]}
  dc.input_tensor.layernorm_2955.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6e50560], [3, 0xab66f60]]}
  lc.input_tensor.layernorm_2955.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x702db20]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa9594c0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xab25700]]}
  lc.input_tensor.layernorm_2969.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6883520]]}
  lc.input_tensor.layernorm_2969.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xac989c0]]}
  dc.input_tensor.layernorm_2969.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x64723a0], [3, 0xacf7940]]}
  lc.input_tensor.layernorm_2969.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x60e19c0]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x6d13e80]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xac98180]]}
  constant_1_multiply_2987:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa99ee40]]}
  lc.input_tensor.softmax_2989.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c78860]]}
  lc.input_tensor.layernorm_3009.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb5b44a0]]}
  lc.input_tensor.layernorm_3009.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7335c60]]}
  dc.input_tensor.layernorm_3009.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x704e3e0], [1, 0x74267e0]]}
  lc.input_tensor.layernorm_3009.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f60900]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb636d20]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x7335420]]}
  lc.input_tensor.layernorm_3023.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x6eac3a0]]}
  lc.input_tensor.layernorm_3023.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xb3d6d40]]}
  dc.input_tensor.layernorm_3023.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x69d4120], [3, 0xb25bf60]]}
  lc.input_tensor.layernorm_3023.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x738d960]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x7511120]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x704dba0]]}
  constant_1_multiply_3041:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6f0ab60]]}
  lc.input_tensor.softmax_3043.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb5b3c60]]}
  lc.input_tensor.layernorm_3063.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaefe300]]}
  lc.input_tensor.layernorm_3063.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x71e7900]]}
  dc.input_tensor.layernorm_3063.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xb258e80], [4, 0xb3d3c60]]}
  lc.input_tensor.layernorm_3063.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x69d38e0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x738d120]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6ca2ae0]]}
  lc.input_tensor.layernorm_3077.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x73426a0]]}
  lc.input_tensor.layernorm_3077.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb4e8180]]}
  dc.input_tensor.layernorm_3077.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x6228a40], [0, 0x6323860]]}
  lc.input_tensor.layernorm_3077.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa911300]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9fecfe0]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x658f9c0]]}
  constant_1_multiply_3095:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6337480]]}
  lc.input_tensor.softmax_3097.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa80ca40]]}
  lc.input_tensor.layernorm_3117.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xaec84a0]]}
  lc.input_tensor.layernorm_3117.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x6642f00]]}
  dc.input_tensor.layernorm_3117.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x71aa980], [1, 0x7036da0]]}
  lc.input_tensor.layernorm_3117.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x6a50fa0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x68730c0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x682cf00]]}
  lc.input_tensor.layernorm_3131.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6337cc0]]}
  lc.input_tensor.layernorm_3131.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x60bb740]]}
  dc.input_tensor.layernorm_3131.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa63c720], [5, 0x5f44280]]}
  lc.input_tensor.layernorm_3131.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9dd8360]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6222840]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6c32ee0]]}
  constant_1_multiply_3149:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x66a6640]]}
  lc.input_tensor.softmax_3151.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x625ff00]]}
  lc.input_tensor.layernorm_3171.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa65dd80]]}
  lc.input_tensor.layernorm_3171.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa355580]]}
  dc.input_tensor.layernorm_3171.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x66e3540], [2, 0x5cf7060]]}
  lc.input_tensor.layernorm_3171.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x661bb80]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x61d5440]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa354d40]]}
  lc.input_tensor.layernorm_3185.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x65c1640]]}
  lc.input_tensor.layernorm_3185.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x64c90c0]]}
  dc.input_tensor.layernorm_3185.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6590200], [2, 0x67539e0]]}
  lc.input_tensor.layernorm_3185.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x69e13a0]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x682d7a0]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x66e2d00]]}
  constant_1_multiply_3203:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0xa7078a0]]}
  lc.input_tensor.softmax_3205.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x605f140]]}
  lc.input_tensor.layernorm_3225.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x69ce6e0]]}
  lc.input_tensor.layernorm_3225.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5f4f560]]}
  dc.input_tensor.layernorm_3225.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xac1dc00], [5, 0x64c5fe0]]}
  lc.input_tensor.layernorm_3225.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa2f98e0]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x69e0b60]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x65b09e0]]}
  lc.input_tensor.layernorm_3239.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x644c140]]}
  lc.input_tensor.layernorm_3239.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa973160]]}
  dc.input_tensor.layernorm_3239.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5fd1de0], [3, 0xa67a540]]}
  lc.input_tensor.layernorm_3239.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x69cdea0]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x6872880]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0xa972920]]}
  constant_1_multiply_3257:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4421580]]}
  lc.input_tensor.softmax_3259.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x426b8e0]]}
  lc.input_tensor.layernorm_3279.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x455fc00]]}
  lc.input_tensor.layernorm_3279.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x46150c0]]}
  dc.input_tensor.layernorm_3279.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46fea20], [0, 0x43939e0]]}
  lc.input_tensor.layernorm_3279.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x86c1da0]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x857ddc0]]}
  lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4614880]]}
  lc.input_tensor.layernorm_3293.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x827f020]]}
  lc.input_tensor.layernorm_3293.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3ef96c0]]}
  dc.input_tensor.layernorm_3293.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3ce3a40], [2, 0x3ca6b40]]}
  lc.input_tensor.layernorm_3293.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x40ac1a0]]}
  lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x82ac300]]}
  lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3ef6de0]]}
  constant_1_multiply_3311:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x420a040]]}
  lc.input_tensor.softmax_3313.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4575080]]}
  lc.input_tensor.layernorm_3333.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x83f2340]]}
  lc.input_tensor.layernorm_3333.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x82ae360]]}
  dc.input_tensor.layernorm_3333.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46fb940], [0, 0x4390900]]}
  lc.input_tensor.layernorm_3333.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x47fdac0]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4629680]]}
  lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x889d460]]}
  lc.input_tensor.layernorm_3347.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x87f5ae0]]}
  lc.input_tensor.layernorm_3347.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x44287e0]]}
  dc.input_tensor.layernorm_3347.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x44c25c0], [1, 0x477f340]]}
  lc.input_tensor.layernorm_3347.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47aadc0]]}
  lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8ae9dc0]]}
  lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x47381e0]]}
  constant_1_multiply_3365:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x46b1840]]}
  lc.input_tensor.softmax_3367.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4766d00]]}
  lc.input_tensor.layernorm_3387.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4420d40]]}
  lc.input_tensor.layernorm_3387.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x478bd80]]}
  dc.input_tensor.layernorm_3387.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x477c260], [2, 0x4425700]]}
  lc.input_tensor.layernorm_3387.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x44c1d80]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47aa580]]}
  lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x426c120]]}
  lc.input_tensor.layernorm_3401.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8789760]]}
  lc.input_tensor.layernorm_3401.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8645780]]}
  dc.input_tensor.layernorm_3401.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x86d0240], [4, 0x8814220]]}
  lc.input_tensor.layernorm_3401.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da100]]}
  lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7cc8d80]]}
  lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3b32640]]}
  constant_1_multiply_3419:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3aaa420]]}
  lc.input_tensor.softmax_3421.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c84420]]}
  lc.input_tensor.layernorm_3441.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8748fc0]]}
  lc.input_tensor.layernorm_3441.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8450bc0]]}
  dc.input_tensor.layernorm_3441.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4421dc0], [2, 0x40cb260]]}
  lc.input_tensor.layernorm_3441.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7dcc5c0]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3b40180]]}
  lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b28bc0]]}
  lc.input_tensor.layernorm_3455.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da940]]}
  lc.input_tensor.layernorm_3455.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38ea520]]}
  dc.input_tensor.layernorm_3455.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100], [5, 0x38da100]]}
  lc.input_tensor.layernorm_3455.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100]]}
  lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4098460]]}
  constant_1_multiply_3473:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x39a2300]]}
  lc.input_tensor.softmax_3475.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3973760]]}
  lc.input_tensor.layernorm_3495.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x38ee640]]}
  lc.input_tensor.layernorm_3495.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38dea60]]}
  dc.input_tensor.layernorm_3495.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ab4940], [5, 0x38ea520]]}
  lc.input_tensor.layernorm_3495.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ab8220]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x38de220]]}
  lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38de220]]}
  lc.input_tensor.layernorm_3509.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3def3e0]]}
  lc.input_tensor.layernorm_3509.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x415a420]]}
  dc.input_tensor.layernorm_3509.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f57fe0], [4, 0x809bfc0]]}
  lc.input_tensor.layernorm_3509.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f0f2e0]]}
  lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e51400]]}
  lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ab4100]]}
  constant_1_multiply_3527:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3c07ba0]]}
  lc.input_tensor.softmax_3529.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3c197e0]]}
  lc.input_tensor.layernorm_3549.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d4ad80]]}
  lc.input_tensor.layernorm_3549.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d8ce20]]}
  dc.input_tensor.layernorm_3549.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8098ee0], [5, 0x4157340]]}
  lc.input_tensor.layernorm_3549.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f577a0]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4097c20]]}
  lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3dde780]]}
  lc.input_tensor.layernorm_3563.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3b91e00]]}
  lc.input_tensor.layernorm_3563.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3bced00]]}
  dc.input_tensor.layernorm_3563.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b8c480], [0, 0x3b7a840]]}
  lc.input_tensor.layernorm_3563.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7dcbd80]]}
  lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d8c5e0]]}
  lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3bce4c0]]}
  constant_1_multiply_3581:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x90b6720]]}
  lc.input_tensor.softmax_3583.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4d51de0]]}
  lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x545d380]]}
  lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9642e80]]}
  dc.input_tensor.layernorm_3603.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x54e14e0], [3, 0x91a9380]]}
  lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5346f00]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5412100]]}
  lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9642640]]}
  lc.input_tensor.layernorm_3617.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4e7b720]]}
  lc.input_tensor.layernorm_3617.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4b6bd20]]}
  dc.input_tensor.layernorm_3617.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4d74600], [0, 0x4d23220]]}
  lc.input_tensor.layernorm_3617.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x90c4220]]}
  lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8d62c40]]}
  lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4cfe0e0]]}
  constant_1_multiply_3635:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x52c4680]]}
  lc.input_tensor.softmax_3637.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x538f880]]}
  lc.input_tensor.layernorm_3657.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5303560]]}
  lc.input_tensor.layernorm_3657.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x94f8c40]]}
  dc.input_tensor.layernorm_3657.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x540e800], [3, 0x90c6ac0]]}
  lc.input_tensor.layernorm_3657.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x54e0ca0]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9384220]]}
  lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x54a6740]]}
  lc.input_tensor.layernorm_3671.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x951e6a0]]}
  lc.input_tensor.layernorm_3671.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x506a2a0]]}
  dc.input_tensor.layernorm_3671.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x56d7980], [1, 0x56055a0]]}
  lc.input_tensor.layernorm_3671.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x5460520]]}
  lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x9842540]]}
  lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x56707e0]]}
  constant_1_multiply_3689:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x5457a80]]}
  lc.input_tensor.softmax_3691.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5419be0]]}
  lc.input_tensor.layernorm_3711.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x54582c0]]}
  lc.input_tensor.layernorm_3711.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x54dfc00]]}
  dc.input_tensor.layernorm_3711.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x56024c0], [2, 0x50671c0]]}
  lc.input_tensor.layernorm_3711.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x56d7140]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x545fce0]]}
  lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x5025960]]}
  lc.input_tensor.layernorm_3725.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x96c5700]]}
  lc.input_tensor.layernorm_3725.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x91f15a0]]}
  dc.input_tensor.layernorm_3725.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x538c880], [2, 0x5529700]]}
  lc.input_tensor.layernorm_3725.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8e14fa0]]}
  lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4b1a040]]}
  lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8af61e0]]}
  constant_1_multiply_3743:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4949d60]]}
  lc.input_tensor.softmax_3745.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4947ce0]]}
  lc.input_tensor.layernorm_3765.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x5305740]]}
  lc.input_tensor.layernorm_3765.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x508e2e0]]}
  dc.input_tensor.layernorm_3765.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8d988a0], [5, 0x4a58860]]}
  lc.input_tensor.layernorm_3765.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x472dfc0]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4a44b20]]}
  lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8a076a0]]}
  lc.input_tensor.layernorm_3779.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x49f5f20]]}
  lc.input_tensor.layernorm_3779.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88df500]]}
  dc.input_tensor.layernorm_3779.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x47baa60], [3, 0x87eb8e0]]}
  lc.input_tensor.layernorm_3779.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x483f320]]}
  lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x466aee0]]}
  lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x88decc0]]}
  constant_1_multiply_3797:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8880de0]]}
  lc.input_tensor.softmax_3799.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x44fcce0]]}
  lc.input_tensor.layernorm_3819.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x447a460]]}
  lc.input_tensor.layernorm_3819.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x47c3c80]]}
  dc.input_tensor.layernorm_3819.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x482d640], [0, 0x4516ae0]]}
  lc.input_tensor.layernorm_3819.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8b6c640]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8837340]]}
  lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x47c3440]]}
  lc.input_tensor.layernorm_3833.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c320a0]]}
  lc.input_tensor.layernorm_3833.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4f79de0]]}
  dc.input_tensor.layernorm_3833.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4e29220], [1, 0x4d5e020]]}
  lc.input_tensor.layernorm_3833.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4f2b9c0]]}
  lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4a4c5a0]]}
  lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4846500]]}
  constant_1_multiply_3851:                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8a931e0]]}
  lc.input_tensor.softmax_3853.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x471ecc0]]}
  lc.input_tensor.layernorm_3873.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8a0c840]]}
  lc.input_tensor.layernorm_3873.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4698320]]}
  dc.input_tensor.layernorm_3873.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4d5af40], [2, 0x4f76d00]]}
  lc.input_tensor.layernorm_3873.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4e289e0]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4f2b180]]}
  lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8c21440]]}
  lc.input_tensor.layernorm_3887.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4a26420]]}
  lc.input_tensor.layernorm_3887.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x472f040]]}
  dc.input_tensor.layernorm_3887.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [6, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8d957c0], [5, 0x4a55780]]}
  lc.input_tensor.layernorm_3887.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8a0c000]]}
  lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4697ae0]]}
  lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x472e800]]}

  # epoch_to_epoch
  e2e_buffer_0_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8_0:     {input: buffer_0_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x303cf040]]}
  e2e_layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                      {input: layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30492060], [0, 0x30498200]]}
  e2e_layernorm_2699.dc.subtract.1_0:                                           {input: layernorm_2699.dc.subtract.1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3049e3a0], [0, 0x305613c0]]}
  e2e_gelu_2745_0:                                                              {input: gelu_2745, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x303cf040], [0, 0x307aa420], [0, 0x3086d440], [0, 0x30930460], [0, 0x309f3480], [0, 0x30ab64a0], [0, 0x30b794c0]]}
  e2e_layernorm_2739.dc.add.10_0:                                               {input: layernorm_2739.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x306243e0], [0, 0x306e7400]]}
  e2e_matmul_2796_0:                                                            {input: matmul_2796, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30c3c4e0], [0, 0x30c9dd00], [0, 0x30cff520], [0, 0x30d60d40], [0, 0x30dc2560], [0, 0x30e23d80], [0, 0x30e855a0], [0, 0x30ee6dc0], [0, 0x30f485e0], [0, 0x30fa9e00], [0, 0x3100b620], [0, 0x3106ce40], [0, 0x310ce660], [0, 0x3112fe80], [0, 0x311916a0], [0, 0x311f2ec0]]}
  e2e_layernorm_2793.dc.add.10_0:                                               {input: layernorm_2793.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30492060], [0, 0x30555080]]}
  e2e_layernorm_2847.dc.multiply.9_0:                                           {input: layernorm_2847.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x312546e0], [0, 0x31317700]]}
  e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:             {input: layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020]]}
  e2e_layernorm_2901.dc.reduce_avg.3.lc1_0:                                     {input: layernorm_2901.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3032c840], [0, 0x303329e0]]}
  e2e_buffer_0_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8_0:     {input: buffer_0_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30338b80], [0, 0x303fbba0]]}
  e2e_softmax_2935.dc.multiply.3_0:                                             {input: softmax_2935.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30644c00], [0, 0x30ad6c20]]}
  e2e_matmul_2939_0:                                                            {input: matmul_2939, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30f68c40], [0, 0x30f99860], [0, 0x30fca480], [0, 0x30ffb0a0], [0, 0x3102bcc0], [0, 0x3105c8e0], [0, 0x3108d500], [0, 0x310be120]]}
  e2e_layernorm_2915.dc.add.10_0:                                               {input: layernorm_2915.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x304bebc0], [0, 0x30581be0]]}
  e2e_layernorm_2969.dc.add.10_0:                                               {input: layernorm_2969.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x310eed40], [0, 0x311b1d60]]}
  e2e_matmul_2972_0:                                                            {input: matmul_2972, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x3033cc40], [0, 0x3036d860], [0, 0x3039e480], [0, 0x303cf0a0], [0, 0x303ffcc0], [0, 0x304308e0], [0, 0x30461500]]}
  e2e_layernorm_3023.dc.multiply.8_0:                                           {input: layernorm_3023.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x31274d80], [0, 0x31337da0]]}
  e2e_layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0:                     {input: layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30492120]]}
  e2e_matmul_3072_0:                                                            {input: matmul_3072, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x304b2940], [0, 0x304caf60], [0, 0x304e3580], [0, 0x304fbba0], [0, 0x305141c0], [0, 0x3052c7e0], [0, 0x30544e00], [0, 0x3055d420], [0, 0x30575a40], [0, 0x3058e060], [0, 0x305a6680], [0, 0x305beca0], [0, 0x305d72c0], [0, 0x305ef8e0], [0, 0x30607f00], [0, 0x30620520]]}
  e2e_buffer_0_layernorm_3063.dc.add.10_add_3076_0:                             {input: buffer_0_layernorm_3063.dc.add.10_add_3076, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x303cf040]]}
  e2e_matmul_3120_0:                                                            {input: matmul_3120, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x307beb80], [0, 0x308203a0], [0, 0x30881bc0], [0, 0x308e33e0], [0, 0x30944c00], [0, 0x309a6420], [0, 0x30a07c40], [0, 0x30a69460], [0, 0x30acac80], [0, 0x30b2c4a0], [0, 0x30b8dcc0], [0, 0x30bef4e0], [0, 0x30c50d00], [0, 0x30cb2520], [0, 0x30d13d40], [0, 0x30d75560]]}
  e2e_layernorm_3117.dc.add.10_0:                                               {input: layernorm_3117.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30638b40], [0, 0x306fbb60]]}
  e2e_layernorm_3171.dc.multiply.9_0:                                           {input: layernorm_3171.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30dd6d80], [0, 0x30e99da0]]}
  e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020]]}
  e2e_layernorm_3225.dc.reduce_avg.3.lc1_0:                                     {input: layernorm_3225.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3032c840], [0, 0x303329e0]]}
  e2e_buffer_0_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8_0:     {input: buffer_0_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30338b80], [0, 0x303fbba0]]}
  e2e_softmax_3259.dc.multiply.3_0:                                             {input: softmax_3259.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30644c00], [0, 0x30ad6c20]]}
  e2e_matmul_3263_0:                                                            {input: matmul_3263, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30f68c40], [0, 0x30f99860], [0, 0x30fca480], [0, 0x30ffb0a0], [0, 0x3102bcc0], [0, 0x3105c8e0], [0, 0x3108d500], [0, 0x310be120]]}
  e2e_layernorm_3239.dc.add.10_0:                                               {input: layernorm_3239.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x304bebc0], [0, 0x30581be0]]}
  e2e_layernorm_3293.dc.add.10_0:                                               {input: layernorm_3293.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x310eed40], [0, 0x311b1d60]]}
  e2e_matmul_3296_0:                                                            {input: matmul_3296, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x3033cc40], [0, 0x3036d860], [0, 0x3039e480], [0, 0x303cf0a0], [0, 0x303ffcc0], [0, 0x304308e0], [0, 0x30461500]]}
  e2e_layernorm_3347.dc.multiply.8_0:                                           {input: layernorm_3347.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x31274d80], [0, 0x31337da0]]}
  e2e_layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0:                    {input: layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30492120]]}
  e2e_matmul_3396_0:                                                            {input: matmul_3396, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x304b2940], [0, 0x304caf60], [0, 0x304e3580], [0, 0x304fbba0], [0, 0x305141c0], [0, 0x3052c7e0], [0, 0x30544e00], [0, 0x3055d420], [0, 0x30575a40], [0, 0x3058e060], [0, 0x305a6680], [0, 0x305beca0], [0, 0x305d72c0], [0, 0x305ef8e0], [0, 0x30607f00], [0, 0x30620520]]}
  e2e_buffer_0_layernorm_3387.dc.add.10_add_3400_0:                             {input: buffer_0_layernorm_3387.dc.add.10_add_3400, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x303cf040]]}
  e2e_matmul_3444_0:                                                            {input: matmul_3444, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x307beb80], [0, 0x308203a0], [0, 0x30881bc0], [0, 0x308e33e0], [0, 0x30944c00], [0, 0x309a6420], [0, 0x30a07c40], [0, 0x30a69460], [0, 0x30acac80], [0, 0x30b2c4a0], [0, 0x30b8dcc0], [0, 0x30bef4e0], [0, 0x30c50d00], [0, 0x30cb2520], [0, 0x30d13d40], [0, 0x30d75560]]}
  e2e_layernorm_3441.dc.add.10_0:                                               {input: layernorm_3441.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30638b40], [0, 0x306fbb60]]}
  e2e_layernorm_3495.dc.multiply.9_0:                                           {input: layernorm_3495.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30dd6d80], [0, 0x30e99da0]]}
  e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020]]}
  e2e_layernorm_3549.dc.reduce_avg.3.lc1_0:                                     {input: layernorm_3549.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3032c840], [0, 0x303329e0]]}
  e2e_buffer_0_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8_0:     {input: buffer_0_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30338b80], [0, 0x303fbba0]]}
  e2e_softmax_3583.dc.multiply.3_0:                                             {input: softmax_3583.dc.multiply.3, type: queue, entries: 2, grid_size: [2, 1], t: 16, mblock: [3, 3], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30644c00], [0, 0x30ad6c20]]}
  e2e_matmul_3587_0:                                                            {input: matmul_3587, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30f68c40], [0, 0x30f99860], [0, 0x30fca480], [0, 0x30ffb0a0], [0, 0x3102bcc0], [0, 0x3105c8e0], [0, 0x3108d500], [0, 0x310be120]]}
  e2e_layernorm_3563.dc.add.10_0:                                               {input: layernorm_3563.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x304bebc0], [0, 0x30581be0]]}
  e2e_layernorm_3617.dc.add.10_0:                                               {input: layernorm_3617.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x310eed40], [0, 0x311b1d60]]}
  e2e_matmul_3620_0:                                                            {input: matmul_3620, type: queue, entries: 2, grid_size: [2, 4], t: 1, mblock: [3, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x3033cc40], [0, 0x3036d860], [0, 0x3039e480], [0, 0x303cf0a0], [0, 0x303ffcc0], [0, 0x304308e0], [0, 0x30461500]]}
  e2e_layernorm_3671.dc.multiply.8_0:                                           {input: layernorm_3671.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x31274d80], [0, 0x31337da0]]}
  e2e_layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0:                    {input: layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30492120]]}
  e2e_matmul_3720_0:                                                            {input: matmul_3720, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 1], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x304b2940], [0, 0x304caf60], [0, 0x304e3580], [0, 0x304fbba0], [0, 0x305141c0], [0, 0x3052c7e0], [0, 0x30544e00], [0, 0x3055d420], [0, 0x30575a40], [0, 0x3058e060], [0, 0x305a6680], [0, 0x305beca0], [0, 0x305d72c0], [0, 0x305ef8e0], [0, 0x30607f00], [0, 0x30620520]]}
  e2e_buffer_0_layernorm_3711.dc.add.10_add_3724_0:                             {input: buffer_0_layernorm_3711.dc.add.10_add_3724, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020], [0, 0x303cf040]]}
  e2e_matmul_3768_0:                                                            {input: matmul_3768, type: queue, entries: 2, grid_size: [2, 8], t: 1, mblock: [3, 4], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x307beb80], [0, 0x308203a0], [0, 0x30881bc0], [0, 0x308e33e0], [0, 0x30944c00], [0, 0x309a6420], [0, 0x30a07c40], [0, 0x30a69460], [0, 0x30acac80], [0, 0x30b2c4a0], [0, 0x30b8dcc0], [0, 0x30bef4e0], [0, 0x30c50d00], [0, 0x30cb2520], [0, 0x30d13d40], [0, 0x30d75560]]}
  e2e_layernorm_3765.dc.add.10_0:                                               {input: layernorm_3765.dc.add.10, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30638b40], [0, 0x306fbb60]]}
  e2e_layernorm_3819.dc.multiply.9_0:                                           {input: layernorm_3819.dc.multiply.9, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30dd6d80], [0, 0x30e99da0]]}
  e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0:            {input: layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3030c020]]}
  e2e_layernorm_3873.dc.reduce_avg.3.lc1_0:                                     {input: layernorm_3873.dc.reduce_avg.3.lc1, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3032c840], [0, 0x303329e0]]}
  e2e_buffer_0_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8_0:     {input: buffer_0_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8, type: queue, entries: 2, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30338b80], [0, 0x303fbba0]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 0
    input_count: 2
    matmul_2594: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2600: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2606: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_2594, matmul_2600],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2609: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_2606, constant_1_multiply_2609],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2610: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [multiply_2609, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2611.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2610],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2611.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2611.dc.exp.0, lc.input_tensor.softmax_2611.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2611.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2611.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_2611.dc.exp.0_softmax_2611.dc.multiply.3: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_2611.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_2611.dc.multiply.3: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_2611.dc.exp.0_softmax_2611.dc.multiply.3, softmax_2611.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2615: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2622: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [softmax_2611.dc.multiply.3, matmul_2615],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_2626: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [matmul_2622, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    add_2630: {type: add, grid_loc: [4, 6], grid_size: [2, 1], inputs: [matmul_2626, hidden_states],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2631.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [add_2630, lc.input_tensor.layernorm_2631.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2630_layernorm_2631.dc.subtract.1: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [add_2630],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2631.dc.subtract.1: {type: subtract, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_add_2630_layernorm_2631.dc.subtract.1, layernorm_2631.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2631.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2631.dc.subtract.1, layernorm_2631.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2631.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_2631.dc.multiply.2, lc.input_tensor.layernorm_2631.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2631.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2631.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2631.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2631.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2631.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2631.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2631.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2631.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2631.dc.reciprocal.7, lc.input_tensor.layernorm_2631.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2631.dc.subtract.1_layernorm_2631.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_2631.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2631.dc.subtract.1_layernorm_2631.dc.multiply.8: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_2631.dc.subtract.1_layernorm_2631.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2631.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_2631.dc.subtract.1_layernorm_2631.dc.multiply.8, layernorm_2631.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_1_temporal_epoch_0:
    target_device: 1
    input_count: 2
    layernorm_2631.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_2631.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2631.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2631.dc.multiply.9, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2634: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_2631.dc.add.10, layer.0.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_2636: {type: add, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_2634, layer.0.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_2637: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_2636],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2640: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2637, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_2631.dc.add.10_add_2644: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2631.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2644: {type: add, grid_loc: [4, 4], grid_size: [2, 1], inputs: [matmul_2640, buffer_0_layernorm_2631.dc.add.10_add_2644],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2645.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [2, 1], inputs: [add_2644, lc.input_tensor.layernorm_2645.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2644_layernorm_2645.dc.subtract.1: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [add_2644],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2645.dc.subtract.1: {type: subtract, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_add_2644_layernorm_2645.dc.subtract.1, layernorm_2645.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2645.dc.multiply.2: {type: multiply, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2645.dc.subtract.1, layernorm_2645.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2645.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2645.dc.multiply.2, lc.input_tensor.layernorm_2645.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2645.dc.add.5: {type: add, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2645.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2645.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2645.dc.sqrt.6: {type: sqrt, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2645.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2645.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2645.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2645.dc.reciprocal.7, lc.input_tensor.layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2645.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2_temporal_epoch_1:
    target_device: 0
    input_count: 2
    layernorm_2645.dc.multiply.8: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8_0, e2e_layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2645.dc.multiply.9: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2645.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2645.dc.add.10: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2645.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2648: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2645.dc.add.10, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2654: {type: matmul, grid_loc: [2, 4], grid_size: [2, 4], inputs: [layernorm_2645.dc.add.10, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2660: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2648, matmul_2654],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2663: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2660, constant_1_multiply_2663],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2664: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [multiply_2663, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2665.dc.exp.0: {type: exp, grid_loc: [4, 0], grid_size: [2, 2], inputs: [add_2664],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2665.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [softmax_2665.dc.exp.0, lc.input_tensor.softmax_2665.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2665.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 4], grid_size: [2, 1], inputs: [softmax_2665.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_2665.dc.exp.0_softmax_2665.dc.multiply.3: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2665.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_2665.dc.multiply.3: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_softmax_2665.dc.exp.0_softmax_2665.dc.multiply.3, softmax_2665.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2669: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [layernorm_2645.dc.add.10, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_2669_matmul_2676: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [matmul_2669],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_2669_matmul_2676: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_matmul_2669_matmul_2676],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_2676: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_2665.dc.multiply.3, buffer_0_matmul_2669_matmul_2676],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_2680: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2676, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_2645.dc.add.10_add_2684: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_2645.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2684: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_2680, buffer_0_layernorm_2645.dc.add.10_add_2684],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2685.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_2684, lc.input_tensor.layernorm_2685.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2684_layernorm_2685.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_2684],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2685.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_2684_layernorm_2685.dc.subtract.1, layernorm_2685.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_layernorm_2685.dc.subtract.1_layernorm_2685.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2685.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2685.dc.subtract.1_layernorm_2685.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_2685.dc.subtract.1_layernorm_2685.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_3_temporal_epoch_1:
    target_device: 1
    input_count: 2
    layernorm_2685.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_2685.dc.subtract.1, layernorm_2685.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2685.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_2685.dc.multiply.2, lc.input_tensor.layernorm_2685.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2685.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2685.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2685.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2685.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2685.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2685.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2685.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2685.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2685.dc.reciprocal.7, lc.input_tensor.layernorm_2685.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2685.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_2685.dc.subtract.1_layernorm_2685.dc.multiply.8, layernorm_2685.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2685.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_2685.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2685.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2685.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2688: {type: matmul, grid_loc: [4, 0], grid_size: [2, 8], inputs: [layernorm_2685.dc.add.10, layer.1.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_2690: {type: add, grid_loc: [2, 2], grid_size: [2, 4], inputs: [matmul_2688, layer.1.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_2691: {type: gelu, grid_loc: [6, 0], grid_size: [2, 4], inputs: [add_2690],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2694: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [gelu_2691, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_2685.dc.add.10_add_2698: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2685.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2698: {type: add, grid_loc: [6, 4], grid_size: [2, 1], inputs: [matmul_2694, buffer_0_layernorm_2685.dc.add.10_add_2698],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2699.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [add_2698, lc.input_tensor.layernorm_2699.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2698_layernorm_2699.dc.subtract.1: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [add_2698],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2699.dc.subtract.1: {type: subtract, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_add_2698_layernorm_2699.dc.subtract.1, layernorm_2699.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_4_temporal_epoch_2:
    target_device: 0
    input_count: 2
    layernorm_2699.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_layernorm_2699.dc.subtract.1_0, e2e_layernorm_2699.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2699.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2699.dc.multiply.2, lc.input_tensor.layernorm_2699.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2699.dc.add.5: {type: add, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2699.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2699.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2699.dc.sqrt.6: {type: sqrt, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2699.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2699.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2699.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2699.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2699.dc.reciprocal.7, lc.input_tensor.layernorm_2699.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2699.dc.subtract.1_layernorm_2699.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2699.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2699.dc.subtract.1_layernorm_2699.dc.multiply.8: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_layernorm_2699.dc.subtract.1_layernorm_2699.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2699.dc.multiply.8: {type: multiply, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_layernorm_2699.dc.subtract.1_layernorm_2699.dc.multiply.8, layernorm_2699.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2699.dc.multiply.9: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2699.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2699.dc.add.10: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2699.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2702: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_2699.dc.add.10, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2708: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_2699.dc.add.10, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2714: {type: matmul, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_2702, matmul_2708],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2717: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_2714, constant_1_multiply_2717],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2718: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [multiply_2717, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2719.dc.exp.0: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [add_2718],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2719.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [softmax_2719.dc.exp.0, lc.input_tensor.softmax_2719.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2719.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 4], grid_size: [2, 1], inputs: [softmax_2719.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_2719.dc.exp.0_softmax_2719.dc.multiply.3: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_2719.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_2719.dc.multiply.3: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_softmax_2719.dc.exp.0_softmax_2719.dc.multiply.3, softmax_2719.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2723: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2699.dc.add.10, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_2723_matmul_2730: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_2723],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_2723_matmul_2730: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_1_matmul_2723_matmul_2730],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_2730: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [softmax_2719.dc.multiply.3, buffer_0_matmul_2723_matmul_2730],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}

  fwd_5_temporal_epoch_2:
    target_device: 1
    input_count: 2
    matmul_2734: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [matmul_2730, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_2699.dc.add.10_add_2738: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2699.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2738: {type: add, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2734, buffer_0_layernorm_2699.dc.add.10_add_2738],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2739.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_2738, lc.input_tensor.layernorm_2739.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2738_layernorm_2739.dc.subtract.1: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [add_2738],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2739.dc.subtract.1: {type: subtract, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_add_2738_layernorm_2739.dc.subtract.1, layernorm_2739.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2739.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_2739.dc.subtract.1, layernorm_2739.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2739.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2739.dc.multiply.2, lc.input_tensor.layernorm_2739.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2739.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_2739.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2739.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2739.dc.sqrt.6: {type: sqrt, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2739.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2739.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2739.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2739.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2739.dc.reciprocal.7, lc.input_tensor.layernorm_2739.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2739.dc.subtract.1_layernorm_2739.dc.multiply.8: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2739.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2739.dc.subtract.1_layernorm_2739.dc.multiply.8: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [buffer_1_layernorm_2739.dc.subtract.1_layernorm_2739.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2739.dc.multiply.8: {type: multiply, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_0_layernorm_2739.dc.subtract.1_layernorm_2739.dc.multiply.8, layernorm_2739.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2739.dc.multiply.9: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2739.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2739.dc.add.10: {type: add, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_2739.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2742: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [layernorm_2739.dc.add.10, layer.2.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_2744: {type: add, grid_loc: [8, 0], grid_size: [2, 4], inputs: [matmul_2742, layer.2.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_2745: {type: gelu, grid_loc: [8, 4], grid_size: [2, 4], inputs: [add_2744],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_6_temporal_epoch_3:
    target_device: 0
    input_count: 2
    matmul_2748: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_gelu_2745_0, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_2739.dc.add.10_add_2752: {type: nop, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2739.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2752: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [matmul_2748, buffer_0_layernorm_2739.dc.add.10_add_2752],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2753.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [add_2752, lc.input_tensor.layernorm_2753.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2752_layernorm_2753.dc.subtract.1: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2752],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2753.dc.subtract.1: {type: subtract, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_add_2752_layernorm_2753.dc.subtract.1, layernorm_2753.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2753.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2753.dc.subtract.1, layernorm_2753.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2753.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2753.dc.multiply.2, lc.input_tensor.layernorm_2753.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2753.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2753.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2753.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2753.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2753.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2753.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2753.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2753.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2753.dc.reciprocal.7, lc.input_tensor.layernorm_2753.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2753.dc.subtract.1_layernorm_2753.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_2753.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2753.dc.subtract.1_layernorm_2753.dc.multiply.8: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2753.dc.subtract.1_layernorm_2753.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2753.dc.multiply.8: {type: multiply, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2753.dc.subtract.1_layernorm_2753.dc.multiply.8, layernorm_2753.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2753.dc.multiply.9: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2753.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2753.dc.add.10: {type: add, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2753.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2756: {type: matmul, grid_loc: [6, 1], grid_size: [2, 4], inputs: [layernorm_2753.dc.add.10, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2762: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2753.dc.add.10, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2768: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [matmul_2756, matmul_2762],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2771: {type: multiply, grid_loc: [6, 6], grid_size: [2, 1], inputs: [matmul_2768, constant_1_multiply_2771],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2772: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [multiply_2771, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2773.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [2, 2], inputs: [add_2772],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2773.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [softmax_2773.dc.exp.0, lc.input_tensor.softmax_2773.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    buffer_0_softmax_2773.dc.exp.0_softmax_2773.dc.multiply.3: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [softmax_2773.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_7_temporal_epoch_3:
    target_device: 1
    input_count: 2
    softmax_2773.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 0], grid_size: [2, 1], inputs: [softmax_2773.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2773.dc.multiply.3: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_0_softmax_2773.dc.exp.0_softmax_2773.dc.multiply.3, softmax_2773.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2777: {type: matmul, grid_loc: [0, 2], grid_size: [2, 4], inputs: [layernorm_2753.dc.add.10, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_2777_matmul_2784: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2777],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_2777_matmul_2784: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1_matmul_2777_matmul_2784],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_2784: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [softmax_2773.dc.multiply.3, buffer_0_matmul_2777_matmul_2784],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_2788: {type: matmul, grid_loc: [2, 1], grid_size: [2, 4], inputs: [matmul_2784, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_2753.dc.add.10_add_2792: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [layernorm_2753.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2792: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [matmul_2788, buffer_0_layernorm_2753.dc.add.10_add_2792],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2793.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [2, 1], inputs: [add_2792, lc.input_tensor.layernorm_2793.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2792_layernorm_2793.dc.subtract.1: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [add_2792],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2793.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [2, 1], inputs: [buffer_0_add_2792_layernorm_2793.dc.subtract.1, layernorm_2793.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2793.dc.multiply.2: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [layernorm_2793.dc.subtract.1, layernorm_2793.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2793.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_2793.dc.multiply.2, lc.input_tensor.layernorm_2793.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2793.dc.add.5: {type: add, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_2793.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2793.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2793.dc.sqrt.6: {type: sqrt, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2793.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2793.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2793.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2793.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_2793.dc.reciprocal.7, lc.input_tensor.layernorm_2793.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2793.dc.subtract.1_layernorm_2793.dc.multiply.8: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2793.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2793.dc.subtract.1_layernorm_2793.dc.multiply.8: {type: nop, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_1_layernorm_2793.dc.subtract.1_layernorm_2793.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2793.dc.multiply.8: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [buffer_0_layernorm_2793.dc.subtract.1_layernorm_2793.dc.multiply.8, layernorm_2793.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2793.dc.multiply.9: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_2793.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2793.dc.add.10: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [layernorm_2793.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2796: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [layernorm_2793.dc.add.10, layer.3.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}

  fwd_8_temporal_epoch_4:
    target_device: 0
    input_count: 2
    add_2798: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_2796_0, layer.3.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_2799: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_2798],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2802: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_2799, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_2793.dc.add.10_add_2806: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2793.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2806: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_2802, buffer_0_layernorm_2793.dc.add.10_add_2806],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2807.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_2806, lc.input_tensor.layernorm_2807.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2806_layernorm_2807.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_2806],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2807.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_add_2806_layernorm_2807.dc.subtract.1, layernorm_2807.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2807.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2807.dc.subtract.1, layernorm_2807.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2807.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2807.dc.multiply.2, lc.input_tensor.layernorm_2807.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2807.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_2807.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2807.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2807.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_2807.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2807.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_2807.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2807.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_2807.dc.reciprocal.7, lc.input_tensor.layernorm_2807.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2807.dc.subtract.1_layernorm_2807.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_2807.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2807.dc.subtract.1_layernorm_2807.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2807.dc.subtract.1_layernorm_2807.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2807.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2807.dc.subtract.1_layernorm_2807.dc.multiply.8, layernorm_2807.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2807.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_2807.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2807.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2807.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2810: {type: matmul, grid_loc: [8, 1], grid_size: [2, 4], inputs: [layernorm_2807.dc.add.10, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_9_temporal_epoch_4:
    target_device: 1
    input_count: 2
    matmul_2816: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [layernorm_2807.dc.add.10, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2822: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_2810, matmul_2816],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2825: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2822, constant_1_multiply_2825],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2826: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [multiply_2825, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2827.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 2], inputs: [add_2826],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2827.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_2827.dc.exp.0, lc.input_tensor.softmax_2827.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2827.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_2827.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_2827.dc.exp.0_softmax_2827.dc.multiply.3: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_2827.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_2827.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_2827.dc.exp.0_softmax_2827.dc.multiply.3, softmax_2827.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2831: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_2807.dc.add.10, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_2831_matmul_2838: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_2831],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_2831_matmul_2838: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_matmul_2831_matmul_2838],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_2838: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_2827.dc.multiply.3, buffer_0_matmul_2831_matmul_2838],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_2842: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_2838, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_2807.dc.add.10_add_2846: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2807.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2846: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [matmul_2842, buffer_0_layernorm_2807.dc.add.10_add_2846],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [add_2846, lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2846_layernorm_2847.dc.subtract.1: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_2846],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_add_2846_layernorm_2847.dc.subtract.1, layernorm_2847.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2847.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_2847.dc.subtract.1, layernorm_2847.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.2, lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2847.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_2847.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2847.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2847.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2847.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2847.dc.reciprocal.7, lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2847.dc.subtract.1_layernorm_2847.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_2847.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2847.dc.subtract.1_layernorm_2847.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2847.dc.subtract.1_layernorm_2847.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2847.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2847.dc.subtract.1_layernorm_2847.dc.multiply.8, layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2847.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2847.dc.multiply.8, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_10_temporal_epoch_5:
    target_device: 0
    input_count: 2
    layernorm_2847.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2847.dc.multiply.9_0, e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2850: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_2847.dc.add.10, layer.4.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_2852: {type: add, grid_loc: [0, 1], grid_size: [2, 4], inputs: [matmul_2850, layer.4.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_2853: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_2852],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2856: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_2853, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_2847.dc.add.10_add_2860: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_2847.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2860: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_2856, buffer_0_layernorm_2847.dc.add.10_add_2860],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_2860, lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2860_layernorm_2861.dc.subtract.1: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_2860],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_add_2860_layernorm_2861.dc.subtract.1, layernorm_2861.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2861.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2861.dc.subtract.1, layernorm_2861.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.2, lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2861.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_2861.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2861.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_2861.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2861.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_2861.dc.reciprocal.7, lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_2861.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2861.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_2861.dc.subtract.1_layernorm_2861.dc.multiply.8, layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_11_temporal_epoch_5:
    target_device: 1
    input_count: 2
    layernorm_2861.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2861.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2861.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2864: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2870: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2876: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_2864, matmul_2870],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2879: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_2876, constant_1_multiply_2879],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2880: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_2879, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2881.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_2880],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2881.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_2881.dc.exp.0, lc.input_tensor.softmax_2881.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2881.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_2881.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_2881.dc.exp.0_softmax_2881.dc.multiply.3: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_2881.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_2881.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_softmax_2881.dc.exp.0_softmax_2881.dc.multiply.3, softmax_2881.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2885: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_2861.dc.add.10, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_2885_matmul_2892: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_2885],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_2885_matmul_2892: {type: nop, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_2885_matmul_2892],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_2892: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_2881.dc.multiply.3, buffer_0_matmul_2885_matmul_2892],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_2896: {type: matmul, grid_loc: [6, 3], grid_size: [2, 4], inputs: [matmul_2892, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_2861.dc.add.10_add_2900: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_2861.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2900: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_2896, buffer_0_layernorm_2861.dc.add.10_add_2900],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2901.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_2900, lc.input_tensor.layernorm_2901.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2900_layernorm_2901.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_2900],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2901.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_0_add_2900_layernorm_2901.dc.subtract.1, layernorm_2901.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2901.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_2901.dc.subtract.1, layernorm_2901.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2901.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_2901.dc.multiply.2, lc.input_tensor.layernorm_2901.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_1_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_2901.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_12_temporal_epoch_6:
    target_device: 0
    input_count: 2
    layernorm_2901.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2901.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_2901.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2901.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_2901.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2901.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_2901.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2901.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_2901.dc.reciprocal.7, lc.input_tensor.layernorm_2901.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2901.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8_0, layernorm_2901.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2901.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2901.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2901.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_2901.dc.multiply.9, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2904: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [layernorm_2901.dc.add.10, layer.5.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_2906: {type: add, grid_loc: [5, 0], grid_size: [2, 4], inputs: [matmul_2904, layer.5.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_2907: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [add_2906],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2910: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [gelu_2907, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_2901.dc.add.10_add_2914: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_2901.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_13_temporal_epoch_6:
    target_device: 1
    input_count: 2
    add_2914: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [matmul_2910, buffer_0_layernorm_2901.dc.add.10_add_2914],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2915.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_2914, lc.input_tensor.layernorm_2915.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2914_layernorm_2915.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_2914],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2915.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_add_2914_layernorm_2915.dc.subtract.1, layernorm_2915.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2915.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_2915.dc.subtract.1, layernorm_2915.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2915.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_2915.dc.multiply.2, lc.input_tensor.layernorm_2915.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2915.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_2915.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2915.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2915.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_2915.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2915.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_2915.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2915.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_2915.dc.reciprocal.7, lc.input_tensor.layernorm_2915.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2915.dc.subtract.1_layernorm_2915.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_2915.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2915.dc.subtract.1_layernorm_2915.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2915.dc.subtract.1_layernorm_2915.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2915.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2915.dc.subtract.1_layernorm_2915.dc.multiply.8, layernorm_2915.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2915.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2915.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2915.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_2915.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2918: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_2915.dc.add.10, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2924: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_2915.dc.add.10, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2930: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_2918, matmul_2924],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2933: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_2930, constant_1_multiply_2933],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2934: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_2933, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2935.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_2934],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2935.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_2935.dc.exp.0, lc.input_tensor.softmax_2935.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2935.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_2935.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_2935.dc.exp.0_softmax_2935.dc.multiply.3: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_2935.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_2935.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [2, 1], inputs: [buffer_0_softmax_2935.dc.exp.0_softmax_2935.dc.multiply.3, softmax_2935.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2939: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_2915.dc.add.10, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_14_temporal_epoch_7:
    target_device: 0
    input_count: 2
    buffer_1_matmul_2939_matmul_2946: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_2939_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_2939_matmul_2946: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_2939_matmul_2946],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_2946: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_2935.dc.multiply.3_0, buffer_0_matmul_2939_matmul_2946],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_2950: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_2946, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_2915.dc.add.10_add_2954: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_2915.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2954: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_2950, buffer_0_layernorm_2915.dc.add.10_add_2954],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2955.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_2954, lc.input_tensor.layernorm_2955.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2954_layernorm_2955.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_2954],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2955.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_2954_layernorm_2955.dc.subtract.1, layernorm_2955.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2955.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_2955.dc.subtract.1, layernorm_2955.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2955.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_2955.dc.multiply.2, lc.input_tensor.layernorm_2955.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2955.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2955.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2955.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2955.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_2955.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2955.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_2955.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2955.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_2955.dc.reciprocal.7, lc.input_tensor.layernorm_2955.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2955.dc.subtract.1_layernorm_2955.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_2955.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2955.dc.subtract.1_layernorm_2955.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_2955.dc.subtract.1_layernorm_2955.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2955.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_2955.dc.subtract.1_layernorm_2955.dc.multiply.8, layernorm_2955.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2955.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_2955.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2955.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_2955.dc.multiply.9, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2958: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [layernorm_2955.dc.add.10, layer.6.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}

  fwd_15_temporal_epoch_7:
    target_device: 1
    input_count: 2
    add_2960: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [matmul_2958, layer.6.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_2961: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_2960],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_2964: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_2961, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_2955.dc.add.10_add_2968: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_2955.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_2968: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_2964, buffer_0_layernorm_2955.dc.add.10_add_2968],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2969.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_2968, lc.input_tensor.layernorm_2969.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_2968_layernorm_2969.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_2968],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2969.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_add_2968_layernorm_2969.dc.subtract.1, layernorm_2969.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_2969.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_2969.dc.subtract.1, layernorm_2969.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2969.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_2969.dc.multiply.2, lc.input_tensor.layernorm_2969.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_2969.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_2969.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_2969.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2969.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_2969.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2969.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_2969.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_2969.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_2969.dc.reciprocal.7, lc.input_tensor.layernorm_2969.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_2969.dc.subtract.1_layernorm_2969.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_2969.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_2969.dc.subtract.1_layernorm_2969.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_2969.dc.subtract.1_layernorm_2969.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_2969.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_2969.dc.subtract.1_layernorm_2969.dc.multiply.8, layernorm_2969.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2969.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_2969.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_2969.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_2969.dc.multiply.9, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_2972: {type: matmul, grid_loc: [8, 1], grid_size: [2, 4], inputs: [layernorm_2969.dc.add.10, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_16_temporal_epoch_8:
    target_device: 0
    input_count: 2
    matmul_2978: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_layernorm_2969.dc.add.10_0, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_2984: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_matmul_2972_0, matmul_2978],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_2987: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_2984, constant_1_multiply_2987],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_2988: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [multiply_2987, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_2989.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 2], inputs: [add_2988],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_2989.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_2989.dc.exp.0, lc.input_tensor.softmax_2989.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_2989.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_2989.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_2989.dc.exp.0_softmax_2989.dc.multiply.3: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_2989.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_2989.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_2989.dc.exp.0_softmax_2989.dc.multiply.3, softmax_2989.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_2993: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [e2e_layernorm_2969.dc.add.10_0, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_2993_matmul_3000: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_2993],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_2993_matmul_3000: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_matmul_2993_matmul_3000],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3000: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_2989.dc.multiply.3, buffer_0_matmul_2993_matmul_3000],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3004: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_3000, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_2969.dc.add.10_add_3008: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [e2e_layernorm_2969.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3008: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [matmul_3004, buffer_0_layernorm_2969.dc.add.10_add_3008],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3009.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [add_3008, lc.input_tensor.layernorm_3009.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3008_layernorm_3009.dc.subtract.1: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_3008],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3009.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_add_3008_layernorm_3009.dc.subtract.1, layernorm_3009.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3009.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3009.dc.subtract.1, layernorm_3009.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3009.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3009.dc.multiply.2, lc.input_tensor.layernorm_3009.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3009.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3009.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3009.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3009.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3009.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3009.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3009.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3009.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3009.dc.reciprocal.7, lc.input_tensor.layernorm_3009.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3009.dc.subtract.1_layernorm_3009.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_3009.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3009.dc.subtract.1_layernorm_3009.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3009.dc.subtract.1_layernorm_3009.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3009.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3009.dc.subtract.1_layernorm_3009.dc.multiply.8, layernorm_3009.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3009.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3009.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_17_temporal_epoch_8:
    target_device: 1
    input_count: 2
    layernorm_3009.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3009.dc.multiply.9, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3012: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_3009.dc.add.10, layer.7.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3014: {type: add, grid_loc: [0, 1], grid_size: [2, 4], inputs: [matmul_3012, layer.7.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3015: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_3014],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3018: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3015, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3009.dc.add.10_add_3022: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3009.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3022: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3018, buffer_0_layernorm_3009.dc.add.10_add_3022],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3023.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_3022, lc.input_tensor.layernorm_3023.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3022_layernorm_3023.dc.subtract.1: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_3022],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3023.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_add_3022_layernorm_3023.dc.subtract.1, layernorm_3023.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3023.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3023.dc.subtract.1, layernorm_3023.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3023.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3023.dc.multiply.2, lc.input_tensor.layernorm_3023.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3023.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3023.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3023.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3023.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3023.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3023.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3023.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3023.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3023.dc.reciprocal.7, lc.input_tensor.layernorm_3023.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3023.dc.subtract.1_layernorm_3023.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3023.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3023.dc.subtract.1_layernorm_3023.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3023.dc.subtract.1_layernorm_3023.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3023.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_3023.dc.subtract.1_layernorm_3023.dc.multiply.8, layernorm_3023.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_18_temporal_epoch_9:
    target_device: 0
    input_count: 2
    layernorm_3023.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3023.dc.multiply.8_0, e2e_layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3023.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3023.dc.multiply.9, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3026: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_3023.dc.add.10, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3032: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3023.dc.add.10, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3038: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3026, matmul_3032],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3041: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3038, constant_1_multiply_3041],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3042: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_3041, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3043.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_3042],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3043.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_3043.dc.exp.0, lc.input_tensor.softmax_3043.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3043.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3043.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3043.dc.exp.0_softmax_3043.dc.multiply.3: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_3043.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3043.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_softmax_3043.dc.exp.0_softmax_3043.dc.multiply.3, softmax_3043.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3047: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_3023.dc.add.10, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3047_matmul_3054: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3047],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3047_matmul_3054: {type: nop, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3047_matmul_3054],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3054: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_3043.dc.multiply.3, buffer_0_matmul_3047_matmul_3054],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3058: {type: matmul, grid_loc: [6, 3], grid_size: [2, 4], inputs: [matmul_3054, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3023.dc.add.10_add_3062: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3023.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3062: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3058, buffer_0_layernorm_3023.dc.add.10_add_3062],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3063.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3062, lc.input_tensor.layernorm_3063.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3062_layernorm_3063.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3062],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3063.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_0_add_3062_layernorm_3063.dc.subtract.1, layernorm_3063.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3063.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3063.dc.subtract.1, layernorm_3063.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3063.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3063.dc.multiply.2, lc.input_tensor.layernorm_3063.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_1_layernorm_3063.dc.subtract.1_layernorm_3063.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3063.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3063.dc.subtract.1_layernorm_3063.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3063.dc.subtract.1_layernorm_3063.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_19_temporal_epoch_9:
    target_device: 1
    input_count: 2
    layernorm_3063.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3063.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3063.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3063.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3063.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3063.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3063.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3063.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3063.dc.reciprocal.7, lc.input_tensor.layernorm_3063.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3063.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3063.dc.subtract.1_layernorm_3063.dc.multiply.8, layernorm_3063.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3063.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3063.dc.multiply.8, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3063.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_3063.dc.multiply.9, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3066: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [layernorm_3063.dc.add.10, layer.8.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3068: {type: add, grid_loc: [5, 0], grid_size: [2, 4], inputs: [matmul_3066, layer.8.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3069: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [add_3068],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3072: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [gelu_3069, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3063.dc.add.10_add_3076: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3063.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_20_temporal_epoch_10:
    target_device: 0
    input_count: 2
    add_3076: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3072_0, e2e_buffer_0_layernorm_3063.dc.add.10_add_3076_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3077.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3076, lc.input_tensor.layernorm_3077.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3076_layernorm_3077.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3076],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3077.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_add_3076_layernorm_3077.dc.subtract.1, layernorm_3077.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3077.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3077.dc.subtract.1, layernorm_3077.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3077.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3077.dc.multiply.2, lc.input_tensor.layernorm_3077.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3077.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3077.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3077.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3077.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3077.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3077.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3077.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3077.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3077.dc.reciprocal.7, lc.input_tensor.layernorm_3077.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3077.dc.subtract.1_layernorm_3077.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3077.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3077.dc.subtract.1_layernorm_3077.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3077.dc.subtract.1_layernorm_3077.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3077.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3077.dc.subtract.1_layernorm_3077.dc.multiply.8, layernorm_3077.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3077.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3077.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3077.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3077.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3080: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3077.dc.add.10, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3086: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3077.dc.add.10, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3092: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3080, matmul_3086],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3095: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3092, constant_1_multiply_3095],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3096: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3095, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3097.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3096],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3097.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3097.dc.exp.0, lc.input_tensor.softmax_3097.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3097.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3097.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3097.dc.exp.0_softmax_3097.dc.multiply.3: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3097.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3097.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [2, 1], inputs: [buffer_0_softmax_3097.dc.exp.0_softmax_3097.dc.multiply.3, softmax_3097.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3101: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3077.dc.add.10, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_21_temporal_epoch_10:
    target_device: 1
    input_count: 2
    buffer_1_matmul_3101_matmul_3108: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [matmul_3101],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3101_matmul_3108: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3101_matmul_3108],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3108: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [softmax_3097.dc.multiply.3, buffer_0_matmul_3101_matmul_3108],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3112: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_3108, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3077.dc.add.10_add_3116: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3077.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3116: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3112, buffer_0_layernorm_3077.dc.add.10_add_3116],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3117.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3116, lc.input_tensor.layernorm_3117.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3116_layernorm_3117.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3116],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3117.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_3116_layernorm_3117.dc.subtract.1, layernorm_3117.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3117.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3117.dc.subtract.1, layernorm_3117.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3117.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3117.dc.multiply.2, lc.input_tensor.layernorm_3117.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3117.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3117.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3117.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3117.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3117.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3117.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3117.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3117.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3117.dc.reciprocal.7, lc.input_tensor.layernorm_3117.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3117.dc.subtract.1_layernorm_3117.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3117.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3117.dc.subtract.1_layernorm_3117.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3117.dc.subtract.1_layernorm_3117.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3117.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3117.dc.subtract.1_layernorm_3117.dc.multiply.8, layernorm_3117.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3117.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3117.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3117.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3117.dc.multiply.9, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3120: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [layernorm_3117.dc.add.10, layer.9.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}

  fwd_22_temporal_epoch_11:
    target_device: 0
    input_count: 2
    add_3122: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_3120_0, layer.9.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3123: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_3122],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3126: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_3123, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3117.dc.add.10_add_3130: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3117.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3130: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_3126, buffer_0_layernorm_3117.dc.add.10_add_3130],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3131.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_3130, lc.input_tensor.layernorm_3131.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3130_layernorm_3131.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3130],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3131.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_add_3130_layernorm_3131.dc.subtract.1, layernorm_3131.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3131.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3131.dc.subtract.1, layernorm_3131.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3131.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3131.dc.multiply.2, lc.input_tensor.layernorm_3131.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3131.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_3131.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3131.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3131.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3131.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3131.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3131.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3131.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3131.dc.reciprocal.7, lc.input_tensor.layernorm_3131.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3131.dc.subtract.1_layernorm_3131.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3131.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3131.dc.subtract.1_layernorm_3131.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3131.dc.subtract.1_layernorm_3131.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3131.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3131.dc.subtract.1_layernorm_3131.dc.multiply.8, layernorm_3131.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3131.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3131.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3131.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3131.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3134: {type: matmul, grid_loc: [8, 1], grid_size: [2, 4], inputs: [layernorm_3131.dc.add.10, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_23_temporal_epoch_11:
    target_device: 1
    input_count: 2
    matmul_3140: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [layernorm_3131.dc.add.10, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3146: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_3134, matmul_3140],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3149: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3146, constant_1_multiply_3149],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3150: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [multiply_3149, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3151.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 2], inputs: [add_3150],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3151.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_3151.dc.exp.0, lc.input_tensor.softmax_3151.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3151.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_3151.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3151.dc.exp.0_softmax_3151.dc.multiply.3: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_3151.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3151.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_3151.dc.exp.0_softmax_3151.dc.multiply.3, softmax_3151.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3155: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3131.dc.add.10, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3155_matmul_3162: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_3155],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3155_matmul_3162: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_matmul_3155_matmul_3162],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3162: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_3151.dc.multiply.3, buffer_0_matmul_3155_matmul_3162],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3166: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_3162, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3131.dc.add.10_add_3170: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3131.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3170: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [matmul_3166, buffer_0_layernorm_3131.dc.add.10_add_3170],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3171.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [add_3170, lc.input_tensor.layernorm_3171.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3170_layernorm_3171.dc.subtract.1: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_3170],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3171.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_add_3170_layernorm_3171.dc.subtract.1, layernorm_3171.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3171.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3171.dc.subtract.1, layernorm_3171.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3171.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3171.dc.multiply.2, lc.input_tensor.layernorm_3171.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3171.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3171.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3171.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3171.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3171.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3171.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3171.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3171.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3171.dc.reciprocal.7, lc.input_tensor.layernorm_3171.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3171.dc.subtract.1_layernorm_3171.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_3171.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3171.dc.subtract.1_layernorm_3171.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3171.dc.subtract.1_layernorm_3171.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3171.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3171.dc.subtract.1_layernorm_3171.dc.multiply.8, layernorm_3171.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3171.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3171.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_24_temporal_epoch_12:
    target_device: 0
    input_count: 2
    layernorm_3171.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3171.dc.multiply.9_0, e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3174: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_3171.dc.add.10, layer.10.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3176: {type: add, grid_loc: [0, 1], grid_size: [2, 4], inputs: [matmul_3174, layer.10.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3177: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_3176],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3180: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3177, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3171.dc.add.10_add_3184: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3171.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3184: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3180, buffer_0_layernorm_3171.dc.add.10_add_3184],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3185.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_3184, lc.input_tensor.layernorm_3185.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3184_layernorm_3185.dc.subtract.1: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_3184],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3185.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_add_3184_layernorm_3185.dc.subtract.1, layernorm_3185.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3185.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3185.dc.subtract.1, layernorm_3185.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3185.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3185.dc.multiply.2, lc.input_tensor.layernorm_3185.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3185.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3185.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3185.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3185.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3185.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3185.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3185.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3185.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3185.dc.reciprocal.7, lc.input_tensor.layernorm_3185.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3185.dc.subtract.1_layernorm_3185.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3185.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3185.dc.subtract.1_layernorm_3185.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3185.dc.subtract.1_layernorm_3185.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3185.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_3185.dc.subtract.1_layernorm_3185.dc.multiply.8, layernorm_3185.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_25_temporal_epoch_12:
    target_device: 1
    input_count: 2
    layernorm_3185.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3185.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3185.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3185.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3188: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_3185.dc.add.10, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3194: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3185.dc.add.10, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3200: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3188, matmul_3194],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3203: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3200, constant_1_multiply_3203],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3204: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_3203, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3205.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_3204],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3205.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_3205.dc.exp.0, lc.input_tensor.softmax_3205.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3205.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3205.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3205.dc.exp.0_softmax_3205.dc.multiply.3: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_3205.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3205.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_softmax_3205.dc.exp.0_softmax_3205.dc.multiply.3, softmax_3205.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3209: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_3185.dc.add.10, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3209_matmul_3216: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3209],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3209_matmul_3216: {type: nop, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3209_matmul_3216],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3216: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_3205.dc.multiply.3, buffer_0_matmul_3209_matmul_3216],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3220: {type: matmul, grid_loc: [6, 3], grid_size: [2, 4], inputs: [matmul_3216, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3185.dc.add.10_add_3224: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3185.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3224: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3220, buffer_0_layernorm_3185.dc.add.10_add_3224],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3225.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3224, lc.input_tensor.layernorm_3225.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3224_layernorm_3225.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3224],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3225.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_0_add_3224_layernorm_3225.dc.subtract.1, layernorm_3225.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3225.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3225.dc.subtract.1, layernorm_3225.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3225.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3225.dc.multiply.2, lc.input_tensor.layernorm_3225.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_1_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3225.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_26_temporal_epoch_13:
    target_device: 0
    input_count: 2
    layernorm_3225.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3225.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_3225.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3225.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3225.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3225.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3225.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3225.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3225.dc.reciprocal.7, lc.input_tensor.layernorm_3225.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3225.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8_0, layernorm_3225.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3225.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3225.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3225.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_3225.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3228: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [layernorm_3225.dc.add.10, layer.11.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3230: {type: add, grid_loc: [5, 0], grid_size: [2, 4], inputs: [matmul_3228, layer.11.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3231: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [add_3230],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3234: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [gelu_3231, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3225.dc.add.10_add_3238: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3225.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_27_temporal_epoch_13:
    target_device: 1
    input_count: 2
    add_3238: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [matmul_3234, buffer_0_layernorm_3225.dc.add.10_add_3238],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3239.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3238, lc.input_tensor.layernorm_3239.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3238_layernorm_3239.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3238],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3239.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_add_3238_layernorm_3239.dc.subtract.1, layernorm_3239.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3239.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3239.dc.subtract.1, layernorm_3239.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3239.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3239.dc.multiply.2, lc.input_tensor.layernorm_3239.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3239.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3239.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3239.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3239.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3239.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3239.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3239.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3239.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3239.dc.reciprocal.7, lc.input_tensor.layernorm_3239.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3239.dc.subtract.1_layernorm_3239.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3239.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3239.dc.subtract.1_layernorm_3239.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3239.dc.subtract.1_layernorm_3239.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3239.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3239.dc.subtract.1_layernorm_3239.dc.multiply.8, layernorm_3239.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3239.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3239.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3239.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3239.dc.multiply.9, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3242: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3239.dc.add.10, layer.12.attention.self.query.weight, layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3248: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3239.dc.add.10, layer.12.attention.self.key.weight, layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3254: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3242, matmul_3248],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3257: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3254, constant_1_multiply_3257],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3258: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3257, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3259.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3258],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3259.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3259.dc.exp.0, lc.input_tensor.softmax_3259.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3259.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3259.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3259.dc.exp.0_softmax_3259.dc.multiply.3: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3259.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3259.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [2, 1], inputs: [buffer_0_softmax_3259.dc.exp.0_softmax_3259.dc.multiply.3, softmax_3259.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3263: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3239.dc.add.10, layer.12.attention.self.value.weight, layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_28_temporal_epoch_14:
    target_device: 0
    input_count: 2
    buffer_1_matmul_3263_matmul_3270: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3263_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3263_matmul_3270: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3263_matmul_3270],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3270: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3259.dc.multiply.3_0, buffer_0_matmul_3263_matmul_3270],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3274: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_3270, layer.12.attention.output.dense.weight, layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3239.dc.add.10_add_3278: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3239.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3278: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3274, buffer_0_layernorm_3239.dc.add.10_add_3278],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3279.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3278, lc.input_tensor.layernorm_3279.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3278_layernorm_3279.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3278],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3279.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_3278_layernorm_3279.dc.subtract.1, layernorm_3279.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3279.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3279.dc.subtract.1, layernorm_3279.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3279.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3279.dc.multiply.2, lc.input_tensor.layernorm_3279.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3279.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3279.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3279.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3279.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3279.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3279.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3279.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3279.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3279.dc.reciprocal.7, lc.input_tensor.layernorm_3279.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3279.dc.subtract.1_layernorm_3279.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3279.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3279.dc.subtract.1_layernorm_3279.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3279.dc.subtract.1_layernorm_3279.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3279.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3279.dc.subtract.1_layernorm_3279.dc.multiply.8, layernorm_3279.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3279.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3279.dc.multiply.8, layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3279.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3279.dc.multiply.9, layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3282: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [layernorm_3279.dc.add.10, layer.12.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}

  fwd_29_temporal_epoch_14:
    target_device: 1
    input_count: 2
    add_3284: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [matmul_3282, layer.12.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3285: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_3284],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3288: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_3285, layer.12.output.dense.weight, layer.12.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3279.dc.add.10_add_3292: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3279.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3292: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_3288, buffer_0_layernorm_3279.dc.add.10_add_3292],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3293.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_3292, lc.input_tensor.layernorm_3293.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3292_layernorm_3293.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3292],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3293.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_add_3292_layernorm_3293.dc.subtract.1, layernorm_3293.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3293.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3293.dc.subtract.1, layernorm_3293.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3293.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3293.dc.multiply.2, lc.input_tensor.layernorm_3293.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3293.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_3293.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3293.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3293.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3293.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3293.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3293.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3293.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3293.dc.reciprocal.7, lc.input_tensor.layernorm_3293.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3293.dc.subtract.1_layernorm_3293.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3293.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3293.dc.subtract.1_layernorm_3293.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3293.dc.subtract.1_layernorm_3293.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3293.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3293.dc.subtract.1_layernorm_3293.dc.multiply.8, layernorm_3293.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3293.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3293.dc.multiply.8, layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.12.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3293.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3293.dc.multiply.9, layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3296: {type: matmul, grid_loc: [8, 1], grid_size: [2, 4], inputs: [layernorm_3293.dc.add.10, layer.13.attention.self.query.weight, layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_30_temporal_epoch_15:
    target_device: 0
    input_count: 2
    matmul_3302: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_layernorm_3293.dc.add.10_0, layer.13.attention.self.key.weight, layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3308: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_matmul_3296_0, matmul_3302],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3311: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3308, constant_1_multiply_3311],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3312: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [multiply_3311, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3313.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 2], inputs: [add_3312],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3313.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_3313.dc.exp.0, lc.input_tensor.softmax_3313.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3313.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_3313.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3313.dc.exp.0_softmax_3313.dc.multiply.3: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_3313.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3313.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_3313.dc.exp.0_softmax_3313.dc.multiply.3, softmax_3313.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3317: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [e2e_layernorm_3293.dc.add.10_0, layer.13.attention.self.value.weight, layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3317_matmul_3324: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_3317],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3317_matmul_3324: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_matmul_3317_matmul_3324],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3324: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_3313.dc.multiply.3, buffer_0_matmul_3317_matmul_3324],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3328: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_3324, layer.13.attention.output.dense.weight, layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3293.dc.add.10_add_3332: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3293.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3332: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [matmul_3328, buffer_0_layernorm_3293.dc.add.10_add_3332],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3333.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [add_3332, lc.input_tensor.layernorm_3333.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3332_layernorm_3333.dc.subtract.1: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_3332],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3333.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_add_3332_layernorm_3333.dc.subtract.1, layernorm_3333.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3333.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3333.dc.subtract.1, layernorm_3333.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3333.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3333.dc.multiply.2, lc.input_tensor.layernorm_3333.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3333.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3333.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3333.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3333.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3333.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3333.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3333.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3333.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3333.dc.reciprocal.7, lc.input_tensor.layernorm_3333.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3333.dc.subtract.1_layernorm_3333.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_3333.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3333.dc.subtract.1_layernorm_3333.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3333.dc.subtract.1_layernorm_3333.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3333.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3333.dc.subtract.1_layernorm_3333.dc.multiply.8, layernorm_3333.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3333.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3333.dc.multiply.8, layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_31_temporal_epoch_15:
    target_device: 1
    input_count: 2
    layernorm_3333.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3333.dc.multiply.9, layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3336: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_3333.dc.add.10, layer.13.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3338: {type: add, grid_loc: [0, 1], grid_size: [2, 4], inputs: [matmul_3336, layer.13.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3339: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_3338],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3342: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3339, layer.13.output.dense.weight, layer.13.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3333.dc.add.10_add_3346: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3333.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3346: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3342, buffer_0_layernorm_3333.dc.add.10_add_3346],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3347.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_3346, lc.input_tensor.layernorm_3347.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3346_layernorm_3347.dc.subtract.1: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_3346],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3347.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_add_3346_layernorm_3347.dc.subtract.1, layernorm_3347.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3347.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3347.dc.subtract.1, layernorm_3347.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3347.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3347.dc.multiply.2, lc.input_tensor.layernorm_3347.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3347.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3347.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3347.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3347.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3347.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3347.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3347.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3347.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3347.dc.reciprocal.7, lc.input_tensor.layernorm_3347.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3347.dc.subtract.1_layernorm_3347.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3347.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3347.dc.subtract.1_layernorm_3347.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3347.dc.subtract.1_layernorm_3347.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3347.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_3347.dc.subtract.1_layernorm_3347.dc.multiply.8, layernorm_3347.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_32_temporal_epoch_16:
    target_device: 0
    input_count: 2
    layernorm_3347.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3347.dc.multiply.8_0, e2e_layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.13.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3347.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3347.dc.multiply.9, layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3350: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_3347.dc.add.10, layer.14.attention.self.query.weight, layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3356: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3347.dc.add.10, layer.14.attention.self.key.weight, layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3362: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3350, matmul_3356],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3365: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3362, constant_1_multiply_3365],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3366: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_3365, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3367.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_3366],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3367.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_3367.dc.exp.0, lc.input_tensor.softmax_3367.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3367.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3367.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3367.dc.exp.0_softmax_3367.dc.multiply.3: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_3367.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3367.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_softmax_3367.dc.exp.0_softmax_3367.dc.multiply.3, softmax_3367.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3371: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_3347.dc.add.10, layer.14.attention.self.value.weight, layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3371_matmul_3378: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3371],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3371_matmul_3378: {type: nop, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3371_matmul_3378],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3378: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_3367.dc.multiply.3, buffer_0_matmul_3371_matmul_3378],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3382: {type: matmul, grid_loc: [6, 3], grid_size: [2, 4], inputs: [matmul_3378, layer.14.attention.output.dense.weight, layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3347.dc.add.10_add_3386: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3347.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3386: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3382, buffer_0_layernorm_3347.dc.add.10_add_3386],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3387.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3386, lc.input_tensor.layernorm_3387.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3386_layernorm_3387.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3386],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3387.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_0_add_3386_layernorm_3387.dc.subtract.1, layernorm_3387.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3387.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3387.dc.subtract.1, layernorm_3387.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3387.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3387.dc.multiply.2, lc.input_tensor.layernorm_3387.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_1_layernorm_3387.dc.subtract.1_layernorm_3387.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3387.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3387.dc.subtract.1_layernorm_3387.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3387.dc.subtract.1_layernorm_3387.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_33_temporal_epoch_16:
    target_device: 1
    input_count: 2
    layernorm_3387.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3387.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3387.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3387.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3387.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3387.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3387.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3387.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3387.dc.reciprocal.7, lc.input_tensor.layernorm_3387.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3387.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3387.dc.subtract.1_layernorm_3387.dc.multiply.8, layernorm_3387.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3387.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3387.dc.multiply.8, layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3387.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_3387.dc.multiply.9, layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3390: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [layernorm_3387.dc.add.10, layer.14.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3392: {type: add, grid_loc: [5, 0], grid_size: [2, 4], inputs: [matmul_3390, layer.14.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3393: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [add_3392],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3396: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [gelu_3393, layer.14.output.dense.weight, layer.14.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3387.dc.add.10_add_3400: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3387.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_34_temporal_epoch_17:
    target_device: 0
    input_count: 2
    add_3400: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3396_0, e2e_buffer_0_layernorm_3387.dc.add.10_add_3400_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3401.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3400, lc.input_tensor.layernorm_3401.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3400_layernorm_3401.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3400],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3401.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_add_3400_layernorm_3401.dc.subtract.1, layernorm_3401.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3401.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3401.dc.subtract.1, layernorm_3401.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3401.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3401.dc.multiply.2, lc.input_tensor.layernorm_3401.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3401.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3401.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3401.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3401.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3401.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3401.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3401.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3401.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3401.dc.reciprocal.7, lc.input_tensor.layernorm_3401.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3401.dc.subtract.1_layernorm_3401.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3401.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3401.dc.subtract.1_layernorm_3401.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3401.dc.subtract.1_layernorm_3401.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3401.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3401.dc.subtract.1_layernorm_3401.dc.multiply.8, layernorm_3401.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3401.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3401.dc.multiply.8, layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.14.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3401.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3401.dc.multiply.9, layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3404: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3401.dc.add.10, layer.15.attention.self.query.weight, layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3410: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3401.dc.add.10, layer.15.attention.self.key.weight, layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3416: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3404, matmul_3410],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3419: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3416, constant_1_multiply_3419],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3420: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3419, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3421.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3420],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3421.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3421.dc.exp.0, lc.input_tensor.softmax_3421.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3421.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3421.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3421.dc.exp.0_softmax_3421.dc.multiply.3: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3421.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3421.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [2, 1], inputs: [buffer_0_softmax_3421.dc.exp.0_softmax_3421.dc.multiply.3, softmax_3421.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3425: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3401.dc.add.10, layer.15.attention.self.value.weight, layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_35_temporal_epoch_17:
    target_device: 1
    input_count: 2
    buffer_1_matmul_3425_matmul_3432: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [matmul_3425],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3425_matmul_3432: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3425_matmul_3432],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3432: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [softmax_3421.dc.multiply.3, buffer_0_matmul_3425_matmul_3432],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3436: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_3432, layer.15.attention.output.dense.weight, layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3401.dc.add.10_add_3440: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3401.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3440: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3436, buffer_0_layernorm_3401.dc.add.10_add_3440],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3441.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3440, lc.input_tensor.layernorm_3441.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3440_layernorm_3441.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3440],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3441.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_3440_layernorm_3441.dc.subtract.1, layernorm_3441.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3441.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3441.dc.subtract.1, layernorm_3441.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3441.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3441.dc.multiply.2, lc.input_tensor.layernorm_3441.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3441.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3441.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3441.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3441.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3441.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3441.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3441.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3441.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3441.dc.reciprocal.7, lc.input_tensor.layernorm_3441.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3441.dc.subtract.1_layernorm_3441.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3441.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3441.dc.subtract.1_layernorm_3441.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3441.dc.subtract.1_layernorm_3441.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3441.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3441.dc.subtract.1_layernorm_3441.dc.multiply.8, layernorm_3441.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3441.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3441.dc.multiply.8, layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3441.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3441.dc.multiply.9, layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3444: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [layernorm_3441.dc.add.10, layer.15.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}

  fwd_36_temporal_epoch_18:
    target_device: 0
    input_count: 2
    add_3446: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_3444_0, layer.15.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3447: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_3446],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3450: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_3447, layer.15.output.dense.weight, layer.15.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3441.dc.add.10_add_3454: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3441.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3454: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_3450, buffer_0_layernorm_3441.dc.add.10_add_3454],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3455.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_3454, lc.input_tensor.layernorm_3455.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3454_layernorm_3455.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3454],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3455.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_add_3454_layernorm_3455.dc.subtract.1, layernorm_3455.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3455.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3455.dc.subtract.1, layernorm_3455.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3455.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3455.dc.multiply.2, lc.input_tensor.layernorm_3455.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3455.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_3455.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3455.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3455.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3455.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3455.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3455.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3455.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3455.dc.reciprocal.7, lc.input_tensor.layernorm_3455.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3455.dc.subtract.1_layernorm_3455.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3455.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3455.dc.subtract.1_layernorm_3455.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3455.dc.subtract.1_layernorm_3455.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3455.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3455.dc.subtract.1_layernorm_3455.dc.multiply.8, layernorm_3455.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3455.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3455.dc.multiply.8, layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.15.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3455.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3455.dc.multiply.9, layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3458: {type: matmul, grid_loc: [8, 1], grid_size: [2, 4], inputs: [layernorm_3455.dc.add.10, layer.16.attention.self.query.weight, layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_37_temporal_epoch_18:
    target_device: 1
    input_count: 2
    matmul_3464: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [layernorm_3455.dc.add.10, layer.16.attention.self.key.weight, layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3470: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_3458, matmul_3464],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3473: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3470, constant_1_multiply_3473],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3474: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [multiply_3473, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3475.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 2], inputs: [add_3474],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3475.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_3475.dc.exp.0, lc.input_tensor.softmax_3475.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3475.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_3475.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3475.dc.exp.0_softmax_3475.dc.multiply.3: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_3475.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3475.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_3475.dc.exp.0_softmax_3475.dc.multiply.3, softmax_3475.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3479: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3455.dc.add.10, layer.16.attention.self.value.weight, layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3479_matmul_3486: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_3479],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3479_matmul_3486: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_matmul_3479_matmul_3486],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3486: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_3475.dc.multiply.3, buffer_0_matmul_3479_matmul_3486],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3490: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_3486, layer.16.attention.output.dense.weight, layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3455.dc.add.10_add_3494: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3455.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3494: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [matmul_3490, buffer_0_layernorm_3455.dc.add.10_add_3494],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3495.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [add_3494, lc.input_tensor.layernorm_3495.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3494_layernorm_3495.dc.subtract.1: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_3494],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3495.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_add_3494_layernorm_3495.dc.subtract.1, layernorm_3495.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3495.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3495.dc.subtract.1, layernorm_3495.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3495.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3495.dc.multiply.2, lc.input_tensor.layernorm_3495.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3495.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3495.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3495.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3495.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3495.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3495.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3495.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3495.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3495.dc.reciprocal.7, lc.input_tensor.layernorm_3495.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3495.dc.subtract.1_layernorm_3495.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_3495.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3495.dc.subtract.1_layernorm_3495.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3495.dc.subtract.1_layernorm_3495.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3495.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3495.dc.subtract.1_layernorm_3495.dc.multiply.8, layernorm_3495.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3495.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3495.dc.multiply.8, layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_38_temporal_epoch_19:
    target_device: 0
    input_count: 2
    layernorm_3495.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3495.dc.multiply.9_0, e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3498: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_3495.dc.add.10, layer.16.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3500: {type: add, grid_loc: [0, 1], grid_size: [2, 4], inputs: [matmul_3498, layer.16.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3501: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_3500],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3504: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3501, layer.16.output.dense.weight, layer.16.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3495.dc.add.10_add_3508: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3495.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3508: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3504, buffer_0_layernorm_3495.dc.add.10_add_3508],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3509.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_3508, lc.input_tensor.layernorm_3509.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3508_layernorm_3509.dc.subtract.1: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_3508],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3509.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_add_3508_layernorm_3509.dc.subtract.1, layernorm_3509.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3509.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3509.dc.subtract.1, layernorm_3509.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3509.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3509.dc.multiply.2, lc.input_tensor.layernorm_3509.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3509.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3509.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3509.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3509.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3509.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3509.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3509.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3509.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3509.dc.reciprocal.7, lc.input_tensor.layernorm_3509.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3509.dc.subtract.1_layernorm_3509.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3509.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3509.dc.subtract.1_layernorm_3509.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3509.dc.subtract.1_layernorm_3509.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3509.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_3509.dc.subtract.1_layernorm_3509.dc.multiply.8, layernorm_3509.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_39_temporal_epoch_19:
    target_device: 1
    input_count: 2
    layernorm_3509.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3509.dc.multiply.8, layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.16.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3509.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3509.dc.multiply.9, layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3512: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_3509.dc.add.10, layer.17.attention.self.query.weight, layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3518: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3509.dc.add.10, layer.17.attention.self.key.weight, layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3524: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3512, matmul_3518],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3527: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3524, constant_1_multiply_3527],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3528: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_3527, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3529.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_3528],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3529.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_3529.dc.exp.0, lc.input_tensor.softmax_3529.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3529.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3529.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3529.dc.exp.0_softmax_3529.dc.multiply.3: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_3529.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3529.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_softmax_3529.dc.exp.0_softmax_3529.dc.multiply.3, softmax_3529.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3533: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_3509.dc.add.10, layer.17.attention.self.value.weight, layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3533_matmul_3540: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3533],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3533_matmul_3540: {type: nop, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3533_matmul_3540],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3540: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_3529.dc.multiply.3, buffer_0_matmul_3533_matmul_3540],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3544: {type: matmul, grid_loc: [6, 3], grid_size: [2, 4], inputs: [matmul_3540, layer.17.attention.output.dense.weight, layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3509.dc.add.10_add_3548: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3509.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3548: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3544, buffer_0_layernorm_3509.dc.add.10_add_3548],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3549.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3548, lc.input_tensor.layernorm_3549.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3548_layernorm_3549.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3548],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3549.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_0_add_3548_layernorm_3549.dc.subtract.1, layernorm_3549.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3549.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3549.dc.subtract.1, layernorm_3549.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3549.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3549.dc.multiply.2, lc.input_tensor.layernorm_3549.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_1_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3549.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_40_temporal_epoch_20:
    target_device: 0
    input_count: 2
    layernorm_3549.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3549.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_3549.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3549.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3549.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3549.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3549.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3549.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3549.dc.reciprocal.7, lc.input_tensor.layernorm_3549.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3549.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8_0, layernorm_3549.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3549.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3549.dc.multiply.8, layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3549.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_3549.dc.multiply.9, layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3552: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [layernorm_3549.dc.add.10, layer.17.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3554: {type: add, grid_loc: [5, 0], grid_size: [2, 4], inputs: [matmul_3552, layer.17.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3555: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [add_3554],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3558: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [gelu_3555, layer.17.output.dense.weight, layer.17.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3549.dc.add.10_add_3562: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3549.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_41_temporal_epoch_20:
    target_device: 1
    input_count: 2
    add_3562: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [matmul_3558, buffer_0_layernorm_3549.dc.add.10_add_3562],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3563.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3562, lc.input_tensor.layernorm_3563.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3562_layernorm_3563.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3562],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3563.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_add_3562_layernorm_3563.dc.subtract.1, layernorm_3563.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3563.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3563.dc.subtract.1, layernorm_3563.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3563.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3563.dc.multiply.2, lc.input_tensor.layernorm_3563.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3563.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3563.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3563.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3563.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3563.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3563.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3563.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3563.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3563.dc.reciprocal.7, lc.input_tensor.layernorm_3563.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3563.dc.subtract.1_layernorm_3563.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3563.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3563.dc.subtract.1_layernorm_3563.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3563.dc.subtract.1_layernorm_3563.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3563.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3563.dc.subtract.1_layernorm_3563.dc.multiply.8, layernorm_3563.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3563.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3563.dc.multiply.8, layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.17.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3563.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3563.dc.multiply.9, layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3566: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3563.dc.add.10, layer.18.attention.self.query.weight, layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3572: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3563.dc.add.10, layer.18.attention.self.key.weight, layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3578: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3566, matmul_3572],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3581: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3578, constant_1_multiply_3581],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3582: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3581, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3583.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3582],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3583.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3583.dc.exp.0, lc.input_tensor.softmax_3583.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3583.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3583.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3583.dc.exp.0_softmax_3583.dc.multiply.3: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3583.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3583.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [2, 1], inputs: [buffer_0_softmax_3583.dc.exp.0_softmax_3583.dc.multiply.3, softmax_3583.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3587: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3563.dc.add.10, layer.18.attention.self.value.weight, layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_42_temporal_epoch_21:
    target_device: 0
    input_count: 2
    buffer_1_matmul_3587_matmul_3594: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3587_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3587_matmul_3594: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3587_matmul_3594],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3594: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [e2e_softmax_3583.dc.multiply.3_0, buffer_0_matmul_3587_matmul_3594],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3598: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_3594, layer.18.attention.output.dense.weight, layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3563.dc.add.10_add_3602: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [e2e_layernorm_3563.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3602: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3598, buffer_0_layernorm_3563.dc.add.10_add_3602],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3602, lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3602_layernorm_3603.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3602],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_3602_layernorm_3603.dc.subtract.1, layernorm_3603.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3603.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3603.dc.subtract.1, layernorm_3603.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.2, lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3603.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3603.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3603.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3603.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3603.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3603.dc.reciprocal.7, lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3603.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3603.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3603.dc.subtract.1_layernorm_3603.dc.multiply.8, layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3603.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.8, layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3603.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3603.dc.multiply.9, layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3606: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [layernorm_3603.dc.add.10, layer.18.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}

  fwd_43_temporal_epoch_21:
    target_device: 1
    input_count: 2
    add_3608: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [matmul_3606, layer.18.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3609: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_3608],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3612: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_3609, layer.18.output.dense.weight, layer.18.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3603.dc.add.10_add_3616: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3603.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3616: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_3612, buffer_0_layernorm_3603.dc.add.10_add_3616],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3617.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_3616, lc.input_tensor.layernorm_3617.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3616_layernorm_3617.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3616],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3617.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_add_3616_layernorm_3617.dc.subtract.1, layernorm_3617.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3617.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3617.dc.subtract.1, layernorm_3617.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3617.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3617.dc.multiply.2, lc.input_tensor.layernorm_3617.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3617.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_3617.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3617.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3617.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3617.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3617.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3617.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3617.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3617.dc.reciprocal.7, lc.input_tensor.layernorm_3617.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3617.dc.subtract.1_layernorm_3617.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3617.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3617.dc.subtract.1_layernorm_3617.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3617.dc.subtract.1_layernorm_3617.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3617.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3617.dc.subtract.1_layernorm_3617.dc.multiply.8, layernorm_3617.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3617.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3617.dc.multiply.8, layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.18.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3617.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3617.dc.multiply.9, layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3620: {type: matmul, grid_loc: [8, 1], grid_size: [2, 4], inputs: [layernorm_3617.dc.add.10, layer.19.attention.self.query.weight, layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_44_temporal_epoch_22:
    target_device: 0
    input_count: 2
    matmul_3626: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_layernorm_3617.dc.add.10_0, layer.19.attention.self.key.weight, layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3632: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_matmul_3620_0, matmul_3626],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3635: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3632, constant_1_multiply_3635],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3636: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [multiply_3635, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3637.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 2], inputs: [add_3636],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3637.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_3637.dc.exp.0, lc.input_tensor.softmax_3637.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3637.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_3637.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3637.dc.exp.0_softmax_3637.dc.multiply.3: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_3637.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3637.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_3637.dc.exp.0_softmax_3637.dc.multiply.3, softmax_3637.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3641: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [e2e_layernorm_3617.dc.add.10_0, layer.19.attention.self.value.weight, layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3641_matmul_3648: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_3641],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3641_matmul_3648: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_matmul_3641_matmul_3648],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3648: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_3637.dc.multiply.3, buffer_0_matmul_3641_matmul_3648],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3652: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_3648, layer.19.attention.output.dense.weight, layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3617.dc.add.10_add_3656: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3617.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3656: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [matmul_3652, buffer_0_layernorm_3617.dc.add.10_add_3656],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3657.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [add_3656, lc.input_tensor.layernorm_3657.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3656_layernorm_3657.dc.subtract.1: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_3656],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3657.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_add_3656_layernorm_3657.dc.subtract.1, layernorm_3657.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3657.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3657.dc.subtract.1, layernorm_3657.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3657.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3657.dc.multiply.2, lc.input_tensor.layernorm_3657.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3657.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3657.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3657.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3657.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3657.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3657.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3657.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3657.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3657.dc.reciprocal.7, lc.input_tensor.layernorm_3657.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3657.dc.subtract.1_layernorm_3657.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_3657.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3657.dc.subtract.1_layernorm_3657.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3657.dc.subtract.1_layernorm_3657.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3657.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3657.dc.subtract.1_layernorm_3657.dc.multiply.8, layernorm_3657.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3657.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3657.dc.multiply.8, layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_45_temporal_epoch_22:
    target_device: 1
    input_count: 2
    layernorm_3657.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3657.dc.multiply.9, layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3660: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_3657.dc.add.10, layer.19.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3662: {type: add, grid_loc: [0, 1], grid_size: [2, 4], inputs: [matmul_3660, layer.19.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3663: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_3662],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3666: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3663, layer.19.output.dense.weight, layer.19.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3657.dc.add.10_add_3670: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3657.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3670: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3666, buffer_0_layernorm_3657.dc.add.10_add_3670],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3671.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_3670, lc.input_tensor.layernorm_3671.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3670_layernorm_3671.dc.subtract.1: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_3670],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3671.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_add_3670_layernorm_3671.dc.subtract.1, layernorm_3671.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3671.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3671.dc.subtract.1, layernorm_3671.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3671.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3671.dc.multiply.2, lc.input_tensor.layernorm_3671.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3671.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3671.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3671.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3671.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3671.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3671.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3671.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3671.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3671.dc.reciprocal.7, lc.input_tensor.layernorm_3671.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3671.dc.subtract.1_layernorm_3671.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3671.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3671.dc.subtract.1_layernorm_3671.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3671.dc.subtract.1_layernorm_3671.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3671.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_3671.dc.subtract.1_layernorm_3671.dc.multiply.8, layernorm_3671.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_46_temporal_epoch_23:
    target_device: 0
    input_count: 2
    layernorm_3671.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3671.dc.multiply.8_0, e2e_layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.19.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3671.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3671.dc.multiply.9, layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3674: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_3671.dc.add.10, layer.20.attention.self.query.weight, layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3680: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3671.dc.add.10, layer.20.attention.self.key.weight, layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3686: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3674, matmul_3680],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3689: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3686, constant_1_multiply_3689],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3690: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_3689, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3691.dc.exp.0: {type: exp, grid_loc: [2, 6], grid_size: [2, 2], inputs: [add_3690],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3691.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 1], inputs: [softmax_3691.dc.exp.0, lc.input_tensor.softmax_3691.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3691.dc.reciprocal.2: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [softmax_3691.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3691.dc.exp.0_softmax_3691.dc.multiply.3: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [softmax_3691.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3691.dc.multiply.3: {type: multiply, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_softmax_3691.dc.exp.0_softmax_3691.dc.multiply.3, softmax_3691.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3695: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [layernorm_3671.dc.add.10, layer.20.attention.self.value.weight, layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3695_matmul_3702: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3695],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3695_matmul_3702: {type: nop, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3695_matmul_3702],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3702: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [softmax_3691.dc.multiply.3, buffer_0_matmul_3695_matmul_3702],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3706: {type: matmul, grid_loc: [6, 3], grid_size: [2, 4], inputs: [matmul_3702, layer.20.attention.output.dense.weight, layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3671.dc.add.10_add_3710: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3671.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3710: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3706, buffer_0_layernorm_3671.dc.add.10_add_3710],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3711.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3710, lc.input_tensor.layernorm_3711.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3710_layernorm_3711.dc.subtract.1: {type: nop, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3710],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3711.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_0_add_3710_layernorm_3711.dc.subtract.1, layernorm_3711.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3711.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3711.dc.subtract.1, layernorm_3711.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3711.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3711.dc.multiply.2, lc.input_tensor.layernorm_3711.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_1_layernorm_3711.dc.subtract.1_layernorm_3711.dc.multiply.8: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3711.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3711.dc.subtract.1_layernorm_3711.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3711.dc.subtract.1_layernorm_3711.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_47_temporal_epoch_23:
    target_device: 1
    input_count: 2
    layernorm_3711.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3711.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3711.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3711.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3711.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3711.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3711.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3711.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3711.dc.reciprocal.7, lc.input_tensor.layernorm_3711.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3711.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3711.dc.subtract.1_layernorm_3711.dc.multiply.8, layernorm_3711.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3711.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3711.dc.multiply.8, layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3711.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_3711.dc.multiply.9, layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3714: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [layernorm_3711.dc.add.10, layer.20.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3716: {type: add, grid_loc: [5, 0], grid_size: [2, 4], inputs: [matmul_3714, layer.20.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3717: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [add_3716],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3720: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [gelu_3717, layer.20.output.dense.weight, layer.20.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3711.dc.add.10_add_3724: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3711.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_48_temporal_epoch_24:
    target_device: 0
    input_count: 2
    add_3724: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_3720_0, e2e_buffer_0_layernorm_3711.dc.add.10_add_3724_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3725.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3724, lc.input_tensor.layernorm_3725.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3724_layernorm_3725.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3724],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3725.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_add_3724_layernorm_3725.dc.subtract.1, layernorm_3725.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3725.dc.multiply.2: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3725.dc.subtract.1, layernorm_3725.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3725.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3725.dc.multiply.2, lc.input_tensor.layernorm_3725.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3725.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3725.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3725.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3725.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3725.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3725.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3725.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3725.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3725.dc.reciprocal.7, lc.input_tensor.layernorm_3725.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3725.dc.subtract.1_layernorm_3725.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3725.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3725.dc.subtract.1_layernorm_3725.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3725.dc.subtract.1_layernorm_3725.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3725.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3725.dc.subtract.1_layernorm_3725.dc.multiply.8, layernorm_3725.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3725.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3725.dc.multiply.8, layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.20.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3725.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3725.dc.multiply.9, layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3728: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3725.dc.add.10, layer.21.attention.self.query.weight, layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3734: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [layernorm_3725.dc.add.10, layer.21.attention.self.key.weight, layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3740: {type: matmul, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_3728, matmul_3734],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3743: {type: multiply, grid_loc: [6, 0], grid_size: [2, 1], inputs: [matmul_3740, constant_1_multiply_3743],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3744: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [multiply_3743, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3745.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [2, 2], inputs: [add_3744],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3745.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [2, 1], inputs: [softmax_3745.dc.exp.0, lc.input_tensor.softmax_3745.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3745.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 6], grid_size: [2, 1], inputs: [softmax_3745.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3745.dc.exp.0_softmax_3745.dc.multiply.3: {type: nop, grid_loc: [7, 4], grid_size: [2, 1], inputs: [softmax_3745.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3745.dc.multiply.3: {type: multiply, grid_loc: [7, 7], grid_size: [2, 1], inputs: [buffer_0_softmax_3745.dc.exp.0_softmax_3745.dc.multiply.3, softmax_3745.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3749: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [layernorm_3725.dc.add.10, layer.21.attention.self.value.weight, layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_49_temporal_epoch_24:
    target_device: 1
    input_count: 2
    buffer_1_matmul_3749_matmul_3756: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [matmul_3749],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3749_matmul_3756: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_1_matmul_3749_matmul_3756],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3756: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [softmax_3745.dc.multiply.3, buffer_0_matmul_3749_matmul_3756],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3760: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [matmul_3756, layer.21.attention.output.dense.weight, layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3725.dc.add.10_add_3764: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [layernorm_3725.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3764: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [matmul_3760, buffer_0_layernorm_3725.dc.add.10_add_3764],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3765.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_3764, lc.input_tensor.layernorm_3765.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3764_layernorm_3765.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_3764],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3765.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_3764_layernorm_3765.dc.subtract.1, layernorm_3765.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3765.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3765.dc.subtract.1, layernorm_3765.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3765.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [layernorm_3765.dc.multiply.2, lc.input_tensor.layernorm_3765.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3765.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [layernorm_3765.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3765.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3765.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [layernorm_3765.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3765.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [layernorm_3765.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3765.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [layernorm_3765.dc.reciprocal.7, lc.input_tensor.layernorm_3765.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3765.dc.subtract.1_layernorm_3765.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [layernorm_3765.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3765.dc.subtract.1_layernorm_3765.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_layernorm_3765.dc.subtract.1_layernorm_3765.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3765.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3765.dc.subtract.1_layernorm_3765.dc.multiply.8, layernorm_3765.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3765.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3765.dc.multiply.8, layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3765.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [layernorm_3765.dc.multiply.9, layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3768: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [layernorm_3765.dc.add.10, layer.21.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}

  fwd_50_temporal_epoch_25:
    target_device: 0
    input_count: 2
    add_3770: {type: add, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_matmul_3768_0, layer.21.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3771: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [add_3770],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3774: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [gelu_3771, layer.21.output.dense.weight, layer.21.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3765.dc.add.10_add_3778: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3765.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3778: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [matmul_3774, buffer_0_layernorm_3765.dc.add.10_add_3778],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3779.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [add_3778, lc.input_tensor.layernorm_3779.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3778_layernorm_3779.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [add_3778],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3779.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_add_3778_layernorm_3779.dc.subtract.1, layernorm_3779.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3779.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [layernorm_3779.dc.subtract.1, layernorm_3779.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3779.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3779.dc.multiply.2, lc.input_tensor.layernorm_3779.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3779.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [layernorm_3779.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3779.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3779.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [2, 1], inputs: [layernorm_3779.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3779.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [layernorm_3779.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3779.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [layernorm_3779.dc.reciprocal.7, lc.input_tensor.layernorm_3779.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3779.dc.subtract.1_layernorm_3779.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [layernorm_3779.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3779.dc.subtract.1_layernorm_3779.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3779.dc.subtract.1_layernorm_3779.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3779.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3779.dc.subtract.1_layernorm_3779.dc.multiply.8, layernorm_3779.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3779.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3779.dc.multiply.8, layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.21.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3779.dc.add.10: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3779.dc.multiply.9, layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3782: {type: matmul, grid_loc: [8, 1], grid_size: [2, 4], inputs: [layernorm_3779.dc.add.10, layer.22.attention.self.query.weight, layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}

  fwd_51_temporal_epoch_25:
    target_device: 1
    input_count: 2
    matmul_3788: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [layernorm_3779.dc.add.10, layer.22.attention.self.key.weight, layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3794: {type: matmul, grid_loc: [0, 4], grid_size: [2, 1], inputs: [matmul_3782, matmul_3788],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3797: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [matmul_3794, constant_1_multiply_3797],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3798: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [multiply_3797, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3799.dc.exp.0: {type: exp, grid_loc: [2, 0], grid_size: [2, 2], inputs: [add_3798],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3799.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [softmax_3799.dc.exp.0, lc.input_tensor.softmax_3799.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3799.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 3], grid_size: [2, 1], inputs: [softmax_3799.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3799.dc.exp.0_softmax_3799.dc.multiply.3: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [softmax_3799.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3799.dc.multiply.3: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_softmax_3799.dc.exp.0_softmax_3799.dc.multiply.3, softmax_3799.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3803: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3779.dc.add.10, layer.22.attention.self.value.weight, layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3803_matmul_3810: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [matmul_3803],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3803_matmul_3810: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [buffer_1_matmul_3803_matmul_3810],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3810: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [softmax_3799.dc.multiply.3, buffer_0_matmul_3803_matmul_3810],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3814: {type: matmul, grid_loc: [4, 4], grid_size: [2, 4], inputs: [matmul_3810, layer.22.attention.output.dense.weight, layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3779.dc.add.10_add_3818: {type: nop, grid_loc: [6, 0], grid_size: [2, 1], inputs: [layernorm_3779.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3818: {type: add, grid_loc: [6, 1], grid_size: [2, 1], inputs: [matmul_3814, buffer_0_layernorm_3779.dc.add.10_add_3818],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3819.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [add_3818, lc.input_tensor.layernorm_3819.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3818_layernorm_3819.dc.subtract.1: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [add_3818],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3819.dc.subtract.1: {type: subtract, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_add_3818_layernorm_3819.dc.subtract.1, layernorm_3819.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3819.dc.multiply.2: {type: multiply, grid_loc: [6, 7], grid_size: [2, 1], inputs: [layernorm_3819.dc.subtract.1, layernorm_3819.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3819.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3819.dc.multiply.2, lc.input_tensor.layernorm_3819.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3819.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3819.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3819.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3819.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3819.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3819.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3819.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3819.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3819.dc.reciprocal.7, lc.input_tensor.layernorm_3819.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3819.dc.subtract.1_layernorm_3819.dc.multiply.8: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [layernorm_3819.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3819.dc.subtract.1_layernorm_3819.dc.multiply.8: {type: nop, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_1_layernorm_3819.dc.subtract.1_layernorm_3819.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3819.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_layernorm_3819.dc.subtract.1_layernorm_3819.dc.multiply.8, layernorm_3819.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3819.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [layernorm_3819.dc.multiply.8, layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_52_temporal_epoch_26:
    target_device: 0
    input_count: 2
    layernorm_3819.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3819.dc.multiply.9_0, e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3822: {type: matmul, grid_loc: [2, 0], grid_size: [2, 8], inputs: [layernorm_3819.dc.add.10, layer.22.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3824: {type: add, grid_loc: [0, 1], grid_size: [2, 4], inputs: [matmul_3822, layer.22.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3825: {type: gelu, grid_loc: [4, 0], grid_size: [2, 4], inputs: [add_3824],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3828: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [gelu_3825, layer.22.output.dense.weight, layer.22.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3819.dc.add.10_add_3832: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3819.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3832: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [matmul_3828, buffer_0_layernorm_3819.dc.add.10_add_3832],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3833.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [add_3832, lc.input_tensor.layernorm_3833.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3832_layernorm_3833.dc.subtract.1: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [add_3832],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3833.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_add_3832_layernorm_3833.dc.subtract.1, layernorm_3833.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3833.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [2, 1], inputs: [layernorm_3833.dc.subtract.1, layernorm_3833.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3833.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [layernorm_3833.dc.multiply.2, lc.input_tensor.layernorm_3833.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3833.dc.add.5: {type: add, grid_loc: [8, 2], grid_size: [2, 1], inputs: [layernorm_3833.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3833.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3833.dc.sqrt.6: {type: sqrt, grid_loc: [8, 3], grid_size: [2, 1], inputs: [layernorm_3833.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3833.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3833.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3833.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3833.dc.reciprocal.7, lc.input_tensor.layernorm_3833.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3833.dc.subtract.1_layernorm_3833.dc.multiply.8: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [layernorm_3833.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3833.dc.subtract.1_layernorm_3833.dc.multiply.8: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3833.dc.subtract.1_layernorm_3833.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3833.dc.multiply.8: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_layernorm_3833.dc.subtract.1_layernorm_3833.dc.multiply.8, layernorm_3833.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_53_temporal_epoch_26:
    target_device: 1
    input_count: 2
    layernorm_3833.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [layernorm_3833.dc.multiply.8, layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.22.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3833.dc.add.10: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3833.dc.multiply.9, layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3836: {type: matmul, grid_loc: [0, 3], grid_size: [2, 4], inputs: [layernorm_3833.dc.add.10, layer.23.attention.self.query.weight, layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3842: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [layernorm_3833.dc.add.10, layer.23.attention.self.key.weight, layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    matmul_3848: {type: matmul, grid_loc: [0, 7], grid_size: [2, 1], inputs: [matmul_3836, matmul_3842],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_3851: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [matmul_3848, constant_1_multiply_3851],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    add_3852: {type: add, grid_loc: [2, 5], grid_size: [2, 1], inputs: [multiply_3851, attention_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    softmax_3853.dc.exp.0: {type: exp, grid_loc: [4, 4], grid_size: [2, 2], inputs: [add_3852],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_3853.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [softmax_3853.dc.exp.0, lc.input_tensor.softmax_3853.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    softmax_3853.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 0], grid_size: [2, 1], inputs: [softmax_3853.dc.reduce_sum.1.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_softmax_3853.dc.exp.0_softmax_3853.dc.multiply.3: {type: nop, grid_loc: [4, 6], grid_size: [2, 1], inputs: [softmax_3853.dc.exp.0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_3853.dc.multiply.3: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_0_softmax_3853.dc.exp.0_softmax_3853.dc.multiply.3, softmax_3853.dc.reciprocal.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    matmul_3857: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [layernorm_3833.dc.add.10, layer.23.attention.self.value.weight, layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_matmul_3857_matmul_3864: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [matmul_3857],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_matmul_3857_matmul_3864: {type: nop, grid_loc: [6, 2], grid_size: [2, 1], inputs: [buffer_1_matmul_3857_matmul_3864],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_3864: {type: matmul, grid_loc: [6, 3], grid_size: [2, 1], inputs: [softmax_3853.dc.multiply.3, buffer_0_matmul_3857_matmul_3864],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    matmul_3868: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_3864, layer.23.attention.output.dense.weight, layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_0_layernorm_3833.dc.add.10_add_3872: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3833.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_3872: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [matmul_3868, buffer_0_layernorm_3833.dc.add.10_add_3872],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3873.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [add_3872, lc.input_tensor.layernorm_3873.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3872_layernorm_3873.dc.subtract.1: {type: nop, grid_loc: [8, 2], grid_size: [2, 1], inputs: [add_3872],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3873.dc.subtract.1: {type: subtract, grid_loc: [8, 3], grid_size: [2, 1], inputs: [buffer_0_add_3872_layernorm_3873.dc.subtract.1, layernorm_3873.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3873.dc.multiply.2: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [layernorm_3873.dc.subtract.1, layernorm_3873.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3873.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [2, 1], inputs: [layernorm_3873.dc.multiply.2, lc.input_tensor.layernorm_3873.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_1_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [layernorm_3873.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_54_temporal_epoch_27:
    target_device: 0
    input_count: 2
    layernorm_3873.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_layernorm_3873.dc.reduce_avg.3.lc1_0, dc.input_tensor.layernorm_3873.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3873.dc.sqrt.6: {type: sqrt, grid_loc: [0, 1], grid_size: [2, 1], inputs: [layernorm_3873.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3873.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 2], grid_size: [2, 1], inputs: [layernorm_3873.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3873.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [2, 1], inputs: [layernorm_3873.dc.reciprocal.7, lc.input_tensor.layernorm_3873.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3873.dc.multiply.8: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [e2e_buffer_0_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8_0, layernorm_3873.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3873.dc.multiply.9: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3873.dc.multiply.8, layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3873.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [2, 1], inputs: [layernorm_3873.dc.multiply.9, layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    matmul_3876: {type: matmul, grid_loc: [3, 0], grid_size: [2, 8], inputs: [layernorm_3873.dc.add.10, layer.23.intermediate.dense.weight],
         t: 1, mblock: [3, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 32, u_kt: 1}}
    add_3878: {type: add, grid_loc: [5, 0], grid_size: [2, 4], inputs: [matmul_3876, layer.23.intermediate.dense.bias],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    gelu_3879: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [add_3878],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_3882: {type: matmul, grid_loc: [7, 0], grid_size: [2, 8], inputs: [gelu_3879, layer.23.output.dense.weight, layer.23.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_layernorm_3873.dc.add.10_add_3886: {type: nop, grid_loc: [1, 7], grid_size: [2, 1], inputs: [layernorm_3873.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_55_temporal_epoch_27:
    target_device: 1
    input_count: 2
    add_3886: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [matmul_3882, buffer_0_layernorm_3873.dc.add.10_add_3886],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3887.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_3886, lc.input_tensor.layernorm_3887.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_3886_layernorm_3887.dc.subtract.1: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [add_3886],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3887.dc.subtract.1: {type: subtract, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_0_add_3886_layernorm_3887.dc.subtract.1, layernorm_3887.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_3887.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [layernorm_3887.dc.subtract.1, layernorm_3887.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3887.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [layernorm_3887.dc.multiply.2, lc.input_tensor.layernorm_3887.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    layernorm_3887.dc.add.5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [layernorm_3887.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_3887.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3887.dc.sqrt.6: {type: sqrt, grid_loc: [2, 1], grid_size: [2, 1], inputs: [layernorm_3887.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3887.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 2], grid_size: [2, 1], inputs: [layernorm_3887.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_3887.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [2, 1], inputs: [layernorm_3887.dc.reciprocal.7, lc.input_tensor.layernorm_3887.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_layernorm_3887.dc.subtract.1_layernorm_3887.dc.multiply.8: {type: nop, grid_loc: [0, 5], grid_size: [2, 1], inputs: [layernorm_3887.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_3887.dc.subtract.1_layernorm_3887.dc.multiply.8: {type: nop, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_1_layernorm_3887.dc.subtract.1_layernorm_3887.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_3887.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_layernorm_3887.dc.subtract.1_layernorm_3887.dc.multiply.8, layernorm_3887.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3887.dc.multiply.9: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [layernorm_3887.dc.multiply.8, layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.23.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_3887.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [2, 1], inputs: [layernorm_3887.dc.multiply.9, layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    layernorm_3887.dc.add.10_output_nop_0: {type: nop, grid_loc: [3, 7], grid_size: [2, 1], inputs: [layernorm_3887.dc.add.10], untilize_output: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_one: 1, $c_zero: 0, $gptr_q23: 0, $lptr_q23: 0, $lptr_q18: 0, $gptr_q16: 0, $c_microbatch_size: 2, $lptr_q12: 0, $lptr_q20: 0, $lptr_q8: 0, $gptr_q21: 0, $lptr_q5: 0, $lptr_q10: 0, $lptr_q3: 0, $gptr_q25: 0, $lptr_q40: 0, $gptr_q44: 0, $lptr_q21: 0, $gptr_q3: 0, $gptr_q34: 0, $lptr_q44: 0, $gptr_q18: 0, $lptr_q14: 0, $gptr_q46: 0, $gptr_q47: 0, $gptr_q10: 0, $gptr_q5: 0, $lptr_q36: 0, $lptr_q29: 0, $gptr_q42: 0, $gptr_q20: 0, $lptr_q46: 0, $lptr_q47: 0, $lptr_q16: 0, $lptr_q49: 0, $gptr_q31: 0, $lptr_q42: 0, $gptr_q14: 0, $gptr_q8: 0, $lptr_q51: 0, $gptr_q36: 0, $gptr_q40: 0, $lptr_q38: 0, $gptr_q27: 0, $gptr_q38: 0, $gptr_q51: 0, $lptr_q34: 0, $gptr_q7: 0, $lptr_q7: 0, $lptr_q33: 0, $gptr_q29: 0, $gptr_q33: 0, $gptr_q49: 0, $lptr_q31: 0, $lptr_q25: 0, $gptr_q12: 0, $lptr_q27: 0}
    - staticvar: {$gptr_q24_shadow: 0, $gptr_q24: 0, $lptr_q24: 0, $gptr_q22_shadow: 0, $gptr_q19_shadow: 0, $gptr_q19: 0, $lptr_q19: 0, $gptr_q17_shadow: 0, $gptr_q17: 0, $lptr_q22: 0, $gptr_q15_shadow: 0, $gptr_q30_shadow: 0, $gptr_q37_shadow: 0, $lptr_q6: 0, $lptr_q48: 0, $lptr_q32: 0, $gptr_q43: 0, $gptr_q35_shadow: 0, $gptr_q35: 0, $gptr_q32: 0, $gptr_q45: 0, $lptr_q35: 0, $gptr_q6: 0, $gptr_q39: 0, $gptr_q41: 0, $gptr_q15: 0, $lptr_q37: 0, $gptr_q0: 0, $lptr_q39: 0, $gptr_q28: 0, $gptr_q28_shadow: 0, $gptr_q32_shadow: 0, $gptr_q48_shadow: 0, $lptr_q0: 0, $lptr_q45: 0, $gptr_q30: 0, $gptr_q45_shadow: 0, $lptr_q26: 0, $gptr_q4: 0, $lptr_q43: 0, $gptr_q48: 0, $gptr_q37: 0, $gptr_q43_shadow: 0, $gptr_q50: 0, $lptr_q4: 0, $lptr_q50: 0, $gptr_q26_shadow: 0, $gptr_q4_shadow: 0, $gptr_q22: 0, $gptr_q13_shadow: 0, $lptr_q1: 0, $gptr_q1: 0, $gptr_q26: 0, $lptr_q9: 0, $gptr_q41_shadow: 0, $gptr_q1_shadow: 0, $lptr_q2: 0, $gptr_q2: 0, $lptr_q30: 0, $gptr_q9_shadow: 0, $gptr_q2_shadow: 0, $gptr_q6_shadow: 0, $gptr_q9: 0, $lptr_q28: 0, $lptr_q11: 0, $lptr_q15: 0, $gptr_q11: 0, $gptr_q11_shadow: 0, $gptr_q39_shadow: 0, $lptr_q13: 0, $lptr_q17: 0, $lptr_q41: 0, $gptr_q13: 0}
    - varinst: [$gptr_q48, set, $gptr_q48_shadow]
    - varinst: [$gptr_q45, set, $gptr_q45_shadow]
    - varinst: [$gptr_q43, set, $gptr_q43_shadow]
    - varinst: [$gptr_q41, set, $gptr_q41_shadow]
    - varinst: [$gptr_q39, set, $gptr_q39_shadow]
    - varinst: [$gptr_q37, set, $gptr_q37_shadow]
    - varinst: [$gptr_q35, set, $gptr_q35_shadow]
    - varinst: [$gptr_q32, set, $gptr_q32_shadow]
    - varinst: [$gptr_q30, set, $gptr_q30_shadow]
    - varinst: [$gptr_q28, set, $gptr_q28_shadow]
    - varinst: [$gptr_q26, set, $gptr_q26_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q9, set, $gptr_q9_shadow]
    - varinst: [$gptr_q11, set, $gptr_q11_shadow]
    - varinst: [$gptr_q13, set, $gptr_q13_shadow]
    - varinst: [$gptr_q15, set, $gptr_q15_shadow]
    - varinst: [$gptr_q17, set, $gptr_q17_shadow]
    - varinst: [$gptr_q19, set, $gptr_q19_shadow]
    - varinst: [$gptr_q22, set, $gptr_q22_shadow]
    - varinst: [$gptr_q24, set, $gptr_q24_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8_0]
    -   execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2609: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2611.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2631.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2631.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2631.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2631.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_1_temporal_epoch_0, queue_settings: {
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2645.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2645.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2645.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_2699.dc.subtract.1_0]
    -   execute: {graph_name: fwd_2_temporal_epoch_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_buffer_0_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2663: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2665.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2685.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_3_temporal_epoch_1, queue_settings: {
               lc.input_tensor.layernorm_2685.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2685.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2685.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2699.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2645.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_buffer_0_layernorm_2645.dc.subtract.1_layernorm_2645.dc.multiply.8_0]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2739.dc.add.10_0, e2e_gelu_2745_0]
    -   execute: {graph_name: fwd_4_temporal_epoch_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layernorm_2699.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.layernorm_2699.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2699.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2699.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2717: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2719.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_5_temporal_epoch_2, queue_settings: {
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2739.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2739.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2739.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2739.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2699.dc.subtract.1_0]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2793.dc.add.10_0, e2e_matmul_2796_0]
    -   execute: {graph_name: fwd_6_temporal_epoch_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_2739.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_gelu_2745_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2753.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2753.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2753.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2753.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2771: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2773.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_7_temporal_epoch_3, queue_settings: {
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2793.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2793.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2793.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2793.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2739.dc.add.10_0, e2e_gelu_2745_0]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_2847.dc.multiply.9_0, e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_8_temporal_epoch_4, queue_settings: {
               e2e_layernorm_2793.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_matmul_2796_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               layer.3.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2807.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2807.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2807.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2807.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_9_temporal_epoch_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2825: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2827.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2847.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2847.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2847.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2847.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2793.dc.add.10_0, e2e_matmul_2796_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q9_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_2901.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8_0]
    -   execute: {graph_name: fwd_10_temporal_epoch_5, queue_settings: {
               e2e_layernorm_2847.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2861.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2861.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2861.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2861.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_11_temporal_epoch_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2879: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2881.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2901.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2901.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2847.dc.multiply.9_0, e2e_layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q11_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_2915.dc.add.10_0, e2e_softmax_2935.dc.multiply.3_0, e2e_matmul_2939_0]
    -   execute: {graph_name: fwd_12_temporal_epoch_6, queue_settings: {
               e2e_layernorm_2901.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_buffer_0_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               dc.input_tensor.layernorm_2901.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2901.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_13_temporal_epoch_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               lc.input_tensor.layernorm_2915.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2915.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2915.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2915.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2933: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2935.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2901.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_2901.dc.subtract.1_layernorm_2901.dc.multiply.8_0]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q13_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_2969.dc.add.10_0, e2e_matmul_2972_0]
    -   execute: {graph_name: fwd_14_temporal_epoch_7, queue_settings: {
               e2e_layernorm_2915.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_softmax_2935.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e_matmul_2939_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2955.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2955.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2955.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2955.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_15_temporal_epoch_7, queue_settings: {
               layer.6.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2969.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2969.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_2969.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_2969.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2915.dc.add.10_0, e2e_softmax_2935.dc.multiply.3_0, e2e_matmul_2939_0]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3023.dc.multiply.8_0, e2e_layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_16_temporal_epoch_8, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_layernorm_2969.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e_matmul_2972_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_2987: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_2989.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3009.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3009.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3009.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3009.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_17_temporal_epoch_8, queue_settings: {
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3023.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3023.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3023.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3023.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_2969.dc.add.10_0, e2e_matmul_2972_0]
    -   varinst: [$gptr_q15_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_3072_0, e2e_buffer_0_layernorm_3063.dc.add.10_add_3076_0]
    -   execute: {graph_name: fwd_18_temporal_epoch_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_layernorm_3023.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3041: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3043.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3063.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3063.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_19_temporal_epoch_9, queue_settings: {
               dc.input_tensor.layernorm_3063.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3063.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3023.dc.multiply.8_0, e2e_layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q17_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3117.dc.add.10_0, e2e_matmul_3120_0]
    -   execute: {graph_name: fwd_20_temporal_epoch_10, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_matmul_3072_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_buffer_0_layernorm_3063.dc.add.10_add_3076_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               lc.input_tensor.layernorm_3077.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3077.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3077.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3077.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3095: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3097.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_21_temporal_epoch_10, queue_settings: {
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3117.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3117.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3117.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3117.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_3072_0, e2e_buffer_0_layernorm_3063.dc.add.10_add_3076_0]
    -   varinst: [$gptr_q19_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q20, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3171.dc.multiply.9_0, e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_22_temporal_epoch_11, queue_settings: {
               e2e_layernorm_3117.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               e2e_matmul_3120_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               layer.9.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3131.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3131.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3131.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3131.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_23_temporal_epoch_11, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3149: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3151.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3171.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3171.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3171.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3171.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3117.dc.add.10_0, e2e_matmul_3120_0]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q22_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_3225.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8_0]
    -   execute: {graph_name: fwd_24_temporal_epoch_12, queue_settings: {
               e2e_layernorm_3171.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3185.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3185.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3185.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3185.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_25_temporal_epoch_12, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3203: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3205.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3225.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3225.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3171.dc.multiply.9_0, e2e_layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q24_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_3239.dc.add.10_0, e2e_softmax_3259.dc.multiply.3_0, e2e_matmul_3263_0]
    -   execute: {graph_name: fwd_26_temporal_epoch_13, queue_settings: {
               e2e_layernorm_3225.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               e2e_buffer_0_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               dc.input_tensor.layernorm_3225.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3225.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_27_temporal_epoch_13, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               lc.input_tensor.layernorm_3239.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3239.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3239.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3239.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3257: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3259.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3225.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_3225.dc.subtract.1_layernorm_3225.dc.multiply.8_0]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q26_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_3293.dc.add.10_0, e2e_matmul_3296_0]
    -   execute: {graph_name: fwd_28_temporal_epoch_14, queue_settings: {
               e2e_layernorm_3239.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_softmax_3259.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               e2e_matmul_3263_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3279.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3279.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3279.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3279.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_29_temporal_epoch_14, queue_settings: {
               layer.12.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3293.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3293.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3293.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3293.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3239.dc.add.10_0, e2e_softmax_3259.dc.multiply.3_0, e2e_matmul_3263_0]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3347.dc.multiply.8_0, e2e_layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_30_temporal_epoch_15, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e_layernorm_3293.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               e2e_matmul_3296_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3311: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3313.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3333.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3333.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3333.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3333.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_31_temporal_epoch_15, queue_settings: {
               layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3347.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3347.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3347.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3347.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3293.dc.add.10_0, e2e_matmul_3296_0]
    -   varinst: [$gptr_q28_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_3396_0, e2e_buffer_0_layernorm_3387.dc.add.10_add_3400_0]
    -   execute: {graph_name: fwd_32_temporal_epoch_16, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e_layernorm_3347.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               e2e_layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               lc.input_tensor.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3365: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3367.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3387.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3387.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_33_temporal_epoch_16, queue_settings: {
               dc.input_tensor.layernorm_3387.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3387.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3347.dc.multiply.8_0, e2e_layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q30_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3441.dc.add.10_0, e2e_matmul_3444_0]
    -   execute: {graph_name: fwd_34_temporal_epoch_17, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e_matmul_3396_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               e2e_buffer_0_layernorm_3387.dc.add.10_add_3400_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               lc.input_tensor.layernorm_3401.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3401.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3401.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3401.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3419: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3421.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_35_temporal_epoch_17, queue_settings: {
               layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3441.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3441.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3441.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3441.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_3396_0, e2e_buffer_0_layernorm_3387.dc.add.10_add_3400_0]
    -   varinst: [$gptr_q32_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3495.dc.multiply.9_0, e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_36_temporal_epoch_18, queue_settings: {
               e2e_layernorm_3441.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e_matmul_3444_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               layer.15.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3455.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3455.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3455.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3455.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_37_temporal_epoch_18, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3473: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3475.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3495.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3495.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3495.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3495.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3441.dc.add.10_0, e2e_matmul_3444_0]
    -   varinst: [$gptr_q34, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q35_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_3549.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8_0]
    -   execute: {graph_name: fwd_38_temporal_epoch_19, queue_settings: {
               e2e_layernorm_3495.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3509.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3509.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3509.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3509.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_39_temporal_epoch_19, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               lc.input_tensor.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3527: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3529.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3549.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3549.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3495.dc.multiply.9_0, e2e_layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q36, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q37_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_3563.dc.add.10_0, e2e_softmax_3583.dc.multiply.3_0, e2e_matmul_3587_0]
    -   execute: {graph_name: fwd_40_temporal_epoch_20, queue_settings: {
               e2e_layernorm_3549.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e_buffer_0_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               dc.input_tensor.layernorm_3549.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3549.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_41_temporal_epoch_20, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               lc.input_tensor.layernorm_3563.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3563.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3563.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3563.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3581: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3583.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3549.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_3549.dc.subtract.1_layernorm_3549.dc.multiply.8_0]
    -   varinst: [$gptr_q38, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q39_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_3617.dc.add.10_0, e2e_matmul_3620_0]
    -   execute: {graph_name: fwd_42_temporal_epoch_21, queue_settings: {
               e2e_layernorm_3563.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e_softmax_3583.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e_matmul_3587_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3603.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3603.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3603.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3603.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_43_temporal_epoch_21, queue_settings: {
               layer.18.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3617.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3617.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3617.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3617.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3563.dc.add.10_0, e2e_softmax_3583.dc.multiply.3_0, e2e_matmul_3587_0]
    -   varinst: [$gptr_q40, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3671.dc.multiply.8_0, e2e_layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_44_temporal_epoch_22, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               e2e_layernorm_3617.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e_matmul_3620_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3635: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3637.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3657.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3657.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3657.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3657.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_45_temporal_epoch_22, queue_settings: {
               layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3671.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3671.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3671.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3671.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3617.dc.add.10_0, e2e_matmul_3620_0]
    -   varinst: [$gptr_q41_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q42, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_matmul_3720_0, e2e_buffer_0_layernorm_3711.dc.add.10_add_3724_0]
    -   execute: {graph_name: fwd_46_temporal_epoch_23, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               e2e_layernorm_3671.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               e2e_layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               lc.input_tensor.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3689: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3691.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3711.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3711.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_47_temporal_epoch_23, queue_settings: {
               dc.input_tensor.layernorm_3711.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3711.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3671.dc.multiply.8_0, e2e_layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q43_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q44, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3765.dc.add.10_0, e2e_matmul_3768_0]
    -   execute: {graph_name: fwd_48_temporal_epoch_24, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               e2e_matmul_3720_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e_buffer_0_layernorm_3711.dc.add.10_add_3724_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               lc.input_tensor.layernorm_3725.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3725.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3725.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3725.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3743: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3745.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_49_temporal_epoch_24, queue_settings: {
               layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3765.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3765.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3765.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3765.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_3720_0, e2e_buffer_0_layernorm_3711.dc.add.10_add_3724_0]
    -   varinst: [$gptr_q45_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_layernorm_3819.dc.multiply.9_0, e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   execute: {graph_name: fwd_50_temporal_epoch_25, queue_settings: {
               e2e_layernorm_3765.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               e2e_matmul_3768_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               layer.21.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3779.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3779.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3779.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3779.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_51_temporal_epoch_25, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3797: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3799.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3819.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3819.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3819.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3819.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3765.dc.add.10_0, e2e_matmul_3768_0]
    -   varinst: [$gptr_q47, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q48_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_3873.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8_0]
    -   execute: {graph_name: fwd_52_temporal_epoch_26, queue_settings: {
               e2e_layernorm_3819.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49},
               layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3833.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3833.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3833.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3833.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_53_temporal_epoch_26, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q50, rd_ptr_global: $gptr_q50},
               lc.input_tensor.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               constant_1_multiply_3851: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_3853.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3873.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3873.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3819.dc.multiply.9_0, e2e_layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q49, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q49, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q50, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q50, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_54_temporal_epoch_27, queue_settings: {
               e2e_layernorm_3873.dc.reduce_avg.3.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               e2e_buffer_0_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q51, rd_ptr_global: $gptr_q51},
               dc.input_tensor.layernorm_3873.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3873.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.intermediate.dense.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_55_temporal_epoch_27, queue_settings: {
               lc.input_tensor.layernorm_3887.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3887.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_3887.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_3887.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_3873.dc.reduce_avg.3.lc1_0, e2e_buffer_0_layernorm_3873.dc.subtract.1_layernorm_3873.dc.multiply.8_0]
    -   varinst: [$gptr_q51, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q51, incwrap, $c_microbatch_size, 4]
    - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 2.0
