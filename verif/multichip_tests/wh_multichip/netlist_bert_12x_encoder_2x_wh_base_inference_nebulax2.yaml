# git checkout 20a214a8
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole-chip2-enc12-base]

devices:
  arch: [wormhole, wormhole_b0]

queues:

  # input
  hidden_states:                                                                {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30000000]]}
  attention_mask:                                                               {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 4], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300c3020]]}

  # output
  bert_encoder.output_layernorm_1907:                                           {input: layernorm_1907.dc.add.10_output_nop_0, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7b59a60], [4, 0x7bdfb60]]}
  layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8558ce0], [4, 0x8398680]]}
  layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x402fc60], [0, 0x4028ac0]]}
  layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4268880], [2, 0x4417260]]}
  layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40baee0], [1, 0x426ea20]]}
  layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x441d400], [3, 0x855f6c0]]}
  layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x839e820], [5, 0x40c28c0]]}
  layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x414d300], [1, 0x4300e40]]}
  layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4306fe0]]}
  layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x85660a0]]}
  layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8432ce0], [5, 0x4155520], [0, 0x4153ce0], [1, 0x4313300], [2, 0x4424620], [3, 0x85723c0], [4, 0x84c5100], [5, 0x41e7940]]}
  layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x41e6100], [1, 0x43a5720], [2, 0x44b6a40], [3, 0x86047e0], [4, 0x8557520], [5, 0x4279d60], [0, 0x41ec2a0], [1, 0x43ab8c0]]}
  layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x44bcbe0], [3, 0x860a980], [4, 0x855d6c0], [5, 0x427ff00], [0, 0x41f2440], [1, 0x43b1a60], [2, 0x454f000], [3, 0x869cda0]]}
  layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x85efae0], [5, 0x4312320], [0, 0x4284860], [1, 0x4443e80], [2, 0x45e1420], [3, 0x872f1c0], [4, 0x85f1360], [5, 0x4313ba0]]}
  layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e50d60]]}
  layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4121ee0]]}
  layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x82fb6c0], [4, 0x813a020]]}
  layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3dd6f60], [0, 0x3e5d080]]}
  layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x400b260], [2, 0x412e200]]}
  layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x838dae0], [4, 0x81cc440]]}
  layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x409d680], [2, 0x41c0620]]}
  layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8393c80], [4, 0x81d25e0]]}
  layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ddd940], [0, 0x3e63a60]]}
  layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x412faa0], [2, 0x4252a40]]}
  layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41c7840]]}
  layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81f6720]]}
  layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f97660], [0, 0x3f91d20], [1, 0x3f8b380], [2, 0x41d3b60], [3, 0x843c680], [4, 0x8202a40], [5, 0x4029a80], [0, 0x4024140]]}
  layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x401d7a0], [2, 0x4265f80], [3, 0x84ceaa0], [4, 0x8294e60], [5, 0x40bbea0], [0, 0x40b6560], [1, 0x4023940], [2, 0x426c120]]}
  layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x84d4c40], [4, 0x829b000], [5, 0x40c2040], [0, 0x40bc700], [1, 0x4029ae0], [2, 0x42722c0], [3, 0x8567060], [4, 0x832d420]]}
  layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4154460], [0, 0x414eb20], [1, 0x40bbf00], [2, 0x43046e0], [3, 0x85f9480], [4, 0x83bf840], [5, 0x4155ce0], [0, 0x41503a0]]}
  layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4151c20]]}
  layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x43067a0]]}
  layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x85fcda0], [4, 0x83c1900]]}
  layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4157da0], [0, 0x415df40]]}
  layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8163280], [5, 0x3f02960]]}
  layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f71460], [2, 0x41b9c40]]}
  layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e6a3a0], [0, 0x3ef6640]]}
  layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f77600], [2, 0x41bfde0]]}
  layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x838f340], [4, 0x80d0e60]]}
  layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3efc7c0], [0, 0x3f88a60]]}
  layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7dfe0]]}
  layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8423800]]}
  layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x81d8780], [5, 0x3e6fd60], [0, 0x3ef5e80], [1, 0x4135c40], [2, 0x4258be0], [3, 0x839a660], [4, 0x826aba0], [5, 0x3f02180]]}
  layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3f882a0], [1, 0x41c8060], [2, 0x42eb000], [3, 0x842ca80], [4, 0x82fcfc0], [5, 0x3f945a0], [0, 0x3f8e440], [1, 0x41ce200]]}
  layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x42f11a0], [3, 0x8432c20], [4, 0x8303160], [5, 0x3f9a740], [0, 0x3f945e0], [1, 0x41d43a0], [2, 0x43835c0], [3, 0x84c5040]]}
  layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8395580], [5, 0x402cb60], [0, 0x4026a00], [1, 0x42667c0], [2, 0x44159e0], [3, 0x8557460], [4, 0x8396e00], [5, 0x402e3e0]]}
  layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8978380]]}
  layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x461c4c0]]}
  layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x47cb6e0], [2, 0x4975fc0]]}
  layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8ab8280], [4, 0x89846a0]]}
  layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x469fd20], [0, 0x46287e0]]}
  layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x485db00], [2, 0x4a083e0]]}
  layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x898a840], [5, 0x4732140]]}
  layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47c4da0], [0, 0x46c7760]]}
  layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8abf4a0], [4, 0x8a1ed00]]}
  layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x486ffc0], [2, 0x4a0edc0]]}
  layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4863ca0]]}
  layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x46bac00]]}
  layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4445700], [2, 0x45e2ca0], [3, 0x8730a40], [4, 0x85f2be0], [5, 0x4315420], [0, 0x4286920], [1, 0x44d7b20], [2, 0x46750c0]]}
  layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x87c2e60], [4, 0x8685000], [5, 0x43a7840], [0, 0x4318d40], [1, 0x4569f40], [2, 0x47074e0], [3, 0x87c9000], [4, 0x868b1a0]]}
  layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x43ad9e0], [0, 0x431eee0], [1, 0x45700e0], [2, 0x470d680], [3, 0x87cf1a0], [4, 0x8691340], [5, 0x443fe00], [0, 0x43b1300]]}
  layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4602500], [2, 0x479faa0], [3, 0x88615c0], [4, 0x8723760], [5, 0x44d2220], [0, 0x4443720], [1, 0x4603d80], [2, 0x47a1320]]}
  layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x842fb20]]}
  layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4570900]]}
  layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8867f40], [4, 0x86b1340]]}
  layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4325060], [0, 0x42aaa60]]}
  layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x43227e0], [2, 0x457cc20]]}
  layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88fa360], [4, 0x8743760]]}
  layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x43b4c00], [2, 0x460f040]]}
  layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8900500], [4, 0x8749900]]}
  layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x432ba40], [0, 0x42b1440]]}
  layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42a48c0], [1, 0x431c640]]}
  layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4447860]]}
  layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8906ee0]]}
  layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8751b40], [5, 0x43be6a0], [0, 0x43440a0], [1, 0x4453b80], [2, 0x46a24e0], [3, 0x8913200], [4, 0x87e3f60], [5, 0x4450ac0]]}
  layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87b7320], [4, 0x857a620], [5, 0x41f3ca0], [0, 0x4205920], [1, 0x41e5940], [2, 0x44456e0], [3, 0x87bd4c0], [4, 0x85807c0]]}
  layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x868f1c0], [4, 0x8453d20], [5, 0x415df40], [0, 0x41640e0], [1, 0x4150c20], [2, 0x43a4ee0], [3, 0x87215e0], [4, 0x84e6140]]}
  layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41f0360], [0, 0x41f6500], [1, 0x41e3040], [2, 0x4437300], [3, 0x87b3a00], [4, 0x8578560], [5, 0x41f1be0], [0, 0x41f7d80]]}
  layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41f9600]]}
  layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44393c0]]}
  layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40be800], [2, 0x4312ac0]]}
  layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41f9e40], [0, 0x420bac0]]}
  layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41ebae0], [2, 0x444b880]]}
  layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87c3660], [4, 0x8586960]]}
  layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x44d3aa0], [0, 0x4444fa0]]}
  layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4605600], [2, 0x47a2ba0]]}
  layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8863680], [4, 0x8725820]]}
  layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4565ec0], [0, 0x44d73c0]]}
  layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x44dd560]]}
  layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x47aade0]]}
  layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x88f7b40], [4, 0x87b8480], [5, 0x456c8a0], [0, 0x44e9880], [1, 0x460c820], [2, 0x47b7100], [3, 0x8989f60], [4, 0x884a8a0]]}
  layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x45fecc0], [0, 0x457bca0], [1, 0x469ec40], [2, 0x4849520], [3, 0x8a1c380], [4, 0x88dccc0], [5, 0x4604e60], [0, 0x4581e40]]}
  layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x46a4de0], [2, 0x484f6c0], [3, 0x8a22520], [4, 0x88e2e60], [5, 0x460b000], [0, 0x4587fe0], [1, 0x4737200], [2, 0x48e1ae0]]}
  layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8ab4940], [4, 0x8975280], [5, 0x469d420], [0, 0x461a400], [1, 0x47c9620], [2, 0x4973f00], [3, 0x8ab61c0], [4, 0x8976b00]]}
  layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3987460]]}
  layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7bebe80]]}
  layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7c71f80], [5, 0x3a1d960]]}
  layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x397a960], [1, 0x3993780]]}
  layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3976860], [3, 0x7bf81a0]]}
  layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d043a0], [5, 0x3aafd80]]}
  layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3a08c80], [3, 0x7c8a5c0]]}
  layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d0a540], [5, 0x3ab5f20]]}
  layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3981340], [1, 0x399b9c0]]}
  layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3a9b0a0], [3, 0x7d1c9e0]]}
  layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x427df00]]}
  layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87c9800]]}
  layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x858cb00], [5, 0x4200820], [0, 0x42124a0], [1, 0x428a220], [2, 0x44de4e0], [3, 0x87d5b20], [4, 0x861ef20], [5, 0x4292c40]]}
  layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3dc9340], [0, 0x3e555e0], [1, 0x3e43160], [2, 0x408d1a0], [3, 0x82f2c00], [4, 0x8028c40], [5, 0x3dcf4e0], [0, 0x3e5b780]]}
  layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3b2bc00], [3, 0x7d0fe80], [4, 0x7bf1800], [5, 0x3a232e0], [0, 0x3a18860], [1, 0x3a18860], [2, 0x3bbe020], [3, 0x7da22a0]]}
  layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c83c20], [5, 0x3ab5700], [0, 0x3aaac80], [1, 0x3aaac80], [2, 0x3c50440], [3, 0x7e346c0], [4, 0x7c854a0], [5, 0x3ab6f80]]}
  layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3ab8800]]}
  layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3aacd40]]}
  layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3986440], [1, 0x3986440]]}
  layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c87560], [5, 0x3ac4b20]]}
  layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3aad580], [1, 0x3ab9060]]}
  layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ce6180], [3, 0x7ec8ba0]]}
  layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3b3f9a0], [1, 0x3b4b480]]}
  layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cec320], [3, 0x7eced40]]}
  layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c8df40], [5, 0x3acb500]]}
  layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3bd1dc0], [1, 0x3bdd8a0]]}
  layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100]]}
  layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da940]]}
  layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38dc1a0], [3, 0x7ab4940], [4, 0x7ab4940], [5, 0x38e6420], [0, 0x38e6c60], [1, 0x38e6c60], [2, 0x396e5c0], [3, 0x7b46d60]]}
  layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b46d60], [5, 0x3978840], [0, 0x3979080], [1, 0x3979080], [2, 0x3a009e0], [3, 0x7bd9180], [4, 0x7b4cf00], [5, 0x397e9e0]]}
  layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3a2dde0], [2, 0x3aa1240], [3, 0x7d22b80], [4, 0x7d10f20], [5, 0x3abc900], [0, 0x3a15800], [1, 0x3ac0200], [2, 0x3b33660]]}
  layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7db4fa0], [4, 0x7da3340], [5, 0x3b4ed20], [0, 0x3aa7c20], [1, 0x3b52620], [2, 0x3bc5a80], [3, 0x7db6820], [4, 0x7da4bc0]]}
  layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ab4100]]}
  layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x38da100]]}
  layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38da940], [1, 0x38e6420]]}
  layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x38da940], [3, 0x7ac0420]]}
  layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ab4940], [5, 0x38e6420]]}
  layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x396cd60], [1, 0x3978840]]}
  layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7b46d60], [5, 0x3978840]]}
  layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3972f00], [1, 0x397e9e0]]}
  layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x38e1320], [3, 0x7ac6e00]]}
  layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bd9180], [5, 0x3a0ac60]]}
  layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3a10e00]]}
  layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x38da100]]}
  layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3e324c0], [3, 0x8014ee0], [4, 0x7ee6c80], [5, 0x3c10e80], [0, 0x3c7f9e0], [1, 0x3db8f00], [2, 0x3ec48e0], [3, 0x80a7300]]}
  layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7e402c0], [5, 0x3b6a4c0], [0, 0x3b50ea0], [1, 0x3c8bc20], [2, 0x3d049e0], [3, 0x7ee7400], [4, 0x7e46460], [5, 0x3b70660]]}
  layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b57040], [1, 0x3c91dc0], [2, 0x3d0ab80], [3, 0x7eed5a0], [4, 0x7e4c600], [5, 0x3b76800], [0, 0x3be9460], [1, 0x3d241e0]]}
  layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3d9cfa0], [3, 0x7f7f9c0], [4, 0x7edea20], [5, 0x3c08c20], [0, 0x3c7b880], [1, 0x3db6600], [2, 0x3d9e820], [3, 0x7f81240]]}
  layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100]]}
  layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bdf320]]}
  layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b530a0], [5, 0x3984b80]]}
  layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x397fa60], [1, 0x397fa60]]}
  layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a073c0], [3, 0x7beb640]]}
  layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7be54c0], [5, 0x3a16fa0]]}
  layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a997e0], [3, 0x7c7da60]]}
  layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7beb660], [5, 0x3a1d140]]}
  layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c53d60], [3, 0x7e36780]]}
  layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d30540], [0, 0x3db0d00]]}
  layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3db6ea0]]}
  layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3feea60]]}
  layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81ce3c0], [4, 0x7f04400], [5, 0x3d36f20], [0, 0x3dc31c0], [1, 0x3db0d40], [2, 0x3ffad80], [3, 0x82607e0], [4, 0x7f96820]]}
  layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81bffe0], [4, 0x7ef7880], [5, 0x3d2a3a0], [0, 0x3daab60], [1, 0x3da9b20], [2, 0x3fe8080], [3, 0x81c6180], [4, 0x7efda20]]}
  layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e49300], [2, 0x4093340], [3, 0x82f8da0], [4, 0x802ede0], [5, 0x3dd5680], [0, 0x3e61920], [1, 0x3edb720], [2, 0x4125760]]}
  layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x838b1c0], [4, 0x80c1200], [5, 0x3e67aa0], [0, 0x3ef3d40], [1, 0x3f6db40], [2, 0x41b7b80], [3, 0x838ca40], [4, 0x80c2a80]]}
  layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x80c4300]]}
  layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ee94a0]]}
  layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7db2780], [5, 0x3befd40]]}
  layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c6a380], [1, 0x3c75e60]]}
  layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e16d00], [3, 0x7ff9720]]}
  layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e44ba0], [5, 0x3c82160]]}
  layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ea9120], [3, 0x808bb40]]}
  layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7e4ad40], [5, 0x3c88300]]}
  layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3da00a0], [3, 0x7f82ac0]]}
  layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ee0ae0], [5, 0x3c0ace0]]}
  layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3d11e00]]}
  layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f56d00]]}
  layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8139720], [4, 0x7f798e0], [5, 0x3ca3ae0], [0, 0x3d1e120], [1, 0x3e4bb60], [2, 0x3f63020], [3, 0x81cbb40], [4, 0x800bd00]]}
  layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d35f00], [0, 0x3db0540], [1, 0x3eddf80], [2, 0x3ff5440], [3, 0x825df60], [4, 0x809e120], [5, 0x3d3c0a0], [0, 0x3db66e0]]}
  layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3ee4120], [2, 0x3ffb5e0], [3, 0x8264100], [4, 0x80a42c0], [5, 0x3d42240], [0, 0x3dbc880], [1, 0x3f76540], [2, 0x408da00]]}
  layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x82f6520], [4, 0x81366e0], [5, 0x3dd4660], [0, 0x3e4eca0], [1, 0x4008960], [2, 0x411fe20], [3, 0x82f7da0], [4, 0x8137f60]]}
  layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b50de0]]}
  layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3be62c0]]}
  layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3c59720], [3, 0x7db88e0]]}
  layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7da6c80], [5, 0x3b5d100]]}
  layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3aaa520], [1, 0x3bf25e0]]}
  layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3cebb40], [3, 0x7e4ad00]]}
  layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3b53ea0], [2, 0x3bc7300]]}
  layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c84a00], [2, 0x3cf1ce0]]}
  layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7e50ea0], [4, 0x7dad660]]}
  layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b63ae0], [0, 0x3b3e9e0]]}
  layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b44b80]]}
  layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3cf86c0]]}
  layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c70d60], [1, 0x3c7c840], [2, 0x3f3b540], [3, 0x811df60], [4, 0x7e50ee0], [5, 0x3c8e4a0], [0, 0x3d03180], [1, 0x3d0ec60]]}
  layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fcd960], [3, 0x81b0380], [4, 0x7ee3300], [5, 0x3d208c0], [0, 0x3d955a0], [1, 0x3da1080], [2, 0x3fd3b00], [3, 0x81b6520]]}
  layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [12, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3cf24c0], [3, 0x7ed4ee0], [4, 0x7d20360], [5, 0x3b5d920], [0, 0x3bd7f60], [1, 0x3be3a40], [2, 0x3d848e0], [3, 0x7f67300]]}
  layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d26a60], [0, 0x3d9b740], [1, 0x3da7220], [2, 0x3fd9ca0], [3, 0x81bc6c0], [4, 0x7ef57c0], [5, 0x3d282e0], [0, 0x3d9cfc0]]}
  layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3d9e840]]}
  layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fdbd60]]}

  # constant
  input_1_multiply_1288_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x855ee80]]}
  lc.input_tensor.softmax_1290.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x40c2080]]}
  lc.input_tensor.layernorm_1310.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4268040]]}
  lc.input_tensor.layernorm_1310.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8565860]]}
  dc.input_tensor.layernorm_1310.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8430c40]]}
  lc.input_tensor.layernorm_1310.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4154ce0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x41534a0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4423de0]]}
  lc.input_tensor.layernorm_1324.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ddd100]]}
  lc.input_tensor.layernorm_1324.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x41216a0]]}
  dc.input_tensor.layernorm_1324.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x82f9620]]}
  lc.input_tensor.layernorm_1324.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x81397e0]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3dd6720]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x400aa20]]}
  input_1_multiply_1341_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x400a1e0]]}
  lc.input_tensor.softmax_1343.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e63220]]}
  lc.input_tensor.layernorm_1363.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8399e20]]}
  lc.input_tensor.layernorm_1363.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4447020]]}
  dc.input_tensor.layernorm_1363.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f955c0]]}
  lc.input_tensor.layernorm_1363.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3f914e0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f8ab40]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x843be40]]}
  lc.input_tensor.layernorm_1377.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81f5ee0]]}
  lc.input_tensor.layernorm_1377.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4305f60]]}
  dc.input_tensor.layernorm_1377.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x85fad00]]}
  lc.input_tensor.layernorm_1377.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x83c10c0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4157560]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40bdfc0]]}
  input_1_multiply_1394_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x838eb00]]}
  lc.input_tensor.softmax_1396.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x80d0620]]}
  lc.input_tensor.layernorm_1416.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f7d7a0]]}
  lc.input_tensor.layernorm_1416.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41c5f80]]}
  dc.input_tensor.layernorm_1416.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8421760]]}
  lc.input_tensor.layernorm_1416.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ef5e00]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3f8ec00]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41c67c0]]}
  lc.input_tensor.layernorm_1430.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4028280]]}
  lc.input_tensor.layernorm_1430.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x47caea0]]}
  dc.input_tensor.layernorm_1430.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x47a8d40]]}
  lc.input_tensor.layernorm_1430.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4975780]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8ab7a40]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x469f4e0]]}
  input_1_multiply_1447_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8abe420]]}
  lc.input_tensor.softmax_1449.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4876160]]}
  lc.input_tensor.layernorm_1469.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x46c6f20]]}
  lc.input_tensor.layernorm_1469.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47c4560]]}
  dc.input_tensor.layernorm_1469.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8a1cc60]]}
  lc.input_tensor.layernorm_1469.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8abec60]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4a0e580]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x461bc80]]}
  lc.input_tensor.layernorm_1483.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81f56a0]]}
  lc.input_tensor.layernorm_1483.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f94d80]]}
  dc.input_tensor.layernorm_1483.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3f8f440]]}
  lc.input_tensor.layernorm_1483.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f8a300]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41c7000]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40bd780]]}
  input_1_multiply_1500_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x432b200]]}
  lc.input_tensor.softmax_1502.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42b0c00]]}
  lc.input_tensor.layernorm_1522.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x46a1460]]}
  lc.input_tensor.layernorm_1522.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x89066a0]]}
  dc.input_tensor.layernorm_1522.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x874faa0]]}
  lc.input_tensor.layernorm_1522.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x43bde60]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4343860]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x46a1ca0]]}
  lc.input_tensor.layernorm_1536.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41e48c0]]}
  lc.input_tensor.layernorm_1536.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4438b80]]}
  dc.input_tensor.layernorm_1536.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87b5280]]}
  lc.input_tensor.layernorm_1536.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8579de0]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41f3460]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41e5100]]}
  input_1_multiply_1553_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8862e40]]}
  lc.input_tensor.softmax_1555.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8724fe0]]}
  lc.input_tensor.layernorm_1575.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x460b7a0]]}
  lc.input_tensor.layernorm_1575.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x42860e0]]}
  dc.input_tensor.layernorm_1575.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x88f5aa0]]}
  lc.input_tensor.layernorm_1575.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x87b7c40]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x456c060]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x460bfe0]]}
  lc.input_tensor.layernorm_1589.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x44235a0]]}
  lc.input_tensor.layernorm_1589.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x469eca0]]}
  dc.input_tensor.layernorm_1589.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x39853c0]]}
  lc.input_tensor.layernorm_1589.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3a1d120]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x397a120]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3976020]]}
  input_1_multiply_1606_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3980b00]]}
  lc.input_tensor.softmax_1608.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x39757e0]]}
  lc.input_tensor.layernorm_1628.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d106e0]]}
  lc.input_tensor.layernorm_1628.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3abc0c0]]}
  dc.input_tensor.layernorm_1628.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a13760]]}
  lc.input_tensor.layernorm_1628.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41fffe0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4211c60]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44ddca0]]}
  lc.input_tensor.layernorm_1642.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3aac500]]}
  lc.input_tensor.layernorm_1642.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3aac500]]}
  dc.input_tensor.layernorm_1642.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c51cc0]]}
  lc.input_tensor.layernorm_1642.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e35f40]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c86d20]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3aacd40]]}
  input_1_multiply_1659_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c8d700]]}
  lc.input_tensor.softmax_1661.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3acacc0]]}
  lc.input_tensor.layernorm_1681.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x397f220]]}
  lc.input_tensor.layernorm_1681.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da100]]}
  dc.input_tensor.layernorm_1681.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100]]}
  lc.input_tensor.layernorm_1681.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38e6420]]}
  lc.input_tensor.layernorm_1695.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b505a0]]}
  lc.input_tensor.layernorm_1695.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3aa94a0]]}
  dc.input_tensor.layernorm_1695.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3999920]]}
  lc.input_tensor.layernorm_1695.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x38da100]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ab4100]]}
  input_1_multiply_1712_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x38e0ae0]]}
  lc.input_tensor.softmax_1714.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ac65c0]]}
  lc.input_tensor.layernorm_1734.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x39790a0]]}
  lc.input_tensor.layernorm_1734.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3984b80]]}
  dc.input_tensor.layernorm_1734.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3973740]]}
  lc.input_tensor.layernorm_1734.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7b59220]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bdf320]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x39798e0]]}
  lc.input_tensor.layernorm_1748.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ee02a0]]}
  lc.input_tensor.layernorm_1748.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3c0a4a0]]}
  dc.input_tensor.layernorm_1748.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3c7d100]]}
  lc.input_tensor.layernorm_1748.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3db7e80]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x397f220]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a06b80]]}
  input_1_multiply_1765_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3985c00]]}
  lc.input_tensor.softmax_1767.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3985c00]]}
  lc.input_tensor.layernorm_1787.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3dafcc0]]}
  lc.input_tensor.layernorm_1787.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fee220]]}
  dc.input_tensor.layernorm_1787.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81cc320]]}
  lc.input_tensor.layernorm_1787.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f03bc0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d366e0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3db0500]]}
  lc.input_tensor.layernorm_1801.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e69320]]}
  lc.input_tensor.layernorm_1801.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ef55c0]]}
  dc.input_tensor.layernorm_1801.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f6f3c0]]}
  lc.input_tensor.layernorm_1801.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41b9400]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x838e2c0]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e69b60]]}
  input_1_multiply_1818_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c70520]]}
  lc.input_tensor.softmax_1820.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c7c000]]}
  lc.input_tensor.layernorm_1840.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3c7f1a0]]}
  lc.input_tensor.layernorm_1840.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3db86c0]]}
  dc.input_tensor.layernorm_1840.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ee5360]]}
  lc.input_tensor.layernorm_1840.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f790a0]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ca32a0]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3e4b320]]}
  lc.input_tensor.layernorm_1854.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3dd5ee0]]}
  lc.input_tensor.layernorm_1854.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e50520]]}
  dc.input_tensor.layernorm_1854.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b3c940]]}
  lc.input_tensor.layernorm_1854.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7db80a0]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7da6440]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3aa9ce0]]}
  input_1_multiply_1871_tile_bcast_tile_bcast:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7dace20]]}
  lc.input_tensor.softmax_1873.dc.reduce_sum.1.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b632a0]]}
  lc.input_tensor.layernorm_1893.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c8aba0]]}
  lc.input_tensor.layernorm_1893.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3cf7e80]]}
  dc.input_tensor.layernorm_1893.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ee32c0]]}
  lc.input_tensor.layernorm_1893.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7e3fa80]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b69c80]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c8b3e0]]}
  lc.input_tensor.layernorm_1907.dc.reduce_avg.0.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3da8aa0]]}
  lc.input_tensor.layernorm_1907.dc.reduce_avg.3.0:                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3fdb520]]}
  dc.input_tensor.layernorm_1907.4:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81bdf40]]}
  lc.input_tensor.layernorm_1907.dc.reciprocal.7_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ef7040]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3d29b60]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3da92e0]]}

  # epoch_to_epoch
  e2e_layernorm_1416.dc.add.10_0:                                               {input: layernorm_1416.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300e3840]]}
  e2e_matmul_1539_0:                                                            {input: matmul_1539, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x301a6880], [0, 0x301d74a0]]}
  e2e_matmul_1545_0:                                                            {input: matmul_1545, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x302080c0], [0, 0x30238ce0]]}
  e2e_layernorm_1536.dc.add.10_0:                                               {input: layernorm_1536.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30145060]]}
  e2e_gelu_1687_0:                                                              {input: gelu_1687, type: queue, entries: 2, grid_size: [1, 2], t: 1, mblock: [2, 12], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x302cb120], [0, 0x3038e140]]}
  e2e_layernorm_1681.dc.add.10_0:                                               {input: layernorm_1681.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30269900]]}
  e2e_matmul_1831_0:                                                            {input: matmul_1831, type: queue, entries: 2, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30145060]]}
  e2e_layernorm_1801.dc.add.10_0:                                               {input: layernorm_1801.dc.add.10, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300e3840]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 1
    input_count: 2
    matmul_1274: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1280: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1286: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_1274, matmul_1280],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1288: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [matmul_1286, input_1_multiply_1288_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1289: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_1288, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1290.dc.exp.0: {type: exp, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_1289],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1290.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_1290.dc.exp.0, lc.input_tensor.softmax_1290.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1290.dc.reciprocal.2: {type: reciprocal, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_1290.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1290.dc.multiply.3: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_1290.dc.exp.0, softmax_1290.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1294: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [hidden_states, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1301: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_1290.dc.multiply.3, matmul_1294],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1305: {type: matmul, grid_loc: [1, 6], grid_size: [1, 2], inputs: [matmul_1301, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1309: {type: add, grid_loc: [2, 0], grid_size: [1, 1], inputs: [matmul_1305, hidden_states],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1310.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [add_1309, lc.input_tensor.layernorm_1310.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1310.dc.subtract.1: {type: subtract, grid_loc: [2, 2], grid_size: [1, 1], inputs: [add_1309, layernorm_1310.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1310.dc.multiply.2: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_1310.dc.subtract.1, layernorm_1310.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1310.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_1310.dc.multiply.2, lc.input_tensor.layernorm_1310.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1310.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_1310.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1310.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1310.dc.sqrt.6: {type: sqrt, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_1310.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1310.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_1310.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1310.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_1310.dc.reciprocal.7, lc.input_tensor.layernorm_1310.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1310.dc.multiply.8: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_1310.dc.subtract.1, layernorm_1310.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1310.dc.multiply.9: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_1310.dc.multiply.8, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1310.dc.add.10: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_1310.dc.multiply.9, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1313: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [layernorm_1310.dc.add.10, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1316: {type: gelu, grid_loc: [3, 6], grid_size: [1, 2], inputs: [matmul_1313],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1319: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_1316, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1323: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_1319, layernorm_1310.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1324.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_1323, lc.input_tensor.layernorm_1324.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1324.dc.subtract.1: {type: subtract, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_1323, layernorm_1324.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1324.dc.multiply.2: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_1324.dc.subtract.1, layernorm_1324.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1324.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_1324.dc.multiply.2, lc.input_tensor.layernorm_1324.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1324.dc.add.5: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_1324.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1324.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1324.dc.sqrt.6: {type: sqrt, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_1324.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1324.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_1324.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1324.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_1324.dc.reciprocal.7, lc.input_tensor.layernorm_1324.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1324.dc.multiply.8: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_1324.dc.subtract.1, layernorm_1324.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1324.dc.multiply.9: {type: multiply, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_1324.dc.multiply.8, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1324.dc.add.10: {type: add, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_1324.dc.multiply.9, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1327: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [layernorm_1324.dc.add.10, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1333: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [layernorm_1324.dc.add.10, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1339: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [matmul_1327, matmul_1333],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1341: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [matmul_1339, input_1_multiply_1341_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1342: {type: add, grid_loc: [8, 4], grid_size: [1, 1], inputs: [multiply_1341, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1343.dc.exp.0: {type: exp, grid_loc: [8, 5], grid_size: [1, 1], inputs: [add_1342],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1343.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_1343.dc.exp.0, lc.input_tensor.softmax_1343.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1343.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_1343.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1343.dc.multiply.3: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_1343.dc.exp.0, softmax_1343.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1347: {type: matmul, grid_loc: [9, 1], grid_size: [1, 2], inputs: [layernorm_1324.dc.add.10, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1354: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [softmax_1343.dc.multiply.3, matmul_1347],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1358: {type: matmul, grid_loc: [9, 4], grid_size: [1, 2], inputs: [matmul_1354, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1362: {type: add, grid_loc: [9, 6], grid_size: [1, 1], inputs: [matmul_1358, layernorm_1324.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1363.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [add_1362, lc.input_tensor.layernorm_1363.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}

  fwd_1_temporal_epoch_0:
    target_device: 0
    input_count: 2
    layernorm_1363.dc.subtract.1: {type: subtract, grid_loc: [0, 0], grid_size: [1, 1], inputs: [add_1362, layernorm_1363.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1363.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_1363.dc.subtract.1, layernorm_1363.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1363.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_1363.dc.multiply.2, lc.input_tensor.layernorm_1363.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1363.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_1363.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1363.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1363.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_1363.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1363.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_1363.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1363.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_1363.dc.reciprocal.7, lc.input_tensor.layernorm_1363.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1363.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_1363.dc.subtract.1, layernorm_1363.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1363.dc.multiply.9: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_1363.dc.multiply.8, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1363.dc.add.10: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_1363.dc.multiply.9, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1366: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layernorm_1363.dc.add.10, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1369: {type: gelu, grid_loc: [1, 4], grid_size: [1, 2], inputs: [matmul_1366],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1372: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_1369, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1376: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_1372, layernorm_1363.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1377.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_1376, lc.input_tensor.layernorm_1377.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1377.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_1376, layernorm_1377.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1377.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_1377.dc.subtract.1, layernorm_1377.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1377.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_1377.dc.multiply.2, lc.input_tensor.layernorm_1377.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1377.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_1377.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1377.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1377.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_1377.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1377.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_1377.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1377.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_1377.dc.reciprocal.7, lc.input_tensor.layernorm_1377.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1377.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_1377.dc.subtract.1, layernorm_1377.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1377.dc.multiply.9: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_1377.dc.multiply.8, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1377.dc.add.10: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_1377.dc.multiply.9, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1380: {type: matmul, grid_loc: [5, 4], grid_size: [1, 2], inputs: [layernorm_1377.dc.add.10, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1386: {type: matmul, grid_loc: [5, 6], grid_size: [1, 2], inputs: [layernorm_1377.dc.add.10, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1392: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_1380, matmul_1386],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1394: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [matmul_1392, input_1_multiply_1394_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1395: {type: add, grid_loc: [6, 2], grid_size: [1, 1], inputs: [multiply_1394, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1396.dc.exp.0: {type: exp, grid_loc: [6, 3], grid_size: [1, 1], inputs: [add_1395],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1396.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_1396.dc.exp.0, lc.input_tensor.softmax_1396.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1396.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 5], grid_size: [1, 1], inputs: [softmax_1396.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1396.dc.multiply.3: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [softmax_1396.dc.exp.0, softmax_1396.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1400: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [layernorm_1377.dc.add.10, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1407: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_1396.dc.multiply.3, matmul_1400],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1411: {type: matmul, grid_loc: [7, 2], grid_size: [1, 2], inputs: [matmul_1407, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1415: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_1411, layernorm_1377.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1416.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [add_1415, lc.input_tensor.layernorm_1416.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1416.dc.subtract.1: {type: subtract, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_1415, layernorm_1416.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1416.dc.multiply.2: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_1416.dc.subtract.1, layernorm_1416.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1416.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_1416.dc.multiply.2, lc.input_tensor.layernorm_1416.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1416.dc.add.5: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_1416.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1416.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1416.dc.sqrt.6: {type: sqrt, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_1416.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1416.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_1416.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1416.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_1416.dc.reciprocal.7, lc.input_tensor.layernorm_1416.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1416.dc.multiply.8: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_1416.dc.subtract.1, layernorm_1416.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1416.dc.multiply.9: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_1416.dc.multiply.8, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1416.dc.add.10: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_1416.dc.multiply.9, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_2_temporal_epoch_1:
    target_device: 1
    input_count: 2
    matmul_1419: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_layernorm_1416.dc.add.10_0, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1422: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_1419],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1425: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_1422, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1429: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [matmul_1425, e2e_layernorm_1416.dc.add.10_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1430.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_1429, lc.input_tensor.layernorm_1430.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1430.dc.subtract.1: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_1429, layernorm_1430.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1430.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_1430.dc.subtract.1, layernorm_1430.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1430.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_1430.dc.multiply.2, lc.input_tensor.layernorm_1430.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1430.dc.add.5: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_1430.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1430.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1430.dc.sqrt.6: {type: sqrt, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_1430.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1430.dc.reciprocal.7: {type: reciprocal, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_1430.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1430.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_1430.dc.reciprocal.7, lc.input_tensor.layernorm_1430.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1430.dc.multiply.8: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_1430.dc.subtract.1, layernorm_1430.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1430.dc.multiply.9: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_1430.dc.multiply.8, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1430.dc.add.10: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_1430.dc.multiply.9, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1433: {type: matmul, grid_loc: [4, 0], grid_size: [1, 2], inputs: [layernorm_1430.dc.add.10, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1439: {type: matmul, grid_loc: [4, 2], grid_size: [1, 2], inputs: [layernorm_1430.dc.add.10, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1445: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [matmul_1433, matmul_1439],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1447: {type: multiply, grid_loc: [4, 5], grid_size: [1, 1], inputs: [matmul_1445, input_1_multiply_1447_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1448: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [multiply_1447, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1449.dc.exp.0: {type: exp, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_1448],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1449.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [softmax_1449.dc.exp.0, lc.input_tensor.softmax_1449.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1449.dc.reciprocal.2: {type: reciprocal, grid_loc: [5, 1], grid_size: [1, 1], inputs: [softmax_1449.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1449.dc.multiply.3: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [softmax_1449.dc.exp.0, softmax_1449.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1453: {type: matmul, grid_loc: [5, 3], grid_size: [1, 2], inputs: [layernorm_1430.dc.add.10, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1460: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [softmax_1449.dc.multiply.3, matmul_1453],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1464: {type: matmul, grid_loc: [5, 6], grid_size: [1, 2], inputs: [matmul_1460, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1468: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_1464, layernorm_1430.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1469.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [add_1468, lc.input_tensor.layernorm_1469.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1469.dc.subtract.1: {type: subtract, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_1468, layernorm_1469.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1469.dc.multiply.2: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_1469.dc.subtract.1, layernorm_1469.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1469.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_1469.dc.multiply.2, lc.input_tensor.layernorm_1469.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1469.dc.add.5: {type: add, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_1469.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1469.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1469.dc.sqrt.6: {type: sqrt, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_1469.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1469.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_1469.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1469.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_1469.dc.reciprocal.7, lc.input_tensor.layernorm_1469.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1469.dc.multiply.8: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_1469.dc.subtract.1, layernorm_1469.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1469.dc.multiply.9: {type: multiply, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_1469.dc.multiply.8, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1469.dc.add.10: {type: add, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_1469.dc.multiply.9, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1472: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [layernorm_1469.dc.add.10, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1475: {type: gelu, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_1472],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1478: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [gelu_1475, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}

  fwd_3_temporal_epoch_1:
    target_device: 0
    input_count: 2
    add_1482: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [matmul_1478, layernorm_1469.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1483.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_1482, lc.input_tensor.layernorm_1483.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1483.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [1, 1], inputs: [add_1482, layernorm_1483.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1483.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_1483.dc.subtract.1, layernorm_1483.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1483.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_1483.dc.multiply.2, lc.input_tensor.layernorm_1483.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1483.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_1483.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1483.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1483.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_1483.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1483.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_1483.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1483.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_1483.dc.reciprocal.7, lc.input_tensor.layernorm_1483.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1483.dc.multiply.8: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_1483.dc.subtract.1, layernorm_1483.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1483.dc.multiply.9: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_1483.dc.multiply.8, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1483.dc.add.10: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_1483.dc.multiply.9, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1486: {type: matmul, grid_loc: [1, 6], grid_size: [1, 2], inputs: [layernorm_1483.dc.add.10, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1492: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [layernorm_1483.dc.add.10, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1498: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_1486, matmul_1492],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1500: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [matmul_1498, input_1_multiply_1500_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1501: {type: add, grid_loc: [2, 4], grid_size: [1, 1], inputs: [multiply_1500, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1502.dc.exp.0: {type: exp, grid_loc: [2, 5], grid_size: [1, 1], inputs: [add_1501],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1502.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_1502.dc.exp.0, lc.input_tensor.softmax_1502.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1502.dc.reciprocal.2: {type: reciprocal, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_1502.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1502.dc.multiply.3: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [softmax_1502.dc.exp.0, softmax_1502.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1506: {type: matmul, grid_loc: [3, 1], grid_size: [1, 2], inputs: [layernorm_1483.dc.add.10, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1513: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [softmax_1502.dc.multiply.3, matmul_1506],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1517: {type: matmul, grid_loc: [3, 4], grid_size: [1, 2], inputs: [matmul_1513, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1521: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [matmul_1517, layernorm_1483.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1522.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [add_1521, lc.input_tensor.layernorm_1522.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1522.dc.subtract.1: {type: subtract, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_1521, layernorm_1522.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1522.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_1522.dc.subtract.1, layernorm_1522.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1522.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_1522.dc.multiply.2, lc.input_tensor.layernorm_1522.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1522.dc.add.5: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_1522.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1522.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1522.dc.sqrt.6: {type: sqrt, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_1522.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1522.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_1522.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1522.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_1522.dc.reciprocal.7, lc.input_tensor.layernorm_1522.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1522.dc.multiply.8: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_1522.dc.subtract.1, layernorm_1522.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1522.dc.multiply.9: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_1522.dc.multiply.8, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1522.dc.add.10: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_1522.dc.multiply.9, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1525: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [layernorm_1522.dc.add.10, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1528: {type: gelu, grid_loc: [5, 4], grid_size: [1, 2], inputs: [matmul_1525],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1531: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [gelu_1528, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1535: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [matmul_1531, layernorm_1522.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1536.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_1535, lc.input_tensor.layernorm_1536.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1536.dc.subtract.1: {type: subtract, grid_loc: [8, 0], grid_size: [1, 1], inputs: [add_1535, layernorm_1536.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1536.dc.multiply.2: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_1536.dc.subtract.1, layernorm_1536.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1536.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_1536.dc.multiply.2, lc.input_tensor.layernorm_1536.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1536.dc.add.5: {type: add, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_1536.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1536.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1536.dc.sqrt.6: {type: sqrt, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_1536.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1536.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_1536.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1536.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_1536.dc.reciprocal.7, lc.input_tensor.layernorm_1536.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1536.dc.multiply.8: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_1536.dc.subtract.1, layernorm_1536.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1536.dc.multiply.9: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_1536.dc.multiply.8, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1536.dc.add.10: {type: add, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1536.dc.multiply.9, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1539: {type: matmul, grid_loc: [9, 4], grid_size: [1, 2], inputs: [layernorm_1536.dc.add.10, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1545: {type: matmul, grid_loc: [9, 6], grid_size: [1, 2], inputs: [layernorm_1536.dc.add.10, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}

  fwd_4_temporal_epoch_2:
    target_device: 1
    input_count: 2
    matmul_1551: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_matmul_1539_0, e2e_matmul_1545_0],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1553: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [matmul_1551, input_1_multiply_1553_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1554: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [multiply_1553, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1555.dc.exp.0: {type: exp, grid_loc: [0, 3], grid_size: [1, 1], inputs: [add_1554],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1555.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [softmax_1555.dc.exp.0, lc.input_tensor.softmax_1555.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1555.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 5], grid_size: [1, 1], inputs: [softmax_1555.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1555.dc.multiply.3: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [softmax_1555.dc.exp.0, softmax_1555.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1559: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [e2e_layernorm_1536.dc.add.10_0, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1566: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [softmax_1555.dc.multiply.3, matmul_1559],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1570: {type: matmul, grid_loc: [1, 2], grid_size: [1, 2], inputs: [matmul_1566, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1574: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_1570, e2e_layernorm_1536.dc.add.10_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1575.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [add_1574, lc.input_tensor.layernorm_1575.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1575.dc.subtract.1: {type: subtract, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_1574, layernorm_1575.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1575.dc.multiply.2: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_1575.dc.subtract.1, layernorm_1575.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1575.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_1575.dc.multiply.2, lc.input_tensor.layernorm_1575.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1575.dc.add.5: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_1575.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1575.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1575.dc.sqrt.6: {type: sqrt, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_1575.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1575.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_1575.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1575.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_1575.dc.reciprocal.7, lc.input_tensor.layernorm_1575.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1575.dc.multiply.8: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_1575.dc.subtract.1, layernorm_1575.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1575.dc.multiply.9: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_1575.dc.multiply.8, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1575.dc.add.10: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_1575.dc.multiply.9, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1578: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [layernorm_1575.dc.add.10, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1581: {type: gelu, grid_loc: [3, 2], grid_size: [1, 2], inputs: [matmul_1578],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1584: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_1581, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1588: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_1584, layernorm_1575.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1589.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [add_1588, lc.input_tensor.layernorm_1589.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1589.dc.subtract.1: {type: subtract, grid_loc: [3, 6], grid_size: [1, 1], inputs: [add_1588, layernorm_1589.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1589.dc.multiply.2: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_1589.dc.subtract.1, layernorm_1589.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1589.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_1589.dc.multiply.2, lc.input_tensor.layernorm_1589.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1589.dc.add.5: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_1589.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1589.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1589.dc.sqrt.6: {type: sqrt, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_1589.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1589.dc.reciprocal.7: {type: reciprocal, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_1589.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1589.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_1589.dc.reciprocal.7, lc.input_tensor.layernorm_1589.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1589.dc.multiply.8: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_1589.dc.subtract.1, layernorm_1589.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1589.dc.multiply.9: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_1589.dc.multiply.8, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1589.dc.add.10: {type: add, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_1589.dc.multiply.9, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1592: {type: matmul, grid_loc: [7, 2], grid_size: [1, 2], inputs: [layernorm_1589.dc.add.10, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1598: {type: matmul, grid_loc: [7, 4], grid_size: [1, 2], inputs: [layernorm_1589.dc.add.10, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1604: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_1592, matmul_1598],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1606: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_1604, input_1_multiply_1606_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1607: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [multiply_1606, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1608.dc.exp.0: {type: exp, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_1607],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1608.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [softmax_1608.dc.exp.0, lc.input_tensor.softmax_1608.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1608.dc.reciprocal.2: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [softmax_1608.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1608.dc.multiply.3: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [softmax_1608.dc.exp.0, softmax_1608.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1612: {type: matmul, grid_loc: [8, 5], grid_size: [1, 2], inputs: [layernorm_1589.dc.add.10, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1619: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_1608.dc.multiply.3, matmul_1612],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1623: {type: matmul, grid_loc: [9, 0], grid_size: [1, 2], inputs: [matmul_1619, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1627: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_1623, layernorm_1589.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1628.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [add_1627, lc.input_tensor.layernorm_1628.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1628.dc.subtract.1: {type: subtract, grid_loc: [9, 4], grid_size: [1, 1], inputs: [add_1627, layernorm_1628.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1628.dc.multiply.2: {type: multiply, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_1628.dc.subtract.1, layernorm_1628.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1628.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_1628.dc.multiply.2, lc.input_tensor.layernorm_1628.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1628.dc.add.5: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_1628.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1628.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_5_temporal_epoch_2:
    target_device: 0
    input_count: 2
    layernorm_1628.dc.sqrt.6: {type: sqrt, grid_loc: [0, 0], grid_size: [1, 1], inputs: [layernorm_1628.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1628.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_1628.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1628.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [layernorm_1628.dc.reciprocal.7, lc.input_tensor.layernorm_1628.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1628.dc.multiply.8: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_1628.dc.subtract.1, layernorm_1628.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1628.dc.multiply.9: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_1628.dc.multiply.8, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1628.dc.add.10: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_1628.dc.multiply.9, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1631: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [layernorm_1628.dc.add.10, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1634: {type: gelu, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_1631],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1637: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_1634, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1641: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_1637, layernorm_1628.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1642.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_1641, lc.input_tensor.layernorm_1642.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1642.dc.subtract.1: {type: subtract, grid_loc: [2, 4], grid_size: [1, 1], inputs: [add_1641, layernorm_1642.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1642.dc.multiply.2: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_1642.dc.subtract.1, layernorm_1642.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1642.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_1642.dc.multiply.2, lc.input_tensor.layernorm_1642.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1642.dc.add.5: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_1642.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1642.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1642.dc.sqrt.6: {type: sqrt, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_1642.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1642.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_1642.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1642.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_1642.dc.reciprocal.7, lc.input_tensor.layernorm_1642.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1642.dc.multiply.8: {type: multiply, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_1642.dc.subtract.1, layernorm_1642.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1642.dc.multiply.9: {type: multiply, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_1642.dc.multiply.8, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1642.dc.add.10: {type: add, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_1642.dc.multiply.9, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1645: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [layernorm_1642.dc.add.10, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1651: {type: matmul, grid_loc: [5, 2], grid_size: [1, 2], inputs: [layernorm_1642.dc.add.10, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1657: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [matmul_1645, matmul_1651],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1659: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [matmul_1657, input_1_multiply_1659_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1660: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [multiply_1659, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1661.dc.exp.0: {type: exp, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_1660],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1661.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_1661.dc.exp.0, lc.input_tensor.softmax_1661.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1661.dc.reciprocal.2: {type: reciprocal, grid_loc: [6, 1], grid_size: [1, 1], inputs: [softmax_1661.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1661.dc.multiply.3: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [softmax_1661.dc.exp.0, softmax_1661.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1665: {type: matmul, grid_loc: [6, 3], grid_size: [1, 2], inputs: [layernorm_1642.dc.add.10, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1672: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [softmax_1661.dc.multiply.3, matmul_1665],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1676: {type: matmul, grid_loc: [6, 6], grid_size: [1, 2], inputs: [matmul_1672, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1680: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_1676, layernorm_1642.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1681.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [add_1680, lc.input_tensor.layernorm_1681.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1681.dc.subtract.1: {type: subtract, grid_loc: [7, 2], grid_size: [1, 1], inputs: [add_1680, layernorm_1681.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1681.dc.multiply.2: {type: multiply, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_1681.dc.subtract.1, layernorm_1681.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1681.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_1681.dc.multiply.2, lc.input_tensor.layernorm_1681.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1681.dc.add.5: {type: add, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_1681.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1681.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1681.dc.sqrt.6: {type: sqrt, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_1681.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1681.dc.reciprocal.7: {type: reciprocal, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_1681.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1681.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_1681.dc.reciprocal.7, lc.input_tensor.layernorm_1681.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1681.dc.multiply.8: {type: multiply, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_1681.dc.subtract.1, layernorm_1681.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1681.dc.multiply.9: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_1681.dc.multiply.8, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1681.dc.add.10: {type: add, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_1681.dc.multiply.9, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1684: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [layernorm_1681.dc.add.10, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1687: {type: gelu, grid_loc: [8, 6], grid_size: [1, 2], inputs: [matmul_1684],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_6_temporal_epoch_3:
    target_device: 1
    input_count: 2
    matmul_1690: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_gelu_1687_0, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1694: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_1690, e2e_layernorm_1681.dc.add.10_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1695.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_1694, lc.input_tensor.layernorm_1695.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1695.dc.subtract.1: {type: subtract, grid_loc: [1, 2], grid_size: [1, 1], inputs: [add_1694, layernorm_1695.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1695.dc.multiply.2: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_1695.dc.subtract.1, layernorm_1695.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1695.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_1695.dc.multiply.2, lc.input_tensor.layernorm_1695.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1695.dc.add.5: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_1695.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1695.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1695.dc.sqrt.6: {type: sqrt, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_1695.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1695.dc.reciprocal.7: {type: reciprocal, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_1695.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1695.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_1695.dc.reciprocal.7, lc.input_tensor.layernorm_1695.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1695.dc.multiply.8: {type: multiply, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_1695.dc.subtract.1, layernorm_1695.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1695.dc.multiply.9: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_1695.dc.multiply.8, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1695.dc.add.10: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_1695.dc.multiply.9, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1698: {type: matmul, grid_loc: [2, 6], grid_size: [1, 2], inputs: [layernorm_1695.dc.add.10, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1704: {type: matmul, grid_loc: [3, 0], grid_size: [1, 2], inputs: [layernorm_1695.dc.add.10, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1710: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [matmul_1698, matmul_1704],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1712: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [matmul_1710, input_1_multiply_1712_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1713: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [multiply_1712, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1714.dc.exp.0: {type: exp, grid_loc: [3, 5], grid_size: [1, 1], inputs: [add_1713],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1714.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [softmax_1714.dc.exp.0, lc.input_tensor.softmax_1714.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1714.dc.reciprocal.2: {type: reciprocal, grid_loc: [3, 7], grid_size: [1, 1], inputs: [softmax_1714.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1714.dc.multiply.3: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [softmax_1714.dc.exp.0, softmax_1714.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1718: {type: matmul, grid_loc: [4, 1], grid_size: [1, 2], inputs: [layernorm_1695.dc.add.10, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1725: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [softmax_1714.dc.multiply.3, matmul_1718],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1729: {type: matmul, grid_loc: [4, 4], grid_size: [1, 2], inputs: [matmul_1725, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1733: {type: add, grid_loc: [4, 6], grid_size: [1, 1], inputs: [matmul_1729, layernorm_1695.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1734.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [add_1733, lc.input_tensor.layernorm_1734.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1734.dc.subtract.1: {type: subtract, grid_loc: [5, 0], grid_size: [1, 1], inputs: [add_1733, layernorm_1734.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1734.dc.multiply.2: {type: multiply, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_1734.dc.subtract.1, layernorm_1734.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1734.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_1734.dc.multiply.2, lc.input_tensor.layernorm_1734.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1734.dc.add.5: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_1734.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1734.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1734.dc.sqrt.6: {type: sqrt, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_1734.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1734.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_1734.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1734.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_1734.dc.reciprocal.7, lc.input_tensor.layernorm_1734.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1734.dc.multiply.8: {type: multiply, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_1734.dc.subtract.1, layernorm_1734.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1734.dc.multiply.9: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_1734.dc.multiply.8, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1734.dc.add.10: {type: add, grid_loc: [6, 3], grid_size: [1, 1], inputs: [layernorm_1734.dc.multiply.9, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1737: {type: matmul, grid_loc: [7, 0], grid_size: [1, 8], inputs: [layernorm_1734.dc.add.10, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1740: {type: gelu, grid_loc: [6, 4], grid_size: [1, 2], inputs: [matmul_1737],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1743: {type: matmul, grid_loc: [8, 0], grid_size: [1, 8], inputs: [gelu_1740, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1747: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [matmul_1743, layernorm_1734.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1748.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [add_1747, lc.input_tensor.layernorm_1748.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1748.dc.subtract.1: {type: subtract, grid_loc: [9, 0], grid_size: [1, 1], inputs: [add_1747, layernorm_1748.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1748.dc.multiply.2: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_1748.dc.subtract.1, layernorm_1748.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1748.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_1748.dc.multiply.2, lc.input_tensor.layernorm_1748.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1748.dc.add.5: {type: add, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1748.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1748.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1748.dc.sqrt.6: {type: sqrt, grid_loc: [9, 4], grid_size: [1, 1], inputs: [layernorm_1748.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1748.dc.reciprocal.7: {type: reciprocal, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_1748.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1748.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_1748.dc.reciprocal.7, lc.input_tensor.layernorm_1748.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1748.dc.multiply.8: {type: multiply, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_1748.dc.subtract.1, layernorm_1748.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}

  fwd_7_temporal_epoch_3:
    target_device: 0
    input_count: 2
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1748.dc.multiply.9: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_1748.dc.multiply.8, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1748.dc.add.10: {type: add, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_1748.dc.multiply.9, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1751: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [layernorm_1748.dc.add.10, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1757: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [layernorm_1748.dc.add.10, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1763: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [matmul_1751, matmul_1757],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1765: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [matmul_1763, input_1_multiply_1765_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1766: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [multiply_1765, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1767.dc.exp.0: {type: exp, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_1766],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1767.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_1767.dc.exp.0, lc.input_tensor.softmax_1767.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1767.dc.reciprocal.2: {type: reciprocal, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_1767.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1767.dc.multiply.3: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [softmax_1767.dc.exp.0, softmax_1767.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1771: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [layernorm_1748.dc.add.10, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1778: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_1767.dc.multiply.3, matmul_1771],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1782: {type: matmul, grid_loc: [2, 2], grid_size: [1, 2], inputs: [matmul_1778, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1786: {type: add, grid_loc: [2, 4], grid_size: [1, 1], inputs: [matmul_1782, layernorm_1748.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1787.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [add_1786, lc.input_tensor.layernorm_1787.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1787.dc.subtract.1: {type: subtract, grid_loc: [2, 6], grid_size: [1, 1], inputs: [add_1786, layernorm_1787.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1787.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_1787.dc.subtract.1, layernorm_1787.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1787.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_1787.dc.multiply.2, lc.input_tensor.layernorm_1787.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1787.dc.add.5: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_1787.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1787.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1787.dc.sqrt.6: {type: sqrt, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_1787.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1787.dc.reciprocal.7: {type: reciprocal, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_1787.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1787.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_1787.dc.reciprocal.7, lc.input_tensor.layernorm_1787.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1787.dc.multiply.8: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_1787.dc.subtract.1, layernorm_1787.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1787.dc.multiply.9: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_1787.dc.multiply.8, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1787.dc.add.10: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_1787.dc.multiply.9, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1790: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layernorm_1787.dc.add.10, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1793: {type: gelu, grid_loc: [4, 2], grid_size: [1, 2], inputs: [matmul_1790],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1796: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [gelu_1793, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1800: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [matmul_1796, layernorm_1787.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1801.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [add_1800, lc.input_tensor.layernorm_1801.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1801.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [1, 1], inputs: [add_1800, layernorm_1801.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1801.dc.multiply.2: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_1801.dc.subtract.1, layernorm_1801.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1801.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_1801.dc.multiply.2, lc.input_tensor.layernorm_1801.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1801.dc.add.5: {type: add, grid_loc: [7, 1], grid_size: [1, 1], inputs: [layernorm_1801.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1801.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1801.dc.sqrt.6: {type: sqrt, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_1801.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1801.dc.reciprocal.7: {type: reciprocal, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_1801.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1801.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_1801.dc.reciprocal.7, lc.input_tensor.layernorm_1801.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1801.dc.multiply.8: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_1801.dc.subtract.1, layernorm_1801.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1801.dc.multiply.9: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_1801.dc.multiply.8, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1801.dc.add.10: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_1801.dc.multiply.9, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1804: {type: matmul, grid_loc: [8, 2], grid_size: [1, 2], inputs: [layernorm_1801.dc.add.10, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1810: {type: matmul, grid_loc: [8, 4], grid_size: [1, 2], inputs: [layernorm_1801.dc.add.10, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1816: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [matmul_1804, matmul_1810],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1818: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [matmul_1816, input_1_multiply_1818_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1819: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [multiply_1818, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1820.dc.exp.0: {type: exp, grid_loc: [9, 1], grid_size: [1, 1], inputs: [add_1819],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1820.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [softmax_1820.dc.exp.0, lc.input_tensor.softmax_1820.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1820.dc.reciprocal.2: {type: reciprocal, grid_loc: [9, 3], grid_size: [1, 1], inputs: [softmax_1820.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1820.dc.multiply.3: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [softmax_1820.dc.exp.0, softmax_1820.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1824: {type: matmul, grid_loc: [9, 5], grid_size: [1, 2], inputs: [layernorm_1801.dc.add.10, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1831: {type: matmul, grid_loc: [9, 7], grid_size: [1, 1], inputs: [softmax_1820.dc.multiply.3, matmul_1824],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}

  fwd_8_temporal_epoch_4:
    target_device: 1
    input_count: 2
    matmul_1835: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_matmul_1831_0, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1839: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_1835, e2e_layernorm_1801.dc.add.10_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1840.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [add_1839, lc.input_tensor.layernorm_1840.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1840.dc.subtract.1: {type: subtract, grid_loc: [0, 4], grid_size: [1, 1], inputs: [add_1839, layernorm_1840.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1840.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_1840.dc.subtract.1, layernorm_1840.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1840.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_1840.dc.multiply.2, lc.input_tensor.layernorm_1840.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1840.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_1840.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1840.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1840.dc.sqrt.6: {type: sqrt, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_1840.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1840.dc.reciprocal.7: {type: reciprocal, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_1840.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1840.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_1840.dc.reciprocal.7, lc.input_tensor.layernorm_1840.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1840.dc.multiply.8: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_1840.dc.subtract.1, layernorm_1840.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1840.dc.multiply.9: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_1840.dc.multiply.8, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1840.dc.add.10: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_1840.dc.multiply.9, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1843: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layernorm_1840.dc.add.10, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1846: {type: gelu, grid_loc: [3, 0], grid_size: [1, 2], inputs: [matmul_1843],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1849: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [gelu_1846, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1853: {type: add, grid_loc: [3, 2], grid_size: [1, 1], inputs: [matmul_1849, layernorm_1840.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1854.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [add_1853, lc.input_tensor.layernorm_1854.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1854.dc.subtract.1: {type: subtract, grid_loc: [3, 4], grid_size: [1, 1], inputs: [add_1853, layernorm_1854.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1854.dc.multiply.2: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_1854.dc.subtract.1, layernorm_1854.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1854.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_1854.dc.multiply.2, lc.input_tensor.layernorm_1854.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1854.dc.add.5: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_1854.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1854.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1854.dc.sqrt.6: {type: sqrt, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_1854.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1854.dc.reciprocal.7: {type: reciprocal, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_1854.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1854.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_1854.dc.reciprocal.7, lc.input_tensor.layernorm_1854.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1854.dc.multiply.8: {type: multiply, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_1854.dc.subtract.1, layernorm_1854.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1854.dc.multiply.9: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [layernorm_1854.dc.multiply.8, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1854.dc.add.10: {type: add, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_1854.dc.multiply.9, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_1857: {type: matmul, grid_loc: [6, 0], grid_size: [1, 2], inputs: [layernorm_1854.dc.add.10, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1863: {type: matmul, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_1854.dc.add.10, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1869: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [matmul_1857, matmul_1863],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12, transpose], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 2}}
    multiply_1871: {type: multiply, grid_loc: [6, 5], grid_size: [1, 1], inputs: [matmul_1869, input_1_multiply_1871_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}]}
    add_1872: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [multiply_1871, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_1873.dc.exp.0: {type: exp, grid_loc: [6, 7], grid_size: [1, 1], inputs: [add_1872],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1873.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_1873.dc.exp.0, lc.input_tensor.softmax_1873.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, u_kt: 4}}
    softmax_1873.dc.reciprocal.2: {type: reciprocal, grid_loc: [7, 3], grid_size: [1, 1], inputs: [softmax_1873.dc.reduce_sum.1.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_1873.dc.multiply.3: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [softmax_1873.dc.exp.0, softmax_1873.dc.reciprocal.2],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [24, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_1877: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [layernorm_1854.dc.add.10, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    matmul_1884: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [softmax_1873.dc.multiply.3, matmul_1877],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, u_kt: 4}}
    matmul_1888: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [matmul_1884, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, u_kt: 2}}
    add_1892: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [matmul_1888, layernorm_1854.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 120], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1893.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_1892, lc.input_tensor.layernorm_1893.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1893.dc.subtract.1: {type: subtract, grid_loc: [8, 2], grid_size: [1, 1], inputs: [add_1892, layernorm_1893.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1893.dc.multiply.2: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_1893.dc.subtract.1, layernorm_1893.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1893.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_1893.dc.multiply.2, lc.input_tensor.layernorm_1893.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1893.dc.add.5: {type: add, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_1893.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1893.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1893.dc.sqrt.6: {type: sqrt, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_1893.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1893.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_1893.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1893.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_1893.dc.reciprocal.7, lc.input_tensor.layernorm_1893.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1893.dc.multiply.8: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_1893.dc.subtract.1, layernorm_1893.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1893.dc.multiply.9: {type: multiply, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1893.dc.multiply.8, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1893.dc.add.10: {type: add, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_1893.dc.multiply.9, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_9_temporal_epoch_4:
    target_device: 0
    input_count: 2
    matmul_1896: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layernorm_1893.dc.add.10, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 3}}
    gelu_1899: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_1896],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_1902: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_1899, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, u_kt: 12}}
    add_1906: {type: add, grid_loc: [1, 2], grid_size: [1, 1], inputs: [matmul_1902, layernorm_1893.dc.add.10],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [0, 116], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1907.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [1, 3], grid_size: [1, 1], inputs: [add_1906, lc.input_tensor.layernorm_1907.dc.reduce_avg.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1907.dc.subtract.1: {type: subtract, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_1906, layernorm_1907.dc.reduce_avg.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_1907.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_1907.dc.subtract.1, layernorm_1907.dc.subtract.1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1907.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_1907.dc.multiply.2, lc.input_tensor.layernorm_1907.dc.reduce_avg.3.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, u_kt: 24}}
    layernorm_1907.dc.add.5: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_1907.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_1907.4],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1907.dc.sqrt.6: {type: sqrt, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_1907.dc.add.5],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1907.dc.reciprocal.7: {type: reciprocal, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_1907.dc.sqrt.6],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_1907.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_1907.dc.reciprocal.7, lc.input_tensor.layernorm_1907.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1907.dc.multiply.8: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_1907.dc.subtract.1, layernorm_1907.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [264, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1907.dc.multiply.9: {type: multiply, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_1907.dc.multiply.8, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_1907.dc.add.10: {type: add, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_1907.dc.multiply.9, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layernorm_1907.dc.add.10_output_nop_0: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_1907.dc.add.10], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 2, $c_zero: 0, $lptr_q3: 0, $gptr_q3: 0, $gptr_q5: 0, $gptr_q7: 0, $c_one: 1, $lptr_q7: 0, $gptr_q9: 0, $lptr_q5: 0, $lptr_q9: 0}
    - staticvar: {$gptr_q0: 0, $gptr_q6_shadow: 0, $lptr_q6: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q8: 0, $gptr_q1: 0, $gptr_q6: 0, $lptr_q4: 0, $gptr_q1_shadow: 0, $gptr_q4: 0, $gptr_q4_shadow: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q8: 0, $gptr_q2_shadow: 0}
    - varinst: [$gptr_q6, set, $gptr_q6_shadow]
    - varinst: [$gptr_q4, set, $gptr_q4_shadow]
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q1, set, $gptr_q1_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_layernorm_1416.dc.add.10_0]
    -   execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1288_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1290.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1310.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1310.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1310.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1310.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1324.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1324.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1324.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1324.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1341_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1343.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1363.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_1_temporal_epoch_0, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.layernorm_1363.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1363.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1363.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1377.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1377.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1377.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1377.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1394_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1396.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1416.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1416.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1416.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1416.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_1536.dc.add.10_0, e2e_matmul_1539_0, e2e_matmul_1545_0]
    -   execute: {graph_name: fwd_2_temporal_epoch_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_1416.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1430.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1430.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1430.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1430.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1447_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1449.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1469.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1469.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1469.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1469.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_3_temporal_epoch_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.layernorm_1483.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1483.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1483.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1483.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1500_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1502.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1522.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1522.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1522.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1522.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1536.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1536.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1536.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1536.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1416.dc.add.10_0]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_1681.dc.add.10_0, e2e_gelu_1687_0]
    -   execute: {graph_name: fwd_4_temporal_epoch_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layernorm_1536.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_1539_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_matmul_1545_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               input_1_multiply_1553_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1555.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1575.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1575.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1575.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1575.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1589.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1589.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1589.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1589.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1606_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1608.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1628.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1628.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1628.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_5_temporal_epoch_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.layernorm_1628.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1642.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1642.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1642.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1642.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1659_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1661.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1681.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1681.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1681.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1681.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1536.dc.add.10_0, e2e_matmul_1539_0, e2e_matmul_1545_0]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 8]
    -   allocate_queue: [e2e_layernorm_1801.dc.add.10_0, e2e_matmul_1831_0]
    -   execute: {graph_name: fwd_6_temporal_epoch_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_1681.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_gelu_1687_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1695.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1695.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1695.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1695.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1712_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1714.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1734.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1734.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1734.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1734.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1748.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1748.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1748.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1748.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_7_temporal_epoch_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1765_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1767.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1787.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1787.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1787.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1787.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1801.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1801.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1801.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1801.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1818_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1820.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1681.dc.add.10_0, e2e_gelu_1687_0]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_8_temporal_epoch_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_layernorm_1801.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_matmul_1831_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1840.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1840.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1840.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1840.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1854.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1854.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1854.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1854.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1871_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1873.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1893.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1893.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1893.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1893.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_9_temporal_epoch_4, queue_settings: {
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1907.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1907.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1907.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1907.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_1801.dc.add.10_0, e2e_matmul_1831_0]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 4]
    - endloop
    
test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pct: 0.0
    check_pcc: 0.98
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: 0.001
    uniform_upper_bound: 2.0

