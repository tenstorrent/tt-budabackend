# git checkout ea96a530
# pytest pybuda/test/backend/models/test_bert.py::test_multichip_wormhole_multi_encoder_split_concurrent[inference-Golden-chip12-enc24-large]

devices:
  arch: [wormhole, wormhole_b0]

queues:

  # input
  encoder_input:                                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300061a0]]}
  attention_mask:                                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30000000]]}

  # output
  encoder23.output_norm_ff_23:                                                                  {input: norm_ff_23.dc.add.10_output_nop_0, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  ff.bert.encoder.layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3b69360], [3, 0x7d43360], [4, 0x7d53f80], [5, 0x3b79f80], [0, 0x3bf9700], [1, 0x3bf9700], [2, 0x3baa380], [3, 0x7d84380]]}
  ff.bert.encoder.layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8137700], [4, 0x8186240], [5, 0x3f2cb00], [0, 0x40164a0]]}
  ff.bert.encoder.layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x40553e0], [2, 0x3f1cf00], [3, 0x80f66e0], [4, 0x8145220], [5, 0x3eebae0], [0, 0x3fd5480], [1, 0x4096400], [2, 0x3f5df20]]}
  ff.bert.encoder.layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x80f25c0], [4, 0x8141100], [5, 0x3ee79c0], [0, 0x3fd1360]]}
  ff.reciprocal_of_sqrt_of_head_size_0:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4054ba0]]}
  ff.bert.encoder.layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3f4eae0], [1, 0x3fd2b60], [2, 0x3edae60], [3, 0x80b0d60], [4, 0x81000e0], [5, 0x3ea6160], [0, 0x3f8fb00], [1, 0x4013b80]]}
  ff.bert.encoder.layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3ed6d40], [3, 0x80acc40], [4, 0x80fbfc0], [5, 0x3ea2040]]}
  ff.bert.encoder.layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3eccaa0], [1, 0x3f50b20], [2, 0x3e95d20], [3, 0x806bc20], [4, 0x80bafa0], [5, 0x3e61020], [0, 0x3f0dac0], [1, 0x3f91b40]]}
  ff.bert.encoder.layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3e91c00], [3, 0x8067b00], [4, 0x80b6e80], [5, 0x3e5cf00]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7e761c0]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3e10d80]]}
  ff.bert.encoder.layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3fdc5a0], [2, 0x40ad920], [3, 0x82a9980], [4, 0x821ae60], [5, 0x3f41fa0], [0, 0x3ee07c0], [1, 0x405e5c0], [2, 0x412f940], [3, 0x832b9a0], [4, 0x829ce80], [5, 0x3fc3fc0], [0, 0x3f627e0], [1, 0x40e05e0], [2, 0x41b1960], [3, 0x83ad9c0], [4, 0x831eea0]]}
  ff.bert.encoder.layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x80fdf60], [4, 0x807f020], [5, 0x3e260e0], [0, 0x3dc4900]]}
  ff.bert.encoder.layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3e43840], [2, 0x3f247a0], [3, 0x810e380], [4, 0x808f440], [5, 0x3e36500], [0, 0x3dd4d20], [1, 0x3ec5860], [2, 0x3fa67c0], [3, 0x81903a0], [4, 0x8111460], [5, 0x3eb8520], [0, 0x3e56d40], [1, 0x3f47880], [2, 0x40287e0], [3, 0x82123c0], [4, 0x8193480]]}
  ff.bert.encoder.layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3f3a540], [0, 0x3ed8d60], [1, 0x3fc98a0], [2, 0x40aa800], [3, 0x82943e0], [4, 0x82154a0], [5, 0x3f3c5e0], [0, 0x3edae00]]}
  ff.bert.encoder.layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3fcc180]]}
  ff.bert.encoder.layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x8299560]]}
  ff.bert.encoder.layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x38da940], [1, 0x391b120], [2, 0x391b120]]}
  ff.bert.encoder.layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x80b6860], [5, 0x3f19760], [0, 0x3f5a780], [1, 0x3ea7360]]}
  ff.bert.encoder.layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3dab520], [3, 0x7ffba40], [4, 0x8075840], [5, 0x3ed8740], [0, 0x3f19760], [1, 0x3e66340], [2, 0x3dec540], [3, 0x803ca60]]}
  ff.bert.encoder.layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x8071720], [5, 0x3ed4620], [0, 0x3f15640], [1, 0x3e62220]]}
  ff.reciprocal_of_sqrt_of_head_size_1:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3daace0]]}
  ff.bert.encoder.layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7f791c0], [4, 0x7fef6e0], [5, 0x3e92dc0], [0, 0x3ed3de0], [1, 0x3e209c0], [2, 0x3d69cc0], [3, 0x7fba1e0], [4, 0x8030700]]}
  ff.bert.encoder.layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3e8eca0], [0, 0x3ecfcc0], [1, 0x3e1c8a0], [2, 0x3d65ba0]]}
  ff.bert.encoder.layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3c54f80], [3, 0x7dee7a0], [4, 0x7ea54a0], [5, 0x3dbf0a0], [0, 0x3dfc7e0], [1, 0x3d493c0], [2, 0x3c95fa0], [3, 0x7e2f7c0]]}
  ff.bert.encoder.layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3ecbba0], [1, 0x3e18780], [2, 0x3d61a80], [3, 0x7f750a0]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3d2eba0]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3d31c80]]}
  ff.bert.encoder.layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3d124c0], [3, 0x7f093e0], [4, 0x7ee9c20], [5, 0x3d3efc0], [0, 0x3d10460], [1, 0x3d420a0], [2, 0x3d944e0], [3, 0x7f8b400], [4, 0x7f6bc40], [5, 0x3dc0fe0], [0, 0x3d92480], [1, 0x3dc40c0], [2, 0x3e16500], [3, 0x800d420], [4, 0x7fedc60], [5, 0x3e43000]]}
  ff.bert.encoder.layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3e144a0], [1, 0x3e460e0], [2, 0x3e98520], [3, 0x808f440]]}
  ff.bert.encoder.layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3e81840], [2, 0x3e27200], [3, 0x807f920], [4, 0x7fdb8c0], [5, 0x3e261c0], [0, 0x3f250a0], [1, 0x3f03860], [2, 0x3ea9220], [3, 0x8101940], [4, 0x805d8e0], [5, 0x3ea81e0], [0, 0x3fa70c0], [1, 0x3f85880], [2, 0x3f2b240], [3, 0x8183960], [4, 0x80df900]]}
  ff.bert.encoder.layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7e4e600], [5, 0x3c96660], [0, 0x3d871c0], [1, 0x3d765a0], [2, 0x3d09ae0], [3, 0x7ef1e60], [4, 0x7e506a0], [5, 0x3c98700]]}
  ff.bert.encoder.layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3d89aa0]]}
  ff.bert.encoder.layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3d0ec60]]}
  ff.bert.encoder.layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x38da940], [1, 0x391b120], [2, 0x391b120]]}
  ff.bert.encoder.layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x80b6860], [5, 0x3f19760], [0, 0x3f5a780], [1, 0x3ea7360]]}
  ff.bert.encoder.layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3dab520], [3, 0x7ffba40], [4, 0x8075840], [5, 0x3ed8740], [0, 0x3f19760], [1, 0x3e66340], [2, 0x3dec540], [3, 0x803ca60]]}
  ff.bert.encoder.layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x8071720], [5, 0x3ed4620], [0, 0x3f15640], [1, 0x3e62220]]}
  ff.reciprocal_of_sqrt_of_head_size_2:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3daace0]]}
  ff.bert.encoder.layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7f791c0], [4, 0x7fef6e0], [5, 0x3e92dc0], [0, 0x3ed3de0], [1, 0x3e209c0], [2, 0x3d69cc0], [3, 0x7fba1e0], [4, 0x8030700]]}
  ff.bert.encoder.layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3e8eca0], [0, 0x3ecfcc0], [1, 0x3e1c8a0], [2, 0x3d65ba0]]}
  ff.bert.encoder.layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3c54f80], [3, 0x7dee7a0], [4, 0x7ea54a0], [5, 0x3dbf0a0], [0, 0x3dfc7e0], [1, 0x3d493c0], [2, 0x3c95fa0], [3, 0x7e2f7c0]]}
  ff.bert.encoder.layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3ecbba0], [1, 0x3e18780], [2, 0x3d61a80], [3, 0x7f750a0]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3d2eba0]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3d31c80]]}
  ff.bert.encoder.layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3d124c0], [3, 0x7f093e0], [4, 0x7ee9c20], [5, 0x3d3efc0], [0, 0x3d10460], [1, 0x3d420a0], [2, 0x3d944e0], [3, 0x7f8b400], [4, 0x7f6bc40], [5, 0x3dc0fe0], [0, 0x3d92480], [1, 0x3dc40c0], [2, 0x3e16500], [3, 0x800d420], [4, 0x7fedc60], [5, 0x3e43000]]}
  ff.bert.encoder.layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3e144a0], [1, 0x3e460e0], [2, 0x3e98520], [3, 0x808f440]]}
  ff.bert.encoder.layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3e81840], [2, 0x3e27200], [3, 0x807f920], [4, 0x7fdb8c0], [5, 0x3e261c0], [0, 0x3f250a0], [1, 0x3f03860], [2, 0x3ea9220], [3, 0x8101940], [4, 0x805d8e0], [5, 0x3ea81e0], [0, 0x3fa70c0], [1, 0x3f85880], [2, 0x3f2b240], [3, 0x8183960], [4, 0x80df900]]}
  ff.bert.encoder.layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7e4e600], [5, 0x3c96660], [0, 0x3d871c0], [1, 0x3d765a0], [2, 0x3d09ae0], [3, 0x7ef1e60], [4, 0x7e506a0], [5, 0x3c98700]]}
  ff.bert.encoder.layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3d89aa0]]}
  ff.bert.encoder.layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3d0ec60]]}
  ff.bert.encoder.layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x38da940], [1, 0x391b120], [2, 0x391b120]]}
  ff.bert.encoder.layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3edd0a0], [1, 0x3f19fa0], [2, 0x3ea3a80], [3, 0x80f7880]]}
  ff.bert.encoder.layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x8035060], [5, 0x3da4360], [0, 0x3e9c080], [1, 0x3ed8f80], [2, 0x3e62a60], [3, 0x80b6860], [4, 0x8076080], [5, 0x3de5380]]}
  ff.bert.encoder.layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3e97f60], [1, 0x3ed4e60], [2, 0x3e5e940], [3, 0x80b2740]]}
  ff.reciprocal_of_sqrt_of_head_size_3:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x8034820]]}
  ff.bert.encoder.layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3d21ae0], [0, 0x3e15f20], [1, 0x3e93600], [2, 0x3e1d0e0], [3, 0x8070ee0], [4, 0x7ff3800], [5, 0x3d62b00], [0, 0x3e56f40]]}
  ff.bert.encoder.layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3e8f4e0], [2, 0x3e18fc0], [3, 0x806cdc0], [4, 0x7fef6e0]]}
  ff.bert.encoder.layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3c9faa0], [0, 0x3d93ee0], [1, 0x3e4e4c0], [2, 0x3dd7fa0], [3, 0x802bda0], [4, 0x7fae6c0], [5, 0x3ce0ac0], [0, 0x3dd4f00]]}
  ff.bert.encoder.layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3d009a0], [2, 0x3d04280], [3, 0x7f1b180], [4, 0x7e9daa0]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81030e0]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e458e0]]}
  ff.bert.encoder.layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e281c0], [1, 0x3e2aa60], [2, 0x3e9ae00], [3, 0x8113500], [4, 0x8082140], [5, 0x3e55d00], [0, 0x3eaa1e0], [1, 0x3eaca80], [2, 0x3f1ce20], [3, 0x8195520], [4, 0x8104160], [5, 0x3ed7d20], [0, 0x3f2c200], [1, 0x3f2eaa0], [2, 0x3f9ee40], [3, 0x8217540]]}
  ff.bert.encoder.layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8186180], [5, 0x3f59d40], [0, 0x3fae220], [1, 0x3fb0ac0]]}
  ff.bert.encoder.layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3dca280], [2, 0x3d0bba0], [3, 0x7ee1aa0], [4, 0x7f30e20], [5, 0x3d58680], [0, 0x3dc8220], [1, 0x3e4c2a0], [2, 0x3d8dbc0], [3, 0x7f63ac0], [4, 0x7fb2e40], [5, 0x3dda6a0], [0, 0x3e4a240], [1, 0x3ece2c0], [2, 0x3e0fbe0], [3, 0x7fe5ae0], [4, 0x8034e60]]}
  ff.bert.encoder.layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x41a2f00], [1, 0x425fd60], [2, 0x40a8140], [3, 0x8244a20], [4, 0x8315d80], [5, 0x40cc220], [0, 0x41a4fa0], [1, 0x4261e00]]}
  ff.bert.encoder.layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4263ea0]]}
  ff.bert.encoder.layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x40bbe00]]}
  ff.bert.encoder.layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3e10540], [1, 0x3dcfd60], [2, 0x3dcfd60], [3, 0x7eef780], [4, 0x7e351a0], [5, 0x3d55f60], [0, 0x3e51560], [1, 0x3e10d80]]}
  ff.bert.encoder.layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3dcbc40], [3, 0x7eeb660], [4, 0x7e31080], [5, 0x3d51e40]]}
  ff.bert.encoder.layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3cbf140], [3, 0x7ddeb60], [4, 0x7d64d60], [5, 0x3c08440], [0, 0x3cc3260], [1, 0x3d00160], [2, 0x3d00160], [3, 0x7e1fb80]]}
  ff.bert.encoder.layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3e08300], [1, 0x3dc7b20], [2, 0x3dc7b20], [3, 0x7ee7540]]}
  ff.reciprocal_of_sqrt_of_head_size_4:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7e2c720]]}
  ff.bert.encoder.layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3ccb4a0], [0, 0x3d862c0], [1, 0x3d862c0], [2, 0x3d862c0], [3, 0x7ea5ce0], [4, 0x7deb700], [5, 0x3d0c4c0], [0, 0x3dc72e0]]}
  ff.bert.encoder.layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3d821a0], [2, 0x3d821a0], [3, 0x7ea1bc0], [4, 0x7de75e0]]}
  ff.bert.encoder.layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3c49460], [0, 0x3d04280], [1, 0x3d41180], [2, 0x3d41180], [3, 0x7e60ba0], [4, 0x7da65c0], [5, 0x3c8a480], [0, 0x3d452a0]]}
  ff.bert.encoder.layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7e2cf60], [5, 0x3d4dd20], [0, 0x3e0c420], [1, 0x3dcbc40]]}
  ff.bert.encoder.layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3fe4800]]}
  ff.bert.encoder.layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x4233980]]}
  ff.bert.encoder.layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x842f9e0], [4, 0x83a0ec0], [5, 0x4046820], [0, 0x3ff4c20], [1, 0x4162e40], [2, 0x4243da0], [3, 0x84b1a00], [4, 0x8422ee0], [5, 0x40c8840], [0, 0x4076c40], [1, 0x41e4e60], [2, 0x42c5dc0], [3, 0x8533a20], [4, 0x84a4f00], [5, 0x414a860], [0, 0x40f8c60]]}
  ff.bert.encoder.layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x4266e80], [2, 0x4347de0], [3, 0x85b5a40], [4, 0x8526f20]]}
  ff.bert.encoder.layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3cf8f00], [1, 0x3d0b380], [2, 0x3da9260], [3, 0x7e51720], [4, 0x7e85c00], [5, 0x3d8c340], [0, 0x3d7af20], [1, 0x3d8d3a0], [2, 0x3e2b280], [3, 0x7ed3740], [4, 0x7f07c20], [5, 0x3e0e360], [0, 0x3dfcf40], [1, 0x3e0f3c0], [2, 0x3ead2a0], [3, 0x7f55760]]}
  ff.bert.encoder.layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3b6c480], [1, 0x3b7e900], [2, 0x3c0e460], [3, 0x7d46480], [4, 0x7d684e0], [5, 0x3bfe880], [0, 0x3b6e520], [1, 0x3b809a0]]}
  ff.bert.encoder.layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3c10d40]]}
  ff.bert.encoder.layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7d6d660]]}
  ff.bert.encoder.layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7ef3060], [4, 0x7f6ce60], [5, 0x3e4d440], [0, 0x3e8ab80], [1, 0x3dd7760], [2, 0x3d20a60], [3, 0x7f34080], [4, 0x7fade80]]}
  ff.bert.encoder.layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3e49320], [0, 0x3e86a60], [1, 0x3dd3640], [2, 0x3d1c940]]}
  ff.bert.encoder.layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7e71020], [4, 0x7eeae20], [5, 0x3e08300], [0, 0x3e45a40], [1, 0x3d92620], [2, 0x3cdb920], [3, 0x7eb2040], [4, 0x7f2be40]]}
  ff.bert.encoder.layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3e041e0], [0, 0x3e41920], [1, 0x3d8e500], [2, 0x3cd7800]]}
  ff.reciprocal_of_sqrt_of_head_size_5:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7e707e0]]}
  ff.bert.encoder.layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x83c7340], [5, 0x416fc60], [0, 0x40fd860], [1, 0x404a440], [2, 0x3fccd60], [3, 0x82d7860], [4, 0x8408360], [5, 0x41b0c80]]}
  ff.bert.encoder.layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x40f9740], [1, 0x4046320], [2, 0x3fc8c40], [3, 0x82d3740]]}
  ff.bert.encoder.layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x8345300], [5, 0x40edc20], [0, 0x40b8720], [1, 0x4005300], [2, 0x3f87c20], [3, 0x8292720], [4, 0x8386320], [5, 0x412ec40]]}
  ff.bert.encoder.layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x40b4600], [1, 0x40011e0], [2, 0x3f83b00], [3, 0x828e600]]}
  ff.bert.encoder.layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x809f860]]}
  ff.bert.encoder.layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3ec8100]]}
  ff.bert.encoder.layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3e279a0], [1, 0x3e56d40], [2, 0x3ea9180], [3, 0x80afc80], [4, 0x8070d00], [5, 0x3ed8520], [0, 0x3ea99c0], [1, 0x3ed8d60], [2, 0x3f2b1a0], [3, 0x8131ca0], [4, 0x80f2d20], [5, 0x3f5a540], [0, 0x3f2b9e0], [1, 0x3f5ad80], [2, 0x3fad1c0], [3, 0x81b3cc0]]}
  ff.bert.encoder.layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x8174d40], [5, 0x3fdc560], [0, 0x3fada00], [1, 0x3fdcda0]]}
  ff.bert.encoder.layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7ef6fe0], [4, 0x7e52f80], [5, 0x3c9afe0], [0, 0x3d99ec0], [1, 0x3d796c0], [2, 0x3d1f080], [3, 0x7f79000], [4, 0x7ed4fa0], [5, 0x3d1d000], [0, 0x3e1bee0], [1, 0x3dfb6e0], [2, 0x3da10a0], [3, 0x7ffb020], [4, 0x7f56fc0], [5, 0x3d9f020], [0, 0x3e9df00]]}
  ff.bert.encoder.layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3e7d700], [2, 0x3e230c0], [3, 0x807d040], [4, 0x7fd8fe0], [5, 0x3e21040], [0, 0x3f1ff20], [1, 0x3e7f7a0], [2, 0x3e25160]]}
  ff.bert.encoder.layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x40290e0]]}
  ff.bert.encoder.layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3fad260]]}
  ff.bert.encoder.layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7ef3060], [4, 0x7f6ce60], [5, 0x3e4d440], [0, 0x3e8ab80], [1, 0x3dd7760], [2, 0x3d20a60], [3, 0x7f34080], [4, 0x7fade80]]}
  ff.bert.encoder.layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3e49320], [0, 0x3e86a60], [1, 0x3dd3640], [2, 0x3d1c940]]}
  ff.bert.encoder.layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7e71020], [4, 0x7eeae20], [5, 0x3e08300], [0, 0x3e45a40], [1, 0x3d92620], [2, 0x3cdb920], [3, 0x7eb2040], [4, 0x7f2be40]]}
  ff.bert.encoder.layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3e041e0], [0, 0x3e41920], [1, 0x3d8e500], [2, 0x3cd7800]]}
  ff.reciprocal_of_sqrt_of_head_size_6:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7e707e0]]}
  ff.bert.encoder.layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x83c7340], [5, 0x416fc60], [0, 0x40fd860], [1, 0x404a440], [2, 0x3fccd60], [3, 0x82d7860], [4, 0x8408360], [5, 0x41b0c80]]}
  ff.bert.encoder.layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x40f9740], [1, 0x4046320], [2, 0x3fc8c40], [3, 0x82d3740]]}
  ff.bert.encoder.layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x8345300], [5, 0x40edc20], [0, 0x40b8720], [1, 0x4005300], [2, 0x3f87c20], [3, 0x8292720], [4, 0x8386320], [5, 0x412ec40]]}
  ff.bert.encoder.layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x40b4600], [1, 0x40011e0], [2, 0x3f83b00], [3, 0x828e600]]}
  ff.bert.encoder.layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x809f860]]}
  ff.bert.encoder.layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3ec8100]]}
  ff.bert.encoder.layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3e279a0], [1, 0x3e56d40], [2, 0x3ea9180], [3, 0x80afc80], [4, 0x8070d00], [5, 0x3ed8520], [0, 0x3ea99c0], [1, 0x3ed8d60], [2, 0x3f2b1a0], [3, 0x8131ca0], [4, 0x80f2d20], [5, 0x3f5a540], [0, 0x3f2b9e0], [1, 0x3f5ad80], [2, 0x3fad1c0], [3, 0x81b3cc0]]}
  ff.bert.encoder.layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x8174d40], [5, 0x3fdc560], [0, 0x3fada00], [1, 0x3fdcda0]]}
  ff.bert.encoder.layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7ef6fe0], [4, 0x7e52f80], [5, 0x3c9afe0], [0, 0x3d99ec0], [1, 0x3d796c0], [2, 0x3d1f080], [3, 0x7f79000], [4, 0x7ed4fa0], [5, 0x3d1d000], [0, 0x3e1bee0], [1, 0x3dfb6e0], [2, 0x3da10a0], [3, 0x7ffb020], [4, 0x7f56fc0], [5, 0x3d9f020], [0, 0x3e9df00]]}
  ff.bert.encoder.layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3e7d700], [2, 0x3e230c0], [3, 0x807d040], [4, 0x7fd8fe0], [5, 0x3e21040], [0, 0x3f1ff20], [1, 0x3e7f7a0], [2, 0x3e25160]]}
  ff.bert.encoder.layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x40290e0]]}
  ff.bert.encoder.layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3fad260]]}
  ff.bert.encoder.layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3cd0e80], [1, 0x3d8b460], [2, 0x3d55720], [3, 0x7f68d40], [4, 0x7eeb660], [5, 0x3c1da60], [0, 0x3d11ea0], [1, 0x3dcc480]]}
  ff.bert.encoder.layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3d51600], [3, 0x7f64c20], [4, 0x7ee7540], [5, 0x3c19940]]}
  ff.bert.encoder.layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3c4ee40], [1, 0x3d09420], [2, 0x3d105e0], [3, 0x7f23c00], [4, 0x7ea6520], [5, 0x3bd8920], [0, 0x3c8fe60], [1, 0x3d4a440]]}
  ff.bert.encoder.layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3d0c4c0], [3, 0x7f1fae0], [4, 0x7ea2400], [5, 0x3bd4800]]}
  ff.reciprocal_of_sqrt_of_head_size_7:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3c4e600]]}
  ff.bert.encoder.layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7fa9d60], [4, 0x7f2c680], [5, 0x3c5ea80], [0, 0x3d52ec0], [1, 0x3e0d4a0], [2, 0x3d96f80], [3, 0x7fead80], [4, 0x7f6d6a0]]}
  ff.bert.encoder.layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x81c7cc0], [5, 0x3fb46a0], [0, 0x406f4c0], [1, 0x402ece0]]}
  ff.bert.encoder.layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x40bd080], [0, 0x40fe0a0], [1, 0x40c11a0], [2, 0x40c11a0], [3, 0x8314fa0], [4, 0x82d47c0], [5, 0x40fe0a0], [0, 0x413f0c0]]}
  ff.bert.encoder.layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x40bd080], [2, 0x40bd080], [3, 0x8310e80], [4, 0x82d06a0]]}
  ff.bert.encoder.layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fc0ee0]]}
  ff.bert.encoder.layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x829c640]]}
  ff.bert.encoder.layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4156380], [1, 0x41783e0], [2, 0x41cb860], [3, 0x83d4c00], [4, 0x82c2060], [5, 0x4114b60], [0, 0x41d83a0], [1, 0x41fa400], [2, 0x424d880], [3, 0x8456c20], [4, 0x8344080], [5, 0x4196b80], [0, 0x425a3c0], [1, 0x427c420], [2, 0x42cf8a0], [3, 0x84d8c40]]}
  ff.bert.encoder.layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8199680], [5, 0x3f6a9a0], [0, 0x3fbee80], [1, 0x3fd1300]]}
  ff.bert.encoder.layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x818f4e0], [5, 0x3f35da0], [0, 0x401cea0], [1, 0x40d9d00], [2, 0x3fa38c0], [3, 0x81401a0], [4, 0x8211500], [5, 0x3fb7dc0], [0, 0x409eec0], [1, 0x415bd20], [2, 0x40258e0], [3, 0x81c21c0], [4, 0x8293520], [5, 0x4039de0], [0, 0x4120ee0], [1, 0x41ddd40]]}
  ff.bert.encoder.layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f9f780], [3, 0x813c060], [4, 0x818d440], [5, 0x3f33d00], [0, 0x401ae00], [1, 0x40d7c60], [2, 0x3fa1820], [3, 0x813e100]]}
  ff.bert.encoder.layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bb8140]]}
  ff.bert.encoder.layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3be92e0]]}
  ff.bert.encoder.layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3f2ed80], [0, 0x406ab60], [1, 0x3ff3600], [2, 0x3ff8740], [3, 0x8142be0], [4, 0x8021cc0], [5, 0x3f6fda0], [0, 0x40abb80]]}
  ff.bert.encoder.layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3fef4e0], [2, 0x3ff4620], [3, 0x813eac0], [4, 0x801dba0]]}
  ff.bert.encoder.layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3eacd40], [0, 0x3fe8b20], [1, 0x3fae4c0], [2, 0x3fb3600], [3, 0x80fdaa0], [4, 0x7fdcb80], [5, 0x3eedd60], [0, 0x4029b40]]}
  ff.bert.encoder.layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3faa3a0], [2, 0x3faf4e0], [3, 0x80f9980], [4, 0x7fd8a60]]}
  ff.reciprocal_of_sqrt_of_head_size_8:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3eac500]]}
  ff.bert.encoder.layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3f662a0], [1, 0x3f28360], [2, 0x3f6dc80], [3, 0x80b8120], [4, 0x7f97200], [5, 0x3e6b4e0], [0, 0x3fa72c0], [1, 0x3f69380]]}
  ff.bert.encoder.layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7e865e0], [5, 0x3d977c0], [0, 0x3e92dc0], [1, 0x3e54e80]]}
  ff.bert.encoder.layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x80360e0], [4, 0x7f151c0], [5, 0x3e29c80], [0, 0x3f25280], [1, 0x3ee7340], [2, 0x3f2cc60], [3, 0x8077100], [4, 0x7f561e0]]}
  ff.bert.encoder.layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3e25b60], [0, 0x3f21160], [1, 0x3ee3220], [2, 0x3f28b40]]}
  ff.bert.encoder.layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x8537340]]}
  ff.bert.encoder.layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x417dd60]]}
  ff.bert.encoder.layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3a811e0], [3, 0x7c4b600], [4, 0x7c5da80], [5, 0x39f26c0], [0, 0x3a126c0], [1, 0x3a842c0], [2, 0x3b03200], [3, 0x7ccd620], [4, 0x7cdfaa0], [5, 0x3a746e0], [0, 0x3a946e0], [1, 0x3b062e0], [2, 0x3b85220], [3, 0x7d4f640], [4, 0x7d61ac0], [5, 0x3af6700]]}
  ff.bert.encoder.layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100]]}
  ff.bert.encoder.layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3c03a00], [0, 0x3b70e00], [1, 0x3b83280], [2, 0x3c21160], [3, 0x7d495a0], [4, 0x7d7da80], [5, 0x3c85a20], [0, 0x3bf2e20], [1, 0x3c052a0], [2, 0x3ca3180], [3, 0x7dcb5c0], [4, 0x7dffaa0], [5, 0x3d07a40], [0, 0x3c74e40], [1, 0x3c872c0], [2, 0x3d251a0]]}
  ff.bert.encoder.layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7e4d5e0], [4, 0x7e81ac0], [5, 0x3d89a60], [0, 0x3cf6e60], [1, 0x3d092e0], [2, 0x3da71c0], [3, 0x7e4f680], [4, 0x7e83b60]]}
  ff.bert.encoder.layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3f2f2c0]]}
  ff.bert.encoder.layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7f8cd20]]}
  ff.bert.encoder.layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x820c5c0], [4, 0x82c32c0], [5, 0x40ac3c0], [0, 0x40735e0], [1, 0x3fc01c0], [2, 0x3f42ae0], [3, 0x824d5e0], [4, 0x83042e0]]}
  ff.bert.encoder.layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x40a82a0], [0, 0x406f4c0], [1, 0x3fbc0a0], [2, 0x3f3e9c0]]}
  ff.bert.encoder.layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x818a580], [4, 0x8241280], [5, 0x4067280], [0, 0x402e4a0], [1, 0x3f7b080], [2, 0x3efd9a0], [3, 0x81cb5a0], [4, 0x82822a0]]}
  ff.bert.encoder.layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x4063160], [0, 0x402a380], [1, 0x3f76f60], [2, 0x3ef9880]]}
  ff.reciprocal_of_sqrt_of_head_size_9:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3e2d560]]}
  ff.bert.encoder.layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x81bea00], [5, 0x3fe1120], [0, 0x3fe8b20], [1, 0x3f35700], [2, 0x3eb8020], [3, 0x8148d20], [4, 0x81ffa20], [5, 0x4022140]]}
  ff.bert.encoder.layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3fe4a00], [1, 0x3f315e0], [2, 0x3eb3f00], [3, 0x8144c00]]}
  ff.bert.encoder.layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x813c9c0], [5, 0x3f5f0e0], [0, 0x3fa39e0], [1, 0x3ef05c0], [2, 0x3e72ee0], [3, 0x8103be0], [4, 0x817d9e0], [5, 0x3fa0100]]}
  ff.bert.encoder.layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3f9f8c0], [1, 0x3eec4a0], [2, 0x3e6edc0], [3, 0x80ffac0]]}
  ff.bert.encoder.layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3fed1c0]]}
  ff.bert.encoder.layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x8238dc0]]}
  ff.bert.encoder.layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x8188240], [5, 0x3fed1c0], [0, 0x3fbe660], [1, 0x3ffd5e0], [2, 0x4030260], [3, 0x82491e0], [4, 0x820a260], [5, 0x406f1e0], [0, 0x4040680], [1, 0x407f600], [2, 0x40b2280], [3, 0x82cb200], [4, 0x828c280], [5, 0x40f1200], [0, 0x40c26a0], [1, 0x4101620]]}
  ff.bert.encoder.layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x41342a0], [3, 0x834d220], [4, 0x830e2a0], [5, 0x4173220]]}
  ff.bert.encoder.layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x8205980], [4, 0x8161920], [5, 0x3f2aa40], [0, 0x4039500], [1, 0x40080e0], [2, 0x3fbd680], [3, 0x82879a0], [4, 0x81e3940], [5, 0x3faca60], [0, 0x40bb520], [1, 0x408a100], [2, 0x403f6a0], [3, 0x83099c0], [4, 0x8265960], [5, 0x402ea80], [0, 0x413d540]]}
  ff.bert.encoder.layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x410c120], [2, 0x40c16c0], [3, 0x838b9e0], [4, 0x82e7980], [5, 0x40b0aa0], [0, 0x41bf560], [1, 0x410e1c0], [2, 0x40c3760]]}
  ff.bert.encoder.layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x838e2c0]]}
  ff.bert.encoder.layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x40b5c20]]}
  ff.bert.encoder.layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x820c5c0], [4, 0x82c32c0], [5, 0x40ac3c0], [0, 0x40735e0], [1, 0x3fc01c0], [2, 0x3f42ae0], [3, 0x824d5e0], [4, 0x83042e0]]}
  ff.bert.encoder.layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x40a82a0], [0, 0x406f4c0], [1, 0x3fbc0a0], [2, 0x3f3e9c0]]}
  ff.bert.encoder.layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x818a580], [4, 0x8241280], [5, 0x4067280], [0, 0x402e4a0], [1, 0x3f7b080], [2, 0x3efd9a0], [3, 0x81cb5a0], [4, 0x82822a0]]}
  ff.bert.encoder.layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x4063160], [0, 0x402a380], [1, 0x3f76f60], [2, 0x3ef9880]]}
  ff.reciprocal_of_sqrt_of_head_size_10:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3e2d560]]}
  ff.bert.encoder.layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x81bea00], [5, 0x3fe1120], [0, 0x3fe8b20], [1, 0x3f35700], [2, 0x3eb8020], [3, 0x8148d20], [4, 0x81ffa20], [5, 0x4022140]]}
  ff.bert.encoder.layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3fe4a00], [1, 0x3f315e0], [2, 0x3eb3f00], [3, 0x8144c00]]}
  ff.bert.encoder.layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x813c9c0], [5, 0x3f5f0e0], [0, 0x3fa39e0], [1, 0x3ef05c0], [2, 0x3e72ee0], [3, 0x8103be0], [4, 0x817d9e0], [5, 0x3fa0100]]}
  ff.bert.encoder.layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3f9f8c0], [1, 0x3eec4a0], [2, 0x3e6edc0], [3, 0x80ffac0]]}
  ff.bert.encoder.layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3fed1c0]]}
  ff.bert.encoder.layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x8238dc0]]}
  ff.bert.encoder.layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x8188240], [5, 0x3fed1c0], [0, 0x3fbe660], [1, 0x3ffd5e0], [2, 0x4030260], [3, 0x82491e0], [4, 0x820a260], [5, 0x406f1e0], [0, 0x4040680], [1, 0x407f600], [2, 0x40b2280], [3, 0x82cb200], [4, 0x828c280], [5, 0x40f1200], [0, 0x40c26a0], [1, 0x4101620]]}
  ff.bert.encoder.layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x41342a0], [3, 0x834d220], [4, 0x830e2a0], [5, 0x4173220]]}
  ff.bert.encoder.layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x8205980], [4, 0x8161920], [5, 0x3f2aa40], [0, 0x4039500], [1, 0x40080e0], [2, 0x3fbd680], [3, 0x82879a0], [4, 0x81e3940], [5, 0x3faca60], [0, 0x40bb520], [1, 0x408a100], [2, 0x403f6a0], [3, 0x83099c0], [4, 0x8265960], [5, 0x402ea80], [0, 0x413d540]]}
  ff.bert.encoder.layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x410c120], [2, 0x40c16c0], [3, 0x838b9e0], [4, 0x82e7980], [5, 0x40b0aa0], [0, 0x41bf560], [1, 0x410e1c0], [2, 0x40c3760]]}
  ff.bert.encoder.layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x838e2c0]]}
  ff.bert.encoder.layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x40b5c20]]}
  ff.bert.encoder.layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x824e660], [5, 0x403b040], [0, 0x40bc840], [1, 0x407c060], [2, 0x407c060], [3, 0x82cfe60], [4, 0x828f680], [5, 0x407c060]]}
  ff.bert.encoder.layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x40b8720], [1, 0x4077f40], [2, 0x4077f40], [3, 0x82cbd40]]}
  ff.bert.encoder.layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x81cc620], [5, 0x3fb9000], [0, 0x4077700], [1, 0x4036f20], [2, 0x4036f20], [3, 0x828ad20], [4, 0x820d640], [5, 0x3ffa020]]}
  ff.bert.encoder.layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x40735e0], [1, 0x4032e00], [2, 0x4032e00], [3, 0x8286c00]]}
  ff.reciprocal_of_sqrt_of_head_size_11:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x81cbde0]]}
  ff.bert.encoder.layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3fb0580], [3, 0x8204380], [4, 0x8186ca0], [5, 0x3f73680], [0, 0x402e4a0], [1, 0x3fedcc0], [2, 0x3ff15a0], [3, 0x82453a0]]}
  ff.bert.encoder.layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x8182b80], [5, 0x3f6f560], [0, 0x402a380], [1, 0x3fe9ba0]]}
  ff.bert.encoder.layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3f2e540], [3, 0x8182340], [4, 0x8141b60], [5, 0x3f2e540], [0, 0x3fe9360], [1, 0x3fa8b80], [2, 0x3f6f560], [3, 0x81c3360]]}
  ff.bert.encoder.layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x813da40], [5, 0x3f2a420], [0, 0x3fe5240], [1, 0x3fa4a60]]}
  ff.bert.encoder.layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4103f00]]}
  ff.bert.encoder.layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x83c1700]]}
  ff.bert.encoder.layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f7dea0], [0, 0x3fcfae0], [1, 0x3ff1b40], [2, 0x4042720], [3, 0x82bd6c0], [4, 0x81bcfa0], [5, 0x3fffec0], [0, 0x4051b00], [1, 0x4073b60], [2, 0x40c4740], [3, 0x833f6e0], [4, 0x823efc0], [5, 0x4081ee0], [0, 0x40d3b20], [1, 0x40f5b80], [2, 0x4146760]]}
  ff.bert.encoder.layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fe1720], [2, 0x4032300], [3, 0x82ad2a0], [4, 0x81acb80]]}
  ff.bert.encoder.layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bcd6e0], [5, 0x39f36e0], [0, 0x3a63280], [1, 0x3a72e60], [2, 0x3a64ae0], [3, 0x7c3eae0], [4, 0x7c4f700], [5, 0x3a75700], [0, 0x3ae52a0], [1, 0x3af4e80], [2, 0x3ae6b00], [3, 0x7cc0b00], [4, 0x7cd1720], [5, 0x3af7720], [0, 0x3b672c0], [1, 0x3b76ea0]]}
  ff.bert.encoder.layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3a609a0], [3, 0x7c3a9a0], [4, 0x7bcb640], [5, 0x39f1640], [0, 0x3a611e0], [1, 0x3a70dc0], [2, 0x3a62a40], [3, 0x7c3ca40]]}
  ff.bert.encoder.layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3a60160]]}
  ff.bert.encoder.layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x39de140]]}
  ff.bert.encoder.layer.12.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3ea6b00], [3, 0x7fb40a0], [4, 0x7ed3960], [5, 0x3de4b40], [0, 0x3ee0140], [1, 0x3ea2200], [2, 0x3ee7b20], [3, 0x7ff50c0]]}
  ff.bert.encoder.layer.12.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7ecf840], [5, 0x3de0a20], [0, 0x3edc020], [1, 0x3e9e0e0]]}
  ff.bert.encoder.layer.12.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3e24ac0], [3, 0x7f32060], [4, 0x7e8e820], [5, 0x3d9fa00], [0, 0x3e9b000], [1, 0x3e5d0c0], [2, 0x3e65ae0], [3, 0x7f73080]]}
  ff.bert.encoder.layer.12.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7e8a700], [5, 0x3d9b8e0], [0, 0x3e96ee0], [1, 0x3e58fa0]]}
  ff.reciprocal_of_sqrt_of_head_size_12:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3e24280]]}
  ff.bert.encoder.layer.12.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3b64a80], [2, 0x3aaa4a0], [3, 0x7c0df80], [4, 0x7c0df80], [5, 0x3a33f80], [0, 0x3aeeda0], [1, 0x3ba5aa0], [2, 0x3aeb4c0]]}
  ff.bert.encoder.layer.12.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7c09e60], [4, 0x7c09e60], [5, 0x3a2fe60], [0, 0x3aeac80]]}
  ff.bert.encoder.layer.12.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3ae2a40], [2, 0x3a28460], [3, 0x7bc8e40], [4, 0x7bc8e40], [5, 0x39eee40], [0, 0x3aa9c60], [1, 0x3b23a60], [2, 0x3a69480]]}
  ff.bert.encoder.layer.12.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7bc4d20], [4, 0x7bc4d20], [5, 0x39ead20], [0, 0x3aa5b40]]}
  ff.bert.encoder.layer.12.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7ac4520]]}
  ff.bert.encoder.layer.12.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x38ed600]]}
  ff.bert.encoder.layer.12.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x38ed600], [2, 0x38ead60], [3, 0x7ac4d60], [4, 0x7ad4940], [5, 0x38db180], [0, 0x38fda20], [1, 0x396f620], [2, 0x396cd80], [3, 0x7b46d80], [4, 0x7b56960], [5, 0x395d1a0], [0, 0x397fa40], [1, 0x39f1640], [2, 0x39eeda0], [3, 0x7bc8da0], [4, 0x7bd8980]]}
  ff.bert.encoder.layer.12.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x39df1c0], [0, 0x3a01a60], [1, 0x3a73660], [2, 0x3a70dc0]]}
  ff.bert.encoder.layer.12.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3e93460], [0, 0x3e7f7a0], [1, 0x3e91c20], [2, 0x3f3f6e0], [3, 0x7fd7fc0], [4, 0x7f9d140], [5, 0x3f15480], [0, 0x3f017c0], [1, 0x3f13c40], [2, 0x3fc1700], [3, 0x8059fe0], [4, 0x801f160], [5, 0x3f974a0], [0, 0x3f837e0], [1, 0x3f95c60], [2, 0x4043720]]}
  ff.bert.encoder.layer.12.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x80dc000], [4, 0x80a1180], [5, 0x40194c0], [0, 0x4005800], [1, 0x4017c80], [2, 0x40c5740], [3, 0x80de0a0], [4, 0x80a3220]]}
  ff.bert.encoder.layer.12.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x38da100]]}
  ff.bert.encoder.layer.12.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7ab4100]]}
  ff.bert.encoder.layer.13.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x807da80], [4, 0x80ba980], [5, 0x3f1d880], [0, 0x3f5e8a0], [1, 0x3eab480], [2, 0x3e2dda0], [3, 0x80beaa0], [4, 0x80fb9a0]]}
  ff.bert.encoder.layer.13.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7ee64c0], [5, 0x3e000c0], [0, 0x3e3d800], [1, 0x3d8a3e0]]}
  ff.bert.encoder.layer.13.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7c844a0], [5, 0x3b242a0], [0, 0x3aae5c0], [1, 0x3a74fa0], [2, 0x3a74fa0], [3, 0x7c0e7c0], [4, 0x7cc54c0], [5, 0x3b652c0]]}
  ff.bert.encoder.layer.13.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3aaa4a0], [1, 0x3a70e80], [2, 0x3a70e80], [3, 0x7c0a6a0]]}
  ff.reciprocal_of_sqrt_of_head_size_13:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7c83c60]]}
  ff.bert.encoder.layer.13.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3aa1a20], [0, 0x3a28460], [1, 0x3a2f620], [2, 0x3a2f620], [3, 0x7bc8e40], [4, 0x7c42c40], [5, 0x3ae2a40], [0, 0x3a69480]]}
  ff.bert.encoder.layer.13.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3a2b500], [2, 0x3a2b500], [3, 0x7bc4d20], [4, 0x7c3eb20]]}
  ff.bert.encoder.layer.13.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3a1f9e0], [0, 0x39a6420], [1, 0x39ea4e0], [2, 0x39ea4e0], [3, 0x7b83d00], [4, 0x7bfdb00], [5, 0x3a60a00], [0, 0x39e7440]]}
  ff.bert.encoder.layer.13.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x39e63c0], [2, 0x39e63c0], [3, 0x7b7fbe0], [4, 0x7bf99e0]]}
  ff.bert.encoder.layer.13.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x38da100]]}
  ff.bert.encoder.layer.13.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x38dd1e0]]}
  ff.bert.encoder.layer.13.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x38dd1e0], [3, 0x7ab4940], [4, 0x7ab4940], [5, 0x38ea520], [0, 0x38db180], [1, 0x38ed600], [2, 0x395f200], [3, 0x7b36960], [4, 0x7b36960], [5, 0x396c540], [0, 0x395d1a0], [1, 0x396f620], [2, 0x39e1220], [3, 0x7bb8980], [4, 0x7bb8980], [5, 0x39ee560]]}
  ff.bert.encoder.layer.13.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x39df1c0], [1, 0x39f1640], [2, 0x3a63240], [3, 0x7c3a9a0]]}
  ff.bert.encoder.layer.13.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7bbf3a0], [5, 0x39f7820], [0, 0x3a68400], [1, 0x3a673c0], [2, 0x3a69c60], [3, 0x7c51fe0], [4, 0x7c413c0], [5, 0x3a79840], [0, 0x3aea420], [1, 0x3ae93e0], [2, 0x3aebc80], [3, 0x7cd4000], [4, 0x7cc33e0], [5, 0x3afb860], [0, 0x3b6c440], [1, 0x3b6b400]]}
  ff.bert.encoder.layer.13.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x38da940], [1, 0x38dc1a0], [2, 0x38dc1a0]]}
  ff.bert.encoder.layer.13.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7ab69e0]]}
  ff.bert.encoder.layer.13.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x38df280]]}
  ff.bert.encoder.layer.14.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x807da80], [4, 0x80ba980], [5, 0x3f1d880], [0, 0x3f5e8a0], [1, 0x3eab480], [2, 0x3e2dda0], [3, 0x80beaa0], [4, 0x80fb9a0]]}
  ff.bert.encoder.layer.14.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7ee64c0], [5, 0x3e000c0], [0, 0x3e3d800], [1, 0x3d8a3e0]]}
  ff.bert.encoder.layer.14.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7c844a0], [5, 0x3b242a0], [0, 0x3aae5c0], [1, 0x3a74fa0], [2, 0x3a74fa0], [3, 0x7c0e7c0], [4, 0x7cc54c0], [5, 0x3b652c0]]}
  ff.bert.encoder.layer.14.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3aaa4a0], [1, 0x3a70e80], [2, 0x3a70e80], [3, 0x7c0a6a0]]}
  ff.reciprocal_of_sqrt_of_head_size_14:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7c83c60]]}
  ff.bert.encoder.layer.14.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3aa1a20], [0, 0x3a28460], [1, 0x3a2f620], [2, 0x3a2f620], [3, 0x7bc8e40], [4, 0x7c42c40], [5, 0x3ae2a40], [0, 0x3a69480]]}
  ff.bert.encoder.layer.14.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3a2b500], [2, 0x3a2b500], [3, 0x7bc4d20], [4, 0x7c3eb20]]}
  ff.bert.encoder.layer.14.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3a1f9e0], [0, 0x39a6420], [1, 0x39ea4e0], [2, 0x39ea4e0], [3, 0x7b83d00], [4, 0x7bfdb00], [5, 0x3a60a00], [0, 0x39e7440]]}
  ff.bert.encoder.layer.14.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x39e63c0], [2, 0x39e63c0], [3, 0x7b7fbe0], [4, 0x7bf99e0]]}
  ff.bert.encoder.layer.14.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x38da100]]}
  ff.bert.encoder.layer.14.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x38dd1e0]]}
  ff.bert.encoder.layer.14.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x38dd1e0], [3, 0x7ab4940], [4, 0x7ab4940], [5, 0x38ea520], [0, 0x38db180], [1, 0x38ed600], [2, 0x395f200], [3, 0x7b36960], [4, 0x7b36960], [5, 0x396c540], [0, 0x395d1a0], [1, 0x396f620], [2, 0x39e1220], [3, 0x7bb8980], [4, 0x7bb8980], [5, 0x39ee560]]}
  ff.bert.encoder.layer.14.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x39df1c0], [1, 0x39f1640], [2, 0x3a63240], [3, 0x7c3a9a0]]}
  ff.bert.encoder.layer.14.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7bbf3a0], [5, 0x39f7820], [0, 0x3a68400], [1, 0x3a673c0], [2, 0x3a69c60], [3, 0x7c51fe0], [4, 0x7c413c0], [5, 0x3a79840], [0, 0x3aea420], [1, 0x3ae93e0], [2, 0x3aebc80], [3, 0x7cd4000], [4, 0x7cc33e0], [5, 0x3afb860], [0, 0x3b6c440], [1, 0x3b6b400]]}
  ff.bert.encoder.layer.14.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x38da940], [1, 0x38dc1a0], [2, 0x38dc1a0]]}
  ff.bert.encoder.layer.14.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7ab69e0]]}
  ff.bert.encoder.layer.14.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x38df280]]}
  ff.bert.encoder.layer.15.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3ea83e0], [0, 0x3f63200], [1, 0x3f63200], [2, 0x3eecce0], [3, 0x8140ae0], [4, 0x80fca20], [5, 0x3ee9400], [0, 0x3fa4220]]}
  ff.bert.encoder.layer.15.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3f5f0e0], [2, 0x3ee8bc0], [3, 0x813c9c0], [4, 0x80f8900]]}
  ff.bert.encoder.layer.15.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3e263a0], [0, 0x3ee11c0], [1, 0x3f1e0c0], [2, 0x3ea7ba0], [3, 0x80fb9a0], [4, 0x80b78e0], [5, 0x3e673c0], [0, 0x3f221e0]]}
  ff.bert.encoder.layer.15.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3bcfea0], [0, 0x3c4a4e0], [1, 0x3d04ac0], [2, 0x3d083a0]]}
  ff.reciprocal_of_sqrt_of_head_size_15:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3aeac80]]}
  ff.bert.encoder.layer.15.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3a68c40], [2, 0x3a68c40], [3, 0x7c42c40], [4, 0x7c42c40], [5, 0x39eee40], [0, 0x39ef680], [1, 0x3aa9c60], [2, 0x3aa9c60]]}
  ff.bert.encoder.layer.15.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7c3eb20], [4, 0x7c3eb20], [5, 0x39ead20], [0, 0x39eb560]]}
  ff.bert.encoder.layer.15.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x39e6c00], [2, 0x39e6c00], [3, 0x7bfdb00], [4, 0x7bfdb00], [5, 0x39a9d00], [0, 0x39aa540], [1, 0x3a27c20], [2, 0x3a27c20]]}
  ff.bert.encoder.layer.15.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7bf99e0], [4, 0x7bf99e0], [5, 0x39a5be0], [0, 0x39a6420]]}
  ff.bert.encoder.layer.15.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4021ee0]]}
  ff.bert.encoder.layer.15.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3b85220]]}
  ff.bert.encoder.layer.15.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a022a0], [2, 0x39f26c0], [3, 0x7c5da80], [4, 0x7c4b600], [5, 0x3a811e0], [0, 0x3a842c0], [1, 0x3a842c0], [2, 0x3a746e0], [3, 0x7cdfaa0], [4, 0x7ccd620], [5, 0x3b03200], [0, 0x3b062e0], [1, 0x3b062e0], [2, 0x3af6700], [3, 0x7d61ac0], [4, 0x7d4f640]]}
  ff.bert.encoder.layer.15.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c4d660], [4, 0x7c3b1e0], [5, 0x3a70dc0], [0, 0x3a73ea0]]}
  ff.bert.encoder.layer.15.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x395c120], [1, 0x395c120], [2, 0x395c120], [3, 0x7b36120], [4, 0x7b36120], [5, 0x395c120], [0, 0x39de140], [1, 0x39de140], [2, 0x39de140], [3, 0x7bb8140]]}
  ff.bert.encoder.layer.15.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3dc5900], [2, 0x3d04980], [3, 0x7ecc500], [4, 0x7f2dd00], [5, 0x3d45980], [0, 0x3dc5100], [1, 0x3dc79a0], [2, 0x3d06a20]]}
  ff.bert.encoder.layer.15.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d47a20]]}
  ff.bert.encoder.layer.15.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ece5a0]]}
  ff.bert.encoder.layer.16.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3a23b00], [1, 0x3a60a00], [2, 0x39e6c00], [3, 0x7b83d00], [4, 0x7b83d00], [5, 0x39a9d00], [0, 0x3a64b20], [1, 0x3aa1a20]]}
  ff.bert.encoder.layer.16.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x39e2ae0], [3, 0x7b7fbe0], [4, 0x7b7fbe0], [5, 0x39a5be0]]}
  ff.bert.encoder.layer.16.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x39a1ac0], [1, 0x39de9c0], [2, 0x39a1ac0], [3, 0x7b3ebc0], [4, 0x7b3ebc0], [5, 0x3964bc0], [0, 0x39e2ae0], [1, 0x3a1f9e0]]}
  ff.bert.encoder.layer.16.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x399d9a0], [3, 0x7b3aaa0], [4, 0x7b3aaa0], [5, 0x3960aa0]]}
  ff.reciprocal_of_sqrt_of_head_size_16:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x39a1280]]}
  ff.bert.encoder.layer.16.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x395c140], [2, 0x391b960], [3, 0x7af9240], [4, 0x7af9240], [5, 0x391f240], [0, 0x3960260], [1, 0x399d160], [2, 0x395c980]]}
  ff.bert.encoder.layer.16.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7af5120], [4, 0x7af5120], [5, 0x391b120], [0, 0x395c140]]}
  ff.bert.encoder.layer.16.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x391b120], [1, 0x391b120]]}
  ff.bert.encoder.layer.16.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3affa60], [0, 0x3bba880], [1, 0x3c71580], [2, 0x3c30da0]]}
  ff.bert.encoder.layer.16.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x38da100]]}
  ff.bert.encoder.layer.16.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3b88300]]}
  ff.bert.encoder.layer.16.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3c07240], [3, 0x7dd1660], [4, 0x7de3ae0], [5, 0x3b78720], [0, 0x3b16f40], [1, 0x3b98720], [2, 0x3c89260], [3, 0x7e53680], [4, 0x7e65b00], [5, 0x3bfa740], [0, 0x3b98f60], [1, 0x3c1a740], [2, 0x3d0b280], [3, 0x7ed56a0], [4, 0x7ee7b20], [5, 0x3c7c760]]}
  ff.bert.encoder.layer.16.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3c1af80], [1, 0x3c9c760], [2, 0x3d8d2a0], [3, 0x7f576c0]]}
  ff.bert.encoder.layer.16.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x38da100], [0, 0x38da940], [1, 0x38da940], [2, 0x38ea520], [3, 0x7ab4940], [4, 0x7ac4520], [5, 0x395c120], [0, 0x395c960], [1, 0x395c960], [2, 0x396c540], [3, 0x7b36960], [4, 0x7b46540], [5, 0x39de140], [0, 0x39de980], [1, 0x39de980], [2, 0x39ee560]]}
  ff.bert.encoder.layer.16.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7bb8980], [4, 0x7bc8560], [5, 0x3a60160], [0, 0x3a609a0], [1, 0x3a609a0], [2, 0x3a70580], [3, 0x7bbaa20], [4, 0x7bca600]]}
  ff.bert.encoder.layer.16.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3a62a40]]}
  ff.bert.encoder.layer.16.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3a65b20]]}
  ff.bert.encoder.layer.17.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7b779a0], [5, 0x399d9a0], [0, 0x3964bc0], [1, 0x39a53a0], [2, 0x39a53a0], [3, 0x7b3ebc0], [4, 0x7bb89c0], [5, 0x39de9c0]]}
  ff.bert.encoder.layer.17.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3960aa0], [1, 0x39a1280], [2, 0x39a1280], [3, 0x7b3aaa0]]}
  ff.bert.encoder.layer.17.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7af5960], [5, 0x391b960], [0, 0x391fa80], [1, 0x3960260], [2, 0x3960260], [3, 0x7af9a80], [4, 0x7b36980], [5, 0x395c980]]}
  ff.bert.encoder.layer.17.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x391b960], [1, 0x395c140], [2, 0x395c140], [3, 0x7af5960]]}
  ff.reciprocal_of_sqrt_of_head_size_17:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7af5120]]}
  ff.bert.encoder.layer.17.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3d7a7a0], [1, 0x3cc7380], [2, 0x3c13f60], [3, 0x7dad780], [4, 0x7e64480], [5, 0x3d7e080], [0, 0x3dbb7c0], [1, 0x3d083a0]]}
  ff.bert.encoder.layer.17.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3c0fe40], [3, 0x7da9660], [4, 0x7e60360], [5, 0x3d79f60]]}
  ff.bert.encoder.layer.17.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3cf8760], [1, 0x3c45340], [2, 0x3bcee20], [3, 0x7d68640], [4, 0x7e1f340], [5, 0x3d38f40], [0, 0x3d39780], [1, 0x3c86360]]}
  ff.bert.encoder.layer.17.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3bcad00], [3, 0x7d64520], [4, 0x7e1b220], [5, 0x3d34e20]]}
  ff.bert.encoder.layer.17.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7c4adc0]]}
  ff.bert.encoder.layer.17.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3a73660]]}
  ff.bert.encoder.layer.17.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x39f26c0], [1, 0x3a022a0], [2, 0x3a73ea0], [3, 0x7c5b1e0], [4, 0x7c3ba20], [5, 0x3a83a80], [0, 0x3a746e0], [1, 0x3a842c0], [2, 0x3af5ec0], [3, 0x7cdd200], [4, 0x7cbda40], [5, 0x3b05aa0], [0, 0x3af6700], [1, 0x3b062e0], [2, 0x3b77ee0], [3, 0x7d5f220]]}
  ff.bert.encoder.layer.17.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7d3fa60], [5, 0x3b87ac0], [0, 0x3b78720], [1, 0x3b88300]]}
  ff.bert.encoder.layer.17.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x38dfac0], [1, 0x38dea80], [2, 0x38dea80], [3, 0x7ac6e00], [4, 0x7ab7220], [5, 0x38ef6a0], [0, 0x3961ae0], [1, 0x3960aa0], [2, 0x3960aa0], [3, 0x7b48e20], [4, 0x7b39240], [5, 0x39716c0], [0, 0x39e3b00], [1, 0x39e2ac0], [2, 0x39e2ac0], [3, 0x7bcae40]]}
  ff.bert.encoder.layer.17.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7bbb260], [5, 0x39f36e0], [0, 0x3a65b20], [1, 0x3a64ae0], [2, 0x3a64ae0], [3, 0x7c4ce60], [4, 0x7bbd300], [5, 0x39f5780]]}
  ff.bert.encoder.layer.17.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7d56020]]}
  ff.bert.encoder.layer.17.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3b7d880]]}
  ff.bert.encoder.layer.18.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7b779a0], [5, 0x399d9a0], [0, 0x3964bc0], [1, 0x39a53a0], [2, 0x39a53a0], [3, 0x7b3ebc0], [4, 0x7bb89c0], [5, 0x39de9c0]]}
  ff.bert.encoder.layer.18.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3960aa0], [1, 0x39a1280], [2, 0x39a1280], [3, 0x7b3aaa0]]}
  ff.bert.encoder.layer.18.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7af5960], [5, 0x391b960], [0, 0x391fa80], [1, 0x3960260], [2, 0x3960260], [3, 0x7af9a80], [4, 0x7b36980], [5, 0x395c980]]}
  ff.bert.encoder.layer.18.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x391b960], [1, 0x395c140], [2, 0x395c140], [3, 0x7af5960]]}
  ff.reciprocal_of_sqrt_of_head_size_18:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7af5120]]}
  ff.bert.encoder.layer.18.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3d7a7a0], [1, 0x3cc7380], [2, 0x3c13f60], [3, 0x7dad780], [4, 0x7e64480], [5, 0x3d7e080], [0, 0x3dbb7c0], [1, 0x3d083a0]]}
  ff.bert.encoder.layer.18.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3c0fe40], [3, 0x7da9660], [4, 0x7e60360], [5, 0x3d79f60]]}
  ff.bert.encoder.layer.18.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3cf8760], [1, 0x3c45340], [2, 0x3bcee20], [3, 0x7d68640], [4, 0x7e1f340], [5, 0x3d38f40], [0, 0x3d39780], [1, 0x3c86360]]}
  ff.bert.encoder.layer.18.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3bcad00], [3, 0x7d64520], [4, 0x7e1b220], [5, 0x3d34e20]]}
  ff.bert.encoder.layer.18.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7c4adc0]]}
  ff.bert.encoder.layer.18.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3a73660]]}
  ff.bert.encoder.layer.18.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x39f26c0], [1, 0x3a022a0], [2, 0x3a73ea0], [3, 0x7c5b1e0], [4, 0x7c3ba20], [5, 0x3a83a80], [0, 0x3a746e0], [1, 0x3a842c0], [2, 0x3af5ec0], [3, 0x7cdd200], [4, 0x7cbda40], [5, 0x3b05aa0], [0, 0x3af6700], [1, 0x3b062e0], [2, 0x3b77ee0], [3, 0x7d5f220]]}
  ff.bert.encoder.layer.18.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7d3fa60], [5, 0x3b87ac0], [0, 0x3b78720], [1, 0x3b88300]]}
  ff.bert.encoder.layer.18.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x38dfac0], [1, 0x38dea80], [2, 0x38dea80], [3, 0x7ac6e00], [4, 0x7ab7220], [5, 0x38ef6a0], [0, 0x3961ae0], [1, 0x3960aa0], [2, 0x3960aa0], [3, 0x7b48e20], [4, 0x7b39240], [5, 0x39716c0], [0, 0x39e3b00], [1, 0x39e2ac0], [2, 0x39e2ac0], [3, 0x7bcae40]]}
  ff.bert.encoder.layer.18.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7bbb260], [5, 0x39f36e0], [0, 0x3a65b20], [1, 0x3a64ae0], [2, 0x3a64ae0], [3, 0x7c4ce60], [4, 0x7bbd300], [5, 0x39f5780]]}
  ff.bert.encoder.layer.18.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7d56020]]}
  ff.bert.encoder.layer.18.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x3b7d880]]}
  ff.bert.encoder.layer.19.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7b779a0], [4, 0x7b779a0], [5, 0x3964380], [0, 0x3964bc0], [1, 0x39a53a0], [2, 0x39a53a0], [3, 0x7bb89c0], [4, 0x7bb89c0]]}
  ff.bert.encoder.layer.19.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3960260], [0, 0x3960aa0], [1, 0x39a1280], [2, 0x39a1280]]}
  ff.bert.encoder.layer.19.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7af5960], [4, 0x7af5960], [5, 0x391f240], [0, 0x391fa80], [1, 0x3960260], [2, 0x3960260], [3, 0x7b36980], [4, 0x7b36980]]}
  ff.bert.encoder.layer.19.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x391b120], [0, 0x391b960], [1, 0x395c140], [2, 0x395c140]]}
  ff.reciprocal_of_sqrt_of_head_size_19:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7af5120]]}
  ff.bert.encoder.layer.19.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7e99140], [4, 0x7e1ba60], [5, 0x3b8e640], [0, 0x3c08c80], [1, 0x3cbf980], [2, 0x3cc3260], [3, 0x7eda160], [4, 0x7e5ca80]]}
  ff.bert.encoder.layer.19.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3b8a520], [0, 0x3c04b60], [1, 0x3cbb860], [2, 0x3cbf140]]}
  ff.bert.encoder.layer.19.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7e17100], [4, 0x7d99a20], [5, 0x3b49500], [0, 0x3bc3b40], [1, 0x3c7a840], [2, 0x3c7e120], [3, 0x7e58120], [4, 0x7ddaa40]]}
  ff.bert.encoder.layer.19.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3b453e0], [0, 0x3bbfa20], [1, 0x3c76720], [2, 0x3c7a000]]}
  ff.bert.encoder.layer.19.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c3d240]]}
  ff.bert.encoder.layer.19.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x39eeda0]]}
  ff.bert.encoder.layer.19.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab71e0], [4, 0x7ab4940], [5, 0x38ea520], [0, 0x38ead60], [1, 0x38ead60], [2, 0x38ed600], [3, 0x7b39200], [4, 0x7b36960], [5, 0x396c540], [0, 0x396cd80], [1, 0x396cd80], [2, 0x396f620], [3, 0x7bbb220], [4, 0x7bb8980], [5, 0x39ee560], [0, 0x39eeda0]]}
  ff.bert.encoder.layer.19.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100], [0, 0x38da940], [1, 0x38da940], [2, 0x38dd1e0]]}
  ff.bert.encoder.layer.19.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d94fa0], [5, 0x3bbafa0], [0, 0x3c3a720], [1, 0x3c3a720], [2, 0x3beb3a0], [3, 0x7dc53a0], [4, 0x7e16fc0], [5, 0x3c3cfc0], [0, 0x3cbc740], [1, 0x3cbc740], [2, 0x3c6d3c0], [3, 0x7e473c0], [4, 0x7e98fe0], [5, 0x3cbefe0], [0, 0x3d3e760], [1, 0x3d3e760]]}
  ff.bert.encoder.layer.19.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d41840], [0, 0x3dc0fc0], [1, 0x3dc3860], [2, 0x3d028e0], [3, 0x7eca460], [4, 0x7f2bc60], [5, 0x3d438e0], [0, 0x3dc3060]]}
  ff.bert.encoder.layer.19.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f1b000]]}
  ff.bert.encoder.layer.19.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3cef3e0]]}
  ff.bert.encoder.layer.20.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3b86400], [0, 0x3c41220], [1, 0x3cbe900], [2, 0x3c7e120], [3, 0x7d9db40], [4, 0x7d23d40], [5, 0x3bc7420], [0, 0x3c82240]]}
  ff.bert.encoder.layer.20.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3cba7e0], [2, 0x3c7a000], [3, 0x7d99a20], [4, 0x7d1fc20]]}
  ff.bert.encoder.layer.20.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3b043c0], [0, 0x3bbf1e0], [1, 0x3c797c0], [2, 0x3c38fe0], [3, 0x7d58a00], [4, 0x7cdec00], [5, 0x3b453e0], [0, 0x3c00200]]}
  ff.bert.encoder.layer.20.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3c756a0], [2, 0x3c34ec0], [3, 0x7d548e0], [4, 0x7cdaae0]]}
  ff.reciprocal_of_sqrt_of_head_size_20:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3b03b80]]}
  ff.bert.encoder.layer.20.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3baed60], [3, 0x7cd2060], [4, 0x7c98a40], [5, 0x3abea40], [0, 0x3b79860], [1, 0x3c30560], [2, 0x3befd80], [3, 0x7d13080]]}
  ff.bert.encoder.layer.20.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7c94920], [5, 0x3aba920], [0, 0x3b75740], [1, 0x3c2c440]]}
  ff.bert.encoder.layer.20.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3b2cd20], [3, 0x7c50020], [4, 0x7c53900], [5, 0x3a79900], [0, 0x3b34720], [1, 0x3beb420], [2, 0x3b6dd40], [3, 0x7c91040]]}
  ff.bert.encoder.layer.20.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7c4f7e0], [5, 0x3a757e0], [0, 0x3b30600], [1, 0x3be7300]]}
  ff.bert.encoder.layer.20.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7f67ae0]]}
  ff.bert.encoder.layer.20.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3d01860]]}
  ff.bert.encoder.layer.20.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3c2e480], [1, 0x3cad3c0], [2, 0x3d9df00], [3, 0x7f77f00], [4, 0x7f6abc0], [5, 0x3d11c80], [0, 0x3cb04a0], [1, 0x3d2f3e0], [2, 0x3e1ff20], [3, 0x7ff9f20], [4, 0x7fecbe0], [5, 0x3d93ca0], [0, 0x3d324c0], [1, 0x3db1400], [2, 0x3ea1f40], [3, 0x807bf40]]}
  ff.bert.encoder.layer.20.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x806ec00], [5, 0x3e15cc0], [0, 0x3db44e0], [1, 0x3e33420]]}
  ff.bert.encoder.layer.20.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3a75700], [3, 0x7bbd300], [4, 0x7bccee0], [5, 0x3a72e60], [0, 0x3a63ac0], [1, 0x3a75f40], [2, 0x3af7720], [3, 0x7c3f320], [4, 0x7c4ef00], [5, 0x3af4e80], [0, 0x3ae5ae0], [1, 0x3af7f60], [2, 0x3b79740], [3, 0x7cc1340], [4, 0x7cd0f20], [5, 0x3b76ea0]]}
  ff.bert.encoder.layer.20.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3b67b00], [1, 0x3b79f80], [2, 0x3bfb760], [3, 0x7d43360], [4, 0x7d52f40], [5, 0x3bf8ec0], [0, 0x3b69ba0], [1, 0x3b7c020]]}
  ff.bert.encoder.layer.20.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3bfe040]]}
  ff.bert.encoder.layer.20.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7d580c0]]}
  ff.bert.encoder.layer.21.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3cb2de0], [0, 0x3c76720], [1, 0x3c03ae0], [2, 0x3b89ce0], [3, 0x7d23500], [4, 0x7dda200], [5, 0x3cf3e00], [0, 0x3cb7740]]}
  ff.bert.encoder.layer.21.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3bff9c0], [2, 0x3b85bc0], [3, 0x7d1f3e0], [4, 0x7dd60e0]]}
  ff.bert.encoder.layer.21.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3c30da0], [0, 0x3bf46e0], [1, 0x3bbe9a0], [2, 0x3b44ba0], [3, 0x7cde3c0], [4, 0x7d950c0], [5, 0x3c71dc0], [0, 0x3c35700]]}
  ff.bert.encoder.layer.21.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3bba880], [2, 0x3b40a80], [3, 0x7cda2a0], [4, 0x7d90fa0]]}
  ff.reciprocal_of_sqrt_of_head_size_21:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3c30560]]}
  ff.bert.encoder.layer.21.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3b71e60], [1, 0x3b38840], [2, 0x3aff220], [3, 0x7c98a40], [4, 0x7d4f740], [5, 0x3bef540], [0, 0x3bb2e80], [1, 0x3b79860]]}
  ff.bert.encoder.layer.21.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3afb100], [3, 0x7c94920], [4, 0x7d4b620], [5, 0x3beb420]]}
  ff.bert.encoder.layer.21.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3aefe20], [1, 0x3ab6800], [2, 0x3aba0e0], [3, 0x7c53900], [4, 0x7d0a600], [5, 0x3baa400], [0, 0x3b30e40], [1, 0x3af7820]]}
  ff.bert.encoder.layer.21.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3ab5fc0], [3, 0x7c4f7e0], [4, 0x7d064e0], [5, 0x3ba62e0]]}
  ff.bert.encoder.layer.21.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3b98720]]}
  ff.bert.encoder.layer.21.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7de4320]]}
  ff.bert.encoder.layer.21.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7d52f60], [5, 0x3b98720], [0, 0x3b89380], [1, 0x3ba8b40], [2, 0x3bfaf80], [3, 0x7df4740], [4, 0x7dd4f80], [5, 0x3c1a740], [0, 0x3c0b3a0], [1, 0x3c2ab60], [2, 0x3c7cfa0], [3, 0x7e76760], [4, 0x7e56fa0], [5, 0x3c9c760], [0, 0x3c8d3c0], [1, 0x3cacb80]]}
  ff.bert.encoder.layer.21.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3cfefc0], [3, 0x7ef8780], [4, 0x7ed8fc0], [5, 0x3d1e780]]}
  ff.bert.encoder.layer.21.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3bee460], [1, 0x3bed420], [2, 0x3b6e4e0], [3, 0x7d66440], [4, 0x7d45c40], [5, 0x3b8dca0], [0, 0x3c70480], [1, 0x3c6f440], [2, 0x3bf0500], [3, 0x7de8460], [4, 0x7dc7c60], [5, 0x3c0fcc0], [0, 0x3cf24a0], [1, 0x3cf1460], [2, 0x3c72520], [3, 0x7e6a480]]}
  ff.bert.encoder.layer.21.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7e49c80], [5, 0x3c91ce0], [0, 0x3d744c0], [1, 0x3d73480], [2, 0x3cf4540], [3, 0x7eec4a0], [4, 0x7e4bd20], [5, 0x3c93d80]]}
  ff.bert.encoder.layer.21.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3d76da0]]}
  ff.bert.encoder.layer.21.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3cf96c0]]}
  ff.bert.encoder.layer.22.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3cb2de0], [0, 0x3c76720], [1, 0x3c03ae0], [2, 0x3b89ce0], [3, 0x7d23500], [4, 0x7dda200], [5, 0x3cf3e00], [0, 0x3cb7740]]}
  ff.bert.encoder.layer.22.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3bff9c0], [2, 0x3b85bc0], [3, 0x7d1f3e0], [4, 0x7dd60e0]]}
  ff.bert.encoder.layer.22.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3c30da0], [0, 0x3bf46e0], [1, 0x3bbe9a0], [2, 0x3b44ba0], [3, 0x7cde3c0], [4, 0x7d950c0], [5, 0x3c71dc0], [0, 0x3c35700]]}
  ff.bert.encoder.layer.22.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3bba880], [2, 0x3b40a80], [3, 0x7cda2a0], [4, 0x7d90fa0]]}
  ff.reciprocal_of_sqrt_of_head_size_22:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3c30560]]}
  ff.bert.encoder.layer.22.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3b71e60], [1, 0x3b38840], [2, 0x3aff220], [3, 0x7c98a40], [4, 0x7d4f740], [5, 0x3bef540], [0, 0x3bb2e80], [1, 0x3b79860]]}
  ff.bert.encoder.layer.22.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3afb100], [3, 0x7c94920], [4, 0x7d4b620], [5, 0x3beb420]]}
  ff.bert.encoder.layer.22.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3aefe20], [1, 0x3ab6800], [2, 0x3aba0e0], [3, 0x7c53900], [4, 0x7d0a600], [5, 0x3baa400], [0, 0x3b30e40], [1, 0x3af7820]]}
  ff.bert.encoder.layer.22.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3ab5fc0], [3, 0x7c4f7e0], [4, 0x7d064e0], [5, 0x3ba62e0]]}
  ff.bert.encoder.layer.22.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3b98720]]}
  ff.bert.encoder.layer.22.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7de4320]]}
  ff.bert.encoder.layer.22.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7d52f60], [5, 0x3b98720], [0, 0x3b89380], [1, 0x3ba8b40], [2, 0x3bfaf80], [3, 0x7df4740], [4, 0x7dd4f80], [5, 0x3c1a740], [0, 0x3c0b3a0], [1, 0x3c2ab60], [2, 0x3c7cfa0], [3, 0x7e76760], [4, 0x7e56fa0], [5, 0x3c9c760], [0, 0x3c8d3c0], [1, 0x3cacb80]]}
  ff.bert.encoder.layer.22.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3cfefc0], [3, 0x7ef8780], [4, 0x7ed8fc0], [5, 0x3d1e780]]}
  ff.bert.encoder.layer.22.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3bee460], [1, 0x3bed420], [2, 0x3b6e4e0], [3, 0x7d66440], [4, 0x7d45c40], [5, 0x3b8dca0], [0, 0x3c70480], [1, 0x3c6f440], [2, 0x3bf0500], [3, 0x7de8460], [4, 0x7dc7c60], [5, 0x3c0fcc0], [0, 0x3cf24a0], [1, 0x3cf1460], [2, 0x3c72520], [3, 0x7e6a480]]}
  ff.bert.encoder.layer.22.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7e49c80], [5, 0x3c91ce0], [0, 0x3d744c0], [1, 0x3d73480], [2, 0x3cf4540], [3, 0x7eec4a0], [4, 0x7e4bd20], [5, 0x3c93d80]]}
  ff.bert.encoder.layer.22.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3d76da0]]}
  ff.bert.encoder.layer.22.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3cf96c0]]}
  ff.bert.encoder.layer.23.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3bf7fc0], [3, 0x7d950c0], [4, 0x7d581c0], [5, 0x3b043c0], [0, 0x3b7ea00], [1, 0x3c35700], [2, 0x3c38fe0], [3, 0x7dd60e0]]}
  ff.bert.encoder.layer.23.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7d540a0], [5, 0x3b002a0], [0, 0x3b7a8e0], [1, 0x3c315e0]]}
  ff.bert.encoder.layer.23.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3b75f80], [3, 0x7d13080], [4, 0x7d13080], [5, 0x3abf280], [0, 0x3b398c0], [1, 0x3bf05c0], [2, 0x3bb6fa0], [3, 0x7d540a0]]}
  ff.bert.encoder.layer.23.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7c844a0], [4, 0x7c844a0], [5, 0x3a306a0], [0, 0x3a30ee0]]}
  ff.reciprocal_of_sqrt_of_head_size_23:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3abea40]]}
  ff.bert.encoder.layer.23.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3ab7040], [1, 0x3b6dd40], [2, 0x3b34720], [3, 0x7cd1820], [4, 0x7cd1820], [5, 0x3a7da20], [0, 0x3af8060], [1, 0x3baed60]]}
  ff.bert.encoder.layer.23.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3b30600], [3, 0x7ccd700], [4, 0x7ccd700], [5, 0x3a79900]]}
  ff.bert.encoder.layer.23.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3a35000], [1, 0x3aebd00], [2, 0x3aef5e0], [3, 0x7c8c6e0], [4, 0x7c8c6e0], [5, 0x3a388e0], [0, 0x3a76020], [1, 0x3b2cd20]]}
  ff.bert.encoder.layer.23.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3aeb4c0], [3, 0x7c885c0], [4, 0x7c885c0], [5, 0x3a347c0]]}
  ff.bert.encoder.layer.23.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7de3ae0]]}
  ff.bert.encoder.layer.23.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3b95640]]}
  ff.bert.encoder.layer.23.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3b88b40], [1, 0x3b8b3e0], [2, 0x3b7b800], [3, 0x7df3f00], [4, 0x7dd1ea0], [5, 0x3ba5a60], [0, 0x3c0ab60], [1, 0x3c0d400], [2, 0x3bfd820], [3, 0x7e75f20], [4, 0x7e53ec0], [5, 0x3c27a80], [0, 0x3c8cb80], [1, 0x3c8f420], [2, 0x3c7f840], [3, 0x7ef7f40]]}
  ff.bert.encoder.layer.23.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ed5ee0], [5, 0x3ca9aa0], [0, 0x3d0eba0], [1, 0x3d11440]]}
  ff.bert.encoder.layer.23.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3d01860], [3, 0x7f79f60], [4, 0x7ee6300], [5, 0x3cb9ec0], [0, 0x3d1efc0], [1, 0x3d21860], [2, 0x3d83880], [3, 0x7ffbf80], [4, 0x7f68320], [5, 0x3d3bee0], [0, 0x3da0fe0], [1, 0x3da3880], [2, 0x3e058a0], [3, 0x807dfa0], [4, 0x7fea340], [5, 0x3dbdf00]]}
  ff.bert.encoder.layer.23.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e23000], [1, 0x3e258a0], [2, 0x3e878c0], [3, 0x80fffc0], [4, 0x806c360], [5, 0x3e3ff20], [0, 0x3e250a0], [1, 0x3e27940]]}
  ff.bert.encoder.layer.23.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e8a1a0]]}
  ff.bert.encoder.layer.23.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x80714e0]]}

  # constant
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f1c6c0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3fd0b20]]}
  lc.input_tensor.attention_mask_s_brcst_m2_23_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ee7180]]}
  lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3dc79e0]]}
  lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f1be80]]}
  lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x391b120]]}
  lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7f30fe0]]}
  dc.input_tensor.norm_mha_0.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3e51da0], [2, 0x3e211a0]]}
  lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3e92580]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3d96f80]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7f307a0]]}
  lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3fcb940]]}
  lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x40ac8a0]]}
  dc.input_tensor.norm_ff_0.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x8296480], [4, 0x8217540]]}
  lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3f3e680]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3edcea0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x40ad0e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7ffb200]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3e619e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_22_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f502e0]]}
  lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3f14e00]]}
  lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3ed3de0]]}
  lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3e8e460]]}
  lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x80704c0]]}
  dc.input_tensor.norm_mha_1.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3d2eba0], [2, 0x3d0f3e0]]}
  lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7f08ba0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7ee93e0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3d0fc20]]}
  lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3d89260]]}
  lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3d78640]]}
  dc.input_tensor.norm_ff_1.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3d0bb80], [3, 0x7ef3f00]]}
  lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7e52740]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3c9a7a0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3d78e80]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7ffb200]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3e619e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_21_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3ecc260]]}
  lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3f14e00]]}
  lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3ed3de0]]}
  lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3e8e460]]}
  lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x80704c0]]}
  dc.input_tensor.norm_mha_2.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3d2eba0], [2, 0x3d0f3e0]]}
  lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7f08ba0]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7ee93e0]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3d0fc20]]}
  lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3d89260]]}
  lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3d78640]]}
  dc.input_tensor.norm_ff_2.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3d0bb80], [3, 0x7ef3f00]]}
  lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7e52740]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x3c9a7a0]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3d78e80]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3da3b20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x80b1f00]]}
  lc.input_tensor.attention_mask_s_brcst_m2_20_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3e5c6c0]]}
  lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3e5e100]]}
  lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3ed4620]]}
  lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3d96740]]}
  lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e27980]]}
  dc.input_tensor.norm_mha_3.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3b88300], [2, 0x3b78720]]}
  lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e2a220]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e9a5c0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8081900]]}
  lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f9ef40]]}
  lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x40aa1e0]]}
  dc.input_tensor.norm_ff_3.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8246ac0], [4, 0x8317e20]]}
  lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x40ce2c0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x41a7040]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x40aaa20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3d4d4e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7ee6d00]]}
  lc.input_tensor.attention_mask_s_brcst_m2_19_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8249ba0]]}
  lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3dc72e0]]}
  lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3dc72e0]]}
  lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3e6aca0]]}
  lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x821a620]]}
  dc.input_tensor.norm_mha_4.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3f3eec0], [0, 0x3edd6e0]]}
  lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3f23f60]]}
  lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x4045fe0]]}
  lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x4162600]]}
  lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3c10500]]}
  lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7d48520]]}
  dc.input_tensor.norm_ff_4.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7d6a580], [5, 0x3c00920]]}
  lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3b705c0]]}
  lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3b82a40]]}
  lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7d48d60]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7eea5e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3cd6fc0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_18_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8315540]]}
  lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7feeea0]]}
  lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x8189d40]]}
  lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x40ed3e0]]}
  lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x806fc80]]}
  dc.input_tensor.norm_mha_5.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3ec5020], [0, 0x3e248c0]]}
  lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3e56500]]}
  lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3ea8940]]}
  lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3d0f3e0]]}
  lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x807f0e0]]}
  lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7fdb080]]}
  dc.input_tensor.norm_ff_5.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3e230e0], [0, 0x3f21fc0]]}
  lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7ef1620]]}
  lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3f2a200]]}
  lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x40078a0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7eea5e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3cd6fc0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_17_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x82441e0]]}
  lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7feeea0]]}
  lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x8189d40]]}
  lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x40ed3e0]]}
  lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x806fc80]]}
  dc.input_tensor.norm_mha_6.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3ec5020], [0, 0x3e248c0]]}
  lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3e56500]]}
  lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3ea8940]]}
  lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3d0f3e0]]}
  lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x807f0e0]]}
  lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7fdb080]]}
  dc.input_tensor.norm_ff_6.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x3e230e0], [0, 0x3f21fc0]]}
  lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7ef1620]]}
  lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x3f2a200]]}
  lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x40078a0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3d08be0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3bd3fc0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_16_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x40a7900]]}
  lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7ea1bc0]]}
  lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7f1f2a0]]}
  lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x40fd860]]}
  lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4020e60]]}
  dc.input_tensor.norm_mha_7.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8299560], [4, 0x81965a0]]}
  lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f6a160]]}
  lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3fbe640]]}
  lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40216a0]]}
  lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x40d7420]]}
  lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x401a5c0]]}
  dc.input_tensor.norm_ff_7.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x818a360], [5, 0x3f30c20]]}
  lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x80f1d80]]}
  lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x813b820]]}
  lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3bf8ec0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3fe82e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7fd8220]]}
  lc.input_tensor.attention_mask_s_brcst_m2_15_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b79740]]}
  lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x80f9140]]}
  lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3faeca0]]}
  lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7f14980]]}
  lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x41cc880]]}
  dc.input_tensor.norm_mha_8.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x417ac80], [1, 0x42772a0]]}
  lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x4358200]]}
  lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x85c5e60]]}
  lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x41cd0c0]]}
  lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3d8bb00]]}
  lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3bfe040]]}
  dc.input_tensor.norm_ff_8.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7f89c40], [5, 0x3e90380]]}
  lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3e7ef60]]}
  lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3e913e0]]}
  lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7fd7780]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x8240a40]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3ef9040]]}
  lc.input_tensor.attention_mask_s_brcst_m2_14_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d53740]]}
  lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3f76720]]}
  lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x4029b40]]}
  lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3f5e8a0]]}
  lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x402f1e0]]}
  dc.input_tensor.norm_mha_9.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x8235ce0], [4, 0x8185160]]}
  lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3fec980]]}
  lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3fbde20]]}
  lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x402fa20]]}
  lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x838da80]]}
  lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x82e9a20]]}
  dc.input_tensor.norm_ff_9.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x40b2b40], [0, 0x41c1600]]}
  lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x4110260]]}
  lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x40c5800]]}
  lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x82ea260]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x8240a40]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3ef9040]]}
  lc.input_tensor.attention_mask_s_brcst_m2_13_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d42b20]]}
  lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3f76720]]}
  lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x4029b40]]}
  lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3f5e8a0]]}
  lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x402f1e0]]}
  dc.input_tensor.norm_mha_10.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x8235ce0], [4, 0x8185160]]}
  lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3fec980]]}
  lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3fbde20]]}
  lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x402fa20]]}
  lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x838da80]]}
  lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x82e9a20]]}
  dc.input_tensor.norm_ff_10.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x40b2b40], [0, 0x41c1600]]}
  lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x4110260]]}
  lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x40c5800]]}
  lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x82ea260]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3fb87c0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x82863c0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_12_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3b68b20]]}
  lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x40325c0]]}
  lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x80b70a0]]}
  lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4114320]]}
  lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x82c1820]]}
  dc.input_tensor.norm_mha_11.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41c8780], [3, 0x83d1b20]]}
  lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4177ba0]]}
  lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4155b40]]}
  lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x82c0fe0]]}
  lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3a70580]]}
  lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a609a0]]}
  dc.input_tensor.norm_ff_11.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bc8560], [5, 0x39ee560]]}
  lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7c3a160]]}
  lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3a60160]]}
  lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a60160]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7f31820]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7da5d80]]}
  lc.input_tensor.attention_mask_s_brcst_m2_11_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x8181b00]]}
  lc.input_tensor.mha_12_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7c4efa0]]}
  lc.input_tensor.mha_12_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7c4efa0]]}
  lc.input_tensor.norm_mha_12.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3a27c20]]}
  lc.input_tensor.norm_mha_12.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x38da100]]}
  dc.input_tensor.norm_mha_12.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x38ea520], [1, 0x38ea520]]}
  lc.input_tensor.norm_mha_12.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x38ea520]]}
  lc.input_tensor.ff.bert.encoder.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7ac4520]]}
  lc.input_tensor.ff.bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x38da940]]}
  lc.input_tensor.norm_ff_12.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x401b560]]}
  lc.input_tensor.norm_ff_12.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x40078a0]]}
  dc.input_tensor.norm_ff_12.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x4019d20], [2, 0x40c77e0]]}
  lc.input_tensor.norm_ff_12.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3a63280]]}
  lc.input_tensor.ff.bert.encoder.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3b23a60]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7c09e60]]}
  lc.input_tensor.attention_mask_s_brcst_m2_10_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3f2dd00]]}
  lc.input_tensor.mha_13_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3a70640]]}
  lc.input_tensor.mha_13_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3a70640]]}
  lc.input_tensor.norm_mha_13.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x39a5be0]]}
  lc.input_tensor.norm_mha_13.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7c3b1e0]]}
  dc.input_tensor.norm_mha_13.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100]]}
  lc.input_tensor.norm_mha_13.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x38da940]]}
  lc.input_tensor.norm_ff_13.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7ab61a0]]}
  lc.input_tensor.norm_ff_13.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7ab61a0]]}
  dc.input_tensor.norm_ff_13.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x38dc1a0], [0, 0x38dc9e0]]}
  lc.input_tensor.norm_ff_13.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x38de240]]}
  lc.input_tensor.ff.bert.encoder.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x38de240]]}
  lc.input_tensor.ff.bert.encoder.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7ab69e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3b23a60]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7c09e60]]}
  lc.input_tensor.attention_mask_s_brcst_m2_9_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3fa4220]]}
  lc.input_tensor.mha_14_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3a70640]]}
  lc.input_tensor.mha_14_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3a70640]]}
  lc.input_tensor.norm_mha_14.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x39a5be0]]}
  lc.input_tensor.norm_mha_14.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7c3b1e0]]}
  dc.input_tensor.norm_mha_14.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100]]}
  lc.input_tensor.norm_mha_14.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x38da940]]}
  lc.input_tensor.norm_ff_14.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7ab61a0]]}
  lc.input_tensor.norm_ff_14.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7ab61a0]]}
  dc.input_tensor.norm_ff_14.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x38dc1a0], [0, 0x38dc9e0]]}
  lc.input_tensor.norm_ff_14.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x38de240]]}
  lc.input_tensor.ff.bert.encoder.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x38de240]]}
  lc.input_tensor.ff.bert.encoder.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7ab69e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3aeac80]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3a306a0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_8_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3a2fe60]]}
  lc.input_tensor.mha_15_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7c83c60]]}
  lc.input_tensor.mha_15_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7c83c60]]}
  lc.input_tensor.norm_mha_15.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x39e63c0]]}
  lc.input_tensor.norm_mha_15.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3fcf2a0]]}
  dc.input_tensor.norm_mha_15.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81a9aa0], [5, 0x3f7adc0]]}
  lc.input_tensor.norm_mha_15.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x82aca60]]}
  lc.input_tensor.ff.bert.encoder.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3e450a0]]}
  lc.input_tensor.ff.bert.encoder.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3b88300]]}
  lc.input_tensor.norm_ff_15.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d57e40]]}
  lc.input_tensor.norm_ff_15.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f305e0]]}
  dc.input_tensor.norm_ff_15.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3d08ac0], [3, 0x7ede9c0]]}
  lc.input_tensor.norm_ff_15.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3dc9a40]]}
  lc.input_tensor.ff.bert.encoder.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3dc71a0]]}
  lc.input_tensor.ff.bert.encoder.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f2fda0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x39de180]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3960260]]}
  lc.input_tensor.attention_mask_s_brcst_m2_7_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x39e63c0]]}
  lc.input_tensor.mha_16_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7b3a260]]}
  lc.input_tensor.mha_16_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7b3a260]]}
  lc.input_tensor.norm_mha_16.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3cff920]]}
  lc.input_tensor.norm_mha_16.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7c4adc0]]}
  dc.input_tensor.norm_mha_16.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7c5a9a0], [5, 0x39ef5e0]]}
  lc.input_tensor.norm_mha_16.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3a11e80]]}
  lc.input_tensor.ff.bert.encoder.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3a83a80]]}
  lc.input_tensor.ff.bert.encoder.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3b16700]]}
  lc.input_tensor.norm_ff_16.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3a62200]]}
  lc.input_tensor.norm_ff_16.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3a62a40]]}
  dc.input_tensor.norm_ff_16.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3a62a40], [2, 0x3a72620]]}
  lc.input_tensor.norm_ff_16.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7bbcac0]]}
  lc.input_tensor.ff.bert.encoder.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7bcc6a0]]}
  lc.input_tensor.ff.bert.encoder.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x391b120]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7af5120]]}
  lc.input_tensor.attention_mask_s_brcst_m2_6_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x39a5be0]]}
  lc.input_tensor.mha_17_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.mha_17_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3bf3ea0]]}
  lc.input_tensor.norm_mha_17.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3c44b00]]}
  lc.input_tensor.norm_mha_17.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7c3a9a0]]}
  dc.input_tensor.norm_mha_17.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3a70580], [0, 0x39ef5e0]]}
  lc.input_tensor.norm_mha_17.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3a01a60]]}
  lc.input_tensor.ff.bert.encoder.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3a73660]]}
  lc.input_tensor.ff.bert.encoder.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.norm_ff_17.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3a67bc0]]}
  lc.input_tensor.norm_ff_17.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3a66b80]]}
  dc.input_tensor.norm_ff_17.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3a66b80], [3, 0x7c4ef00]]}
  lc.input_tensor.norm_ff_17.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3b6dca0]]}
  lc.input_tensor.ff.bert.encoder.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7d45400]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x391b120]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7af5120]]}
  lc.input_tensor.attention_mask_s_brcst_m2_5_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x39a53a0]]}
  lc.input_tensor.mha_18_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.mha_18_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3bf3ea0]]}
  lc.input_tensor.norm_mha_18.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3c44b00]]}
  lc.input_tensor.norm_mha_18.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7c3a9a0]]}
  dc.input_tensor.norm_mha_18.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3a70580], [0, 0x39ef5e0]]}
  lc.input_tensor.norm_mha_18.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3a01a60]]}
  lc.input_tensor.ff.bert.encoder.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3a73660]]}
  lc.input_tensor.ff.bert.encoder.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.norm_ff_18.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3a67bc0]]}
  lc.input_tensor.norm_ff_18.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3a66b80]]}
  dc.input_tensor.norm_ff_18.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3a66b80], [3, 0x7c4ef00]]}
  lc.input_tensor.norm_ff_18.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3b6dca0]]}
  lc.input_tensor.ff.bert.encoder.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7d45400]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7af5120]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.attention_mask_s_brcst_m2_4_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3befd80]]}
  lc.input_tensor.mha_19_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3c49ca0]]}
  lc.input_tensor.mha_19_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3bcf660]]}
  lc.input_tensor.norm_mha_19.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7d991e0]]}
  lc.input_tensor.norm_mha_19.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39f1e80]]}
  dc.input_tensor.norm_mha_19.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a70dc0], [1, 0x39ff1c0]]}
  lc.input_tensor.norm_mha_19.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a70580]]}
  lc.input_tensor.ff.bert.encoder.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39f1640]]}
  lc.input_tensor.norm_ff_19.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f2b420]]}
  lc.input_tensor.norm_ff_19.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ec9c20]]}
  dc.input_tensor.norm_ff_19.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3dc0780], [2, 0x3cff800]]}
  lc.input_tensor.norm_ff_19.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3dc0780]]}
  lc.input_tensor.ff.bert.encoder.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d41000]]}
  lc.input_tensor.ff.bert.encoder.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ec93e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3bbe9a0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7cda2a0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7d540a0]]}
  lc.input_tensor.mha_20_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3a74fa0]]}
  lc.input_tensor.mha_20_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7cd9a60]]}
  lc.input_tensor.norm_mha_20.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7c4f7e0]]}
  lc.input_tensor.norm_mha_20.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7f69b40]]}
  dc.input_tensor.norm_mha_20.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3cfe780], [0, 0x3c2b3a0]]}
  lc.input_tensor.norm_mha_20.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3cacb80]]}
  lc.input_tensor.ff.bert.encoder.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3d9d6c0]]}
  lc.input_tensor.ff.bert.encoder.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7f6a380]]}
  lc.input_tensor.norm_ff_20.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3bfd800]]}
  lc.input_tensor.norm_ff_20.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7d45400]]}
  dc.input_tensor.norm_ff_20.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7d54fe0], [5, 0x3bfaf60]]}
  lc.input_tensor.norm_ff_20.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3b6bc40]]}
  lc.input_tensor.ff.bert.encoder.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3b7e0c0]]}
  lc.input_tensor.ff.bert.encoder.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7d45c40]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3aef5e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7d90760]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3b2c4e0]]}
  lc.input_tensor.mha_21_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7cd9a60]]}
  lc.input_tensor.mha_21_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3b40240]]}
  lc.input_tensor.norm_mha_21.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3ab5fc0]]}
  lc.input_tensor.norm_mha_21.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3bf9f00]]}
  dc.input_tensor.norm_mha_21.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7de1240], [4, 0x7d4fe80]]}
  lc.input_tensor.norm_mha_21.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3b97ee0]]}
  lc.input_tensor.ff.bert.encoder.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3b88b40]]}
  lc.input_tensor.ff.bert.encoder.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3bfa740]]}
  lc.input_tensor.norm_ff_21.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3d76560]]}
  lc.input_tensor.norm_ff_21.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3d75520]]}
  dc.input_tensor.norm_ff_21.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3cf65e0], [3, 0x7eee540]]}
  lc.input_tensor.norm_ff_21.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7e4ddc0]]}
  lc.input_tensor.ff.bert.encoder.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3c95e20]]}
  lc.input_tensor.ff.bert.encoder.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3d75d60]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3aef5e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7d90760]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3be6ac0]]}
  lc.input_tensor.mha_22_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7cd9a60]]}
  lc.input_tensor.mha_22_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3b40240]]}
  lc.input_tensor.norm_mha_22.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3ab5fc0]]}
  lc.input_tensor.norm_mha_22.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3bf9f00]]}
  dc.input_tensor.norm_mha_22.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7de1240], [4, 0x7d4fe80]]}
  lc.input_tensor.norm_mha_22.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3b97ee0]]}
  lc.input_tensor.ff.bert.encoder.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3b88b40]]}
  lc.input_tensor.ff.bert.encoder.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3bfa740]]}
  lc.input_tensor.norm_ff_22.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3d76560]]}
  lc.input_tensor.norm_ff_22.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3d75520]]}
  dc.input_tensor.norm_ff_22.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3cf65e0], [3, 0x7eee540]]}
  lc.input_tensor.norm_ff_22.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7e4ddc0]]}
  lc.input_tensor.ff.bert.encoder.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x3c95e20]]}
  lc.input_tensor.ff.bert.encoder.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3d75d60]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3b39080]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7d12840]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3b2fdc0]]}
  lc.input_tensor.mha_23_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7d12840]]}
  lc.input_tensor.mha_23_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3b75740]]}
  lc.input_tensor.norm_mha_23.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3aeb4c0]]}
  lc.input_tensor.norm_mha_23.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  dc.input_tensor.norm_mha_23.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100], [3, 0x7ab4100]]}
  lc.input_tensor.norm_mha_23.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c3a9a0]]}
  lc.input_tensor.ff.bert.encoder.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7dd1660]]}
  lc.input_tensor.norm_ff_23.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e89960]]}
  lc.input_tensor.norm_ff_23.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8102060]]}
  dc.input_tensor.norm_ff_23.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x806e400], [5, 0x3e41fc0]]}
  lc.input_tensor.norm_ff_23.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e27140]]}
  lc.input_tensor.ff.bert.encoder.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e299e0]]}
  lc.input_tensor.ff.bert.encoder.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81028a0]]}

  # epoch_to_epoch
  e2e_ff3_gelu_0:                                                                               {input: ff3_gelu, type: queue, entries: 1, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3018c200], [0, 0x301eda20], [0, 0x3024f240], [0, 0x302b0a60], [0, 0x30312280], [0, 0x30373aa0], [0, 0x303d52c0], [0, 0x30436ae0]]}
  e2e_buffer_0_norm_mha_3.dc.add.10_add_ff_3_0:                                                 {input: buffer_0_norm_mha_3.dc.add.10_add_ff_3, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300c91c0], [0, 0x3012a9e0]]}
  e2e_attention_mask_s_brcst_m2_19_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_19_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_18_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_18_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_17_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_17_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_16_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_16_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x30000000]]}
  e2e_ff7_gelu_0:                                                                               {input: ff7_gelu, type: queue, entries: 1, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30498300], [0, 0x304f9b20], [0, 0x3055b340], [0, 0x305bcb60], [0, 0x3061e380], [0, 0x3067fba0], [0, 0x306e13c0], [0, 0x30742be0]]}
  e2e_buffer_0_norm_mha_7.dc.add.10_add_ff_7_0:                                                 {input: buffer_0_norm_mha_7.dc.add.10_add_ff_7, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x307a4400], [0, 0x30805c20]]}
  e2e_attention_mask_s_brcst_m2_15_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_15_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x300061a0]]}
  e2e_attention_mask_s_brcst_m2_14_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_14_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x300061a0]]}
  e2e_attention_mask_s_brcst_m2_13_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_13_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x300061a0]]}
  e2e_attention_mask_s_brcst_m2_12_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_12_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3000c340]]}
  e2e_ff11_gelu_0:                                                                              {input: ff11_gelu, type: queue, entries: 1, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30867440], [0, 0x308c8c60], [0, 0x3092a480], [0, 0x3098bca0], [0, 0x309ed4c0], [0, 0x30a4ece0], [0, 0x30ab0500], [0, 0x30b11d20]]}
  e2e_buffer_0_norm_mha_11.dc.add.10_add_ff_11_0:                                               {input: buffer_0_norm_mha_11.dc.add.10_add_ff_11, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300c91c0], [0, 0x3012a9e0]]}
  e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0:                                   {input: attention_mask_input_op_fork_nop1_input_op_fork_nop0, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x300061a0]]}
  e2e_attention_mask_s_brcst_m2_11_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_11_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3000c340]]}
  e2e_attention_mask_s_brcst_m2_10_1.lc1_0:                                                     {input: attention_mask_s_brcst_m2_10_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x300124e0]]}
  e2e_attention_mask_s_brcst_m2_9_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_9_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x300124e0]]}
  e2e_attention_mask_s_brcst_m2_8_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_8_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x413f0c0]]}
  e2e_ff15_gelu_0:                                                                              {input: ff15_gelu, type: queue, entries: 1, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3018c200], [0, 0x301eda20], [0, 0x3024f240], [0, 0x302b0a60], [0, 0x30312280], [0, 0x30373aa0], [0, 0x303d52c0], [0, 0x30436ae0]]}
  e2e_buffer_0_norm_mha_15.dc.add.10_add_ff_15_0:                                               {input: buffer_0_norm_mha_15.dc.add.10_add_ff_15, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30498300], [0, 0x304f9b20]]}
  e2e_attention_mask_s_brcst_m2_7_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_7_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x300124e0]]}
  e2e_attention_mask_s_brcst_m2_6_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_6_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x30018680]]}
  e2e_attention_mask_s_brcst_m2_5_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_5_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x30018680]]}
  e2e_attention_mask_s_brcst_m2_4_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_4_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x41800e0]]}
  e2e_ff19_gelu_0:                                                                              {input: ff19_gelu, type: queue, entries: 1, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3055b340], [0, 0x305bcb60], [0, 0x3061e380], [0, 0x3067fba0], [0, 0x306e13c0], [0, 0x30742be0], [0, 0x307a4400], [0, 0x30805c20]]}
  e2e_buffer_0_norm_mha_19.dc.add.10_add_ff_19_0:                                               {input: buffer_0_norm_mha_19.dc.add.10_add_ff_19, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300c91c0], [0, 0x3012a9e0]]}
  e2e_attention_mask_s_brcst_m2_3_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_3_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x8183c00]]}
  e2e_attention_mask_s_brcst_m2_2_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_2_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3000c340]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3000c340]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x300124e0]]}
  e2e_add_ff_23_0:                                                                              {input: add_ff_23, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x83c60a0], [4, 0x84278c0]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 1
    input_count: 1
    mha_0_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.query.weight, ff.bert.encoder.layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.key.weight, ff.bert.encoder.layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_as: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_0_query, mha_0_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_div: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_0_as, ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    attention_mask_input_op_fork_nop0: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_input_op_fork_nop0_input_op_fork_nop0: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_23_1.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_23_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_mask: {type: add, grid_loc: [4, 1], grid_size: [2, 1], inputs: [mha_0_as_div, attention_mask_s_brcst_m2_23_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_0_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [4, 3], grid_size: [2, 1], inputs: [mha_0_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_0_as_mask_mha_0_as_softmax.dc.subtract.1: {type: nop, grid_loc: [4, 2], grid_size: [2, 1], inputs: [mha_0_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_0_mha_0_as_mask_mha_0_as_softmax.dc.subtract.1, mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_0_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 6], grid_size: [2, 2], inputs: [mha_0_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_0_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.exp.2, lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_0_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 2], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_0_as_softmax.dc.exp.2_mha_0_as_softmax.dc.multiply.5: {type: nop, grid_loc: [5, 0], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [6, 3], grid_size: [2, 1], inputs: [buffer_0_mha_0_as_softmax.dc.exp.2_mha_0_as_softmax.dc.multiply.5, mha_0_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_0_value: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.value.weight, ff.bert.encoder.layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_ac: {type: matmul, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.multiply.5, mha_0_value],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_0_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_0_ac, ff.bert.encoder.layer.0.attention.output.dense.weight, ff.bert.encoder.layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_0: {type: add, grid_loc: [6, 5], grid_size: [2, 1], inputs: [encoder_input, mha_0_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_22_1.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_22_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_21_1.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_21_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_20_1.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_20_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_19_1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_19_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_18_1.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_18_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_17_1.lc1: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_17_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_16_1.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_16_1.0, attention_mask_input_op_fork_nop0_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_15_1.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_15_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_14_1.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_14_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_13_1.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_13_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_12_1.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_12_1.0, attention_mask_input_op_fork_nop0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_input_op_fork_nop1: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_1_temporal_epoch_0:
    target_device: 2
    input_count: 1
    norm_mha_0.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [add_mha_0, lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_0_norm_mha_0.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_mha_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.subtract.1: {type: subtract, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_add_mha_0_norm_mha_0.dc.subtract.1, norm_mha_0.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_mha_0.dc.multiply.2: {type: multiply, grid_loc: [1, 4], grid_size: [2, 1], inputs: [norm_mha_0.dc.subtract.1, norm_mha_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.2, lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_0.dc.add.5: {type: add, grid_loc: [1, 6], grid_size: [2, 1], inputs: [norm_mha_0.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_0.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [norm_mha_0.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_0.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_mha_0.dc.reciprocal.7, lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8: {type: nop, grid_loc: [1, 2], grid_size: [2, 1], inputs: [norm_mha_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8: {type: nop, grid_loc: [1, 3], grid_size: [2, 1], inputs: [buffer_1_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.multiply.8: {type: multiply, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8, norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0.dc.multiply.9: {type: multiply, grid_loc: [3, 4], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.8, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0.dc.add.10: {type: add, grid_loc: [3, 6], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.9, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_mha_0.dc.add.10_add_ff_0: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_mha_0.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_input_op_fork_nop1_input_op_fork_nop0: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [attention_mask_input_op_fork_nop1],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [0, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, attention_mask_input_op_fork_nop1],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, attention_mask_input_op_fork_nop1],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, attention_mask_input_op_fork_nop1],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, attention_mask_input_op_fork_nop1],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_2_temporal_epoch_0:
    target_device: 3
    input_count: 1
    ff_0_ff1: {type: matmul, grid_loc: [0, 0], grid_size: [6, 4], inputs: [norm_mha_0.dc.add.10, ff.bert.encoder.layer.0.intermediate.dense.weight, ff.bert.encoder.layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff0_gelu: {type: gelu, grid_loc: [0, 4], grid_size: [2, 4], inputs: [ff_0_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    ff_0_ff2: {type: matmul, grid_loc: [6, 0], grid_size: [2, 8], inputs: [ff0_gelu, ff.bert.encoder.layer.0.output.dense.weight, ff.bert.encoder.layer.0.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_0: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [buffer_0_norm_mha_0.dc.add.10_add_ff_0, ff_0_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [2, 1], inputs: [add_ff_0, lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_0_norm_ff_0.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [add_ff_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.subtract.1: {type: subtract, grid_loc: [2, 7], grid_size: [2, 1], inputs: [buffer_0_add_ff_0_norm_ff_0.dc.subtract.1, norm_ff_0.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_0.dc.multiply.2: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_0.dc.subtract.1, norm_ff_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.2, lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_0.dc.add.5: {type: add, grid_loc: [8, 0], grid_size: [2, 1], inputs: [norm_ff_0.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_0.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.sqrt.6: {type: sqrt, grid_loc: [8, 1], grid_size: [2, 1], inputs: [norm_ff_0.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reciprocal.7: {type: reciprocal, grid_loc: [8, 2], grid_size: [2, 1], inputs: [norm_ff_0.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [2, 1], inputs: [norm_ff_0.dc.reciprocal.7, lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [norm_ff_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8: {type: nop, grid_loc: [4, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.multiply.8: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8, norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_0.dc.multiply.9: {type: multiply, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.8, ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_3_temporal_epoch_0:
    target_device: 4
    input_count: 1
    norm_ff_0.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.9, ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_0.dc.add.10_add_mha_1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_ff_0.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_4_temporal_epoch_0:
    target_device: 5
    input_count: 1
    mha_1_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.query.weight, ff.bert.encoder.layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_1_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.key.weight, ff.bert.encoder.layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_1_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_1_query, mha_1_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_1],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_1_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_1_as, ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_1_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_1_as_div, attention_mask_s_brcst_m2_22_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_1_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_1_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_1_as_mask_mha_1_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_1_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_1_as_mask_mha_1_as_softmax.dc.subtract.1, mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_1_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_1_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_1_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.exp.2, lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_1_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_1_as_softmax.dc.exp.2_mha_1_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_1_as_softmax.dc.exp.2_mha_1_as_softmax.dc.multiply.5, mha_1_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_1_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.value.weight, ff.bert.encoder.layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_1_value_mha_1_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_1_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_1_value_mha_1_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_1_value_mha_1_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.multiply.5, buffer_0_mha_1_value_mha_1_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_1_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_1_ac, ff.bert.encoder.layer.1.attention.output.dense.weight, ff.bert.encoder.layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_1: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_0.dc.add.10_add_mha_1, mha_1_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_1, lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_1_norm_mha_1.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_1_norm_mha_1.dc.subtract.1, norm_mha_1.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_5_temporal_epoch_0:
    target_device: 6
    input_count: 1
    norm_mha_1.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_1.dc.subtract.1, norm_mha_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.2, lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_1.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_1.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_1.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_1.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_1.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_1.dc.reciprocal.7, lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8, norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.8, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.9, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_1_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_1.dc.add.10, ff.bert.encoder.layer.1.intermediate.dense.weight, ff.bert.encoder.layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff1_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_1_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_1.dc.add.10_add_ff_1: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_1.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_6_temporal_epoch_0:
    target_device: 7
    input_count: 1
    ff_1_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff1_gelu, ff.bert.encoder.layer.1.output.dense.weight, ff.bert.encoder.layer.1.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_1: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_1.dc.add.10_add_ff_1, ff_1_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_1, lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_1_norm_ff_1.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_1_norm_ff_1.dc.subtract.1, norm_ff_1.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_1.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_1.dc.subtract.1, norm_ff_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.2, lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_1.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_1.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_1.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_1.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_1.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_1.dc.reciprocal.7, lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8, norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.8, ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.9, ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_1.dc.add.10_add_mha_2: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_1.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_7_temporal_epoch_0:
    target_device: 8
    input_count: 1
    mha_2_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.query.weight, ff.bert.encoder.layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_2_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.key.weight, ff.bert.encoder.layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_2_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_2_query, mha_2_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_2],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_2_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_2_as, ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_2_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_2_as_div, attention_mask_s_brcst_m2_21_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_2_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_2_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_2_as_mask_mha_2_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_2_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_2_as_mask_mha_2_as_softmax.dc.subtract.1, mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_2_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_2_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_2_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.exp.2, lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_2_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_2_as_softmax.dc.exp.2_mha_2_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_2_as_softmax.dc.exp.2_mha_2_as_softmax.dc.multiply.5, mha_2_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_2_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.value.weight, ff.bert.encoder.layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_2_value_mha_2_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_2_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_2_value_mha_2_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_2_value_mha_2_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.multiply.5, buffer_0_mha_2_value_mha_2_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_2_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_2_ac, ff.bert.encoder.layer.2.attention.output.dense.weight, ff.bert.encoder.layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_2: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_1.dc.add.10_add_mha_2, mha_2_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_2, lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_2_norm_mha_2.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_2_norm_mha_2.dc.subtract.1, norm_mha_2.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_8_temporal_epoch_0:
    target_device: 9
    input_count: 1
    norm_mha_2.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_2.dc.subtract.1, norm_mha_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.2, lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_2.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_2.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_2.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_2.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_2.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_2.dc.reciprocal.7, lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8, norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.8, ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.9, ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_2_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_2.dc.add.10, ff.bert.encoder.layer.2.intermediate.dense.weight, ff.bert.encoder.layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff2_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_2_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_2.dc.add.10_add_ff_2: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_2.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_9_temporal_epoch_0:
    target_device: 10
    input_count: 1
    ff_2_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff2_gelu, ff.bert.encoder.layer.2.output.dense.weight, ff.bert.encoder.layer.2.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_2: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_2.dc.add.10_add_ff_2, ff_2_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_2, lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_2_norm_ff_2.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_2_norm_ff_2.dc.subtract.1, norm_ff_2.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_2.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_2.dc.subtract.1, norm_ff_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.2, lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_2.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_2.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_2.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_2.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_2.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_2.dc.reciprocal.7, lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8, norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_2.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.8, ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_2.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.9, ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_2.dc.add.10_add_mha_3: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_2.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_10_temporal_epoch_0:
    target_device: 11
    input_count: 1
    mha_3_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.query.weight, ff.bert.encoder.layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_3_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.key.weight, ff.bert.encoder.layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_3_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_3_query, mha_3_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_3],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_3_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_3_as, ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_3_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_3_as_div, attention_mask_s_brcst_m2_20_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_3_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_3_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_3_as_mask_mha_3_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_3_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_3_as_mask_mha_3_as_softmax.dc.subtract.1, mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_3_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_3_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_3_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.exp.2, lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_3_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_3_as_softmax.dc.exp.2_mha_3_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_3_as_softmax.dc.exp.2_mha_3_as_softmax.dc.multiply.5, mha_3_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_3_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.value.weight, ff.bert.encoder.layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_3_value_mha_3_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_3_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_3_value_mha_3_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_3_value_mha_3_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.multiply.5, buffer_0_mha_3_value_mha_3_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_3_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_3_ac, ff.bert.encoder.layer.3.attention.output.dense.weight, ff.bert.encoder.layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_3: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_2.dc.add.10_add_mha_3, mha_3_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_3, lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_3_norm_mha_3.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_3_norm_mha_3.dc.subtract.1, norm_mha_3.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_11_temporal_epoch_0:
    target_device: 0
    input_count: 1
    norm_mha_3.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_3.dc.subtract.1, norm_mha_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.2, lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_3.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_3.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_3.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_3.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_3.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_3.dc.reciprocal.7, lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8, norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.8, ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.9, ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_3_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_3.dc.add.10, ff.bert.encoder.layer.3.intermediate.dense.weight, ff.bert.encoder.layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff3_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_3_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_3.dc.add.10_add_ff_3: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_3.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_12_temporal_epoch_1:
    target_device: 1
    input_count: 1
    ff_3_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_ff3_gelu_0, ff.bert.encoder.layer.3.output.dense.weight, ff.bert.encoder.layer.3.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_3: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_norm_mha_3.dc.add.10_add_ff_3_0, ff_3_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_3, lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_3_norm_ff_3.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_3_norm_ff_3.dc.subtract.1, norm_ff_3.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_3.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_3.dc.subtract.1, norm_ff_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.2, lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_3.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_3.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_3.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_3.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_3.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_3.dc.reciprocal.7, lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8, norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_3.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.8, ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_3.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.9, ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_3.dc.add.10_add_mha_4: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_3.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_13_temporal_epoch_1:
    target_device: 2
    input_count: 1
    mha_4_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_3.dc.add.10, ff.bert.encoder.layer.4.attention.self.query.weight, ff.bert.encoder.layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_4_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_3.dc.add.10, ff.bert.encoder.layer.4.attention.self.key.weight, ff.bert.encoder.layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_4_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_4_query, mha_4_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_4],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_4_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_4_as, ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_4_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_4_as_div, e2e_attention_mask_s_brcst_m2_19_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_4_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_4_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_4_as_mask_mha_4_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_4_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_4_as_mask_mha_4_as_softmax.dc.subtract.1, mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_4_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_4_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_4_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.exp.2, lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_4_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_4_as_softmax.dc.exp.2_mha_4_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_4_as_softmax.dc.exp.2_mha_4_as_softmax.dc.multiply.5, mha_4_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_4_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_3.dc.add.10, ff.bert.encoder.layer.4.attention.self.value.weight, ff.bert.encoder.layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_4_value_mha_4_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_4_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_4_value_mha_4_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_4_value_mha_4_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.multiply.5, buffer_0_mha_4_value_mha_4_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_4_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_4_ac, ff.bert.encoder.layer.4.attention.output.dense.weight, ff.bert.encoder.layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_4: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_3.dc.add.10_add_mha_4, mha_4_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_4, lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_4_norm_mha_4.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_4_norm_mha_4.dc.subtract.1, norm_mha_4.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_14_temporal_epoch_1:
    target_device: 3
    input_count: 1
    norm_mha_4.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_4.dc.subtract.1, norm_mha_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.2, lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_4.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_4.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_4.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_4.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_4.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_4.dc.reciprocal.7, lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_4.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8, norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_4.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.8, ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_4.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.9, ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_4_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_4.dc.add.10, ff.bert.encoder.layer.4.intermediate.dense.weight, ff.bert.encoder.layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff4_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_4_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_4.dc.add.10_add_ff_4: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_4.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_15_temporal_epoch_1:
    target_device: 4
    input_count: 1
    ff_4_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff4_gelu, ff.bert.encoder.layer.4.output.dense.weight, ff.bert.encoder.layer.4.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_4: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_4.dc.add.10_add_ff_4, ff_4_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_4, lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_4_norm_ff_4.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_4_norm_ff_4.dc.subtract.1, norm_ff_4.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_4.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_4.dc.subtract.1, norm_ff_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.2, lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_4.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_4.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_4.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_4.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_4.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_4.dc.reciprocal.7, lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8, norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_4.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.8, ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_4.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.9, ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_4.dc.add.10_add_mha_5: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_4.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_16_temporal_epoch_1:
    target_device: 5
    input_count: 1
    mha_5_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.query.weight, ff.bert.encoder.layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_5_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.key.weight, ff.bert.encoder.layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_5_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_5_query, mha_5_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_5],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_5_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_5_as, ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_5_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_5_as_div, e2e_attention_mask_s_brcst_m2_18_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_5_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_5_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_5_as_mask_mha_5_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_5_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_5_as_mask_mha_5_as_softmax.dc.subtract.1, mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_5_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_5_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_5_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.exp.2, lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_5_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_5_as_softmax.dc.exp.2_mha_5_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_5_as_softmax.dc.exp.2_mha_5_as_softmax.dc.multiply.5, mha_5_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_5_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.value.weight, ff.bert.encoder.layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_5_value_mha_5_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_5_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_5_value_mha_5_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_5_value_mha_5_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.multiply.5, buffer_0_mha_5_value_mha_5_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_5_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_5_ac, ff.bert.encoder.layer.5.attention.output.dense.weight, ff.bert.encoder.layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_5: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_4.dc.add.10_add_mha_5, mha_5_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_5, lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_5_norm_mha_5.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_5_norm_mha_5.dc.subtract.1, norm_mha_5.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_17_temporal_epoch_1:
    target_device: 6
    input_count: 1
    norm_mha_5.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_5.dc.subtract.1, norm_mha_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.2, lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_5.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_5.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_5.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_5.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_5.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_5.dc.reciprocal.7, lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_5.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8, norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_5.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.8, ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_5.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.9, ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_5_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_5.dc.add.10, ff.bert.encoder.layer.5.intermediate.dense.weight, ff.bert.encoder.layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff5_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_5_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_5.dc.add.10_add_ff_5: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_5.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_18_temporal_epoch_1:
    target_device: 7
    input_count: 1
    ff_5_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff5_gelu, ff.bert.encoder.layer.5.output.dense.weight, ff.bert.encoder.layer.5.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_5.dc.add.10_add_ff_5, ff_5_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_5, lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_5_norm_ff_5.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_5_norm_ff_5.dc.subtract.1, norm_ff_5.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_5.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_5.dc.subtract.1, norm_ff_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.2, lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_5.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_5.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_5.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_5.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_5.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_5.dc.reciprocal.7, lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8, norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_5.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.8, ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_5.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.9, ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_5.dc.add.10_add_mha_6: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_5.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_19_temporal_epoch_1:
    target_device: 8
    input_count: 1
    mha_6_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.query.weight, ff.bert.encoder.layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_6_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.key.weight, ff.bert.encoder.layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_6_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_6_query, mha_6_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_6],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_6_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_6_as, ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_6_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_6_as_div, e2e_attention_mask_s_brcst_m2_17_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_6_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_6_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_6_as_mask_mha_6_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_6_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_6_as_mask_mha_6_as_softmax.dc.subtract.1, mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_6_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_6_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_6_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.exp.2, lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_6_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_6_as_softmax.dc.exp.2_mha_6_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_6_as_softmax.dc.exp.2_mha_6_as_softmax.dc.multiply.5, mha_6_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_6_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.value.weight, ff.bert.encoder.layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_6_value_mha_6_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_6_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_6_value_mha_6_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_6_value_mha_6_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.multiply.5, buffer_0_mha_6_value_mha_6_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_6_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_6_ac, ff.bert.encoder.layer.6.attention.output.dense.weight, ff.bert.encoder.layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_6: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_5.dc.add.10_add_mha_6, mha_6_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_6, lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_6_norm_mha_6.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_6_norm_mha_6.dc.subtract.1, norm_mha_6.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_20_temporal_epoch_1:
    target_device: 9
    input_count: 1
    norm_mha_6.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_6.dc.subtract.1, norm_mha_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.2, lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_6.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_6.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_6.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_6.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_6.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_6.dc.reciprocal.7, lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_6.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8, norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_6.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.8, ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_6.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.9, ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_6_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_6.dc.add.10, ff.bert.encoder.layer.6.intermediate.dense.weight, ff.bert.encoder.layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff6_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_6_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_6.dc.add.10_add_ff_6: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_6.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_21_temporal_epoch_1:
    target_device: 10
    input_count: 1
    ff_6_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff6_gelu, ff.bert.encoder.layer.6.output.dense.weight, ff.bert.encoder.layer.6.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_6: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_6.dc.add.10_add_ff_6, ff_6_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_6, lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_6_norm_ff_6.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_6_norm_ff_6.dc.subtract.1, norm_ff_6.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_6.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_6.dc.subtract.1, norm_ff_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.2, lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_6.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_6.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_6.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_6.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_6.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_6.dc.reciprocal.7, lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8, norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_6.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.8, ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_6.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.9, ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_6.dc.add.10_add_mha_7: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_6.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_22_temporal_epoch_1:
    target_device: 11
    input_count: 1
    mha_7_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.query.weight, ff.bert.encoder.layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_7_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.key.weight, ff.bert.encoder.layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_7_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_7_query, mha_7_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_7],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_7_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_7_as, ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_7_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_7_as_div, e2e_attention_mask_s_brcst_m2_16_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_7_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_7_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_7_as_mask_mha_7_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_7_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_7_as_mask_mha_7_as_softmax.dc.subtract.1, mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_7_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_7_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_7_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.exp.2, lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_7_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_7_as_softmax.dc.exp.2_mha_7_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_7_as_softmax.dc.exp.2_mha_7_as_softmax.dc.multiply.5, mha_7_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_7_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.value.weight, ff.bert.encoder.layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_7_value_mha_7_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_7_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_7_value_mha_7_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_7_value_mha_7_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.multiply.5, buffer_0_mha_7_value_mha_7_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_7_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_7_ac, ff.bert.encoder.layer.7.attention.output.dense.weight, ff.bert.encoder.layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_7: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_6.dc.add.10_add_mha_7, mha_7_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_7, lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_7_norm_mha_7.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_7_norm_mha_7.dc.subtract.1, norm_mha_7.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_23_temporal_epoch_1:
    target_device: 0
    input_count: 1
    norm_mha_7.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_7.dc.subtract.1, norm_mha_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.2, lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_7.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_7.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_7.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_7.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_7.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_7.dc.reciprocal.7, lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_7.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8, norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_7.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.8, ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_7.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.9, ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_7_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_7.dc.add.10, ff.bert.encoder.layer.7.intermediate.dense.weight, ff.bert.encoder.layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff7_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_7_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_7.dc.add.10_add_ff_7: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_7.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_24_temporal_epoch_2:
    target_device: 1
    input_count: 1
    ff_7_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_ff7_gelu_0, ff.bert.encoder.layer.7.output.dense.weight, ff.bert.encoder.layer.7.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_7: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_norm_mha_7.dc.add.10_add_ff_7_0, ff_7_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_7, lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_7_norm_ff_7.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_7_norm_ff_7.dc.subtract.1, norm_ff_7.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_7.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_7.dc.subtract.1, norm_ff_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.2, lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_7.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_7.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_7.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_7.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_7.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_7.dc.reciprocal.7, lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8, norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_7.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.8, ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_7.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.9, ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_25_temporal_epoch_2:
    target_device: 2
    input_count: 1
    mha_8_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_7.dc.add.10, ff.bert.encoder.layer.8.attention.self.query.weight, ff.bert.encoder.layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_8_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_7.dc.add.10, ff.bert.encoder.layer.8.attention.self.key.weight, ff.bert.encoder.layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_8_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_8_query, mha_8_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_8],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_8_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_8_as, ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_8_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_8_as_div, e2e_attention_mask_s_brcst_m2_15_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_8_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_8_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_8_as_mask_mha_8_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_8_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_8_as_mask_mha_8_as_softmax.dc.subtract.1, mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_8_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_8_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_8_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.exp.2, lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_8_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_8_as_softmax.dc.exp.2_mha_8_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_8_as_softmax.dc.exp.2_mha_8_as_softmax.dc.multiply.5, mha_8_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_8_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_7.dc.add.10, ff.bert.encoder.layer.8.attention.self.value.weight, ff.bert.encoder.layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_8_value_mha_8_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_8_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_8_value_mha_8_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_8_value_mha_8_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.multiply.5, buffer_0_mha_8_value_mha_8_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_8_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_8_ac, ff.bert.encoder.layer.8.attention.output.dense.weight, ff.bert.encoder.layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_7.dc.add.10_add_mha_8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_7.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_8: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_7.dc.add.10_add_mha_8, mha_8_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_8, lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_8_norm_mha_8.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_8_norm_mha_8.dc.subtract.1, norm_mha_8.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_26_temporal_epoch_2:
    target_device: 3
    input_count: 1
    norm_mha_8.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_8.dc.subtract.1, norm_mha_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.2, lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_8.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_8.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_8.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_8.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_8.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_8.dc.reciprocal.7, lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8, norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_8.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.8, ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_8.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.9, ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_8_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_8.dc.add.10, ff.bert.encoder.layer.8.intermediate.dense.weight, ff.bert.encoder.layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff8_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_8_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_8.dc.add.10_add_ff_8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_8.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_27_temporal_epoch_2:
    target_device: 4
    input_count: 1
    ff_8_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff8_gelu, ff.bert.encoder.layer.8.output.dense.weight, ff.bert.encoder.layer.8.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_8: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_8.dc.add.10_add_ff_8, ff_8_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_8, lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_8_norm_ff_8.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_8_norm_ff_8.dc.subtract.1, norm_ff_8.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_8.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_8.dc.subtract.1, norm_ff_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.2, lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_8.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_8.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_8.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_8.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_8.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_8.dc.reciprocal.7, lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8, norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_8.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.8, ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_8.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.9, ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_28_temporal_epoch_2:
    target_device: 5
    input_count: 1
    mha_9_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.query.weight, ff.bert.encoder.layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_9_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.key.weight, ff.bert.encoder.layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_9_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_9_query, mha_9_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_9],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_9_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_9_as, ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_9_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_9_as_div, e2e_attention_mask_s_brcst_m2_14_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_9_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_9_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_9_as_mask_mha_9_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_9_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_9_as_mask_mha_9_as_softmax.dc.subtract.1, mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_9_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_9_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_9_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.exp.2, lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_9_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_9_as_softmax.dc.exp.2_mha_9_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_9_as_softmax.dc.exp.2_mha_9_as_softmax.dc.multiply.5, mha_9_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_9_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.value.weight, ff.bert.encoder.layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_9_value_mha_9_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_9_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_9_value_mha_9_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_9_value_mha_9_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.multiply.5, buffer_0_mha_9_value_mha_9_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_9_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_9_ac, ff.bert.encoder.layer.9.attention.output.dense.weight, ff.bert.encoder.layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_8.dc.add.10_add_mha_9: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_8.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_9: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_8.dc.add.10_add_mha_9, mha_9_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_9, lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_9_norm_mha_9.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_9],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_9_norm_mha_9.dc.subtract.1, norm_mha_9.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_29_temporal_epoch_2:
    target_device: 6
    input_count: 1
    norm_mha_9.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_9.dc.subtract.1, norm_mha_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.2, lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_9.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_9.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_9.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_9.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_9.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_9.dc.reciprocal.7, lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8, norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_9.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.8, ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_9.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.9, ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_9_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_9.dc.add.10, ff.bert.encoder.layer.9.intermediate.dense.weight, ff.bert.encoder.layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff9_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_9_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_9.dc.add.10_add_ff_9: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_9.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_30_temporal_epoch_2:
    target_device: 7
    input_count: 1
    ff_9_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff9_gelu, ff.bert.encoder.layer.9.output.dense.weight, ff.bert.encoder.layer.9.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_9: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_9.dc.add.10_add_ff_9, ff_9_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_9, lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_9_norm_ff_9.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_9],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_9_norm_ff_9.dc.subtract.1, norm_ff_9.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_9.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_9.dc.subtract.1, norm_ff_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.2, lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_9.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_9.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_9.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_9.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_9.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_9.dc.reciprocal.7, lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8, norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_9.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.8, ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_9.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.9, ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_31_temporal_epoch_2:
    target_device: 8
    input_count: 1
    mha_10_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.query.weight, ff.bert.encoder.layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_10_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.key.weight, ff.bert.encoder.layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_10_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_10_query, mha_10_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_10],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_10_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_10_as, ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_10_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_10_as_div, e2e_attention_mask_s_brcst_m2_13_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_10_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_10_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_10_as_mask_mha_10_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_10_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_10_as_mask_mha_10_as_softmax.dc.subtract.1, mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_10_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_10_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_10_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.exp.2, lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_10_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_10_as_softmax.dc.exp.2_mha_10_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_10_as_softmax.dc.exp.2_mha_10_as_softmax.dc.multiply.5, mha_10_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_10_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.value.weight, ff.bert.encoder.layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_10_value_mha_10_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_10_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_10_value_mha_10_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_10_value_mha_10_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.multiply.5, buffer_0_mha_10_value_mha_10_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_10_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_10_ac, ff.bert.encoder.layer.10.attention.output.dense.weight, ff.bert.encoder.layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_9.dc.add.10_add_mha_10: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_9.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_10: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_9.dc.add.10_add_mha_10, mha_10_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_10, lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_10_norm_mha_10.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_10_norm_mha_10.dc.subtract.1, norm_mha_10.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_32_temporal_epoch_2:
    target_device: 9
    input_count: 1
    norm_mha_10.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_10.dc.subtract.1, norm_mha_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.2, lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_10.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_10.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_10.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_10.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_10.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_10.dc.reciprocal.7, lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8, norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_10.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.8, ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_10.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.9, ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_10_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_10.dc.add.10, ff.bert.encoder.layer.10.intermediate.dense.weight, ff.bert.encoder.layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff10_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_10_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_10.dc.add.10_add_ff_10: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_10.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_33_temporal_epoch_2:
    target_device: 10
    input_count: 1
    ff_10_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff10_gelu, ff.bert.encoder.layer.10.output.dense.weight, ff.bert.encoder.layer.10.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_10: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_10.dc.add.10_add_ff_10, ff_10_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_10, lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_10_norm_ff_10.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_10_norm_ff_10.dc.subtract.1, norm_ff_10.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_10.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_10.dc.subtract.1, norm_ff_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.2, lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_10.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_10.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_10.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_10.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_10.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_10.dc.reciprocal.7, lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8, norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_10.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.8, ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_10.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.9, ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_34_temporal_epoch_2:
    target_device: 11
    input_count: 1
    mha_11_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.query.weight, ff.bert.encoder.layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_11_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.key.weight, ff.bert.encoder.layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_11_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_11_query, mha_11_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_11],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_11_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_11_as, ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_11_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_11_as_div, e2e_attention_mask_s_brcst_m2_12_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_11_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [4, 2], grid_size: [2, 1], inputs: [mha_11_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_11_as_mask_mha_11_as_softmax.dc.subtract.1: {type: nop, grid_loc: [4, 1], grid_size: [2, 1], inputs: [mha_11_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_mha_11_as_mask_mha_11_as_softmax.dc.subtract.1, mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_11_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 5], grid_size: [2, 2], inputs: [mha_11_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_11_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.exp.2, lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_11_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 1], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_11_as_softmax.dc.exp.2_mha_11_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [6, 2], grid_size: [2, 1], inputs: [buffer_0_mha_11_as_softmax.dc.exp.2_mha_11_as_softmax.dc.multiply.5, mha_11_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_11_value: {type: matmul, grid_loc: [6, 3], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.value.weight, ff.bert.encoder.layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_11_value_mha_11_ac: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [mha_11_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_11_value_mha_11_ac: {type: nop, grid_loc: [8, 0], grid_size: [2, 1], inputs: [buffer_1_mha_11_value_mha_11_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_ac: {type: matmul, grid_loc: [8, 1], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.multiply.5, buffer_0_mha_11_value_mha_11_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_11_output: {type: matmul, grid_loc: [8, 2], grid_size: [2, 4], inputs: [mha_11_ac, ff.bert.encoder.layer.11.attention.output.dense.weight, ff.bert.encoder.layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_10.dc.add.10_add_mha_11: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_10.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_11: {type: add, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_10.dc.add.10_add_mha_11, mha_11_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_add_mha_11_norm_mha_11.dc.subtract.1: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [add_mha_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_11_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_10_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_9_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_8_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_7_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_6_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_5_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_4_1.0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_35_temporal_epoch_2:
    target_device: 0
    input_count: 1
    norm_mha_11.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [add_mha_11, lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_11.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [2, 1], inputs: [buffer_0_add_mha_11_norm_mha_11.dc.subtract.1, norm_mha_11.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_mha_11.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_11.dc.subtract.1, norm_mha_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.2, lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_11.dc.add.5: {type: add, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_11.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_11.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.sqrt.6: {type: sqrt, grid_loc: [0, 7], grid_size: [2, 1], inputs: [norm_mha_11.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 0], grid_size: [2, 1], inputs: [norm_mha_11.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_11.dc.reciprocal.7, lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [buffer_1_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.multiply.8: {type: multiply, grid_loc: [2, 2], grid_size: [2, 1], inputs: [buffer_0_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8, norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_11.dc.multiply.9: {type: multiply, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.8, ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_11.dc.add.10: {type: add, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.9, ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_11_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_11.dc.add.10, ff.bert.encoder.layer.11.intermediate.dense.weight, ff.bert.encoder.layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff11_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_11_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_11.dc.add.10_add_ff_11: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_mha_11.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_36_temporal_epoch_3:
    target_device: 1
    input_count: 1
    ff_11_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_ff11_gelu_0, ff.bert.encoder.layer.11.output.dense.weight, ff.bert.encoder.layer.11.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_11: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_norm_mha_11.dc.add.10_add_ff_11_0, ff_11_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_11, lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_11_norm_ff_11.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_11_norm_ff_11.dc.subtract.1, norm_ff_11.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_11.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_11.dc.subtract.1, norm_ff_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.2, lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_11.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_11.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_11.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_11.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_11.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_11.dc.reciprocal.7, lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8, norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_11.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.8, ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_11.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.9, ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_11.dc.add.10_add_mha_12: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_11.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_37_temporal_epoch_3:
    target_device: 2
    input_count: 1
    mha_12_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_11.dc.add.10, ff.bert.encoder.layer.12.attention.self.query.weight, ff.bert.encoder.layer.12.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_12_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_11.dc.add.10, ff.bert.encoder.layer.12.attention.self.key.weight, ff.bert.encoder.layer.12.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_12_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_12_query, mha_12_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_12],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_12_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_12_as, ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_12_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_12_as_div, e2e_attention_mask_s_brcst_m2_11_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_12_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_12_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_12_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_12_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_12_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_12_as_mask_mha_12_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_12_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_12_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_12_as_mask_mha_12_as_softmax.dc.subtract.1, mha_12_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_12_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_12_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_12_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_12_as_softmax.dc.exp.2, lc.input_tensor.mha_12_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_12_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_12_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_12_as_softmax.dc.exp.2_mha_12_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_12_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_12_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_12_as_softmax.dc.exp.2_mha_12_as_softmax.dc.multiply.5, mha_12_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_12_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_11.dc.add.10, ff.bert.encoder.layer.12.attention.self.value.weight, ff.bert.encoder.layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_12_value_mha_12_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_12_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_12_value_mha_12_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_12_value_mha_12_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_12_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_12_as_softmax.dc.multiply.5, buffer_0_mha_12_value_mha_12_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_12_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_12_ac, ff.bert.encoder.layer.12.attention.output.dense.weight, ff.bert.encoder.layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_12: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_11.dc.add.10_add_mha_12, mha_12_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_12.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_12, lc.input_tensor.norm_mha_12.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_12_norm_mha_12.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_12],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_12.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_12_norm_mha_12.dc.subtract.1, norm_mha_12.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_12.dc.subtract.1_norm_mha_12.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_12.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_12.dc.subtract.1_norm_mha_12.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_12.dc.subtract.1_norm_mha_12.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_38_temporal_epoch_3:
    target_device: 3
    input_count: 1
    norm_mha_12.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_12.dc.subtract.1, norm_mha_12.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_12.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_12.dc.multiply.2, lc.input_tensor.norm_mha_12.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_12.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_12.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_12.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_12.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_12.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_12.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_12.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_12.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_12.dc.reciprocal.7, lc.input_tensor.norm_mha_12.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_12.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_12.dc.subtract.1_norm_mha_12.dc.multiply.8, norm_mha_12.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.12.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_12.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_12.dc.multiply.8, ff.bert.encoder.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.12.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_12.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_12.dc.multiply.9, ff.bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_12_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_12.dc.add.10, ff.bert.encoder.layer.12.intermediate.dense.weight, ff.bert.encoder.layer.12.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff12_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_12_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_12.dc.add.10_add_ff_12: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_12.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_39_temporal_epoch_3:
    target_device: 4
    input_count: 1
    ff_12_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff12_gelu, ff.bert.encoder.layer.12.output.dense.weight, ff.bert.encoder.layer.12.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_12: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_12.dc.add.10_add_ff_12, ff_12_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_12.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_12, lc.input_tensor.norm_ff_12.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_12_norm_ff_12.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_12],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_12.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_12_norm_ff_12.dc.subtract.1, norm_ff_12.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_12.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_12.dc.subtract.1, norm_ff_12.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_12.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_12.dc.multiply.2, lc.input_tensor.norm_ff_12.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_12.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_12.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_12.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_12.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_12.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_12.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_12.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_12.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_12.dc.reciprocal.7, lc.input_tensor.norm_ff_12.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_12.dc.subtract.1_norm_ff_12.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_12.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_12.dc.subtract.1_norm_ff_12.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_12.dc.subtract.1_norm_ff_12.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_12.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_12.dc.subtract.1_norm_ff_12.dc.multiply.8, norm_ff_12.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.12.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_12.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_12.dc.multiply.8, ff.bert.encoder.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.12.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_12.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_12.dc.multiply.9, ff.bert.encoder.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_12.dc.add.10_add_mha_13: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_12.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_40_temporal_epoch_3:
    target_device: 5
    input_count: 1
    mha_13_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_12.dc.add.10, ff.bert.encoder.layer.13.attention.self.query.weight, ff.bert.encoder.layer.13.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_13_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_12.dc.add.10, ff.bert.encoder.layer.13.attention.self.key.weight, ff.bert.encoder.layer.13.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_13_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_13_query, mha_13_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_13],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_13_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_13_as, ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_13_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_13_as_div, e2e_attention_mask_s_brcst_m2_10_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_13_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_13_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_13_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_13_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_13_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_13_as_mask_mha_13_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_13_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_13_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_13_as_mask_mha_13_as_softmax.dc.subtract.1, mha_13_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_13_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_13_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_13_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_13_as_softmax.dc.exp.2, lc.input_tensor.mha_13_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_13_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_13_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_13_as_softmax.dc.exp.2_mha_13_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_13_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_13_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_13_as_softmax.dc.exp.2_mha_13_as_softmax.dc.multiply.5, mha_13_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_13_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_12.dc.add.10, ff.bert.encoder.layer.13.attention.self.value.weight, ff.bert.encoder.layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_13_value_mha_13_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_13_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_13_value_mha_13_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_13_value_mha_13_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_13_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_13_as_softmax.dc.multiply.5, buffer_0_mha_13_value_mha_13_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_13_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_13_ac, ff.bert.encoder.layer.13.attention.output.dense.weight, ff.bert.encoder.layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_13: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_12.dc.add.10_add_mha_13, mha_13_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_13.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_13, lc.input_tensor.norm_mha_13.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_13_norm_mha_13.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_13],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_13.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_13_norm_mha_13.dc.subtract.1, norm_mha_13.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_13.dc.subtract.1_norm_mha_13.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_13.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_13.dc.subtract.1_norm_mha_13.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_13.dc.subtract.1_norm_mha_13.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_41_temporal_epoch_3:
    target_device: 6
    input_count: 1
    norm_mha_13.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_13.dc.subtract.1, norm_mha_13.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_13.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_13.dc.multiply.2, lc.input_tensor.norm_mha_13.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_13.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_13.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_13.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_13.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_13.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_13.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_13.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_13.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_13.dc.reciprocal.7, lc.input_tensor.norm_mha_13.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_13.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_13.dc.subtract.1_norm_mha_13.dc.multiply.8, norm_mha_13.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.13.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_13.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_13.dc.multiply.8, ff.bert.encoder.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.13.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_13.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_13.dc.multiply.9, ff.bert.encoder.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_13_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_13.dc.add.10, ff.bert.encoder.layer.13.intermediate.dense.weight, ff.bert.encoder.layer.13.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff13_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_13_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_13.dc.add.10_add_ff_13: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_13.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_42_temporal_epoch_3:
    target_device: 7
    input_count: 1
    ff_13_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff13_gelu, ff.bert.encoder.layer.13.output.dense.weight, ff.bert.encoder.layer.13.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_13: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_13.dc.add.10_add_ff_13, ff_13_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_13.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_13, lc.input_tensor.norm_ff_13.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_13_norm_ff_13.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_13],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_13.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_13_norm_ff_13.dc.subtract.1, norm_ff_13.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_13.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_13.dc.subtract.1, norm_ff_13.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_13.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_13.dc.multiply.2, lc.input_tensor.norm_ff_13.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_13.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_13.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_13.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_13.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_13.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_13.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_13.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_13.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_13.dc.reciprocal.7, lc.input_tensor.norm_ff_13.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_13.dc.subtract.1_norm_ff_13.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_13.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_13.dc.subtract.1_norm_ff_13.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_13.dc.subtract.1_norm_ff_13.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_13.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_13.dc.subtract.1_norm_ff_13.dc.multiply.8, norm_ff_13.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.13.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_13.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_13.dc.multiply.8, ff.bert.encoder.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.13.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_13.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_13.dc.multiply.9, ff.bert.encoder.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_13.dc.add.10_add_mha_14: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_13.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_43_temporal_epoch_3:
    target_device: 8
    input_count: 1
    mha_14_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_13.dc.add.10, ff.bert.encoder.layer.14.attention.self.query.weight, ff.bert.encoder.layer.14.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_14_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_13.dc.add.10, ff.bert.encoder.layer.14.attention.self.key.weight, ff.bert.encoder.layer.14.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_14_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_14_query, mha_14_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_14],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_14_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_14_as, ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_14_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_14_as_div, e2e_attention_mask_s_brcst_m2_9_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_14_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_14_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_14_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_14_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_14_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_14_as_mask_mha_14_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_14_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_14_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_14_as_mask_mha_14_as_softmax.dc.subtract.1, mha_14_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_14_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_14_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_14_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_14_as_softmax.dc.exp.2, lc.input_tensor.mha_14_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_14_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_14_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_14_as_softmax.dc.exp.2_mha_14_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_14_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_14_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_14_as_softmax.dc.exp.2_mha_14_as_softmax.dc.multiply.5, mha_14_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_14_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_13.dc.add.10, ff.bert.encoder.layer.14.attention.self.value.weight, ff.bert.encoder.layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_14_value_mha_14_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_14_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_14_value_mha_14_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_14_value_mha_14_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_14_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_14_as_softmax.dc.multiply.5, buffer_0_mha_14_value_mha_14_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_14_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_14_ac, ff.bert.encoder.layer.14.attention.output.dense.weight, ff.bert.encoder.layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_14: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_13.dc.add.10_add_mha_14, mha_14_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_14.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_14, lc.input_tensor.norm_mha_14.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_14_norm_mha_14.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_14],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_14.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_14_norm_mha_14.dc.subtract.1, norm_mha_14.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_14.dc.subtract.1_norm_mha_14.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_14.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_14.dc.subtract.1_norm_mha_14.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_14.dc.subtract.1_norm_mha_14.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_44_temporal_epoch_3:
    target_device: 9
    input_count: 1
    norm_mha_14.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_14.dc.subtract.1, norm_mha_14.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_14.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_14.dc.multiply.2, lc.input_tensor.norm_mha_14.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_14.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_14.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_14.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_14.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_14.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_14.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_14.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_14.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_14.dc.reciprocal.7, lc.input_tensor.norm_mha_14.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_14.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_14.dc.subtract.1_norm_mha_14.dc.multiply.8, norm_mha_14.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.14.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_14.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_14.dc.multiply.8, ff.bert.encoder.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.14.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_14.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_14.dc.multiply.9, ff.bert.encoder.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_14_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_14.dc.add.10, ff.bert.encoder.layer.14.intermediate.dense.weight, ff.bert.encoder.layer.14.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff14_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_14_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_14.dc.add.10_add_ff_14: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_14.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_45_temporal_epoch_3:
    target_device: 10
    input_count: 1
    ff_14_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff14_gelu, ff.bert.encoder.layer.14.output.dense.weight, ff.bert.encoder.layer.14.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_14: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_14.dc.add.10_add_ff_14, ff_14_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_14.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_14, lc.input_tensor.norm_ff_14.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_14_norm_ff_14.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_14],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_14.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_14_norm_ff_14.dc.subtract.1, norm_ff_14.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_14.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_14.dc.subtract.1, norm_ff_14.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_14.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_14.dc.multiply.2, lc.input_tensor.norm_ff_14.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_14.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_14.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_14.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_14.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_14.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_14.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_14.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_14.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_14.dc.reciprocal.7, lc.input_tensor.norm_ff_14.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_14.dc.subtract.1_norm_ff_14.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_14.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_14.dc.subtract.1_norm_ff_14.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_14.dc.subtract.1_norm_ff_14.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_14.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_14.dc.subtract.1_norm_ff_14.dc.multiply.8, norm_ff_14.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.14.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_14.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_14.dc.multiply.8, ff.bert.encoder.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.14.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_14.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_14.dc.multiply.9, ff.bert.encoder.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_14.dc.add.10_add_mha_15: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_14.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_46_temporal_epoch_3:
    target_device: 11
    input_count: 1
    mha_15_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_14.dc.add.10, ff.bert.encoder.layer.15.attention.self.query.weight, ff.bert.encoder.layer.15.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_15_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_14.dc.add.10, ff.bert.encoder.layer.15.attention.self.key.weight, ff.bert.encoder.layer.15.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_15_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_15_query, mha_15_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_15],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_15_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_15_as, ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_15_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_15_as_div, e2e_attention_mask_s_brcst_m2_8_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_15_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_15_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_15_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_15_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_15_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_15_as_mask_mha_15_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_15_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_15_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_15_as_mask_mha_15_as_softmax.dc.subtract.1, mha_15_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_15_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_15_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_15_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_15_as_softmax.dc.exp.2, lc.input_tensor.mha_15_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_15_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_15_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_15_as_softmax.dc.exp.2_mha_15_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_15_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_15_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_15_as_softmax.dc.exp.2_mha_15_as_softmax.dc.multiply.5, mha_15_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_15_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_14.dc.add.10, ff.bert.encoder.layer.15.attention.self.value.weight, ff.bert.encoder.layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_15_value_mha_15_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_15_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_15_value_mha_15_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_15_value_mha_15_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_15_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_15_as_softmax.dc.multiply.5, buffer_0_mha_15_value_mha_15_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_15_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_15_ac, ff.bert.encoder.layer.15.attention.output.dense.weight, ff.bert.encoder.layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_15: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_14.dc.add.10_add_mha_15, mha_15_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_15.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_15, lc.input_tensor.norm_mha_15.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_15_norm_mha_15.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_15],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_15.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_15_norm_mha_15.dc.subtract.1, norm_mha_15.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_15.dc.subtract.1_norm_mha_15.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_15.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_15.dc.subtract.1_norm_mha_15.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_15.dc.subtract.1_norm_mha_15.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_47_temporal_epoch_3:
    target_device: 0
    input_count: 1
    norm_mha_15.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_15.dc.subtract.1, norm_mha_15.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_15.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_15.dc.multiply.2, lc.input_tensor.norm_mha_15.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_15.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_15.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_15.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_15.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_15.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_15.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_15.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_15.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_15.dc.reciprocal.7, lc.input_tensor.norm_mha_15.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_15.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_15.dc.subtract.1_norm_mha_15.dc.multiply.8, norm_mha_15.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.15.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_15.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_15.dc.multiply.8, ff.bert.encoder.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.15.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_15.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_15.dc.multiply.9, ff.bert.encoder.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_15_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_15.dc.add.10, ff.bert.encoder.layer.15.intermediate.dense.weight, ff.bert.encoder.layer.15.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff15_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_15_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_15.dc.add.10_add_ff_15: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_15.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_48_temporal_epoch_4:
    target_device: 1
    input_count: 1
    ff_15_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_ff15_gelu_0, ff.bert.encoder.layer.15.output.dense.weight, ff.bert.encoder.layer.15.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_15: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_norm_mha_15.dc.add.10_add_ff_15_0, ff_15_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_15.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_15, lc.input_tensor.norm_ff_15.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_15_norm_ff_15.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_15],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_15.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_15_norm_ff_15.dc.subtract.1, norm_ff_15.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_15.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_15.dc.subtract.1, norm_ff_15.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_15.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_15.dc.multiply.2, lc.input_tensor.norm_ff_15.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_15.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_15.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_15.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_15.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_15.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_15.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_15.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_15.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_15.dc.reciprocal.7, lc.input_tensor.norm_ff_15.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_15.dc.subtract.1_norm_ff_15.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_15.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_15.dc.subtract.1_norm_ff_15.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_15.dc.subtract.1_norm_ff_15.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_15.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_15.dc.subtract.1_norm_ff_15.dc.multiply.8, norm_ff_15.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.15.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_15.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_15.dc.multiply.8, ff.bert.encoder.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.15.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_15.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_15.dc.multiply.9, ff.bert.encoder.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_15.dc.add.10_add_mha_16: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_15.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_49_temporal_epoch_4:
    target_device: 2
    input_count: 1
    mha_16_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_15.dc.add.10, ff.bert.encoder.layer.16.attention.self.query.weight, ff.bert.encoder.layer.16.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_16_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_15.dc.add.10, ff.bert.encoder.layer.16.attention.self.key.weight, ff.bert.encoder.layer.16.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_16_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_16_query, mha_16_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_16],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_16_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_16_as, ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_16_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_16_as_div, e2e_attention_mask_s_brcst_m2_7_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_16_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_16_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_16_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_16_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_16_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_16_as_mask_mha_16_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_16_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_16_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_16_as_mask_mha_16_as_softmax.dc.subtract.1, mha_16_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_16_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_16_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_16_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_16_as_softmax.dc.exp.2, lc.input_tensor.mha_16_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_16_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_16_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_16_as_softmax.dc.exp.2_mha_16_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_16_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_16_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_16_as_softmax.dc.exp.2_mha_16_as_softmax.dc.multiply.5, mha_16_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_16_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_15.dc.add.10, ff.bert.encoder.layer.16.attention.self.value.weight, ff.bert.encoder.layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_16_value_mha_16_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_16_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_16_value_mha_16_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_16_value_mha_16_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_16_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_16_as_softmax.dc.multiply.5, buffer_0_mha_16_value_mha_16_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_16_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_16_ac, ff.bert.encoder.layer.16.attention.output.dense.weight, ff.bert.encoder.layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_16: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_15.dc.add.10_add_mha_16, mha_16_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_16.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_16, lc.input_tensor.norm_mha_16.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_16_norm_mha_16.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_16],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_16.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_16_norm_mha_16.dc.subtract.1, norm_mha_16.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_16.dc.subtract.1_norm_mha_16.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_16.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_16.dc.subtract.1_norm_mha_16.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_16.dc.subtract.1_norm_mha_16.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_50_temporal_epoch_4:
    target_device: 3
    input_count: 1
    norm_mha_16.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_16.dc.subtract.1, norm_mha_16.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_16.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_16.dc.multiply.2, lc.input_tensor.norm_mha_16.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_16.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_16.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_16.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_16.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_16.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_16.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_16.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_16.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_16.dc.reciprocal.7, lc.input_tensor.norm_mha_16.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_16.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_16.dc.subtract.1_norm_mha_16.dc.multiply.8, norm_mha_16.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.16.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_16.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_16.dc.multiply.8, ff.bert.encoder.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.16.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_16.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_16.dc.multiply.9, ff.bert.encoder.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_16_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_16.dc.add.10, ff.bert.encoder.layer.16.intermediate.dense.weight, ff.bert.encoder.layer.16.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff16_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_16_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_16.dc.add.10_add_ff_16: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_16.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_51_temporal_epoch_4:
    target_device: 4
    input_count: 1
    ff_16_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff16_gelu, ff.bert.encoder.layer.16.output.dense.weight, ff.bert.encoder.layer.16.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_16: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_16.dc.add.10_add_ff_16, ff_16_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_16.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_16, lc.input_tensor.norm_ff_16.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_16_norm_ff_16.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_16],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_16.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_16_norm_ff_16.dc.subtract.1, norm_ff_16.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_16.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_16.dc.subtract.1, norm_ff_16.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_16.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_16.dc.multiply.2, lc.input_tensor.norm_ff_16.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_16.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_16.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_16.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_16.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_16.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_16.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_16.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_16.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_16.dc.reciprocal.7, lc.input_tensor.norm_ff_16.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_16.dc.subtract.1_norm_ff_16.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_16.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_16.dc.subtract.1_norm_ff_16.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_16.dc.subtract.1_norm_ff_16.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_16.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_16.dc.subtract.1_norm_ff_16.dc.multiply.8, norm_ff_16.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.16.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_16.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_16.dc.multiply.8, ff.bert.encoder.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.16.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_16.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_16.dc.multiply.9, ff.bert.encoder.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_16.dc.add.10_add_mha_17: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_16.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_52_temporal_epoch_4:
    target_device: 5
    input_count: 1
    mha_17_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_16.dc.add.10, ff.bert.encoder.layer.17.attention.self.query.weight, ff.bert.encoder.layer.17.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_17_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_16.dc.add.10, ff.bert.encoder.layer.17.attention.self.key.weight, ff.bert.encoder.layer.17.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_17_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_17_query, mha_17_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_17],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_17_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_17_as, ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_17_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_17_as_div, e2e_attention_mask_s_brcst_m2_6_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_17_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_17_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_17_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_17_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_17_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_17_as_mask_mha_17_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_17_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_17_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_17_as_mask_mha_17_as_softmax.dc.subtract.1, mha_17_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_17_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_17_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_17_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_17_as_softmax.dc.exp.2, lc.input_tensor.mha_17_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_17_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_17_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_17_as_softmax.dc.exp.2_mha_17_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_17_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_17_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_17_as_softmax.dc.exp.2_mha_17_as_softmax.dc.multiply.5, mha_17_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_17_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_16.dc.add.10, ff.bert.encoder.layer.17.attention.self.value.weight, ff.bert.encoder.layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_17_value_mha_17_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_17_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_17_value_mha_17_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_17_value_mha_17_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_17_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_17_as_softmax.dc.multiply.5, buffer_0_mha_17_value_mha_17_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_17_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_17_ac, ff.bert.encoder.layer.17.attention.output.dense.weight, ff.bert.encoder.layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_17: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_16.dc.add.10_add_mha_17, mha_17_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_17.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_17, lc.input_tensor.norm_mha_17.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_17_norm_mha_17.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_17],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_17.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_17_norm_mha_17.dc.subtract.1, norm_mha_17.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_17.dc.subtract.1_norm_mha_17.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_17.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_17.dc.subtract.1_norm_mha_17.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_17.dc.subtract.1_norm_mha_17.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_53_temporal_epoch_4:
    target_device: 6
    input_count: 1
    norm_mha_17.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_17.dc.subtract.1, norm_mha_17.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_17.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_17.dc.multiply.2, lc.input_tensor.norm_mha_17.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_17.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_17.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_17.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_17.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_17.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_17.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_17.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_17.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_17.dc.reciprocal.7, lc.input_tensor.norm_mha_17.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_17.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_17.dc.subtract.1_norm_mha_17.dc.multiply.8, norm_mha_17.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.17.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_17.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_17.dc.multiply.8, ff.bert.encoder.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.17.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_17.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_17.dc.multiply.9, ff.bert.encoder.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_17_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_17.dc.add.10, ff.bert.encoder.layer.17.intermediate.dense.weight, ff.bert.encoder.layer.17.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff17_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_17_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_17.dc.add.10_add_ff_17: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_17.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_54_temporal_epoch_4:
    target_device: 7
    input_count: 1
    ff_17_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff17_gelu, ff.bert.encoder.layer.17.output.dense.weight, ff.bert.encoder.layer.17.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_17: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_17.dc.add.10_add_ff_17, ff_17_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_17.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_17, lc.input_tensor.norm_ff_17.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_17_norm_ff_17.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_17],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_17.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_17_norm_ff_17.dc.subtract.1, norm_ff_17.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_17.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_17.dc.subtract.1, norm_ff_17.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_17.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_17.dc.multiply.2, lc.input_tensor.norm_ff_17.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_17.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_17.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_17.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_17.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_17.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_17.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_17.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_17.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_17.dc.reciprocal.7, lc.input_tensor.norm_ff_17.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_17.dc.subtract.1_norm_ff_17.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_17.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_17.dc.subtract.1_norm_ff_17.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_17.dc.subtract.1_norm_ff_17.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_17.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_17.dc.subtract.1_norm_ff_17.dc.multiply.8, norm_ff_17.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.17.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_17.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_17.dc.multiply.8, ff.bert.encoder.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.17.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_17.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_17.dc.multiply.9, ff.bert.encoder.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_17.dc.add.10_add_mha_18: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_17.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_55_temporal_epoch_4:
    target_device: 8
    input_count: 1
    mha_18_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_17.dc.add.10, ff.bert.encoder.layer.18.attention.self.query.weight, ff.bert.encoder.layer.18.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_18_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_17.dc.add.10, ff.bert.encoder.layer.18.attention.self.key.weight, ff.bert.encoder.layer.18.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_18_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_18_query, mha_18_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_18],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_18_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_18_as, ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_18_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_18_as_div, e2e_attention_mask_s_brcst_m2_5_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_18_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_18_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_18_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_18_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_18_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_18_as_mask_mha_18_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_18_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_18_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_18_as_mask_mha_18_as_softmax.dc.subtract.1, mha_18_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_18_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_18_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_18_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_18_as_softmax.dc.exp.2, lc.input_tensor.mha_18_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_18_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_18_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_18_as_softmax.dc.exp.2_mha_18_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_18_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_18_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_18_as_softmax.dc.exp.2_mha_18_as_softmax.dc.multiply.5, mha_18_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_18_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_17.dc.add.10, ff.bert.encoder.layer.18.attention.self.value.weight, ff.bert.encoder.layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_18_value_mha_18_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_18_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_18_value_mha_18_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_18_value_mha_18_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_18_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_18_as_softmax.dc.multiply.5, buffer_0_mha_18_value_mha_18_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_18_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_18_ac, ff.bert.encoder.layer.18.attention.output.dense.weight, ff.bert.encoder.layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_18: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_17.dc.add.10_add_mha_18, mha_18_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_18.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_18, lc.input_tensor.norm_mha_18.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_18_norm_mha_18.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_18],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_18.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_18_norm_mha_18.dc.subtract.1, norm_mha_18.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_18.dc.subtract.1_norm_mha_18.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_18.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_18.dc.subtract.1_norm_mha_18.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_18.dc.subtract.1_norm_mha_18.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_56_temporal_epoch_4:
    target_device: 9
    input_count: 1
    norm_mha_18.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_18.dc.subtract.1, norm_mha_18.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_18.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_18.dc.multiply.2, lc.input_tensor.norm_mha_18.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_18.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_18.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_18.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_18.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_18.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_18.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_18.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_18.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_18.dc.reciprocal.7, lc.input_tensor.norm_mha_18.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_18.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_18.dc.subtract.1_norm_mha_18.dc.multiply.8, norm_mha_18.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.18.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_18.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_18.dc.multiply.8, ff.bert.encoder.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.18.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_18.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_18.dc.multiply.9, ff.bert.encoder.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_18_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_18.dc.add.10, ff.bert.encoder.layer.18.intermediate.dense.weight, ff.bert.encoder.layer.18.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff18_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_18_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_18.dc.add.10_add_ff_18: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_18.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_57_temporal_epoch_4:
    target_device: 10
    input_count: 1
    ff_18_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff18_gelu, ff.bert.encoder.layer.18.output.dense.weight, ff.bert.encoder.layer.18.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_18: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_18.dc.add.10_add_ff_18, ff_18_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_18.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_18, lc.input_tensor.norm_ff_18.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_18_norm_ff_18.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_18],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_18.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_18_norm_ff_18.dc.subtract.1, norm_ff_18.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_18.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_18.dc.subtract.1, norm_ff_18.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_18.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_18.dc.multiply.2, lc.input_tensor.norm_ff_18.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_18.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_18.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_18.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_18.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_18.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_18.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_18.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_18.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_18.dc.reciprocal.7, lc.input_tensor.norm_ff_18.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_18.dc.subtract.1_norm_ff_18.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_18.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_18.dc.subtract.1_norm_ff_18.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_18.dc.subtract.1_norm_ff_18.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_18.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_18.dc.subtract.1_norm_ff_18.dc.multiply.8, norm_ff_18.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.18.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_18.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_18.dc.multiply.8, ff.bert.encoder.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.18.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_18.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_18.dc.multiply.9, ff.bert.encoder.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    buffer_0_norm_ff_18.dc.add.10_add_mha_19: {type: nop, grid_loc: [5, 7], grid_size: [2, 1], inputs: [norm_ff_18.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_58_temporal_epoch_4:
    target_device: 11
    input_count: 1
    mha_19_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_18.dc.add.10, ff.bert.encoder.layer.19.attention.self.query.weight, ff.bert.encoder.layer.19.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_19_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_18.dc.add.10, ff.bert.encoder.layer.19.attention.self.key.weight, ff.bert.encoder.layer.19.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_19_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_19_query, mha_19_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_19],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_19_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_19_as, ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_19_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_19_as_div, e2e_attention_mask_s_brcst_m2_4_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_19_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_19_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_19_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_19_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_19_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_19_as_mask_mha_19_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [mha_19_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_19_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 1], grid_size: [2, 1], inputs: [buffer_0_mha_19_as_mask_mha_19_as_softmax.dc.subtract.1, mha_19_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_19_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 2], grid_size: [2, 2], inputs: [mha_19_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_19_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_19_as_softmax.dc.exp.2, lc.input_tensor.mha_19_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_19_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_19_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_19_as_softmax.dc.exp.2_mha_19_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_19_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_19_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_19_as_softmax.dc.exp.2_mha_19_as_softmax.dc.multiply.5, mha_19_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_19_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_18.dc.add.10, ff.bert.encoder.layer.19.attention.self.value.weight, ff.bert.encoder.layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_19_value_mha_19_ac: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_19_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_19_value_mha_19_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_1_mha_19_value_mha_19_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_19_ac: {type: matmul, grid_loc: [6, 5], grid_size: [2, 1], inputs: [mha_19_as_softmax.dc.multiply.5, buffer_0_mha_19_value_mha_19_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_19_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_19_ac, ff.bert.encoder.layer.19.attention.output.dense.weight, ff.bert.encoder.layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_19: {type: add, grid_loc: [6, 6], grid_size: [2, 1], inputs: [buffer_0_norm_ff_18.dc.add.10_add_mha_19, mha_19_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_19.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_19, lc.input_tensor.norm_mha_19.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_19_norm_mha_19.dc.subtract.1: {type: nop, grid_loc: [6, 7], grid_size: [2, 1], inputs: [add_mha_19],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_19.dc.subtract.1: {type: subtract, grid_loc: [8, 5], grid_size: [2, 1], inputs: [buffer_0_add_mha_19_norm_mha_19.dc.subtract.1, norm_mha_19.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_19.dc.subtract.1_norm_mha_19.dc.multiply.8: {type: nop, grid_loc: [8, 6], grid_size: [2, 1], inputs: [norm_mha_19.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_19.dc.subtract.1_norm_mha_19.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [buffer_1_norm_mha_19.dc.subtract.1_norm_mha_19.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_59_temporal_epoch_4:
    target_device: 0
    input_count: 1
    norm_mha_19.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_19.dc.subtract.1, norm_mha_19.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_19.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_19.dc.multiply.2, lc.input_tensor.norm_mha_19.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_19.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_19.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_19.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_19.dc.sqrt.6: {type: sqrt, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_19.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_19.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_19.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_19.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_19.dc.reciprocal.7, lc.input_tensor.norm_mha_19.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_19.dc.multiply.8: {type: multiply, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_0_norm_mha_19.dc.subtract.1_norm_mha_19.dc.multiply.8, norm_mha_19.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.19.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_19.dc.multiply.9: {type: multiply, grid_loc: [1, 7], grid_size: [2, 1], inputs: [norm_mha_19.dc.multiply.8, ff.bert.encoder.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.19.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_19.dc.add.10: {type: add, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_19.dc.multiply.9, ff.bert.encoder.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_19_ff1: {type: matmul, grid_loc: [2, 3], grid_size: [6, 4], inputs: [norm_mha_19.dc.add.10, ff.bert.encoder.layer.19.intermediate.dense.weight, ff.bert.encoder.layer.19.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff19_gelu: {type: gelu, grid_loc: [8, 0], grid_size: [2, 4], inputs: [ff_19_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_19.dc.add.10_add_ff_19: {type: nop, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_19.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_60_temporal_epoch_5:
    target_device: 1
    input_count: 1
    ff_19_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_ff19_gelu_0, ff.bert.encoder.layer.19.output.dense.weight, ff.bert.encoder.layer.19.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_19: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_norm_mha_19.dc.add.10_add_ff_19_0, ff_19_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_19.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_19, lc.input_tensor.norm_ff_19.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_19_norm_ff_19.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_19],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_19.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_19_norm_ff_19.dc.subtract.1, norm_ff_19.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_19.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_19.dc.subtract.1, norm_ff_19.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_19.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_19.dc.multiply.2, lc.input_tensor.norm_ff_19.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_19.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_19.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_19.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_19.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_19.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_19.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_19.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_19.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_19.dc.reciprocal.7, lc.input_tensor.norm_ff_19.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_19.dc.subtract.1_norm_ff_19.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_19.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_19.dc.subtract.1_norm_ff_19.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_19.dc.subtract.1_norm_ff_19.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_19.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_19.dc.subtract.1_norm_ff_19.dc.multiply.8, norm_ff_19.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.19.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_19.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_19.dc.multiply.8, ff.bert.encoder.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.19.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_19.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_19.dc.multiply.9, ff.bert.encoder.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_61_temporal_epoch_5:
    target_device: 2
    input_count: 1
    mha_20_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_19.dc.add.10, ff.bert.encoder.layer.20.attention.self.query.weight, ff.bert.encoder.layer.20.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_20_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_19.dc.add.10, ff.bert.encoder.layer.20.attention.self.key.weight, ff.bert.encoder.layer.20.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_20_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_20_query, mha_20_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_20],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_20_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_20_as, ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_20_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_20_as_div, e2e_attention_mask_s_brcst_m2_3_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_20_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_20_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_20_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_20_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_20_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_20_as_mask_mha_20_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_20_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_20_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_20_as_mask_mha_20_as_softmax.dc.subtract.1, mha_20_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_20_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_20_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_20_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_20_as_softmax.dc.exp.2, lc.input_tensor.mha_20_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_20_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_20_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_20_as_softmax.dc.exp.2_mha_20_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_20_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_20_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_20_as_softmax.dc.exp.2_mha_20_as_softmax.dc.multiply.5, mha_20_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_20_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_19.dc.add.10, ff.bert.encoder.layer.20.attention.self.value.weight, ff.bert.encoder.layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_20_value_mha_20_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_20_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_20_value_mha_20_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_20_value_mha_20_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_20_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_20_as_softmax.dc.multiply.5, buffer_0_mha_20_value_mha_20_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_20_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_20_ac, ff.bert.encoder.layer.20.attention.output.dense.weight, ff.bert.encoder.layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_19.dc.add.10_add_mha_20: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_19.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_20: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_19.dc.add.10_add_mha_20, mha_20_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_20.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_20, lc.input_tensor.norm_mha_20.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_20_norm_mha_20.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_20],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_20.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_20_norm_mha_20.dc.subtract.1, norm_mha_20.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_20.dc.subtract.1_norm_mha_20.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_20.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_62_temporal_epoch_5:
    target_device: 3
    input_count: 1
    norm_mha_20.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_20.dc.subtract.1, norm_mha_20.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_20.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_20.dc.multiply.2, lc.input_tensor.norm_mha_20.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_20.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_20.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_20.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_20.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_20.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_20.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_20.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_20.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_20.dc.reciprocal.7, lc.input_tensor.norm_mha_20.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_20.dc.subtract.1_norm_mha_20.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_20.dc.subtract.1_norm_mha_20.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_20.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_20.dc.subtract.1_norm_mha_20.dc.multiply.8, norm_mha_20.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.20.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_20.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_20.dc.multiply.8, ff.bert.encoder.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.20.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_20.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_20.dc.multiply.9, ff.bert.encoder.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_20_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_20.dc.add.10, ff.bert.encoder.layer.20.intermediate.dense.weight, ff.bert.encoder.layer.20.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff20_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_20_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_20.dc.add.10_add_ff_20: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_20.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_63_temporal_epoch_5:
    target_device: 4
    input_count: 1
    ff_20_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff20_gelu, ff.bert.encoder.layer.20.output.dense.weight, ff.bert.encoder.layer.20.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_20: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_20.dc.add.10_add_ff_20, ff_20_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_20.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_20, lc.input_tensor.norm_ff_20.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_20_norm_ff_20.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_20],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_20.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_20_norm_ff_20.dc.subtract.1, norm_ff_20.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_20.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_20.dc.subtract.1, norm_ff_20.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_20.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_20.dc.multiply.2, lc.input_tensor.norm_ff_20.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_20.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_20.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_20.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_20.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_20.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_20.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_20.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_20.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_20.dc.reciprocal.7, lc.input_tensor.norm_ff_20.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_20.dc.subtract.1_norm_ff_20.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_20.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_20.dc.subtract.1_norm_ff_20.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_20.dc.subtract.1_norm_ff_20.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_20.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_20.dc.subtract.1_norm_ff_20.dc.multiply.8, norm_ff_20.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.20.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_20.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_20.dc.multiply.8, ff.bert.encoder.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.20.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_20.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_20.dc.multiply.9, ff.bert.encoder.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_64_temporal_epoch_5:
    target_device: 5
    input_count: 1
    mha_21_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_20.dc.add.10, ff.bert.encoder.layer.21.attention.self.query.weight, ff.bert.encoder.layer.21.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_21_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_20.dc.add.10, ff.bert.encoder.layer.21.attention.self.key.weight, ff.bert.encoder.layer.21.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_21_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_21_query, mha_21_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_21],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_21_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_21_as, ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_21_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_21_as_div, e2e_attention_mask_s_brcst_m2_2_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_21_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_21_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_21_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_21_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_21_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_21_as_mask_mha_21_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_21_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_21_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_21_as_mask_mha_21_as_softmax.dc.subtract.1, mha_21_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_21_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_21_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_21_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_21_as_softmax.dc.exp.2, lc.input_tensor.mha_21_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_21_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_21_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_21_as_softmax.dc.exp.2_mha_21_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_21_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_21_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_21_as_softmax.dc.exp.2_mha_21_as_softmax.dc.multiply.5, mha_21_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_21_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_20.dc.add.10, ff.bert.encoder.layer.21.attention.self.value.weight, ff.bert.encoder.layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_21_value_mha_21_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_21_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_21_value_mha_21_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_21_value_mha_21_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_21_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_21_as_softmax.dc.multiply.5, buffer_0_mha_21_value_mha_21_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_21_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_21_ac, ff.bert.encoder.layer.21.attention.output.dense.weight, ff.bert.encoder.layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_20.dc.add.10_add_mha_21: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_20.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_21: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_20.dc.add.10_add_mha_21, mha_21_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_21.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_21, lc.input_tensor.norm_mha_21.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_21_norm_mha_21.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_21],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_21.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_21_norm_mha_21.dc.subtract.1, norm_mha_21.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_21.dc.subtract.1_norm_mha_21.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_21.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_65_temporal_epoch_5:
    target_device: 6
    input_count: 1
    norm_mha_21.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_21.dc.subtract.1, norm_mha_21.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_21.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_21.dc.multiply.2, lc.input_tensor.norm_mha_21.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_21.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_21.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_21.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_21.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_21.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_21.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_21.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_21.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_21.dc.reciprocal.7, lc.input_tensor.norm_mha_21.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_21.dc.subtract.1_norm_mha_21.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_21.dc.subtract.1_norm_mha_21.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_21.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_21.dc.subtract.1_norm_mha_21.dc.multiply.8, norm_mha_21.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.21.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_21.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_21.dc.multiply.8, ff.bert.encoder.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.21.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_21.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_21.dc.multiply.9, ff.bert.encoder.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_21_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_21.dc.add.10, ff.bert.encoder.layer.21.intermediate.dense.weight, ff.bert.encoder.layer.21.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff21_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_21_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_21.dc.add.10_add_ff_21: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_21.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_66_temporal_epoch_5:
    target_device: 7
    input_count: 1
    ff_21_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff21_gelu, ff.bert.encoder.layer.21.output.dense.weight, ff.bert.encoder.layer.21.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_21: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_21.dc.add.10_add_ff_21, ff_21_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_21.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_21, lc.input_tensor.norm_ff_21.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_21_norm_ff_21.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_21],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_21.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_21_norm_ff_21.dc.subtract.1, norm_ff_21.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_21.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_21.dc.subtract.1, norm_ff_21.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_21.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_21.dc.multiply.2, lc.input_tensor.norm_ff_21.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_21.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_21.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_21.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_21.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_21.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_21.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_21.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_21.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_21.dc.reciprocal.7, lc.input_tensor.norm_ff_21.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_21.dc.subtract.1_norm_ff_21.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_21.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_21.dc.subtract.1_norm_ff_21.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_21.dc.subtract.1_norm_ff_21.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_21.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_21.dc.subtract.1_norm_ff_21.dc.multiply.8, norm_ff_21.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.21.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_21.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_21.dc.multiply.8, ff.bert.encoder.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.21.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_21.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_21.dc.multiply.9, ff.bert.encoder.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_67_temporal_epoch_5:
    target_device: 8
    input_count: 1
    mha_22_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_21.dc.add.10, ff.bert.encoder.layer.22.attention.self.query.weight, ff.bert.encoder.layer.22.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_22_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_21.dc.add.10, ff.bert.encoder.layer.22.attention.self.key.weight, ff.bert.encoder.layer.22.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_22_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_22_query, mha_22_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_22],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_22_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_22_as, ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_22_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_22_as_div, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_22_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_22_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_22_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_22_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_22_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_22_as_mask_mha_22_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_22_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_22_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_22_as_mask_mha_22_as_softmax.dc.subtract.1, mha_22_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_22_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_22_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_22_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_22_as_softmax.dc.exp.2, lc.input_tensor.mha_22_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_22_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_22_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_22_as_softmax.dc.exp.2_mha_22_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_22_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_22_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_22_as_softmax.dc.exp.2_mha_22_as_softmax.dc.multiply.5, mha_22_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_22_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_21.dc.add.10, ff.bert.encoder.layer.22.attention.self.value.weight, ff.bert.encoder.layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_22_value_mha_22_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_22_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_22_value_mha_22_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_22_value_mha_22_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_22_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_22_as_softmax.dc.multiply.5, buffer_0_mha_22_value_mha_22_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_22_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_22_ac, ff.bert.encoder.layer.22.attention.output.dense.weight, ff.bert.encoder.layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_21.dc.add.10_add_mha_22: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_21.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_22: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_21.dc.add.10_add_mha_22, mha_22_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_22.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_22, lc.input_tensor.norm_mha_22.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_22_norm_mha_22.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_22],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_22.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_22_norm_mha_22.dc.subtract.1, norm_mha_22.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_22.dc.subtract.1_norm_mha_22.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_22.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_68_temporal_epoch_5:
    target_device: 9
    input_count: 1
    norm_mha_22.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_22.dc.subtract.1, norm_mha_22.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_22.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_22.dc.multiply.2, lc.input_tensor.norm_mha_22.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_22.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_22.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_22.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_22.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_22.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_22.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_22.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_22.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_22.dc.reciprocal.7, lc.input_tensor.norm_mha_22.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_22.dc.subtract.1_norm_mha_22.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_22.dc.subtract.1_norm_mha_22.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_22.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_22.dc.subtract.1_norm_mha_22.dc.multiply.8, norm_mha_22.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.22.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_22.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_22.dc.multiply.8, ff.bert.encoder.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.22.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_22.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_22.dc.multiply.9, ff.bert.encoder.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_22_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_22.dc.add.10, ff.bert.encoder.layer.22.intermediate.dense.weight, ff.bert.encoder.layer.22.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff22_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_22_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_22.dc.add.10_add_ff_22: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_22.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_69_temporal_epoch_5:
    target_device: 10
    input_count: 1
    ff_22_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff22_gelu, ff.bert.encoder.layer.22.output.dense.weight, ff.bert.encoder.layer.22.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_22: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_22.dc.add.10_add_ff_22, ff_22_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_22.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_22, lc.input_tensor.norm_ff_22.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_22_norm_ff_22.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_22],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_22.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_22_norm_ff_22.dc.subtract.1, norm_ff_22.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_22.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_22.dc.subtract.1, norm_ff_22.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_22.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_22.dc.multiply.2, lc.input_tensor.norm_ff_22.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_22.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_22.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_22.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_22.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_22.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_22.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_22.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_22.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_22.dc.reciprocal.7, lc.input_tensor.norm_ff_22.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_22.dc.subtract.1_norm_ff_22.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_22.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_22.dc.subtract.1_norm_ff_22.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_22.dc.subtract.1_norm_ff_22.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_22.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_22.dc.subtract.1_norm_ff_22.dc.multiply.8, norm_ff_22.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.22.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_22.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_22.dc.multiply.8, ff.bert.encoder.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.22.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_22.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_22.dc.multiply.9, ff.bert.encoder.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_70_temporal_epoch_5:
    target_device: 11
    input_count: 1
    mha_23_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_22.dc.add.10, ff.bert.encoder.layer.23.attention.self.query.weight, ff.bert.encoder.layer.23.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_23_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_22.dc.add.10, ff.bert.encoder.layer.23.attention.self.key.weight, ff.bert.encoder.layer.23.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_23_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_23_query, mha_23_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_23],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_23_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_23_as, ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_23_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_23_as_div, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_23_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_23_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_23_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_23_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_23_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_23_as_mask_mha_23_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_23_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_23_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_23_as_mask_mha_23_as_softmax.dc.subtract.1, mha_23_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_23_as_softmax.dc.exp.2: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [mha_23_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_23_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [mha_23_as_softmax.dc.exp.2, lc.input_tensor.mha_23_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_23_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [mha_23_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_23_as_softmax.dc.exp.2_mha_23_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_23_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_23_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_mha_23_as_softmax.dc.exp.2_mha_23_as_softmax.dc.multiply.5, mha_23_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_23_value: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [norm_ff_22.dc.add.10, ff.bert.encoder.layer.23.attention.self.value.weight, ff.bert.encoder.layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_23_value_mha_23_ac: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_23_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_23_value_mha_23_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_23_value_mha_23_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_23_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_23_as_softmax.dc.multiply.5, buffer_0_mha_23_value_mha_23_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_23_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_23_ac, ff.bert.encoder.layer.23.attention.output.dense.weight, ff.bert.encoder.layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_22.dc.add.10_add_mha_23: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_22.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_23: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_22.dc.add.10_add_mha_23, mha_23_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_23.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_23, lc.input_tensor.norm_mha_23.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_23_norm_mha_23.dc.subtract.1: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_23],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_23.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_23_norm_mha_23.dc.subtract.1, norm_mha_23.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_mha_23.dc.multiply.2: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_23.dc.subtract.1, norm_mha_23.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_71_temporal_epoch_5:
    target_device: 0
    input_count: 1
    norm_mha_23.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_23.dc.multiply.2, lc.input_tensor.norm_mha_23.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_23.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_23.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_23.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_23.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_23.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_23.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_23.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_23.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_23.dc.reciprocal.7, lc.input_tensor.norm_mha_23.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_mha_23.dc.subtract.1_norm_mha_23.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_23.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_23.dc.subtract.1_norm_mha_23.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_norm_mha_23.dc.subtract.1_norm_mha_23.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_23.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_23.dc.subtract.1_norm_mha_23.dc.multiply.8, norm_mha_23.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.23.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_23.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_23.dc.multiply.8, ff.bert.encoder.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.23.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_23.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_23.dc.multiply.9, ff.bert.encoder.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_23_ff1: {type: matmul, grid_loc: [2, 4], grid_size: [6, 4], inputs: [norm_mha_23.dc.add.10, ff.bert.encoder.layer.23.intermediate.dense.weight, ff.bert.encoder.layer.23.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff23_gelu: {type: gelu, grid_loc: [5, 0], grid_size: [2, 4], inputs: [ff_23_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    ff_23_ff2: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [ff23_gelu, ff.bert.encoder.layer.23.output.dense.weight, ff.bert.encoder.layer.23.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_norm_mha_23.dc.add.10_add_ff_23: {type: nop, grid_loc: [3, 0], grid_size: [2, 1], inputs: [norm_mha_23.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_ff_23: {type: add, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_norm_mha_23.dc.add.10_add_ff_23, ff_23_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_72_temporal_epoch_6:
    target_device: 0
    input_count: 1
    norm_ff_23.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_add_ff_23_0, lc.input_tensor.norm_ff_23.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_23_norm_ff_23.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_add_ff_23_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_23.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_0_add_ff_23_norm_ff_23.dc.subtract.1, norm_ff_23.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_23.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_ff_23.dc.subtract.1, norm_ff_23.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_23.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_ff_23.dc.multiply.2, lc.input_tensor.norm_ff_23.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_23.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [norm_ff_23.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_23.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_23.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [norm_ff_23.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_23.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_ff_23.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_23.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_ff_23.dc.reciprocal.7, lc.input_tensor.norm_ff_23.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_23.dc.subtract.1_norm_ff_23.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_ff_23.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_23.dc.subtract.1_norm_ff_23.dc.multiply.8: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_1_norm_ff_23.dc.subtract.1_norm_ff_23.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_23.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_norm_ff_23.dc.subtract.1_norm_ff_23.dc.multiply.8, norm_ff_23.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.23.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_23.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_23.dc.multiply.8, ff.bert.encoder.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.23.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_23.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_23.dc.multiply.9, ff.bert.encoder.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    norm_ff_23.dc.add.10_output_nop_0: {type: nop, grid_loc: [3, 4], grid_size: [2, 1], inputs: [norm_ff_23.dc.add.10], untilize_output: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_73_temporal_epoch_6:
    target_device: 1
    input_count: 1

  fwd_74_temporal_epoch_6:
    target_device: 2
    input_count: 1

  fwd_75_temporal_epoch_6:
    target_device: 3
    input_count: 1

  fwd_76_temporal_epoch_6:
    target_device: 4
    input_count: 1

  fwd_77_temporal_epoch_6:
    target_device: 5
    input_count: 1

  fwd_78_temporal_epoch_6:
    target_device: 6
    input_count: 1

  fwd_79_temporal_epoch_6:
    target_device: 7
    input_count: 1

  fwd_80_temporal_epoch_6:
    target_device: 8
    input_count: 1

  fwd_81_temporal_epoch_6:
    target_device: 9
    input_count: 1

  fwd_82_temporal_epoch_6:
    target_device: 10
    input_count: 1

  fwd_83_temporal_epoch_6:
    target_device: 11
    input_count: 1


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 1, $lptr_q6: 0, $gptr_q1: 0, $gptr_q6: 0, $lptr_q5: 0, $gptr_q5: 0, $lptr_q2: 0, $lptr_q4: 0, $lptr_q3: 0, $gptr_q4: 0, $gptr_q3: 0, $gptr_q2: 0, $c_one: 1, $lptr_q1: 0, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e_ff3_gelu_0, e2e_buffer_0_norm_mha_3.dc.add.10_add_ff_3_0, e2e_attention_mask_s_brcst_m2_19_1.lc1_0, e2e_attention_mask_s_brcst_m2_18_1.lc1_0, e2e_attention_mask_s_brcst_m2_17_1.lc1_0, e2e_attention_mask_s_brcst_m2_16_1.lc1_0, e2e_attention_mask_s_brcst_m2_15_1.lc1_0, e2e_attention_mask_s_brcst_m2_14_1.lc1_0, e2e_attention_mask_s_brcst_m2_13_1.lc1_0, e2e_attention_mask_s_brcst_m2_12_1.lc1_0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {
               encoder_input: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               ff.bert.encoder.layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_0: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_23_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_22_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_21_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_20_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_19_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_18_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_17_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_16_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_15_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_14_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_13_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_12_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_1_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_0.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_2_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_0.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_3_temporal_epoch_0}
    -   execute: {graph_name: fwd_4_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_1: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_5_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_1.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_6_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_1.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_7_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_2: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_8_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_2.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_9_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_2.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_10_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_3: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_11_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_3.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_ff7_gelu_0, e2e_buffer_0_norm_mha_7.dc.add.10_add_ff_7_0]
    -   execute: {graph_name: fwd_12_temporal_epoch_1, queue_settings: {
               e2e_ff3_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_buffer_0_norm_mha_3.dc.add.10_add_ff_3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_3.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_13_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_19_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_4: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_14_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_4.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_15_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_4.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_16_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_18_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_5: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_17_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_5.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_18_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_5.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_19_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_17_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_6: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_20_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_6.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_21_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_6.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_22_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_16_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_7: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_23_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_7.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_ff3_gelu_0, e2e_buffer_0_norm_mha_3.dc.add.10_add_ff_3_0, e2e_attention_mask_s_brcst_m2_19_1.lc1_0, e2e_attention_mask_s_brcst_m2_18_1.lc1_0, e2e_attention_mask_s_brcst_m2_17_1.lc1_0, e2e_attention_mask_s_brcst_m2_16_1.lc1_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_ff11_gelu_0, e2e_buffer_0_norm_mha_11.dc.add.10_add_ff_11_0, e2e_attention_mask_s_brcst_m2_11_1.lc1_0, e2e_attention_mask_s_brcst_m2_10_1.lc1_0, e2e_attention_mask_s_brcst_m2_9_1.lc1_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0]
    -   execute: {graph_name: fwd_24_temporal_epoch_2, queue_settings: {
               e2e_ff7_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_buffer_0_norm_mha_7.dc.add.10_add_ff_7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_7.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_25_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_15_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_8: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_26_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_8.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_27_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_8.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_28_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_14_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_9: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_29_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_9.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_30_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_9.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_31_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_13_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_10: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_32_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_10.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_33_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_10.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_34_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_12_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_11: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_11_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_10_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_9_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_8_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_7_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_6_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_5_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_4_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_35_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_11.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_ff7_gelu_0, e2e_buffer_0_norm_mha_7.dc.add.10_add_ff_7_0, e2e_attention_mask_s_brcst_m2_15_1.lc1_0, e2e_attention_mask_s_brcst_m2_14_1.lc1_0, e2e_attention_mask_s_brcst_m2_13_1.lc1_0, e2e_attention_mask_s_brcst_m2_12_1.lc1_0, e2e_attention_mask_input_op_fork_nop1_input_op_fork_nop0_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_ff15_gelu_0, e2e_buffer_0_norm_mha_15.dc.add.10_add_ff_15_0]
    -   execute: {graph_name: fwd_36_temporal_epoch_3, queue_settings: {
               e2e_ff11_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_buffer_0_norm_mha_11.dc.add.10_add_ff_11_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_11.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_37_temporal_epoch_3, queue_settings: {
               e2e_attention_mask_s_brcst_m2_11_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_12: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_12_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_12_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_12_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_12.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_38_temporal_epoch_3, queue_settings: {
               lc.input_tensor.norm_mha_12.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_12.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_12.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.12.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.12.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_39_temporal_epoch_3, queue_settings: {
               ff.bert.encoder.layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_12.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_12.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_12.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_12.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.12.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.12.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_40_temporal_epoch_3, queue_settings: {
               e2e_attention_mask_s_brcst_m2_10_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_13: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_13_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_13_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_13_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_13.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_41_temporal_epoch_3, queue_settings: {
               lc.input_tensor.norm_mha_13.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_13.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_13.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.13.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.13.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_42_temporal_epoch_3, queue_settings: {
               ff.bert.encoder.layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_13.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_13.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_13.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_13.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.13.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.13.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_43_temporal_epoch_3, queue_settings: {
               e2e_attention_mask_s_brcst_m2_9_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_14: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_14_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_14_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_14_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_14.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_44_temporal_epoch_3, queue_settings: {
               lc.input_tensor.norm_mha_14.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_14.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_14.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.14.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.14.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_45_temporal_epoch_3, queue_settings: {
               ff.bert.encoder.layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_14.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_14.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_14.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_14.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.14.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.14.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_46_temporal_epoch_3, queue_settings: {
               e2e_attention_mask_s_brcst_m2_8_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_15: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_15_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_15_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_15_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_15.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_47_temporal_epoch_3, queue_settings: {
               lc.input_tensor.norm_mha_15.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_15.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_15.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.15.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.15.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_ff11_gelu_0, e2e_buffer_0_norm_mha_11.dc.add.10_add_ff_11_0, e2e_attention_mask_s_brcst_m2_11_1.lc1_0, e2e_attention_mask_s_brcst_m2_10_1.lc1_0, e2e_attention_mask_s_brcst_m2_9_1.lc1_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_ff19_gelu_0, e2e_buffer_0_norm_mha_19.dc.add.10_add_ff_19_0]
    -   execute: {graph_name: fwd_48_temporal_epoch_4, queue_settings: {
               e2e_ff15_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_buffer_0_norm_mha_15.dc.add.10_add_ff_15_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               ff.bert.encoder.layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_15.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_15.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_15.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_15.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.15.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.15.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_49_temporal_epoch_4, queue_settings: {
               e2e_attention_mask_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               ff.bert.encoder.layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_16: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_16_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_16_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_16_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_16.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_50_temporal_epoch_4, queue_settings: {
               lc.input_tensor.norm_mha_16.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_16.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_16.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.16.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.16.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_51_temporal_epoch_4, queue_settings: {
               ff.bert.encoder.layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_16.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_16.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_16.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_16.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.16.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.16.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_52_temporal_epoch_4, queue_settings: {
               e2e_attention_mask_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               ff.bert.encoder.layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_17: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_17_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_17_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_17_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_17.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_53_temporal_epoch_4, queue_settings: {
               lc.input_tensor.norm_mha_17.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_17.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_17.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.17.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.17.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_54_temporal_epoch_4, queue_settings: {
               ff.bert.encoder.layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_17.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_17.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_17.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_17.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.17.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.17.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_55_temporal_epoch_4, queue_settings: {
               e2e_attention_mask_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               ff.bert.encoder.layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_18: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_18_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_18_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_18_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_18.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_56_temporal_epoch_4, queue_settings: {
               lc.input_tensor.norm_mha_18.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_18.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.18.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.18.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_57_temporal_epoch_4, queue_settings: {
               ff.bert.encoder.layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_18.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_18.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_18.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_18.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.18.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.18.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_58_temporal_epoch_4, queue_settings: {
               e2e_attention_mask_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               ff.bert.encoder.layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_19: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_19_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_19_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_19_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_19.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_59_temporal_epoch_4, queue_settings: {
               lc.input_tensor.norm_mha_19.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_19.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_19.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.19.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.19.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_ff15_gelu_0, e2e_buffer_0_norm_mha_15.dc.add.10_add_ff_15_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_add_ff_23_0]
    -   execute: {graph_name: fwd_60_temporal_epoch_5, queue_settings: {
               e2e_ff19_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_buffer_0_norm_mha_19.dc.add.10_add_ff_19_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               ff.bert.encoder.layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_19.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_19.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_19.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_19.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.19.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.19.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_61_temporal_epoch_5, queue_settings: {
               e2e_attention_mask_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               ff.bert.encoder.layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_20: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_20_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_20_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_20_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_20.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_62_temporal_epoch_5, queue_settings: {
               lc.input_tensor.norm_mha_20.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_20.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_20.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.20.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.20.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_63_temporal_epoch_5, queue_settings: {
               ff.bert.encoder.layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_20.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_20.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_20.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_20.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.20.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.20.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_64_temporal_epoch_5, queue_settings: {
               e2e_attention_mask_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               ff.bert.encoder.layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_21: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_21_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_21_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_21_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_21.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_65_temporal_epoch_5, queue_settings: {
               lc.input_tensor.norm_mha_21.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_21.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_21.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.21.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.21.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_66_temporal_epoch_5, queue_settings: {
               ff.bert.encoder.layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_21.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_21.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_21.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_21.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.21.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.21.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_67_temporal_epoch_5, queue_settings: {
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               ff.bert.encoder.layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_22: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_22_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_22_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_22_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_22.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_68_temporal_epoch_5, queue_settings: {
               lc.input_tensor.norm_mha_22.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_22.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_22.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.22.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.22.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_69_temporal_epoch_5, queue_settings: {
               ff.bert.encoder.layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_22.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_22.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_22.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_22.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.22.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.22.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_70_temporal_epoch_5, queue_settings: {
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               ff.bert.encoder.layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_23: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_23_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_23_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_23_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_23.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_71_temporal_epoch_5, queue_settings: {
               lc.input_tensor.norm_mha_23.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_23.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_23.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.23.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.23.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_ff19_gelu_0, e2e_buffer_0_norm_mha_19.dc.add.10_add_ff_19_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_72_temporal_epoch_6, queue_settings: {
               e2e_add_ff_23_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               lc.input_tensor.norm_ff_23.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_23.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_23.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_23.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.23.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.23.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_73_temporal_epoch_6}
    -   execute: {graph_name: fwd_74_temporal_epoch_6}
    -   execute: {graph_name: fwd_75_temporal_epoch_6}
    -   execute: {graph_name: fwd_76_temporal_epoch_6}
    -   execute: {graph_name: fwd_77_temporal_epoch_6}
    -   execute: {graph_name: fwd_78_temporal_epoch_6}
    -   execute: {graph_name: fwd_79_temporal_epoch_6}
    -   execute: {graph_name: fwd_80_temporal_epoch_6}
    -   execute: {graph_name: fwd_81_temporal_epoch_6}
    -   execute: {graph_name: fwd_82_temporal_epoch_6}
    -   execute: {graph_name: fwd_83_temporal_epoch_6}
    -   deallocate_queue: [e2e_add_ff_23_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    - endloop


