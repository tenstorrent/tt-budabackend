# git checkout 9da0fd7
# pytest decode40.py --mode sequential --kv-cache last_1layer_split_64_prefill.pt -l 1 --version split-mq --load-weights -d golden --num-tokens 1 --user-rows 32 --precision bf16 --num-chips 32 --net-name buf_error --log-level DEBUG

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 256], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3882a520]]}
  cos:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x388294c0]]}
  sin:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38828460]]}
  past_key:                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 8, mblock: [65, 64], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x34624440]]}
  attn_mask:                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 65], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x34204020]]}
  past_value:                                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 8, mblock: [65, 64], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}

  # output
  falcon_32c_0mf_0af_1l_2048s_buf_error.output_add_220:                {input: add_220_output_nop_0, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 64], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}
  falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_37:             {input: falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_37_tm_nop_output_nop_0, type: queue, entries: 1, grid_size: [1, 1], t: 8, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x80020]}
  falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_56:             {input: falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_56_tm_nop_output_nop_0, type: queue, entries: 1, grid_size: [1, 1], t: 8, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x180040]}

  # parameter
  layers.0.ln_mlp.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 64], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x400262a0]]}
  layers.0.ln_mlp.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 64], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40027300]]}
  layers.0.mlp.dense_h_to_4h.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [4, 32], ublock: [64, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[1, 0x40035e80], [3, 0x40035ec0], [4, 0x40086160], [5, 0x40086160], [2, 0x400a82c0], [0, 0x400a9320], [1, 0x42335ea0], [3, 0x42335ee0]]}
  layers.0.mlp.dense_4h_to_h.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [128, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 1, loc: dram, dram: [[4, 0x42386180], [5, 0x42386180], [2, 0x423a82e0], [0, 0x423a9340], [1, 0x44635ec0], [3, 0x44635f00], [1, 0x465a100], [2, 0x465a100]]}
  layers.0.ln_attn.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 64], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4123e0a0]]}
  layers.0.ln_attn.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 64], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4120bdc0]]}
  layers.0.self_attention.wq.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [32, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x409474a0], [4, 0x4094b5a0], [5, 0x4094bda0], [0, 0x4097e080], [1, 0x41040040], [2, 0x41040040], [3, 0x412074c0], [4, 0x4120b5c0]]}
  layers.0.self_attention.wk.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [2, 1], ublock: [128, 2], ublock_order: r, df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40820020], [3, 0x40820860], [4, 0x40820860], [0, 0x40866040], [5, 0x408ac040], [3, 0x408ac880], [4, 0x408ac880], [0, 0x408f2060]]}
  layers.0.self_attention.wv.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [2, 1], ublock: [128, 2], ublock_order: r, df: Bfp8_b, target_device: 2, loc: dram, dram: [[1, 0x40000000], [2, 0x40000000], [3, 0x40000000], [4, 0x40000000], [5, 0x40000000], [0, 0x40046020], [1, 0x4008c020], [2, 0x4008c020]]}
  layers.0.self_attention.dense.weight:                                {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [32, 8], ublock: [8, 4], ublock_order: r, df: Bfp8_b, target_device: 7, loc: dram, dram: [[4, 0x400290e0], [2, 0x40029180], [5, 0x40029720], [3, 0x400297c0], [1, 0x400299c0], [0, 0x40082020], [4, 0x408e9100], [2, 0x408e91a0]]}

  # constant
  lc.input_tensor.layernorm_0.dc.reduce_sum.0.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x400061e0]]}
  dc.input_tensor.layernorm_0.1:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x465b180], [4, 0x446861a0]]}
  lc.input_tensor.layernorm_0.dc.reduce_sum.5.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x465a940]]}
  dc.input_tensor.layernorm_0.6:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x465a100]]}
  dc.input_tensor.layernorm_0.8:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40026ac0]]}
  lc.input_tensor.layernorm_0.dc.reciprocal.11_s_brcst_m1_0_0.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40026280]]}
  lc.input_tensor.layernorm_10.dc.reduce_sum.0.0:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40820020]]}
  dc.input_tensor.layernorm_10.1:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 32], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4128fee0], [0, 0x412c00c0]]}
  lc.input_tensor.layernorm_10.dc.reduce_sum.5.0:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4128f6a0]]}
  dc.input_tensor.layernorm_10.6:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4128ee60]]}
  dc.input_tensor.layernorm_10.8:                                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4128e620]]}
  lc.input_tensor.layernorm_10.dc.reciprocal.11_s_brcst_m1_0_0.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4128dde0]]}
  input_1_multiply_17:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40946c60]]}
  lc.input_tensor.transpose_23.dc.sparse_matmul.4.0:                   {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 0, loc: dram, dram: [[4, 0x4093a920], [5, 0x4093b120], [3, 0x4093b960], [4, 0x409402a0], [5, 0x40940aa0], [3, 0x409412e0], [4, 0x40945c20], [5, 0x40946420]]}
  lc.input_tensor.transpose_23.dc.sparse_matmul.4.1:                   {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 0, loc: dram, dram: [[5, 0x40938060], [3, 0x409388a0], [4, 0x409388a0], [5, 0x409390a0], [3, 0x409398e0], [4, 0x409398e0], [5, 0x4093a0e0], [3, 0x4093a920]]}
  input_1_multiply_32:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40820020]]}
  lc.input_tensor.reshape_37.dc.sparse_matmul.4.0:                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 31, loc: dram, dram: [[0, 0x40000000], [1, 0x40000000], [2, 0x40000000], [3, 0x40000000]]}
  lc.input_tensor.reshape_37.dc.sparse_matmul.4.1:                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 31, loc: dram, dram: [[4, 0x40000000], [5, 0x40000000], [0, 0x40000c80], [1, 0x40000c80]]}
  lc.input_tensor.reshape_37.dc.sparse_matmul.10.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 31, loc: dram, dram: [[2, 0x40000c80]]}
  lc.input_tensor.reshape_37.dc.sparse_matmul.10.1:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 31, loc: dram, dram: [[3, 0x40000c80]]}
  lc.input_tensor.reshape_37.dc.sparse_matmul.14.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 31, loc: dram, dram: [[4, 0x40001040]]}
  lc.input_tensor.reshape_37.dc.sparse_matmul.14.1:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 31, loc: dram, dram: [[5, 0x40001040]]}
  lc.input_tensor.reshape_37.dc.transpose.23_s_brcst_m2_1_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 31, loc: dram, dram: [[2, 0x40001900]]}
  input_1_multiply_46:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x446861a0]]}
  lc.input_tensor.attn_mask_s_brcst_m2_0_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x446869e0]]}
  lc.input_tensor.softmax_48.dc.reduce_max.0_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x44687220]]}
  lc.input_tensor.softmax_48.dc.reduce_sum.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x44687a60]]}
  dc.input_tensor.softmax_48.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x40025aa0]]}
  lc.input_tensor.softmax_48.dc.reciprocal.6_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x400061e0]]}
  lc.input_tensor.reshape_56.dc.sparse_matmul.4.0:                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 2, loc: dram, dram: [[3, 0x4008c020], [4, 0x4008c020], [5, 0x4008c020], [3, 0x4008cca0]]}
  lc.input_tensor.reshape_56.dc.sparse_matmul.4.1:                     {input: HOST, type: queue, entries: 1, grid_size: [4, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 2, loc: dram, dram: [[4, 0x4008cca0], [5, 0x4008cca0], [3, 0x4008d920], [4, 0x4008dce0]]}
  lc.input_tensor.reshape_56.dc.sparse_matmul.10.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 2, loc: dram, dram: [[5, 0x4008dce0]]}
  lc.input_tensor.reshape_56.dc.sparse_matmul.10.1:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 2, loc: dram, dram: [[3, 0x4008e960]]}
  lc.input_tensor.reshape_56.dc.sparse_matmul.14.0:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 9], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 3, loc: dram, dram: [[0, 0x40000000]]}
  lc.input_tensor.reshape_56.dc.sparse_matmul.14.1:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 3, loc: dram, dram: [[1, 0x40000000]]}
  lc.input_tensor.reshape_56.dc.transpose.23_s_brcst_m2_1_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x40000000]]}
  lc.input_tensor.transpose_67.dc.sparse_matmul.4.0:                   {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 4, loc: dram, dram: [[1, 0x40000000], [2, 0x40000000], [3, 0x40000000], [4, 0x40000000], [5, 0x40000000], [1, 0x40005980], [2, 0x40005980], [3, 0x40005980]]}
  lc.input_tensor.transpose_67.dc.sparse_matmul.4.1:                   {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 4, loc: dram, dram: [[4, 0x40005980], [5, 0x40005980], [4, 0x400069c0], [5, 0x400069c0], [4, 0x40007a00], [5, 0x40007a00], [4, 0x40008a40], [5, 0x40008a40]]}
  input_1_multiply_75:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x40000000]]}
  lc.input_tensor.attn_mask_s_brcst_m2_1_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x40000000]]}
  lc.input_tensor.softmax_77.dc.reduce_max.0_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x40000000]]}
  lc.input_tensor.softmax_77.dc.reduce_sum.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x40000000]]}
  dc.input_tensor.softmax_77.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x40000000]]}
  lc.input_tensor.softmax_77.dc.reciprocal.6_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x40000000]]}
  lc.input_tensor.transpose_88.dc.sparse_matmul.4.0:                   {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 6, loc: dram, dram: [[5, 0x40000000], [2, 0x40000440], [0, 0x40000840], [3, 0x40000840], [1, 0x40001040], [5, 0x40005980], [2, 0x40005dc0], [0, 0x400061c0]]}
  lc.input_tensor.transpose_88.dc.sparse_matmul.4.1:                   {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 6, loc: dram, dram: [[3, 0x40008b00], [3, 0x40009b40], [3, 0x4000ab80], [5, 0x4000b300], [2, 0x4000b740], [0, 0x4000bb40], [3, 0x4000bbc0], [5, 0x4000c340]]}
  input_1_multiply_96:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x400082c0]]}
  lc.input_tensor.attn_mask_s_brcst_m2_2_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x40007a80]]}
  lc.input_tensor.softmax_98.dc.reduce_max.0_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x40007240]]}
  lc.input_tensor.softmax_98.dc.reduce_sum.3.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x40006a00]]}
  dc.input_tensor.softmax_98.4:                                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x400069c0]]}
  lc.input_tensor.softmax_98.dc.reciprocal.6_s_brcst_m1_0_0.0:         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x400061c0]]}
  lc.input_tensor.transpose_109.dc.sparse_matmul.4.0:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 7, loc: dram, dram: [[4, 0x40010420], [5, 0x40010420], [4, 0x40015da0], [5, 0x40015da0], [4, 0x4001b720], [5, 0x4001b720], [1, 0x40020840], [2, 0x40020840]]}
  lc.input_tensor.transpose_109.dc.sparse_matmul.4.1:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 7, loc: dram, dram: [[3, 0x40020840], [4, 0x400210a0], [5, 0x400210a0], [3, 0x40021880], [4, 0x400220e0], [5, 0x400220e0], [3, 0x400228c0], [4, 0x40023120]]}
  input_1_multiply_117:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x40000000]]}
  lc.input_tensor.attn_mask_s_brcst_m2_3_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x40000000]]}
  lc.input_tensor.softmax_119.dc.reduce_max.0_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x400061e0]]}
  lc.input_tensor.softmax_119.dc.reduce_sum.3.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x400061e0]]}
  dc.input_tensor.softmax_119.4:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40006a20]]}
  lc.input_tensor.softmax_119.dc.reciprocal.6_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x40006a20]]}
  lc.input_tensor.transpose_130.dc.sparse_matmul.4.0:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 1, loc: dram, dram: [[2, 0x40006a20], [3, 0x40006a20], [1, 0x40007260], [2, 0x4000c3a0], [3, 0x4000c3a0], [1, 0x4000cbe0], [2, 0x40011d20], [3, 0x40011d20]]}
  lc.input_tensor.transpose_130.dc.sparse_matmul.4.1:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 1, loc: dram, dram: [[1, 0x40012560], [1, 0x400135a0], [1, 0x400145e0], [1, 0x40015620], [1, 0x40016660], [0, 0x40016e40], [1, 0x400176a0], [2, 0x400176a0]]}
  input_1_multiply_138:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x4008e960]]}
  lc.input_tensor.attn_mask_s_brcst_m2_4_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x4008ed20]]}
  lc.input_tensor.softmax_140.dc.reduce_max.0_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x4008f1a0]]}
  lc.input_tensor.softmax_140.dc.reduce_sum.3.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x4008f560]]}
  dc.input_tensor.softmax_140.4:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x4008f9a0]]}
  lc.input_tensor.softmax_140.dc.reciprocal.6_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x4008f9e0]]}
  lc.input_tensor.transpose_151.dc.sparse_matmul.4.0:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 1, loc: dram, dram: [[3, 0x400176a0], [0, 0x40017e80], [1, 0x400186e0], [2, 0x400186e0], [3, 0x4001d020], [0, 0x4001d800], [1, 0x4001e060], [2, 0x4001e060]]}
  lc.input_tensor.transpose_151.dc.sparse_matmul.4.1:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 1, loc: dram, dram: [[3, 0x400229a0], [0, 0x40023180], [1, 0x400239e0], [2, 0x400239e0], [3, 0x400239e0], [0, 0x400241c0], [1, 0x40024a20], [2, 0x40024a20]]}
  input_1_multiply_159:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x40024a20]]}
  lc.input_tensor.attn_mask_s_brcst_m2_5_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40025200]]}
  lc.input_tensor.softmax_161.dc.reduce_max.0_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x40025260]]}
  lc.input_tensor.softmax_161.dc.reduce_sum.3.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40025a40]]}
  dc.input_tensor.softmax_161.4:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x40025a60]]}
  lc.input_tensor.softmax_161.dc.reciprocal.6_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x40025a60]]}
  lc.input_tensor.transpose_172.dc.sparse_matmul.4.0:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 2, loc: dram, dram: [[4, 0x4008fda0], [5, 0x40090220], [4, 0x40095720], [5, 0x40095ba0], [4, 0x4009b0a0], [5, 0x4009b520], [3, 0x4009fdc0], [4, 0x400a0a20]]}
  lc.input_tensor.transpose_172.dc.sparse_matmul.4.1:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 2, loc: dram, dram: [[5, 0x400a0ea0], [5, 0x400a1ee0], [5, 0x400a2f20], [5, 0x400a3f60], [5, 0x400a4fa0], [3, 0x400a5740], [5, 0x400a5fe0], [4, 0x400a63a0]]}
  input_1_multiply_180:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x40000000]]}
  lc.input_tensor.attn_mask_s_brcst_m2_6_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x40000000]]}
  lc.input_tensor.softmax_182.dc.reduce_max.0_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x40000000]]}
  lc.input_tensor.softmax_182.dc.reduce_sum.3.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x40000840]]}
  dc.input_tensor.softmax_182.4:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x40000840]]}
  lc.input_tensor.softmax_182.dc.reciprocal.6_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x40000840]]}
  lc.input_tensor.transpose_193.dc.sparse_matmul.4.0:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 65], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 4, loc: dram, dram: [[4, 0x40009a80], [5, 0x40009a80], [1, 0x4000b300], [2, 0x4000b300], [3, 0x4000b300], [4, 0x4000f400], [5, 0x4000f400], [1, 0x40010c80]]}
  lc.input_tensor.transpose_193.dc.sparse_matmul.4.1:                  {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 4, loc: dram, dram: [[2, 0x40010c80], [3, 0x40010c80], [2, 0x40011cc0], [3, 0x40011cc0], [2, 0x40012d00], [3, 0x40012d00], [2, 0x40013d40], [3, 0x40013d40]]}
  input_1_multiply_201:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x40000840]]}
  lc.input_tensor.attn_mask_s_brcst_m2_7_0.0:                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x40000840]]}
  lc.input_tensor.softmax_203.dc.reduce_max.0_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x40000840]]}
  lc.input_tensor.softmax_203.dc.reduce_sum.3.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x40000000]]}
  dc.input_tensor.softmax_203.4:                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x40000000]]}
  lc.input_tensor.softmax_203.dc.reciprocal.6_s_brcst_m1_0_0.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x40000000]]}
  lc.input_tensor.concatenate_213.dc.sparse_matmul.10.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 6, loc: dram, dram: [[2, 0x40000000]]}
  lc.input_tensor.concatenate_213.dc.sparse_matmul.10.1:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 6, loc: dram, dram: [[1, 0x40000000]]}
  lc.input_tensor.reshape_214.dc.sparse_matmul.4.0:                    {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 17], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 7, loc: dram, dram: [[5, 0x40023120], [3, 0x40023900], [4, 0x40024160], [5, 0x400248a0], [3, 0x40025080], [4, 0x400258e0], [5, 0x40026020], [1, 0x400261c0]]}
  lc.input_tensor.reshape_214.dc.sparse_matmul.4.1:                    {input: HOST, type: queue, entries: 1, grid_size: [8, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 7, loc: dram, dram: [[2, 0x400261c0], [3, 0x40026800], [4, 0x40027060], [2, 0x40027200], [5, 0x400277a0], [3, 0x40027840], [1, 0x40027940], [4, 0x400280a0]]}
  lc.input_tensor.reshape_214.dc.sparse_matmul.10.0:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 3], t: 1, mblock: [1, 11], ublock: [1, 1], ublock_order: c, df: Bfp2_b, target_device: 7, loc: dram, dram: [[2, 0x40028240], [5, 0x400287e0], [3, 0x40028880]]}
  lc.input_tensor.reshape_214.dc.sparse_matmul.10.1:                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: RawUInt32, target_device: 7, loc: dram, dram: [[1, 0x40028980]]}

  # epoch_to_epoch
  e2e_index_25.dc.select.0_0:                                          {input: index_25.dc.select.0, type: queue, entries: 1, grid_size: [1, 8], t: 32, mblock: [64, 1], ublock: [1, 2], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40000000], [2, 0x40000000], [3, 0x40000000], [4, 0x40000000], [5, 0x40000000], [0, 0x40046020], [1, 0x40820020], [2, 0x40820020]]}
  e2e_layernorm_10.dc.add.14_0:                                        {input: layernorm_10.dc.add.14, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 64], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40000000]]}
  e2e_index_25.dc.buffer.1_0:                                          {input: index_25.dc.buffer.1, type: queue, entries: 1, grid_size: [8, 8], t: 8, mblock: [4, 2], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x446882a0], [5, 0x469c1a0], [2, 0x446a8300], [0, 0x446a9360], [4, 0x446c71c0], [5, 0x4478c2c0], [5, 0x47a01c0], [2, 0x447ac320], [0, 0x447ad380], [4, 0x447cb1e0], [5, 0x448902e0], [5, 0x48a41e0], [2, 0x448b0340], [0, 0x448b13a0], [4, 0x448cf200], [5, 0x44994300], [5, 0x49a8200], [2, 0x449b4360], [0, 0x449b53c0], [4, 0x449d3220], [5, 0x44a98320], [5, 0x4aac220], [2, 0x44ab8380], [0, 0x44ab93e0], [4, 0x44ad7240], [5, 0x44b9c340], [5, 0x4bb0240], [2, 0x44bbc3a0], [0, 0x44bbd400], [4, 0x44bdb260], [5, 0x44ca0360], [5, 0x4cb4260], [2, 0x44cc03c0], [0, 0x44cc1420], [4, 0x44cdf280], [5, 0x44da4380], [5, 0x4db8280], [2, 0x44dc43e0], [0, 0x44dc5440], [4, 0x44de32a0], [5, 0x44ea83a0], [5, 0x4ebc2a0], [2, 0x44ec8400], [0, 0x44ec9460], [4, 0x44ee72c0], [5, 0x44fac3c0], [5, 0x4fc02c0], [2, 0x44fcc420], [0, 0x44fcd480], [4, 0x44feb2e0], [5, 0x450b03e0], [5, 0x50c42e0], [2, 0x450d0440], [0, 0x450d14a0], [4, 0x450ef300], [5, 0x451b4400], [5, 0x51c8300], [2, 0x451d4460], [0, 0x451d54c0], [4, 0x451f3320], [5, 0x452b8420], [5, 0x52cc320], [2, 0x452d8480], [0, 0x452d94e0]]}
  e2e_add_38_0:                                                        {input: add_38, type: queue, entries: 1, grid_size: [1, 1], t: 8, mblock: [1, 16], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x452f7340]]}
  e2e_transpose_23.dc.sparse_matmul.4.lc2_0:                           {input: transpose_23.dc.sparse_matmul.4.lc2, type: queue, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40000000], [1, 0x40000000], [2, 0x40000000], [3, 0x40000000], [4, 0x40000000], [5, 0x40000000], [0, 0x400020a0], [1, 0x400020a0], [2, 0x400020a0], [3, 0x400020a0], [4, 0x400020a0], [5, 0x400020a0], [0, 0x40004140], [1, 0x40004140], [2, 0x40004140], [3, 0x40004140]]}
  e2e_index_40.dc.select.0_0:                                          {input: index_40.dc.select.0, type: queue, entries: 1, grid_size: [5, 4], t: 1, mblock: [13, 4], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x453bc440], [5, 0x53d0340], [2, 0x453dc4a0], [0, 0x453dd500], [4, 0x453fb360], [5, 0x45425e60], [5, 0x5439d60], [2, 0x45445ec0], [0, 0x45446f20], [4, 0x45464d80], [5, 0x4548f880], [5, 0x54a3780], [2, 0x454af8e0], [0, 0x454b0940], [4, 0x454ce7a0], [5, 0x454f92a0], [5, 0x550d1a0], [2, 0x45519300], [0, 0x4551a360], [4, 0x455381c0]]}
  e2e_layernorm_10.dc.add.14_1:                                        {input: layernorm_10.dc.add.14, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 64], ublock: [1, 4], ublock_order: c, df: Bfp8_b, target_device: 2, loc: dram, dram: [[0, 0x40000000]]}
  e2e_add_21_0:                                                        {input: add_21, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x40000000]]}
  e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0_0:  {input: concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0, type: queue, entries: 1, grid_size: [5, 8], t: 8, mblock: [13, 2], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x40000000], [3, 0x40000000], [4, 0x40000000], [5, 0x40000000], [0, 0x40000840], [1, 0x40000840], [2, 0x401a6820], [3, 0x401a6820], [4, 0x401a6820], [5, 0x401a6820], [0, 0x401a7060], [1, 0x401a7060], [2, 0x4034d040], [3, 0x4034d040], [4, 0x4034d040], [5, 0x4034d040], [0, 0x4034d880], [1, 0x4034d880], [2, 0x404f3860], [3, 0x404f3860], [4, 0x404f3860], [5, 0x404f3860], [0, 0x404f40a0], [1, 0x404f40a0], [2, 0x4069a080], [3, 0x4069a080], [4, 0x4069a080], [5, 0x4069a080], [0, 0x4069a8c0], [1, 0x4069a8c0], [2, 0x408408a0], [3, 0x408408a0], [4, 0x408408a0], [5, 0x408408a0], [0, 0x408410e0], [1, 0x408410e0], [2, 0x409e70c0], [3, 0x409e70c0], [4, 0x409e70c0], [5, 0x409e70c0]]}
  e2e_add_118_0:                                                       {input: add_118, type: queue, entries: 1, grid_size: [1, 1], t: 32, mblock: [1, 65], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x45dc45a0]]}
  e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0_0:  {input: concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0, type: queue, entries: 1, grid_size: [5, 8], t: 8, mblock: [13, 2], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x45de2400], [4, 0x45f88c20], [4, 0x4612f440], [0, 0x461e49c0], [4, 0x462d5c60], [0, 0x4638b1e0], [4, 0x4647c480], [0, 0x46531a00], [5, 0x465e3d00], [5, 0x65f7c00], [2, 0x46603d60], [4, 0x46622ca0], [0, 0x466d8220], [5, 0x4678a520], [5, 0x679e420], [2, 0x467aa580], [4, 0x467c94c0], [0, 0x4687ea40], [5, 0x46930d40], [1, 0x46935ee0], [3, 0x46935f20], [5, 0x6944c40], [2, 0x46950da0], [1, 0x695a120], [2, 0x695a120], [4, 0x4696fce0], [0, 0x46a25260], [5, 0x46ad7560], [1, 0x46adc700], [3, 0x46adc740], [5, 0x6aeb460], [2, 0x46af75c0], [1, 0x6b00940], [2, 0x6b00940], [4, 0x46b16500], [0, 0x46bcba80], [5, 0x46c7dd80], [1, 0x46c82f20], [3, 0x46c82f60], [5, 0x6c91c80]]}
  e2e_add_21_3:                                                        {input: add_21, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x40004140]]}
  e2e_concatenate_39.dc.concatenate.0_0:                               {input: concatenate_39.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 8], t: 32, mblock: [65, 1], ublock: [1, 2], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x45562cc0], [5, 0x5576bc0], [2, 0x45582d20], [0, 0x45583d80], [4, 0x455a1be0], [5, 0x45da34e0], [5, 0x5db73e0], [2, 0x45dc3540]]}
  e2e_transpose_130.dc.sparse_matmul.4.lc2_0:                          {input: transpose_130.dc.sparse_matmul.4.lc2, type: queue, entries: 1, grid_size: [8, 2], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x400a6780], [5, 0x400a7020], [4, 0x400a73e0], [3, 0x400a8820], [5, 0x400a90c0], [4, 0x400a9480], [3, 0x400aa8c0], [5, 0x400ab160], [4, 0x400ab520], [3, 0x400ac960], [5, 0x400ad200], [4, 0x400ad5c0], [3, 0x400aea00], [5, 0x400af2a0], [4, 0x400af660], [3, 0x400b0aa0]]}
  e2e_concatenate_58.dc.concatenate.0_0:                               {input: concatenate_58.dc.concatenate.0, type: queue, entries: 1, grid_size: [1, 8], t: 32, mblock: [65, 1], ublock: [1, 2], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x46c9dde0], [1, 0x6ca7160], [2, 0x6ca7160], [4, 0x46cbcd20], [0, 0x46d722a0], [5, 0x46e245a0], [1, 0x46e29740], [3, 0x46e29780]]}
  e2e_softmax_140.dc.multiply.7_0:                                     {input: softmax_140.dc.multiply.7, type: queue, entries: 1, grid_size: [1, 5], t: 32, mblock: [1, 13], ublock: [1, 1], ublock_order: c, df: Bfp8_b, target_device: 1, loc: dram, dram: [[5, 0x6e384a0], [5, 0x6eaa0c0], [5, 0x6f1bce0], [5, 0x6f8d900], [5, 0x6fff520]]}
  e2e_add_21_4:                                                        {input: add_21, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x40004140]]}
  e2e_index_150.dc.select.0_0:                                         {input: index_150.dc.select.0, type: queue, entries: 1, grid_size: [1, 1], t: 16, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x7071140]]}
  e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1_0:  {input: concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1, type: queue, entries: 1, grid_size: [5, 8], t: 8, mblock: [13, 2], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x40000840], [5, 0x40000840], [0, 0x40001080], [1, 0x40001080], [2, 0x40001080], [4, 0x40010420], [3, 0x401a7060], [5, 0x401a7060], [0, 0x401a78a0], [1, 0x401a78a0], [2, 0x401a78a0], [4, 0x401b6c40], [3, 0x4034d880], [5, 0x4034d880], [0, 0x4034e0c0], [1, 0x4034e0c0], [2, 0x4034e0c0], [4, 0x4035d460], [3, 0x404f40a0], [5, 0x404f40a0], [0, 0x404f48e0], [1, 0x404f48e0], [2, 0x404f48e0], [4, 0x40503c80], [3, 0x4069a8c0], [5, 0x4069a8c0], [0, 0x4069b100], [1, 0x4069b100], [2, 0x4069b100], [4, 0x406aa4a0], [3, 0x408410e0], [5, 0x408410e0], [0, 0x40841920], [1, 0x40841920], [2, 0x40841920], [4, 0x40850cc0], [3, 0x409e7900], [5, 0x409e7900], [0, 0x409e8140], [1, 0x409e8140]]}
  e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1_0:  {input: concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1, type: queue, entries: 1, grid_size: [5, 8], t: 8, mblock: [13, 2], ublock: [1, 4], ublock_order: c, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x4001d020], [5, 0x4001d7a0], [4, 0x40020840], [1, 0x40027200], [2, 0x4002cfc0], [0, 0x4002d3c0], [3, 0x401c3840], [5, 0x401c3fc0], [4, 0x401c7060], [1, 0x401cda20], [2, 0x401d37e0], [0, 0x401d3be0], [3, 0x4036a060], [5, 0x4036a7e0], [4, 0x4036d880], [1, 0x40374240], [2, 0x4037a000], [0, 0x4037a400], [3, 0x40510880], [5, 0x40511000], [4, 0x405140a0], [1, 0x4051aa60], [2, 0x40520820], [0, 0x40520c20], [3, 0x406b70a0], [5, 0x406b7820], [4, 0x406ba8c0], [1, 0x406c1280], [2, 0x406c7040], [0, 0x406c7440], [3, 0x4085d8c0], [5, 0x4085e040], [4, 0x408610e0], [1, 0x40867aa0], [2, 0x4086d860], [0, 0x4086dc60], [3, 0x40a040e0], [5, 0x40a04860], [4, 0x40a07900], [1, 0x40a0e2c0]]}
  e2e_add_21_5:                                                        {input: add_21, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [1, 2], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x40000000]]}
  e2e_matmul_64_0:                                                     {input: matmul_64, type: queue, entries: 1, grid_size: [1, 2], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x4000c780], [0, 0x4000cb80]]}
  e2e_matmul_85_0:                                                     {input: matmul_85, type: queue, entries: 1, grid_size: [1, 2], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x4000cc00], [5, 0x4000d380]]}
  e2e_matmul_106_0:                                                    {input: matmul_106, type: queue, entries: 1, grid_size: [1, 2], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x40010420], [1, 0x40016de0]]}
  e2e_matmul_127_0:                                                    {input: matmul_127, type: queue, entries: 1, grid_size: [1, 2], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x4001cba0], [0, 0x4001cfa0]]}
  e2e_matmul_148_0:                                                    {input: matmul_148, type: queue, entries: 1, grid_size: [1, 2], t: 32, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x40a14080], [0, 0x40a14480]]}
  e2e_matmul_8_0:                                                      {input: matmul_8, type: queue, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x40000000], [2, 0x40000000], [3, 0x40000000], [4, 0x40000000], [5, 0x40000000], [1, 0x40010420], [2, 0x40010420], [3, 0x40010420]]}
  e2e_reshape_37.dc.sparse_matmul.14.lc2_0:                            {input: reshape_37.dc.sparse_matmul.14.lc2, type: queue, entries: 1, grid_size: [1, 2], t: 8, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x412d0f00], [0, 0x413010e0]]}
  e2e_reshape_56.dc.sparse_matmul.14.lc2_0:                            {input: reshape_56.dc.sparse_matmul.14.lc2, type: queue, entries: 1, grid_size: [1, 2], t: 8, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41352f20], [0, 0x41383100]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 1
    input_count: 1
    layernorm_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [input_1, lc.input_tensor.layernorm_0.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 256}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 8}}
    layernorm_0.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [1, 2], inputs: [dc.input_tensor.layernorm_0.1, layernorm_0.dc.reduce_sum.0.lc1],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 256}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    layernorm_0.dc.subtract.3: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [input_1, layernorm_0.dc.multiply.2],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [hslice: 2]}
    layernorm_0.dc.multiply.4: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_0.dc.subtract.3, layernorm_0.dc.subtract.3],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_0.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_0.dc.multiply.4, lc.input_tensor.layernorm_0.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 256}], input_0_tms: [hstack: 2],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 8}}
    layernorm_0.dc.multiply.7: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_0.6, layernorm_0.dc.reduce_sum.5.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_0.dc.add.9: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_0.dc.multiply.7, dc.input_tensor.layernorm_0.8],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_0.dc.sqrt.10: {type: sqrt, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_0.dc.add.9],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_0.dc.reciprocal.11: {type: reciprocal, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_0.dc.sqrt.10],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_0.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_0.dc.reciprocal.11, lc.input_tensor.layernorm_0.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_0.dc.multiply.12: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_0.dc.subtract.3, layernorm_0.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 256}, hslice: 2],
         attributes: {kernel_broadcast: {input_1: 1}}}
    layernorm_0.dc.multiply.13: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_0.dc.multiply.12, layers.0.ln_mlp.weight],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, hslice: 2]}
    layernorm_0.dc.add.14: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_0.dc.multiply.13, layers.0.ln_mlp.bias],
         t: 1, mblock: [1, 64], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}], input_0_tms: [hstack: 2]}
    matmul_3: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layernorm_0.dc.add.14, layers.0.mlp.dense_h_to_4h.weight],
         t: 32, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32], input_0_tms: [broadcast: {r: 32}, vslice: 32],
         attributes: {m_k: 4, min_buffer_input: 0, u_kt: 64}}
    gelu_5: {type: gelu, grid_loc: [3, 0], grid_size: [1, 8], inputs: [matmul_3],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 32],
         attributes: {approximate_mode: false}}
    matmul_8: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [gelu_5, layers.0.mlp.dense_4h_to_h.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 128, min_buffer_input: 0, u_kt: 8}}

  fwd_0_1_temporal_epoch_0:
    target_device: 0
    input_count: 1
    layernorm_10.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [input_1, lc.input_tensor.layernorm_10.dc.reduce_sum.0.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 256}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 8}}
    layernorm_10.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [1, 2], inputs: [dc.input_tensor.layernorm_10.1, layernorm_10.dc.reduce_sum.0.lc1],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 256}],
         attributes: {kernel_broadcast: {input_1: 1}}}
    layernorm_10.dc.subtract.3: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [input_1, layernorm_10.dc.multiply.2],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 2], input_0_tms: [hslice: 2]}
    layernorm_10.dc.multiply.4: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_10.dc.subtract.3, layernorm_10.dc.subtract.3],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_10.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_10.dc.multiply.4, lc.input_tensor.layernorm_10.dc.reduce_sum.5.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 256}], input_0_tms: [hstack: 2],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 32, min_buffer_input: 0, u_kt: 8}}
    layernorm_10.dc.multiply.7: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_10.6, layernorm_10.dc.reduce_sum.5.lc1],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_10.dc.add.9: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_10.dc.multiply.7, dc.input_tensor.layernorm_10.8],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_10.dc.sqrt.10: {type: sqrt, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_10.dc.add.9],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_10.dc.reciprocal.11: {type: reciprocal, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_10.dc.sqrt.10],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_10.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_10.dc.reciprocal.11, lc.input_tensor.layernorm_10.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_10.dc.multiply.12: {type: multiply, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_10.dc.subtract.3, layernorm_10.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 256}, hslice: 2],
         attributes: {kernel_broadcast: {input_1: 1}}}
    layernorm_10.dc.multiply.13: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_10.dc.multiply.12, layers.0.ln_attn.weight],
         t: 2, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}, hslice: 2]}
    layernorm_10.dc.add.14: {type: add, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_10.dc.multiply.13, layers.0.ln_attn.bias],
         t: 1, mblock: [1, 64], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}], input_0_tms: [hstack: 2]}
    matmul_13: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layernorm_10.dc.add.14, layers.0.self_attention.wq.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 8}}
    multiply_15: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [matmul_13, cos],
         t: 128, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 128}], input_0_tms: [hslice: 128],
         attributes: {kernel_broadcast: {input_1: 2}}}
    index_16.dc.select.0: {type: splice, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_13],
         t: 128, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 128],
         attributes: {input0: [1, 1, 1]}}
    index_16.dc.buffer.1: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [index_16.dc.select.0],
         t: 128, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_17_splt_brcst_1_0: {type: nop, grid_loc: [3, 1], grid_size: [1, 1], inputs: [input_1_multiply_17],
         t: 128, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 128}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_17_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [input_1_multiply_17_splt_brcst_1_0],
         t: 128, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 1}]}
    multiply_17: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [index_16.dc.buffer.1, input_1_multiply_17_splt_brcst_1_0_splt_brcst_3_0],
         t: 128, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}]}
    index_18.dc.select.0: {type: splice, grid_loc: [3, 4], grid_size: [1, 1], inputs: [matmul_13],
         t: 128, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 128],
         attributes: {input0: [0, 1, 2]}}
    index_18.dc.buffer.1: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [index_18.dc.select.0],
         t: 128, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_19.dc.concatenate.0: {type: splice, grid_loc: [3, 6], grid_size: [1, 1], inputs: [multiply_17, index_18.dc.buffer.1],
         t: 128, mblock: [1, 2], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {input0: [0, 1, 1], input1: [0, 1, 1]}}
    multiply_20: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [concatenate_19.dc.concatenate.0, sin],
         t: 128, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 128}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_21: {type: add, grid_loc: [4, 0], grid_size: [1, 1], inputs: [multiply_15, multiply_20],
         t: 128, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    index_22.dc.select.0: {type: splice, grid_loc: [4, 1], grid_size: [1, 1], inputs: [add_21],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [0, 16, 128]}}
    transpose_23.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [5, 0], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_23.dc.sparse_matmul.4.0, index_22.dc.select.0, lc.input_tensor.transpose_23.dc.sparse_matmul.4.1], grid_transpose: true,
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}
    index_25.dc.select.0: {type: splice, grid_loc: [7, 0], grid_size: [1, 8], inputs: [past_key],
         t: 32, mblock: [64, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 4],
         attributes: {input0: [0, 64, 65]}}

  fwd_0_2_temporal_epoch_1:
    target_device: 0
    input_count: 1
    index_25.dc.buffer.1: {type: nop, grid_loc: [0, 0], grid_size: [8, 8], inputs: [e2e_index_25.dc.select.0_0],
         t: 8, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4]}
    index_26.dc.select.0: {type: splice, grid_loc: [8, 0], grid_size: [1, 2], inputs: [past_key],
         t: 128, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 16],
         attributes: {input0: [64, 1, 1]}}
    index_26.dc.buffer.1: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [index_26.dc.select.0],
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 16]}
    matmul_28: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [e2e_layernorm_10.dc.add.14_0, layers.0.self_attention.wk.weight],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 128}}
    multiply_30: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [matmul_28, cos],
         t: 8, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 8}], input_0_tms: [hslice: 8],
         attributes: {kernel_broadcast: {input_1: 2}}}
    index_31.dc.select.0: {type: splice, grid_loc: [8, 4], grid_size: [1, 1], inputs: [matmul_28],
         t: 8, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8],
         attributes: {input0: [1, 1, 1]}}
    index_31.dc.buffer.1: {type: nop, grid_loc: [8, 5], grid_size: [1, 1], inputs: [index_31.dc.select.0],
         t: 8, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    input_1_multiply_32_splt_brcst_1_0: {type: nop, grid_loc: [8, 6], grid_size: [1, 1], inputs: [input_1_multiply_32],
         t: 8, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 8}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    input_1_multiply_32_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [8, 7], grid_size: [1, 1], inputs: [input_1_multiply_32_splt_brcst_1_0],
         t: 8, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 1}]}

  fwd_0_3_temporal_epoch_1:
    target_device: 31
    input_count: 1
    multiply_32: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [index_31.dc.buffer.1, input_1_multiply_32_splt_brcst_1_0_splt_brcst_3_0],
         t: 8, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}]}
    index_33.dc.select.0: {type: splice, grid_loc: [0, 1], grid_size: [1, 1], inputs: [matmul_28],
         t: 8, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8],
         attributes: {input0: [0, 1, 2]}}
    index_33.dc.buffer.1: {type: nop, grid_loc: [0, 2], grid_size: [1, 1], inputs: [index_33.dc.select.0],
         t: 8, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    concatenate_34.dc.concatenate.0: {type: splice, grid_loc: [0, 3], grid_size: [1, 1], inputs: [multiply_32, index_33.dc.buffer.1],
         t: 8, mblock: [1, 2], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {input0: [0, 1, 1], input1: [0, 1, 1]}}
    multiply_35: {type: multiply, grid_loc: [0, 4], grid_size: [1, 1], inputs: [concatenate_34.dc.concatenate.0, sin],
         t: 8, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 8}],
         attributes: {kernel_broadcast: {input_1: 2}}}
    add_36: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [multiply_30, multiply_35],
         t: 8, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    reshape_37.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 6], grid_size: [4, 1], inputs: [lc.input_tensor.reshape_37.dc.sparse_matmul.4.0, add_36, lc.input_tensor.reshape_37.dc.sparse_matmul.4.1],
         t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 7, u_kt: 1}}
    reshape_37.dc.sparse_matmul.10.lc2: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.reshape_37.dc.sparse_matmul.10.0, reshape_37.dc.sparse_matmul.4.lc2, lc.input_tensor.reshape_37.dc.sparse_matmul.10.1],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 8, vslice: 32, hstack: 32, vstack: 8],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 8, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, u_kt: 1}}
    reshape_37.dc.sparse_matmul.14.lc2: {type: matmul, grid_loc: [1, 0], grid_size: [1, 2], inputs: [lc.input_tensor.reshape_37.dc.sparse_matmul.14.0, reshape_37.dc.sparse_matmul.10.lc2, lc.input_tensor.reshape_37.dc.sparse_matmul.14.1],
         t: 8, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, u_kt: 1}}
    reshape_37.dc.transpose.23_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 4], inputs: [lc.input_tensor.reshape_37.dc.transpose.23_s_brcst_m2_1_0.0, reshape_37.dc.sparse_matmul.14.lc2],
         t: 8, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 8}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_38: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [index_26.dc.buffer.1, reshape_37.dc.transpose.23_s_brcst_m2_1_0.lc1],
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}]}

  fwd_0_4_temporal_epoch_2:
    target_device: 1
    input_count: 1
    concatenate_39.dc.concatenate.0: {type: splice, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_index_25.dc.buffer.1_0, e2e_add_38_0],
         t: 32, mblock: [65, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 4], input_0_tms: [hslice: 4],
         attributes: {input0: [0, 64, 64], input1: [0, 1, 1]}}
    concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0: {type: nop, grid_loc: [1, 0], grid_size: [5, 8], inputs: [concatenate_39.dc.concatenate.0],
         t: 8, mblock: [13, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4]}
    index_40.dc.select.0: {type: splice, grid_loc: [6, 0], grid_size: [5, 4], inputs: [concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0], grid_transpose: true,
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [0, 1, 8]}}

  fwd_0_5_temporal_epoch_3:
    target_device: 1
    input_count: 1
    matmul_44: {type: matmul, grid_loc: [0, 0], grid_size: [1, 5], inputs: [e2e_transpose_23.dc.sparse_matmul.4.lc2_0, e2e_index_40.dc.select.0_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_46_splt_brcst_1_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [input_1_multiply_46],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_46: {type: multiply, grid_loc: [1, 0], grid_size: [1, 5], inputs: [matmul_44, input_1_multiply_46_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 5], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_0_0.0, attn_mask],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_47: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [multiply_46, attn_mask_s_brcst_m2_0_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_48.dc.reduce_max.0: {type: reduce, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_47],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_48.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_48.dc.reduce_max.0, lc.input_tensor.softmax_48.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_48.dc.subtract.1: {type: subtract, grid_loc: [3, 0], grid_size: [1, 5], inputs: [add_47, softmax_48.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_48.dc.exp.2: {type: exp, grid_loc: [4, 0], grid_size: [1, 5], inputs: [softmax_48.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_48.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_48.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_48.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_48.dc.exp.2, lc.input_tensor.softmax_48.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_48.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [softmax_48.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_48.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_48.dc.reciprocal.6: {type: reciprocal, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_48.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_48.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_48.dc.reciprocal.6, lc.input_tensor.softmax_48.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_48.dc.multiply.7: {type: multiply, grid_loc: [5, 0], grid_size: [1, 5], inputs: [softmax_48.dc.exp.2, softmax_48.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    index_51.dc.select.0: {type: splice, grid_loc: [6, 0], grid_size: [1, 8], inputs: [past_value],
         t: 32, mblock: [64, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 4],
         attributes: {input0: [0, 64, 65]}}

  fwd_0_6_temporal_epoch_3:
    target_device: 2
    input_count: 1
    index_51.dc.buffer.1: {type: nop, grid_loc: [0, 0], grid_size: [8, 8], inputs: [index_51.dc.select.0],
         t: 8, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4]}
    index_52.dc.select.0: {type: splice, grid_loc: [8, 0], grid_size: [1, 2], inputs: [past_value],
         t: 128, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 16],
         attributes: {input0: [64, 1, 1]}}
    index_52.dc.buffer.1: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [index_52.dc.select.0],
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 16]}
    matmul_54: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [e2e_layernorm_10.dc.add.14_1, layers.0.self_attention.wv.weight],
         t: 1, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 2, min_buffer_input: 0, u_kt: 128}}
    reshape_56.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [8, 3], grid_size: [4, 1], inputs: [lc.input_tensor.reshape_56.dc.sparse_matmul.4.0, matmul_54, lc.input_tensor.reshape_56.dc.sparse_matmul.4.1], grid_transpose: true,
         t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 7, u_kt: 1}}
    reshape_56.dc.sparse_matmul.10.lc2: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.reshape_56.dc.sparse_matmul.10.0, reshape_56.dc.sparse_matmul.4.lc2, lc.input_tensor.reshape_56.dc.sparse_matmul.10.1],
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 8, vslice: 32, hstack: 32, vstack: 8],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 8, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, u_kt: 1}}

  fwd_0_7_temporal_epoch_3:
    target_device: 3
    input_count: 1
    reshape_56.dc.sparse_matmul.14.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [lc.input_tensor.reshape_56.dc.sparse_matmul.14.0, reshape_56.dc.sparse_matmul.10.lc2, lc.input_tensor.reshape_56.dc.sparse_matmul.14.1],
         t: 8, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 9, sparse_tile_ptr_bits: 5, u_kt: 1}}
    reshape_56.dc.transpose.23_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 4], inputs: [lc.input_tensor.reshape_56.dc.transpose.23_s_brcst_m2_1_0.0, reshape_56.dc.sparse_matmul.14.lc2],
         t: 8, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 8}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_57: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [index_52.dc.buffer.1, reshape_56.dc.transpose.23_s_brcst_m2_1_0.lc1],
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 1}]}
    concatenate_58.dc.concatenate.0: {type: splice, grid_loc: [1, 0], grid_size: [1, 8], inputs: [index_51.dc.buffer.1, add_57],
         t: 32, mblock: [65, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 4], input_0_tms: [hslice: 4],
         attributes: {input0: [0, 64, 64], input1: [0, 1, 1]}}
    concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0: {type: nop, grid_loc: [2, 0], grid_size: [5, 8], inputs: [concatenate_58.dc.concatenate.0],
         t: 8, mblock: [13, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4]}

  fwd_0_8_temporal_epoch_3:
    target_device: 4
    input_count: 1
    index_59.dc.select.0: {type: splice, grid_loc: [0, 0], grid_size: [5, 4], inputs: [concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [0, 1, 8]}}
    matmul_64: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [softmax_48.dc.multiply.7, index_59.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    index_66.dc.select.0: {type: splice, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e_add_21_0],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [16, 16, 112]}}
    transpose_67.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_67.dc.sparse_matmul.4.0, index_66.dc.select.0, lc.input_tensor.transpose_67.dc.sparse_matmul.4.1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}
    index_69.dc.select.0: {type: splice, grid_loc: [5, 0], grid_size: [5, 4], inputs: [e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [1, 1, 7]}}

  fwd_0_9_temporal_epoch_3:
    target_device: 5
    input_count: 1
    matmul_73: {type: matmul, grid_loc: [0, 0], grid_size: [1, 5], inputs: [transpose_67.dc.sparse_matmul.4.lc2, index_69.dc.select.0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_75_splt_brcst_1_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [input_1_multiply_75],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_75: {type: multiply, grid_loc: [1, 0], grid_size: [1, 5], inputs: [matmul_73, input_1_multiply_75_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 5], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_1_0.0, attn_mask],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_76: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [multiply_75, attn_mask_s_brcst_m2_1_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_77.dc.reduce_max.0: {type: reduce, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_76],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_77.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_77.dc.reduce_max.0, lc.input_tensor.softmax_77.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_77.dc.subtract.1: {type: subtract, grid_loc: [3, 0], grid_size: [1, 5], inputs: [add_76, softmax_77.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_77.dc.exp.2: {type: exp, grid_loc: [4, 0], grid_size: [1, 5], inputs: [softmax_77.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_77.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_77.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_77.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_77.dc.exp.2, lc.input_tensor.softmax_77.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_77.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [softmax_77.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_77.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_77.dc.reciprocal.6: {type: reciprocal, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_77.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_77.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_77.dc.reciprocal.6, lc.input_tensor.softmax_77.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_77.dc.multiply.7: {type: multiply, grid_loc: [5, 0], grid_size: [1, 5], inputs: [softmax_77.dc.exp.2, softmax_77.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    index_80.dc.select.0: {type: splice, grid_loc: [6, 0], grid_size: [5, 4], inputs: [concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0], grid_transpose: true,
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [1, 1, 7]}}
    matmul_85: {type: matmul, grid_loc: [3, 5], grid_size: [1, 2], inputs: [softmax_77.dc.multiply.7, index_80.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    index_87.dc.select.0: {type: splice, grid_loc: [3, 7], grid_size: [1, 1], inputs: [e2e_add_21_0],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [32, 16, 96]}}

  fwd_0_10_temporal_epoch_3:
    target_device: 6
    input_count: 1
    transpose_88.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_88.dc.sparse_matmul.4.0, index_87.dc.select.0, lc.input_tensor.transpose_88.dc.sparse_matmul.4.1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}
    index_90.dc.select.0: {type: splice, grid_loc: [0, 2], grid_size: [5, 4], inputs: [e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [2, 1, 6]}}
    matmul_94: {type: matmul, grid_loc: [5, 2], grid_size: [1, 5], inputs: [transpose_88.dc.sparse_matmul.4.lc2, index_90.dc.select.0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_96_splt_brcst_1_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [input_1_multiply_96],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_96: {type: multiply, grid_loc: [6, 2], grid_size: [1, 5], inputs: [matmul_94, input_1_multiply_96_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_2_0.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 5], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_2_0.0, attn_mask],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_97: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [multiply_96, attn_mask_s_brcst_m2_2_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_98.dc.reduce_max.0: {type: reduce, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_97],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_98.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_98.dc.reduce_max.0, lc.input_tensor.softmax_98.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_98.dc.subtract.1: {type: subtract, grid_loc: [8, 0], grid_size: [1, 5], inputs: [add_97, softmax_98.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_98.dc.exp.2: {type: exp, grid_loc: [9, 0], grid_size: [1, 5], inputs: [softmax_98.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_98.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_98.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_98.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_98.dc.exp.2, lc.input_tensor.softmax_98.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_98.dc.add.5: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [softmax_98.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_98.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_98.dc.reciprocal.6: {type: reciprocal, grid_loc: [3, 7], grid_size: [1, 1], inputs: [softmax_98.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_98.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [softmax_98.dc.reciprocal.6, lc.input_tensor.softmax_98.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}

  fwd_0_11_temporal_epoch_3:
    target_device: 7
    input_count: 1
    softmax_98.dc.multiply.7: {type: multiply, grid_loc: [0, 0], grid_size: [1, 5], inputs: [softmax_98.dc.exp.2, softmax_98.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    index_101.dc.select.0: {type: splice, grid_loc: [1, 0], grid_size: [5, 4], inputs: [concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [2, 1, 6]}}
    matmul_106: {type: matmul, grid_loc: [0, 5], grid_size: [1, 2], inputs: [softmax_98.dc.multiply.7, index_101.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    index_108.dc.select.0: {type: splice, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e_add_21_0],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [48, 16, 80]}}
    transpose_109.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_109.dc.sparse_matmul.4.0, index_108.dc.select.0, lc.input_tensor.transpose_109.dc.sparse_matmul.4.1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}

  fwd_0_12_temporal_epoch_3:
    target_device: 8
    input_count: 1
    index_111.dc.select.0: {type: splice, grid_loc: [0, 0], grid_size: [5, 4], inputs: [e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [3, 1, 5]}}
    matmul_115: {type: matmul, grid_loc: [5, 0], grid_size: [1, 5], inputs: [transpose_109.dc.sparse_matmul.4.lc2, index_111.dc.select.0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_117_splt_brcst_1_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [input_1_multiply_117],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_117: {type: multiply, grid_loc: [6, 0], grid_size: [1, 5], inputs: [matmul_115, input_1_multiply_117_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_3_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 5], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_3_0.0, attn_mask],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_118: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [multiply_117, attn_mask_s_brcst_m2_3_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}

  fwd_0_13_temporal_epoch_4:
    target_device: 1
    input_count: 1
    softmax_119.dc.reduce_max.0: {type: reduce, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_add_118_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_119.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [softmax_119.dc.reduce_max.0, lc.input_tensor.softmax_119.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_119.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [1, 5], inputs: [e2e_add_118_0, softmax_119.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_119.dc.exp.2: {type: exp, grid_loc: [1, 0], grid_size: [1, 5], inputs: [softmax_119.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_119.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_119.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_119.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_119.dc.exp.2, lc.input_tensor.softmax_119.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_119.dc.add.5: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [softmax_119.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_119.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_119.dc.reciprocal.6: {type: reciprocal, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_119.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_119.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [softmax_119.dc.reciprocal.6, lc.input_tensor.softmax_119.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_119.dc.multiply.7: {type: multiply, grid_loc: [2, 1], grid_size: [1, 5], inputs: [softmax_119.dc.exp.2, softmax_119.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    index_122.dc.select.0: {type: splice, grid_loc: [3, 0], grid_size: [5, 4], inputs: [e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [3, 1, 5]}}
    matmul_127: {type: matmul, grid_loc: [2, 6], grid_size: [1, 2], inputs: [softmax_119.dc.multiply.7, index_122.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    index_129.dc.select.0: {type: splice, grid_loc: [3, 4], grid_size: [1, 1], inputs: [e2e_add_21_3],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [64, 16, 64]}}
    transpose_130.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [8, 0], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_130.dc.sparse_matmul.4.0, index_129.dc.select.0, lc.input_tensor.transpose_130.dc.sparse_matmul.4.1], grid_transpose: true,
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}

  fwd_0_14_temporal_epoch_5:
    target_device: 1
    input_count: 1
    concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1: {type: nop, grid_loc: [0, 0], grid_size: [5, 8], inputs: [e2e_concatenate_39.dc.concatenate.0_0],
         t: 8, mblock: [13, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4]}
    index_132.dc.select.0: {type: splice, grid_loc: [5, 0], grid_size: [5, 4], inputs: [concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [4, 1, 4]}}

  fwd_0_15_temporal_epoch_5:
    target_device: 2
    input_count: 1
    matmul_136: {type: matmul, grid_loc: [0, 0], grid_size: [1, 5], inputs: [e2e_transpose_130.dc.sparse_matmul.4.lc2_0, index_132.dc.select.0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_138_splt_brcst_1_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [input_1_multiply_138],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_138: {type: multiply, grid_loc: [1, 0], grid_size: [1, 5], inputs: [matmul_136, input_1_multiply_138_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_4_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 5], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_4_0.0, attn_mask],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_139: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [multiply_138, attn_mask_s_brcst_m2_4_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_140.dc.reduce_max.0: {type: reduce, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_139],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_140.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_140.dc.reduce_max.0, lc.input_tensor.softmax_140.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_140.dc.subtract.1: {type: subtract, grid_loc: [3, 0], grid_size: [1, 5], inputs: [add_139, softmax_140.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_140.dc.exp.2: {type: exp, grid_loc: [4, 0], grid_size: [1, 5], inputs: [softmax_140.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_140.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_140.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_140.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_140.dc.exp.2, lc.input_tensor.softmax_140.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_140.dc.add.5: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [softmax_140.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_140.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_140.dc.reciprocal.6: {type: reciprocal, grid_loc: [2, 6], grid_size: [1, 1], inputs: [softmax_140.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_140.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_140.dc.reciprocal.6, lc.input_tensor.softmax_140.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_140.dc.multiply.7: {type: multiply, grid_loc: [5, 0], grid_size: [1, 5], inputs: [softmax_140.dc.exp.2, softmax_140.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}

  fwd_0_16_temporal_epoch_6:
    target_device: 1
    input_count: 1
    concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1: {type: nop, grid_loc: [0, 0], grid_size: [5, 8], inputs: [e2e_concatenate_58.dc.concatenate.0_0],
         t: 8, mblock: [13, 2], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4]}
    index_143.dc.select.0: {type: splice, grid_loc: [5, 0], grid_size: [5, 4], inputs: [concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [4, 1, 4]}}
    matmul_148: {type: matmul, grid_loc: [5, 4], grid_size: [1, 2], inputs: [e2e_softmax_140.dc.multiply.7_0, index_143.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    index_150.dc.select.0: {type: splice, grid_loc: [5, 6], grid_size: [1, 1], inputs: [e2e_add_21_4],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [80, 16, 48]}}

  fwd_0_17_temporal_epoch_7:
    target_device: 1
    input_count: 1
    transpose_151.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_151.dc.sparse_matmul.4.0, e2e_index_150.dc.select.0_0, lc.input_tensor.transpose_151.dc.sparse_matmul.4.1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}
    index_153.dc.select.0: {type: splice, grid_loc: [0, 2], grid_size: [5, 4], inputs: [e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [5, 1, 3]}}
    matmul_157: {type: matmul, grid_loc: [5, 2], grid_size: [1, 5], inputs: [transpose_151.dc.sparse_matmul.4.lc2, index_153.dc.select.0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_159_splt_brcst_1_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [input_1_multiply_159],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_159: {type: multiply, grid_loc: [6, 2], grid_size: [1, 5], inputs: [matmul_157, input_1_multiply_159_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_5_0.lc1: {type: matmul, grid_loc: [7, 2], grid_size: [1, 5], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_5_0.0, attn_mask],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_160: {type: add, grid_loc: [0, 7], grid_size: [1, 1], inputs: [multiply_159, attn_mask_s_brcst_m2_5_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_161.dc.reduce_max.0: {type: reduce, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_160],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_161.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_161.dc.reduce_max.0, lc.input_tensor.softmax_161.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_161.dc.subtract.1: {type: subtract, grid_loc: [8, 0], grid_size: [1, 5], inputs: [add_160, softmax_161.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_161.dc.exp.2: {type: exp, grid_loc: [9, 0], grid_size: [1, 5], inputs: [softmax_161.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_161.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_161.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_161.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [softmax_161.dc.exp.2, lc.input_tensor.softmax_161.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_161.dc.add.5: {type: add, grid_loc: [3, 6], grid_size: [1, 1], inputs: [softmax_161.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_161.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_161.dc.reciprocal.6: {type: reciprocal, grid_loc: [3, 7], grid_size: [1, 1], inputs: [softmax_161.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_161.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [softmax_161.dc.reciprocal.6, lc.input_tensor.softmax_161.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}

  fwd_0_18_temporal_epoch_7:
    target_device: 2
    input_count: 1
    softmax_161.dc.multiply.7: {type: multiply, grid_loc: [0, 0], grid_size: [1, 5], inputs: [softmax_161.dc.exp.2, softmax_161.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    index_164.dc.select.0: {type: splice, grid_loc: [1, 0], grid_size: [5, 4], inputs: [e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [5, 1, 3]}}
    matmul_169: {type: matmul, grid_loc: [0, 5], grid_size: [1, 2], inputs: [softmax_161.dc.multiply.7, index_164.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    index_171.dc.select.0: {type: splice, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e_add_21_5],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [96, 16, 32]}}
    transpose_172.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_172.dc.sparse_matmul.4.0, index_171.dc.select.0, lc.input_tensor.transpose_172.dc.sparse_matmul.4.1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}

  fwd_0_19_temporal_epoch_7:
    target_device: 3
    input_count: 1
    index_174.dc.select.0: {type: splice, grid_loc: [0, 0], grid_size: [5, 4], inputs: [e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [6, 1, 2]}}
    matmul_178: {type: matmul, grid_loc: [5, 0], grid_size: [1, 5], inputs: [transpose_172.dc.sparse_matmul.4.lc2, index_174.dc.select.0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_180_splt_brcst_1_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [input_1_multiply_180],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_180: {type: multiply, grid_loc: [6, 0], grid_size: [1, 5], inputs: [matmul_178, input_1_multiply_180_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_6_0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 5], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_6_0.0, attn_mask],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_181: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [multiply_180, attn_mask_s_brcst_m2_6_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_182.dc.reduce_max.0: {type: reduce, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_181],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_182.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [softmax_182.dc.reduce_max.0, lc.input_tensor.softmax_182.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_182.dc.subtract.1: {type: subtract, grid_loc: [8, 0], grid_size: [1, 5], inputs: [add_181, softmax_182.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_182.dc.exp.2: {type: exp, grid_loc: [9, 0], grid_size: [1, 5], inputs: [softmax_182.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_182.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [1, 4], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_182.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_182.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_182.dc.exp.2, lc.input_tensor.softmax_182.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_182.dc.add.5: {type: add, grid_loc: [1, 6], grid_size: [1, 1], inputs: [softmax_182.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_182.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_182.dc.reciprocal.6: {type: reciprocal, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_182.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_182.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [softmax_182.dc.reciprocal.6, lc.input_tensor.softmax_182.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}

  fwd_0_20_temporal_epoch_7:
    target_device: 4
    input_count: 1
    softmax_182.dc.multiply.7: {type: multiply, grid_loc: [0, 0], grid_size: [1, 5], inputs: [softmax_182.dc.exp.2, softmax_182.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    index_185.dc.select.0: {type: splice, grid_loc: [1, 0], grid_size: [5, 4], inputs: [e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [6, 1, 2]}}
    matmul_190: {type: matmul, grid_loc: [0, 5], grid_size: [1, 2], inputs: [softmax_182.dc.multiply.7, index_185.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    index_192.dc.select.0: {type: splice, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e_add_21_5],
         t: 16, mblock: [1, 1], ublock: [1, 2], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [112, 16, 16]}}
    transpose_193.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [1, 4], grid_size: [8, 2], inputs: [lc.input_tensor.transpose_193.dc.sparse_matmul.4.0, index_192.dc.select.0, lc.input_tensor.transpose_193.dc.sparse_matmul.4.1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_1_tms: [vstack: 16], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 16, num_index_tiles: 1, num_sparse_tiles: 65, sparse_tile_ptr_bits: 11, u_kt: 1}}

  fwd_0_21_temporal_epoch_7:
    target_device: 5
    input_count: 1
    index_195.dc.select.0: {type: splice, grid_loc: [0, 0], grid_size: [5, 4], inputs: [e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [7, 1, 1]}}
    matmul_199: {type: matmul, grid_loc: [5, 0], grid_size: [1, 5], inputs: [transpose_193.dc.sparse_matmul.4.lc2, index_195.dc.select.0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 32], input_0_tms: [vslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    input_1_multiply_201_splt_brcst_1_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [input_1_multiply_201],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    multiply_201: {type: multiply, grid_loc: [6, 0], grid_size: [1, 5], inputs: [matmul_199, input_1_multiply_201_splt_brcst_1_0],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    attn_mask_s_brcst_m2_7_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attn_mask_s_brcst_m2_7_0.0, attn_mask],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    add_202: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [multiply_201, attn_mask_s_brcst_m2_7_0.lc1],
         t: 32, mblock: [1, 65], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_203.dc.reduce_max.0: {type: reduce, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_202],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 65}}
    softmax_203.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_203.dc.reduce_max.0, lc.input_tensor.softmax_203.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_203.dc.subtract.1: {type: subtract, grid_loc: [7, 0], grid_size: [1, 5], inputs: [add_202, softmax_203.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    softmax_203.dc.exp.2: {type: exp, grid_loc: [8, 0], grid_size: [1, 5], inputs: [softmax_203.dc.subtract.1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}

  fwd_0_22_temporal_epoch_7:
    target_device: 6
    input_count: 1
    lc.input_tensor.softmax_203.dc.reduce_sum.3.0_splt_brcst_1_0: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_203.dc.reduce_sum.3.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_0: 1}}}
    softmax_203.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [softmax_203.dc.exp.2, lc.input_tensor.softmax_203.dc.reduce_sum.3.0_splt_brcst_1_0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 65}],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    softmax_203.dc.add.5: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [softmax_203.dc.reduce_sum.3.lc1, dc.input_tensor.softmax_203.4],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    softmax_203.dc.reciprocal.6: {type: reciprocal, grid_loc: [0, 3], grid_size: [1, 1], inputs: [softmax_203.dc.add.5],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_203.dc.reciprocal.6_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [softmax_203.dc.reciprocal.6, lc.input_tensor.softmax_203.dc.reciprocal.6_s_brcst_m1_0_0.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 1, min_buffer_input: 0, u_kt: 1}}
    softmax_203.dc.multiply.7: {type: multiply, grid_loc: [1, 0], grid_size: [1, 5], inputs: [softmax_203.dc.exp.2, softmax_203.dc.reciprocal.6_s_brcst_m1_0_0.lc1],
         t: 32, mblock: [1, 13], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 65}]}
    index_206.dc.select.0: {type: splice, grid_loc: [2, 0], grid_size: [5, 4], inputs: [e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1_0],
         t: 1, mblock: [13, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {granularity: t, input0: [7, 1, 1]}}
    matmul_211: {type: matmul, grid_loc: [0, 5], grid_size: [1, 2], inputs: [softmax_203.dc.multiply.7, index_206.dc.select.0],
         t: 32, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 32],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 65}}
    concatenate_213.dc.concatenate.8: {type: splice, grid_loc: [1, 5], grid_size: [1, 2], inputs: [e2e_matmul_64_0, e2e_matmul_85_0, e2e_matmul_106_0, e2e_matmul_127_0, e2e_matmul_148_0, matmul_169, matmul_190, matmul_211],
         t: 32, mblock: [8, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {input0: [0, 1, 1], input1: [0, 1, 1], input2: [0, 1, 1], input3: [0, 1, 1], input4: [0, 1, 1], input5: [0, 1, 1], input6: [0, 1, 1], input7: [0, 1, 1]}}
    concatenate_213.dc.sparse_matmul.10.lc2: {type: matmul, grid_loc: [2, 4], grid_size: [1, 2], inputs: [lc.input_tensor.concatenate_213.dc.sparse_matmul.10.0, concatenate_213.dc.concatenate.8, lc.input_tensor.concatenate_213.dc.sparse_matmul.10.1],
         t: 32, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 64, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 2}], input_0_tms: [broadcast: {c: 2}],
         attributes: {act_t: 32, fracture_factor: 1, identity: true, m_k: 2, num_index_tiles: 1, num_sparse_tiles: 3, sparse_tile_ptr_bits: 3, u_kt: 4}}

  fwd_0_23_temporal_epoch_7:
    target_device: 7
    input_count: 1
    reshape_214.dc.sparse_matmul.4.lc2: {type: matmul, grid_loc: [0, 0], grid_size: [8, 8], inputs: [lc.input_tensor.reshape_214.dc.sparse_matmul.4.0, concatenate_213.dc.sparse_matmul.10.lc2, lc.input_tensor.reshape_214.dc.sparse_matmul.4.1],
         t: 1, mblock: [8, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 8}], input_1_tms: [hstack: 32], input_0_tms: [broadcast: {c: 8}],
         attributes: {act_t: 1, fracture_factor: 1, identity: true, m_k: 1, num_index_tiles: 1, num_sparse_tiles: 17, sparse_tile_ptr_bits: 7, u_kt: 4}}
    reshape_214.dc.sparse_matmul.10.lc2: {type: matmul, grid_loc: [8, 0], grid_size: [1, 4], inputs: [lc.input_tensor.reshape_214.dc.sparse_matmul.10.0, reshape_214.dc.sparse_matmul.4.lc2, lc.input_tensor.reshape_214.dc.sparse_matmul.10.1],
         t: 4, mblock: [1, 4], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Bfp2_b, Float16_b, RawUInt32], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {c: 4}], input_1_tms: [hslice: 32, vslice: 128, hstack: 128, vstack: 32, hslice: 4], input_0_tms: [broadcast: {c: 4}],
         attributes: {act_t: 4, fracture_factor: 1, identity: true, m_k: 8, num_index_tiles: 1, num_sparse_tiles: 33, sparse_tile_ptr_bits: 7, u_kt: 4}}
    matmul_216: {type: matmul, grid_loc: [9, 0], grid_size: [1, 8], inputs: [reshape_214.dc.sparse_matmul.10.lc2, layers.0.self_attention.dense.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Bfp8_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 4],
         attributes: {m_k: 32, min_buffer_input: 0, u_kt: 8}}
    add_218: {type: add, grid_loc: [8, 4], grid_size: [1, 2], inputs: [e2e_matmul_8_0, matmul_216],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    add_220: {type: add, grid_loc: [8, 6], grid_size: [1, 2], inputs: [input_1, add_218],
         t: 1, mblock: [1, 32], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}

  fwd_0_24_temporal_epoch_7:
    target_device: 0
    input_count: 1
    add_220_output_nop_0: {type: nop, grid_loc: [0, 2], grid_size: [1, 4], inputs: [add_220], untilize_output: true,
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_37_tm_nop: {type: nop, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_reshape_37.dc.sparse_matmul.14.lc2_0],
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_37_tm_nop_output_nop_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_37_tm_nop], untilize_output: true,
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_56_tm_nop: {type: nop, grid_loc: [0, 1], grid_size: [1, 1], inputs: [e2e_reshape_56.dc.sparse_matmul.14.lc2_0],
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_56_tm_nop_output_nop_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [falcon_32c_0mf_0af_1l_2048s_buf_error.output_reshape_56_tm_nop], untilize_output: true,
         t: 8, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_25_temporal_epoch_0:
    target_device: 2
    input_count: 1

  fwd_0_26_temporal_epoch_0:
    target_device: 3
    input_count: 1

  fwd_0_27_temporal_epoch_0:
    target_device: 4
    input_count: 1

  fwd_0_28_temporal_epoch_0:
    target_device: 5
    input_count: 1

  fwd_0_29_temporal_epoch_0:
    target_device: 6
    input_count: 1

  fwd_0_30_temporal_epoch_0:
    target_device: 7
    input_count: 1

  fwd_0_31_temporal_epoch_0:
    target_device: 8
    input_count: 1

  fwd_0_32_temporal_epoch_0:
    target_device: 9
    input_count: 1

  fwd_0_33_temporal_epoch_0:
    target_device: 10
    input_count: 1

  fwd_0_34_temporal_epoch_0:
    target_device: 11
    input_count: 1

  fwd_0_35_temporal_epoch_0:
    target_device: 12
    input_count: 1

  fwd_0_36_temporal_epoch_0:
    target_device: 13
    input_count: 1

  fwd_0_37_temporal_epoch_0:
    target_device: 14
    input_count: 1

  fwd_0_38_temporal_epoch_0:
    target_device: 15
    input_count: 1

  fwd_0_39_temporal_epoch_0:
    target_device: 16
    input_count: 1

  fwd_0_40_temporal_epoch_0:
    target_device: 17
    input_count: 1

  fwd_0_41_temporal_epoch_0:
    target_device: 18
    input_count: 1

  fwd_0_42_temporal_epoch_0:
    target_device: 19
    input_count: 1

  fwd_0_43_temporal_epoch_0:
    target_device: 20
    input_count: 1

  fwd_0_44_temporal_epoch_0:
    target_device: 21
    input_count: 1

  fwd_0_45_temporal_epoch_0:
    target_device: 22
    input_count: 1

  fwd_0_46_temporal_epoch_0:
    target_device: 23
    input_count: 1

  fwd_0_47_temporal_epoch_0:
    target_device: 24
    input_count: 1

  fwd_0_48_temporal_epoch_0:
    target_device: 25
    input_count: 1

  fwd_0_49_temporal_epoch_0:
    target_device: 26
    input_count: 1

  fwd_0_50_temporal_epoch_0:
    target_device: 27
    input_count: 1

  fwd_0_51_temporal_epoch_0:
    target_device: 28
    input_count: 1

  fwd_0_52_temporal_epoch_0:
    target_device: 29
    input_count: 1

  fwd_0_53_temporal_epoch_0:
    target_device: 30
    input_count: 1

  fwd_0_54_temporal_epoch_0:
    target_device: 31
    input_count: 1

  fwd_0_55_temporal_epoch_1:
    target_device: 1
    input_count: 1

  fwd_0_56_temporal_epoch_1:
    target_device: 2
    input_count: 1

  fwd_0_57_temporal_epoch_1:
    target_device: 3
    input_count: 1

  fwd_0_58_temporal_epoch_1:
    target_device: 4
    input_count: 1

  fwd_0_59_temporal_epoch_1:
    target_device: 5
    input_count: 1

  fwd_0_60_temporal_epoch_1:
    target_device: 6
    input_count: 1

  fwd_0_61_temporal_epoch_1:
    target_device: 7
    input_count: 1

  fwd_0_62_temporal_epoch_1:
    target_device: 8
    input_count: 1

  fwd_0_63_temporal_epoch_1:
    target_device: 9
    input_count: 1

  fwd_0_64_temporal_epoch_1:
    target_device: 10
    input_count: 1

  fwd_0_65_temporal_epoch_1:
    target_device: 11
    input_count: 1

  fwd_0_66_temporal_epoch_1:
    target_device: 12
    input_count: 1

  fwd_0_67_temporal_epoch_1:
    target_device: 13
    input_count: 1

  fwd_0_68_temporal_epoch_1:
    target_device: 14
    input_count: 1

  fwd_0_69_temporal_epoch_1:
    target_device: 15
    input_count: 1

  fwd_0_70_temporal_epoch_1:
    target_device: 16
    input_count: 1

  fwd_0_71_temporal_epoch_1:
    target_device: 17
    input_count: 1

  fwd_0_72_temporal_epoch_1:
    target_device: 18
    input_count: 1

  fwd_0_73_temporal_epoch_1:
    target_device: 19
    input_count: 1

  fwd_0_74_temporal_epoch_1:
    target_device: 20
    input_count: 1

  fwd_0_75_temporal_epoch_1:
    target_device: 21
    input_count: 1

  fwd_0_76_temporal_epoch_1:
    target_device: 22
    input_count: 1

  fwd_0_77_temporal_epoch_1:
    target_device: 23
    input_count: 1

  fwd_0_78_temporal_epoch_1:
    target_device: 24
    input_count: 1

  fwd_0_79_temporal_epoch_1:
    target_device: 25
    input_count: 1

  fwd_0_80_temporal_epoch_1:
    target_device: 26
    input_count: 1

  fwd_0_81_temporal_epoch_1:
    target_device: 27
    input_count: 1

  fwd_0_82_temporal_epoch_1:
    target_device: 28
    input_count: 1

  fwd_0_83_temporal_epoch_1:
    target_device: 29
    input_count: 1

  fwd_0_84_temporal_epoch_1:
    target_device: 30
    input_count: 1

  fwd_0_85_temporal_epoch_2:
    target_device: 0
    input_count: 1

  fwd_0_86_temporal_epoch_2:
    target_device: 2
    input_count: 1

  fwd_0_87_temporal_epoch_2:
    target_device: 3
    input_count: 1

  fwd_0_88_temporal_epoch_2:
    target_device: 4
    input_count: 1

  fwd_0_89_temporal_epoch_2:
    target_device: 5
    input_count: 1

  fwd_0_90_temporal_epoch_2:
    target_device: 6
    input_count: 1

  fwd_0_91_temporal_epoch_2:
    target_device: 7
    input_count: 1

  fwd_0_92_temporal_epoch_2:
    target_device: 8
    input_count: 1

  fwd_0_93_temporal_epoch_2:
    target_device: 9
    input_count: 1

  fwd_0_94_temporal_epoch_2:
    target_device: 10
    input_count: 1

  fwd_0_95_temporal_epoch_2:
    target_device: 11
    input_count: 1

  fwd_0_96_temporal_epoch_2:
    target_device: 12
    input_count: 1

  fwd_0_97_temporal_epoch_2:
    target_device: 13
    input_count: 1

  fwd_0_98_temporal_epoch_2:
    target_device: 14
    input_count: 1

  fwd_0_99_temporal_epoch_2:
    target_device: 15
    input_count: 1

  fwd_0_100_temporal_epoch_2:
    target_device: 16
    input_count: 1

  fwd_0_101_temporal_epoch_2:
    target_device: 17
    input_count: 1

  fwd_0_102_temporal_epoch_2:
    target_device: 18
    input_count: 1

  fwd_0_103_temporal_epoch_2:
    target_device: 19
    input_count: 1

  fwd_0_104_temporal_epoch_2:
    target_device: 20
    input_count: 1

  fwd_0_105_temporal_epoch_2:
    target_device: 21
    input_count: 1

  fwd_0_106_temporal_epoch_2:
    target_device: 22
    input_count: 1

  fwd_0_107_temporal_epoch_2:
    target_device: 23
    input_count: 1

  fwd_0_108_temporal_epoch_2:
    target_device: 24
    input_count: 1

  fwd_0_109_temporal_epoch_2:
    target_device: 25
    input_count: 1

  fwd_0_110_temporal_epoch_2:
    target_device: 26
    input_count: 1

  fwd_0_111_temporal_epoch_2:
    target_device: 27
    input_count: 1

  fwd_0_112_temporal_epoch_2:
    target_device: 28
    input_count: 1

  fwd_0_113_temporal_epoch_2:
    target_device: 29
    input_count: 1

  fwd_0_114_temporal_epoch_2:
    target_device: 30
    input_count: 1

  fwd_0_115_temporal_epoch_2:
    target_device: 31
    input_count: 1

  fwd_0_116_temporal_epoch_3:
    target_device: 0
    input_count: 1

  fwd_0_117_temporal_epoch_3:
    target_device: 9
    input_count: 1

  fwd_0_118_temporal_epoch_3:
    target_device: 10
    input_count: 1

  fwd_0_119_temporal_epoch_3:
    target_device: 11
    input_count: 1

  fwd_0_120_temporal_epoch_3:
    target_device: 12
    input_count: 1

  fwd_0_121_temporal_epoch_3:
    target_device: 13
    input_count: 1

  fwd_0_122_temporal_epoch_3:
    target_device: 14
    input_count: 1

  fwd_0_123_temporal_epoch_3:
    target_device: 15
    input_count: 1

  fwd_0_124_temporal_epoch_3:
    target_device: 16
    input_count: 1

  fwd_0_125_temporal_epoch_3:
    target_device: 17
    input_count: 1

  fwd_0_126_temporal_epoch_3:
    target_device: 18
    input_count: 1

  fwd_0_127_temporal_epoch_3:
    target_device: 19
    input_count: 1

  fwd_0_128_temporal_epoch_3:
    target_device: 20
    input_count: 1

  fwd_0_129_temporal_epoch_3:
    target_device: 21
    input_count: 1

  fwd_0_130_temporal_epoch_3:
    target_device: 22
    input_count: 1

  fwd_0_131_temporal_epoch_3:
    target_device: 23
    input_count: 1

  fwd_0_132_temporal_epoch_3:
    target_device: 24
    input_count: 1

  fwd_0_133_temporal_epoch_3:
    target_device: 25
    input_count: 1

  fwd_0_134_temporal_epoch_3:
    target_device: 26
    input_count: 1

  fwd_0_135_temporal_epoch_3:
    target_device: 27
    input_count: 1

  fwd_0_136_temporal_epoch_3:
    target_device: 28
    input_count: 1

  fwd_0_137_temporal_epoch_3:
    target_device: 29
    input_count: 1

  fwd_0_138_temporal_epoch_3:
    target_device: 30
    input_count: 1

  fwd_0_139_temporal_epoch_3:
    target_device: 31
    input_count: 1

  fwd_0_140_temporal_epoch_4:
    target_device: 0
    input_count: 1

  fwd_0_141_temporal_epoch_4:
    target_device: 2
    input_count: 1

  fwd_0_142_temporal_epoch_4:
    target_device: 3
    input_count: 1

  fwd_0_143_temporal_epoch_4:
    target_device: 4
    input_count: 1

  fwd_0_144_temporal_epoch_4:
    target_device: 5
    input_count: 1

  fwd_0_145_temporal_epoch_4:
    target_device: 6
    input_count: 1

  fwd_0_146_temporal_epoch_4:
    target_device: 7
    input_count: 1

  fwd_0_147_temporal_epoch_4:
    target_device: 8
    input_count: 1

  fwd_0_148_temporal_epoch_4:
    target_device: 9
    input_count: 1

  fwd_0_149_temporal_epoch_4:
    target_device: 10
    input_count: 1

  fwd_0_150_temporal_epoch_4:
    target_device: 11
    input_count: 1

  fwd_0_151_temporal_epoch_4:
    target_device: 12
    input_count: 1

  fwd_0_152_temporal_epoch_4:
    target_device: 13
    input_count: 1

  fwd_0_153_temporal_epoch_4:
    target_device: 14
    input_count: 1

  fwd_0_154_temporal_epoch_4:
    target_device: 15
    input_count: 1

  fwd_0_155_temporal_epoch_4:
    target_device: 16
    input_count: 1

  fwd_0_156_temporal_epoch_4:
    target_device: 17
    input_count: 1

  fwd_0_157_temporal_epoch_4:
    target_device: 18
    input_count: 1

  fwd_0_158_temporal_epoch_4:
    target_device: 19
    input_count: 1

  fwd_0_159_temporal_epoch_4:
    target_device: 20
    input_count: 1

  fwd_0_160_temporal_epoch_4:
    target_device: 21
    input_count: 1

  fwd_0_161_temporal_epoch_4:
    target_device: 22
    input_count: 1

  fwd_0_162_temporal_epoch_4:
    target_device: 23
    input_count: 1

  fwd_0_163_temporal_epoch_4:
    target_device: 24
    input_count: 1

  fwd_0_164_temporal_epoch_4:
    target_device: 25
    input_count: 1

  fwd_0_165_temporal_epoch_4:
    target_device: 26
    input_count: 1

  fwd_0_166_temporal_epoch_4:
    target_device: 27
    input_count: 1

  fwd_0_167_temporal_epoch_4:
    target_device: 28
    input_count: 1

  fwd_0_168_temporal_epoch_4:
    target_device: 29
    input_count: 1

  fwd_0_169_temporal_epoch_4:
    target_device: 30
    input_count: 1

  fwd_0_170_temporal_epoch_4:
    target_device: 31
    input_count: 1

  fwd_0_171_temporal_epoch_5:
    target_device: 0
    input_count: 1

  fwd_0_172_temporal_epoch_5:
    target_device: 3
    input_count: 1

  fwd_0_173_temporal_epoch_5:
    target_device: 4
    input_count: 1

  fwd_0_174_temporal_epoch_5:
    target_device: 5
    input_count: 1

  fwd_0_175_temporal_epoch_5:
    target_device: 6
    input_count: 1

  fwd_0_176_temporal_epoch_5:
    target_device: 7
    input_count: 1

  fwd_0_177_temporal_epoch_5:
    target_device: 8
    input_count: 1

  fwd_0_178_temporal_epoch_5:
    target_device: 9
    input_count: 1

  fwd_0_179_temporal_epoch_5:
    target_device: 10
    input_count: 1

  fwd_0_180_temporal_epoch_5:
    target_device: 11
    input_count: 1

  fwd_0_181_temporal_epoch_5:
    target_device: 12
    input_count: 1

  fwd_0_182_temporal_epoch_5:
    target_device: 13
    input_count: 1

  fwd_0_183_temporal_epoch_5:
    target_device: 14
    input_count: 1

  fwd_0_184_temporal_epoch_5:
    target_device: 15
    input_count: 1

  fwd_0_185_temporal_epoch_5:
    target_device: 16
    input_count: 1

  fwd_0_186_temporal_epoch_5:
    target_device: 17
    input_count: 1

  fwd_0_187_temporal_epoch_5:
    target_device: 18
    input_count: 1

  fwd_0_188_temporal_epoch_5:
    target_device: 19
    input_count: 1

  fwd_0_189_temporal_epoch_5:
    target_device: 20
    input_count: 1

  fwd_0_190_temporal_epoch_5:
    target_device: 21
    input_count: 1

  fwd_0_191_temporal_epoch_5:
    target_device: 22
    input_count: 1

  fwd_0_192_temporal_epoch_5:
    target_device: 23
    input_count: 1

  fwd_0_193_temporal_epoch_5:
    target_device: 24
    input_count: 1

  fwd_0_194_temporal_epoch_5:
    target_device: 25
    input_count: 1

  fwd_0_195_temporal_epoch_5:
    target_device: 26
    input_count: 1

  fwd_0_196_temporal_epoch_5:
    target_device: 27
    input_count: 1

  fwd_0_197_temporal_epoch_5:
    target_device: 28
    input_count: 1

  fwd_0_198_temporal_epoch_5:
    target_device: 29
    input_count: 1

  fwd_0_199_temporal_epoch_5:
    target_device: 30
    input_count: 1

  fwd_0_200_temporal_epoch_5:
    target_device: 31
    input_count: 1

  fwd_0_201_temporal_epoch_6:
    target_device: 0
    input_count: 1

  fwd_0_202_temporal_epoch_6:
    target_device: 2
    input_count: 1

  fwd_0_203_temporal_epoch_6:
    target_device: 3
    input_count: 1

  fwd_0_204_temporal_epoch_6:
    target_device: 4
    input_count: 1

  fwd_0_205_temporal_epoch_6:
    target_device: 5
    input_count: 1

  fwd_0_206_temporal_epoch_6:
    target_device: 6
    input_count: 1

  fwd_0_207_temporal_epoch_6:
    target_device: 7
    input_count: 1

  fwd_0_208_temporal_epoch_6:
    target_device: 8
    input_count: 1

  fwd_0_209_temporal_epoch_6:
    target_device: 9
    input_count: 1

  fwd_0_210_temporal_epoch_6:
    target_device: 10
    input_count: 1

  fwd_0_211_temporal_epoch_6:
    target_device: 11
    input_count: 1

  fwd_0_212_temporal_epoch_6:
    target_device: 12
    input_count: 1

  fwd_0_213_temporal_epoch_6:
    target_device: 13
    input_count: 1

  fwd_0_214_temporal_epoch_6:
    target_device: 14
    input_count: 1

  fwd_0_215_temporal_epoch_6:
    target_device: 15
    input_count: 1

  fwd_0_216_temporal_epoch_6:
    target_device: 16
    input_count: 1

  fwd_0_217_temporal_epoch_6:
    target_device: 17
    input_count: 1

  fwd_0_218_temporal_epoch_6:
    target_device: 18
    input_count: 1

  fwd_0_219_temporal_epoch_6:
    target_device: 19
    input_count: 1

  fwd_0_220_temporal_epoch_6:
    target_device: 20
    input_count: 1

  fwd_0_221_temporal_epoch_6:
    target_device: 21
    input_count: 1

  fwd_0_222_temporal_epoch_6:
    target_device: 22
    input_count: 1

  fwd_0_223_temporal_epoch_6:
    target_device: 23
    input_count: 1

  fwd_0_224_temporal_epoch_6:
    target_device: 24
    input_count: 1

  fwd_0_225_temporal_epoch_6:
    target_device: 25
    input_count: 1

  fwd_0_226_temporal_epoch_6:
    target_device: 26
    input_count: 1

  fwd_0_227_temporal_epoch_6:
    target_device: 27
    input_count: 1

  fwd_0_228_temporal_epoch_6:
    target_device: 28
    input_count: 1

  fwd_0_229_temporal_epoch_6:
    target_device: 29
    input_count: 1

  fwd_0_230_temporal_epoch_6:
    target_device: 30
    input_count: 1

  fwd_0_231_temporal_epoch_6:
    target_device: 31
    input_count: 1

  fwd_0_232_temporal_epoch_7:
    target_device: 8
    input_count: 1

  fwd_0_233_temporal_epoch_7:
    target_device: 9
    input_count: 1

  fwd_0_234_temporal_epoch_7:
    target_device: 10
    input_count: 1

  fwd_0_235_temporal_epoch_7:
    target_device: 11
    input_count: 1

  fwd_0_236_temporal_epoch_7:
    target_device: 12
    input_count: 1

  fwd_0_237_temporal_epoch_7:
    target_device: 13
    input_count: 1

  fwd_0_238_temporal_epoch_7:
    target_device: 14
    input_count: 1

  fwd_0_239_temporal_epoch_7:
    target_device: 15
    input_count: 1

  fwd_0_240_temporal_epoch_7:
    target_device: 16
    input_count: 1

  fwd_0_241_temporal_epoch_7:
    target_device: 17
    input_count: 1

  fwd_0_242_temporal_epoch_7:
    target_device: 18
    input_count: 1

  fwd_0_243_temporal_epoch_7:
    target_device: 19
    input_count: 1

  fwd_0_244_temporal_epoch_7:
    target_device: 20
    input_count: 1

  fwd_0_245_temporal_epoch_7:
    target_device: 21
    input_count: 1

  fwd_0_246_temporal_epoch_7:
    target_device: 22
    input_count: 1

  fwd_0_247_temporal_epoch_7:
    target_device: 23
    input_count: 1

  fwd_0_248_temporal_epoch_7:
    target_device: 24
    input_count: 1

  fwd_0_249_temporal_epoch_7:
    target_device: 25
    input_count: 1

  fwd_0_250_temporal_epoch_7:
    target_device: 26
    input_count: 1

  fwd_0_251_temporal_epoch_7:
    target_device: 27
    input_count: 1

  fwd_0_252_temporal_epoch_7:
    target_device: 28
    input_count: 1

  fwd_0_253_temporal_epoch_7:
    target_device: 29
    input_count: 1

  fwd_0_254_temporal_epoch_7:
    target_device: 30
    input_count: 1

  fwd_0_255_temporal_epoch_7:
    target_device: 31
    input_count: 1


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0_shadow: 0, $gptr_q4_shadow: 0, $gptr_q9_shadow: 0, $gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $lptr_q3: 0, $lptr_q4: 0, $gptr_q5: 0, $lptr_q5: 0, $gptr_q6: 0, $lptr_q12: 0, $gptr_q1: 0, $lptr_q8: 0, $lptr_q11: 0, $gptr_q10: 0, $gptr_q12: 0, $lptr_q10: 0, $lptr_q7: 0, $gptr_q11: 0, $lptr_q9: 0, $gptr_q8: 0, $gptr_q9: 0, $gptr_q4: 0, $gptr_q7: 0, $lptr_q6: 0}
    - loop: $p_loop_count
    -   varinst: [$gptr_q9, set, $gptr_q9_shadow]
    -   varinst: [$gptr_q4, set, $gptr_q4_shadow]
    -   varinst: [$gptr_q0, set, $gptr_q0_shadow]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.layernorm_0.dc.reduce_sum.0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_0.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_0.dc.reduce_sum.5.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_0.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_0.8: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_0.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.ln_mlp.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.ln_mlp.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.mlp.dense_h_to_4h.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.mlp.dense_4h_to_h.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_1_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               cos: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               sin: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               past_key: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.layernorm_10.dc.reduce_sum.0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_10.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_10.dc.reduce_sum.5.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_10.6: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_10.8: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_10.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.ln_attn.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.ln_attn.bias: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.self_attention.wq.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_17: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_23.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_23.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_25_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_26_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_27_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_28_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_29_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_30_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_31_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_32_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_33_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_34_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_35_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_36_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_37_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_38_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_39_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_40_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_41_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_42_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_43_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_44_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_45_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_46_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_47_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_48_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_49_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_50_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_51_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_52_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_53_temporal_epoch_0}
    -   execute: {graph_name: fwd_0_54_temporal_epoch_0}
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_1, queue_settings: {
               cos: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               past_key: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_10.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_index_25.dc.select.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               layers.0.self_attention.wk.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_32: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_3_temporal_epoch_1, queue_settings: {
               sin: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.reshape_37.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_37.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_37.dc.sparse_matmul.10.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_37.dc.sparse_matmul.10.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_37.dc.sparse_matmul.14.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_37.dc.sparse_matmul.14.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_37.dc.transpose.23_s_brcst_m2_1_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_55_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_56_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_57_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_58_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_59_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_60_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_61_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_62_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_63_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_64_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_65_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_66_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_67_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_68_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_69_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_70_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_71_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_72_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_73_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_74_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_75_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_76_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_77_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_78_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_79_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_80_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_81_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_82_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_83_temporal_epoch_1}
    -   execute: {graph_name: fwd_0_84_temporal_epoch_1}
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_2, queue_settings: {
               e2e_index_25.dc.buffer.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_add_38_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3}} }
    -   execute: {graph_name: fwd_0_85_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_86_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_87_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_88_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_89_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_90_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_91_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_92_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_93_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_94_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_95_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_96_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_97_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_98_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_99_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_100_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_101_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_102_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_103_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_104_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_105_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_106_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_107_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_108_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_109_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_110_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_111_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_112_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_113_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_114_temporal_epoch_2}
    -   execute: {graph_name: fwd_0_115_temporal_epoch_2}
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_3, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               past_value: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_transpose_23.dc.sparse_matmul.4.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_index_40.dc.select.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_multiply_46: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_48.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_48.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_48.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_48.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_6_temporal_epoch_3, queue_settings: {
               past_value: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_layernorm_10.dc.add.14_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layers.0.self_attention.wv.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.reshape_56.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_56.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_56.dc.sparse_matmul.10.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_56.dc.sparse_matmul.10.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_7_temporal_epoch_3, queue_settings: {
               lc.input_tensor.reshape_56.dc.sparse_matmul.14.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_56.dc.sparse_matmul.14.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_56.dc.transpose.23_s_brcst_m2_1_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_8_temporal_epoch_3, queue_settings: {
               e2e_add_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               lc.input_tensor.transpose_67.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_67.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_9_temporal_epoch_3, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_add_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_multiply_75: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_1_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_77.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_77.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_77.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_77.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_10_temporal_epoch_3, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               lc.input_tensor.transpose_88.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_88.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_96: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_2_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_98.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_98.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_98.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_98.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_11_temporal_epoch_3, queue_settings: {
               e2e_add_21_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               lc.input_tensor.transpose_109.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_109.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_12_temporal_epoch_3, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               input_1_multiply_117: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_3_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_116_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_117_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_118_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_119_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_120_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_121_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_122_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_123_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_124_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_125_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_126_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_127_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_128_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_129_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_130_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_131_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_132_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_133_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_134_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_135_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_136_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_137_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_138_temporal_epoch_3}
    -   execute: {graph_name: fwd_0_139_temporal_epoch_3}
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_13_temporal_epoch_4, queue_settings: {
               e2e_add_21_3: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               e2e_add_118_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.softmax_119.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_119.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_119.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_119.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_130.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_130.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_140_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_141_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_142_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_143_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_144_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_145_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_146_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_147_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_148_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_149_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_150_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_151_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_152_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_153_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_154_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_155_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_156_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_157_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_158_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_159_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_160_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_161_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_162_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_163_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_164_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_165_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_166_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_167_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_168_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_169_temporal_epoch_4}
    -   execute: {graph_name: fwd_0_170_temporal_epoch_4}
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_14_temporal_epoch_5, queue_settings: {
               e2e_concatenate_39.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}} }
    -   execute: {graph_name: fwd_0_15_temporal_epoch_5, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_transpose_130.dc.sparse_matmul.4.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               input_1_multiply_138: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_4_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_140.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_140.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_140.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_140.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_171_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_172_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_173_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_174_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_175_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_176_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_177_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_178_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_179_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_180_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_181_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_182_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_183_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_184_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_185_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_186_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_187_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_188_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_189_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_190_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_191_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_192_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_193_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_194_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_195_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_196_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_197_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_198_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_199_temporal_epoch_5}
    -   execute: {graph_name: fwd_0_200_temporal_epoch_5}
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q9_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_16_temporal_epoch_6, queue_settings: {
               e2e_add_21_4: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_concatenate_58.dc.concatenate.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_softmax_140.dc.multiply.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10}} }
    -   execute: {graph_name: fwd_0_201_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_202_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_203_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_204_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_205_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_206_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_207_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_208_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_209_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_210_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_211_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_212_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_213_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_214_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_215_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_216_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_217_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_218_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_219_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_220_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_221_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_222_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_223_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_224_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_225_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_226_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_227_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_228_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_229_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_230_temporal_epoch_6}
    -   execute: {graph_name: fwd_0_231_temporal_epoch_6}
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_0_17_temporal_epoch_7, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_index_150.dc.select.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.transpose_151.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_151.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_multiply_159: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_5_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_161.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_161.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_161.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_161.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_18_temporal_epoch_7, queue_settings: {
               e2e_add_21_5: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.transpose_172.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_172.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_19_temporal_epoch_7, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               input_1_multiply_180: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_6_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_182.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_182.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_182.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_182.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_20_temporal_epoch_7, queue_settings: {
               e2e_add_21_5: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.transpose_193.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.transpose_193.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_21_temporal_epoch_7, queue_settings: {
               attn_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_concatenate_39.dc.concatenate.0_attempt_1_input_op_fork_nop1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               input_1_multiply_201: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attn_mask_s_brcst_m2_7_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_203.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_22_temporal_epoch_7, queue_settings: {
               e2e_matmul_64_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_matmul_85_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_matmul_106_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_matmul_127_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_concatenate_58.dc.concatenate.0_attempt_2_input_op_fork_nop1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_matmul_148_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.softmax_203.dc.reduce_sum.3.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_203.4: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_203.dc.reciprocal.6_s_brcst_m1_0_0.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_213.dc.sparse_matmul.10.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.concatenate_213.dc.sparse_matmul.10.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_23_temporal_epoch_7, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_matmul_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.reshape_214.dc.sparse_matmul.4.0: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_214.dc.sparse_matmul.4.1: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_214.dc.sparse_matmul.10.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.reshape_214.dc.sparse_matmul.10.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.self_attention.dense.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_24_temporal_epoch_7, queue_settings: {
               e2e_reshape_37.dc.sparse_matmul.14.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_reshape_56.dc.sparse_matmul.14.lc2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12}} }
    -   execute: {graph_name: fwd_0_232_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_233_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_234_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_235_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_236_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_237_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_238_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_239_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_240_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_241_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_242_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_243_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_244_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_245_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_246_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_247_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_248_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_249_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_250_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_251_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_252_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_253_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_254_temporal_epoch_7}
    -   execute: {graph_name: fwd_0_255_temporal_epoch_7}
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 2]
    - endloop


