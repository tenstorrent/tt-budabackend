# git checkout ea96a530
# pytest pybuda/test/backend/models/test_bert.py::test_multichip_wormhole_multi_encoder_split_concurrent[inference-Golden-chip12-enc12-large]

devices:
  arch: [wormhole, wormhole_b0]

queues:

  # input
  encoder_input:                                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300061a0]]}
  attention_mask:                                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30000000]]}

  # output
  encoder11.output_norm_ff_11:                                                                  {input: norm_ff_11.dc.add.10_output_nop_0, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  ff.bert.encoder.layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7dc5420], [5, 0x3beb420], [0, 0x3af8060], [1, 0x3af8060], [2, 0x3afb940], [3, 0x7cd5940], [4, 0x7e06440], [5, 0x3c2c440]]}
  ff.bert.encoder.layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b39080], [1, 0x3b39080], [2, 0x3b3c960], [3, 0x7d16960]]}
  ff.bert.encoder.layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7e47460], [5, 0x3c6d460], [0, 0x3b3d1a0], [1, 0x3b3d1a0], [2, 0x3b40a80], [3, 0x7d1aa80], [4, 0x7e88480], [5, 0x3cae480]]}
  ff.bert.encoder.layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b7e1c0], [1, 0x3b7e1c0], [2, 0x3b81aa0], [3, 0x7d5baa0]]}
  ff.reciprocal_of_sqrt_of_head_size_0:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3cef4a0]]}
  ff.bert.encoder.layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3cefce0], [0, 0x3b82b20], [1, 0x3b82b20], [2, 0x3b85bc0], [3, 0x7d60400], [4, 0x7eca520], [5, 0x3d30d00], [0, 0x3bc3b40]]}
  ff.bert.encoder.layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3bc3b40], [2, 0x3bc6be0], [3, 0x7da1420], [4, 0x7f0b540]]}
  ff.bert.encoder.layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3a79900], [3, 0x7c53900], [4, 0x7d84400], [5, 0x3baa400], [0, 0x3ab7040], [1, 0x3ab7040], [2, 0x3aba920], [3, 0x7c94920]]}
  ff.bert.encoder.layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3c04b60], [1, 0x3bc7c60], [2, 0x3bcad00], [3, 0x7da5540]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3d1e780]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3d1b6a0]]}
  ff.bert.encoder.layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3b98720], [2, 0x3b88b40], [3, 0x7d72720], [4, 0x7d602a0], [5, 0x3c17660], [0, 0x3c1a740], [1, 0x3c1a740], [2, 0x3c0ab60], [3, 0x7df4740], [4, 0x7de22c0], [5, 0x3c99680], [0, 0x3c9c760], [1, 0x3c9c760], [2, 0x3c8cb80], [3, 0x7e76760], [4, 0x7e642e0]]}
  ff.bert.encoder.layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7d62300], [4, 0x7d4fe80], [5, 0x3c07240], [0, 0x3c0a320]]}
  ff.bert.encoder.layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3a63280], [3, 0x7c4ce60], [4, 0x7bccee0], [5, 0x39e3300], [0, 0x3a75f40], [1, 0x3a75f40], [2, 0x3ae52a0], [3, 0x7ccee80], [4, 0x7c4ef00], [5, 0x3a65320], [0, 0x3af7f60], [1, 0x3af7f60], [2, 0x3b672c0], [3, 0x7d50ea0], [4, 0x7cd0f20], [5, 0x3ae7340]]}
  ff.bert.encoder.layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3d059a0], [2, 0x3d72460], [3, 0x7f5e8e0], [4, 0x7e6e5c0], [5, 0x3c72560], [0, 0x3d15dc0], [1, 0x3d07a40], [2, 0x3d74500]]}
  ff.bert.encoder.layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3d05160]]}
  ff.bert.encoder.layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7e5b0c0]]}
  ff.bert.encoder.layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3c797c0], [2, 0x3d2cbe0], [3, 0x7e168c0], [4, 0x7d22cc0], [5, 0x3b48cc0], [0, 0x3b89ce0], [1, 0x3cba7e0], [2, 0x3d6dc00]]}
  ff.bert.encoder.layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7c940e0], [5, 0x3aba0e0], [0, 0x3afb100], [1, 0x3bae520]]}
  ff.bert.encoder.layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3c28320], [3, 0x7d4ef00], [4, 0x7c98200], [5, 0x3abe200], [0, 0x3aff220], [1, 0x3bb2640], [2, 0x3c69340], [3, 0x7d8ff20]]}
  ff.bert.encoder.layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7cd9220], [5, 0x3aff220], [0, 0x3b40240], [1, 0x3bf3660]]}
  ff.reciprocal_of_sqrt_of_head_size_1:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7dd0f40]]}
  ff.bert.encoder.layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3bf7780], [2, 0x3caaba0], [3, 0x7dd1780], [4, 0x7cddb80], [5, 0x3b03b80], [0, 0x3b44ba0], [1, 0x3c387a0], [2, 0x3cebbc0]]}
  ff.bert.encoder.layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7e127a0], [4, 0x7d1eba0], [5, 0x3b44ba0], [0, 0x3b85bc0]]}
  ff.bert.encoder.layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3ba62e0], [3, 0x7cccec0], [4, 0x7c530c0], [5, 0x3a790c0], [0, 0x3aba0e0], [1, 0x3b6d500], [2, 0x3be7300], [3, 0x7d0dee0]]}
  ff.bert.encoder.layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7e578e0], [4, 0x7d63ce0], [5, 0x3b89ce0], [0, 0x3bcad00]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3d1e780]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3d1b6a0]]}
  ff.bert.encoder.layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3b98720], [2, 0x3b88b40], [3, 0x7d72720], [4, 0x7d602a0], [5, 0x3c17660], [0, 0x3c1a740], [1, 0x3c1a740], [2, 0x3c0ab60], [3, 0x7df4740], [4, 0x7de22c0], [5, 0x3c99680], [0, 0x3c9c760], [1, 0x3c9c760], [2, 0x3c8cb80], [3, 0x7e76760], [4, 0x7e642e0]]}
  ff.bert.encoder.layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7d62300], [4, 0x7d4fe80], [5, 0x3c07240], [0, 0x3c0a320]]}
  ff.bert.encoder.layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3a63280], [3, 0x7c4ce60], [4, 0x7bccee0], [5, 0x39e3300], [0, 0x3a75f40], [1, 0x3a75f40], [2, 0x3ae52a0], [3, 0x7ccee80], [4, 0x7c4ef00], [5, 0x3a65320], [0, 0x3af7f60], [1, 0x3af7f60], [2, 0x3b672c0], [3, 0x7d50ea0], [4, 0x7cd0f20], [5, 0x3ae7340]]}
  ff.bert.encoder.layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3d059a0], [2, 0x3d72460], [3, 0x7f5e8e0], [4, 0x7e6e5c0], [5, 0x3c72560], [0, 0x3d15dc0], [1, 0x3d07a40], [2, 0x3d74500]]}
  ff.bert.encoder.layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3d05160]]}
  ff.bert.encoder.layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7e5b0c0]]}
  ff.bert.encoder.layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3c797c0], [2, 0x3d2cbe0], [3, 0x7e168c0], [4, 0x7d22cc0], [5, 0x3b48cc0], [0, 0x3b89ce0], [1, 0x3cba7e0], [2, 0x3d6dc00]]}
  ff.bert.encoder.layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7c940e0], [5, 0x3aba0e0], [0, 0x3afb100], [1, 0x3bae520]]}
  ff.bert.encoder.layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3c28320], [3, 0x7d4ef00], [4, 0x7c98200], [5, 0x3abe200], [0, 0x3aff220], [1, 0x3bb2640], [2, 0x3c69340], [3, 0x7d8ff20]]}
  ff.bert.encoder.layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7cd9220], [5, 0x3aff220], [0, 0x3b40240], [1, 0x3bf3660]]}
  ff.reciprocal_of_sqrt_of_head_size_2:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7dd0f40]]}
  ff.bert.encoder.layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3bf7780], [2, 0x3caaba0], [3, 0x7dd1780], [4, 0x7cddb80], [5, 0x3b03b80], [0, 0x3b44ba0], [1, 0x3c387a0], [2, 0x3cebbc0]]}
  ff.bert.encoder.layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7e127a0], [4, 0x7d1eba0], [5, 0x3b44ba0], [0, 0x3b85bc0]]}
  ff.bert.encoder.layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3ba62e0], [3, 0x7cccec0], [4, 0x7c530c0], [5, 0x3a790c0], [0, 0x3aba0e0], [1, 0x3b6d500], [2, 0x3be7300], [3, 0x7d0dee0]]}
  ff.bert.encoder.layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7e578e0], [4, 0x7d63ce0], [5, 0x3b89ce0], [0, 0x3bcad00]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3d1e780]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3d1b6a0]]}
  ff.bert.encoder.layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3b98720], [2, 0x3b88b40], [3, 0x7d72720], [4, 0x7d602a0], [5, 0x3c17660], [0, 0x3c1a740], [1, 0x3c1a740], [2, 0x3c0ab60], [3, 0x7df4740], [4, 0x7de22c0], [5, 0x3c99680], [0, 0x3c9c760], [1, 0x3c9c760], [2, 0x3c8cb80], [3, 0x7e76760], [4, 0x7e642e0]]}
  ff.bert.encoder.layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7d62300], [4, 0x7d4fe80], [5, 0x3c07240], [0, 0x3c0a320]]}
  ff.bert.encoder.layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3a63280], [3, 0x7c4ce60], [4, 0x7bccee0], [5, 0x39e3300], [0, 0x3a75f40], [1, 0x3a75f40], [2, 0x3ae52a0], [3, 0x7ccee80], [4, 0x7c4ef00], [5, 0x3a65320], [0, 0x3af7f60], [1, 0x3af7f60], [2, 0x3b672c0], [3, 0x7d50ea0], [4, 0x7cd0f20], [5, 0x3ae7340]]}
  ff.bert.encoder.layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3d059a0], [2, 0x3d72460], [3, 0x7f5e8e0], [4, 0x7e6e5c0], [5, 0x3c72560], [0, 0x3d15dc0], [1, 0x3d07a40], [2, 0x3d74500]]}
  ff.bert.encoder.layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3d05160]]}
  ff.bert.encoder.layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7e5b0c0]]}
  ff.bert.encoder.layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3c797c0], [2, 0x3d2cbe0], [3, 0x7e168c0], [4, 0x7d22cc0], [5, 0x3b48cc0], [0, 0x3b89ce0], [1, 0x3cba7e0], [2, 0x3d6dc00]]}
  ff.bert.encoder.layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7c940e0], [5, 0x3aba0e0], [0, 0x3afb100], [1, 0x3bae520]]}
  ff.bert.encoder.layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3c28320], [3, 0x7d4ef00], [4, 0x7c98200], [5, 0x3abe200], [0, 0x3aff220], [1, 0x3bb2640], [2, 0x3c69340], [3, 0x7d8ff20]]}
  ff.bert.encoder.layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7cd9220], [5, 0x3aff220], [0, 0x3b40240], [1, 0x3bf3660]]}
  ff.reciprocal_of_sqrt_of_head_size_3:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7dd0f40]]}
  ff.bert.encoder.layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3bf7780], [2, 0x3caaba0], [3, 0x7dd1780], [4, 0x7cddb80], [5, 0x3b03b80], [0, 0x3b44ba0], [1, 0x3c387a0], [2, 0x3cebbc0]]}
  ff.bert.encoder.layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7e127a0], [4, 0x7d1eba0], [5, 0x3b44ba0], [0, 0x3b85bc0]]}
  ff.bert.encoder.layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3ba62e0], [3, 0x7cccec0], [4, 0x7c530c0], [5, 0x3a790c0], [0, 0x3aba0e0], [1, 0x3b6d500], [2, 0x3be7300], [3, 0x7d0dee0]]}
  ff.bert.encoder.layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7e578e0], [4, 0x7d63ce0], [5, 0x3b89ce0], [0, 0x3bcad00]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3e26900]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3ea37a0]]}
  ff.bert.encoder.layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3ca08a0], [2, 0x3c90cc0], [3, 0x7f0a400], [4, 0x7ee83a0], [5, 0x3d9f760], [0, 0x3d92c60], [1, 0x3d228c0], [2, 0x3d12ce0], [3, 0x7f8c420], [4, 0x7f6a3c0], [5, 0x3e21780], [0, 0x3e14c80], [1, 0x3da48e0], [2, 0x3d94d00], [3, 0x800e440], [4, 0x7fec3e0]]}
  ff.bert.encoder.layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7ef9fe0], [4, 0x7ed7f80], [5, 0x3d8f340], [0, 0x3d82840]]}
  ff.bert.encoder.layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ac4d60], [5, 0x38ead60], [0, 0x38de260], [1, 0x38f06e0], [2, 0x38ede40], [3, 0x7ab59c0], [4, 0x7b46d80], [5, 0x396cd80], [0, 0x3960280], [1, 0x3972700], [2, 0x396fe60], [3, 0x7b379e0], [4, 0x7bc8da0], [5, 0x39eeda0], [0, 0x39e22a0], [1, 0x39f4720]]}
  ff.bert.encoder.layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3bff080], [2, 0x3b7b000], [3, 0x7d45420], [4, 0x7d66440], [5, 0x3b79fc0], [0, 0x3bfd020], [1, 0x3c01120], [2, 0x3b7d0a0]]}
  ff.bert.encoder.layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3bec3c0]]}
  ff.bert.encoder.layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d52f40]]}
  ff.bert.encoder.layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3bcbd80], [2, 0x3bcee20], [3, 0x7da9660], [4, 0x7f0fea0], [5, 0x3d72560], [0, 0x3c094c0], [1, 0x3c0cda0], [2, 0x3c0fe40]]}
  ff.bert.encoder.layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7dea680], [4, 0x7f50ec0], [5, 0x3db3580], [0, 0x3c4a4e0]]}
  ff.bert.encoder.layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c4ddc0], [2, 0x3c50e60], [3, 0x7dee7a0], [4, 0x7f54fe0], [5, 0x3db76a0], [0, 0x3c4e600], [1, 0x3c8ede0], [2, 0x3c91e80]]}
  ff.bert.encoder.layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7e2f7c0], [4, 0x7f96000], [5, 0x3df86c0], [0, 0x3c8f620]]}
  ff.reciprocal_of_sqrt_of_head_size_4:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3cd2ea0]]}
  ff.bert.encoder.layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ab4100], [5, 0x38da100], [0, 0x38de220], [1, 0x38de220], [2, 0x38de220], [3, 0x7ab8220], [4, 0x7af5120], [5, 0x391b120]]}
  ff.bert.encoder.layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100]]}
  ff.bert.encoder.layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x391f240], [1, 0x391f240], [2, 0x391f240], [3, 0x7af9240], [4, 0x7b36140], [5, 0x395c140], [0, 0x3960260], [1, 0x3960260]]}
  ff.bert.encoder.layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3960260], [3, 0x7b3a260], [4, 0x7b77160], [5, 0x399d160]]}
  ff.bert.encoder.layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x38da100]]}
  ff.bert.encoder.layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3b85220]]}
  ff.bert.encoder.layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7bdba60], [4, 0x7bc95e0], [5, 0x3a809a0], [0, 0x3a811e0], [1, 0x3a811e0], [2, 0x3a83a80], [3, 0x7c5da80], [4, 0x7c4b600], [5, 0x3b029c0], [0, 0x3b03200], [1, 0x3b03200], [2, 0x3b05aa0], [3, 0x7cdfaa0], [4, 0x7ccd620], [5, 0x3b849e0], [0, 0x3b85220]]}
  ff.bert.encoder.layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3a70580], [0, 0x3a70dc0], [1, 0x3a70dc0], [2, 0x3a73660]]}
  ff.bert.encoder.layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3b7f100], [1, 0x3b7f100], [2, 0x3bebbc0], [3, 0x7dd57a0], [4, 0x7d57080], [5, 0x3b6d4a0], [0, 0x3c01120], [1, 0x3c01120], [2, 0x3c6dbe0], [3, 0x7e577c0], [4, 0x7dd90a0], [5, 0x3bef4c0], [0, 0x3c83140], [1, 0x3c83140], [2, 0x3cefc00], [3, 0x7ed97e0]]}
  ff.bert.encoder.layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7d52f40], [5, 0x3b69360], [0, 0x3b7d060], [1, 0x3b7d060], [2, 0x3be9b20], [3, 0x7dd3700], [4, 0x7d54fe0], [5, 0x3b6b400]]}
  ff.bert.encoder.layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3a65b20]]}
  ff.bert.encoder.layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7bbcac0]]}
  ff.bert.encoder.layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3daec20], [3, 0x7e5ba00], [4, 0x7d67e00], [5, 0x3b8de00], [0, 0x3bcee20], [1, 0x3cfc040], [2, 0x3defc40], [3, 0x7e9ca20]]}
  ff.bert.encoder.layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7da8e20], [5, 0x3bcee20], [0, 0x3c0fe40], [1, 0x3d3d060]]}
  ff.bert.encoder.layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3e30c60], [3, 0x7edda40], [4, 0x7dacf40], [5, 0x3bd2f40], [0, 0x3c13f60], [1, 0x3d41180], [2, 0x3e71c80], [3, 0x7f1ea60]]}
  ff.bert.encoder.layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7dedf60], [5, 0x3c13f60], [0, 0x3c54f80], [1, 0x3d821a0]]}
  ff.reciprocal_of_sqrt_of_head_size_5:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7f5fa80]]}
  ff.bert.encoder.layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x391b120], [1, 0x391b120]]}
  ff.bert.encoder.layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7af5120], [4, 0x7af5120], [5, 0x391b120], [0, 0x395c140]]}
  ff.bert.encoder.layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x395c140], [2, 0x391b960], [3, 0x7af9240], [4, 0x7af9240], [5, 0x391f240], [0, 0x3960260], [1, 0x399d160], [2, 0x395c980]]}
  ff.bert.encoder.layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7b3a260], [4, 0x7b3a260], [5, 0x3960260], [0, 0x39a1280]]}
  ff.bert.encoder.layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x38da100]]}
  ff.bert.encoder.layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3b85220]]}
  ff.bert.encoder.layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7bdba60], [4, 0x7bc95e0], [5, 0x3a809a0], [0, 0x3a811e0], [1, 0x3a811e0], [2, 0x3a83a80], [3, 0x7c5da80], [4, 0x7c4b600], [5, 0x3b029c0], [0, 0x3b03200], [1, 0x3b03200], [2, 0x3b05aa0], [3, 0x7cdfaa0], [4, 0x7ccd620], [5, 0x3b849e0], [0, 0x3b85220]]}
  ff.bert.encoder.layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3a70580], [0, 0x3a70dc0], [1, 0x3a70dc0], [2, 0x3a73660]]}
  ff.bert.encoder.layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3b7f100], [1, 0x3b7f100], [2, 0x3bebbc0], [3, 0x7dd57a0], [4, 0x7d57080], [5, 0x3b6d4a0], [0, 0x3c01120], [1, 0x3c01120], [2, 0x3c6dbe0], [3, 0x7e577c0], [4, 0x7dd90a0], [5, 0x3bef4c0], [0, 0x3c83140], [1, 0x3c83140], [2, 0x3cefc00], [3, 0x7ed97e0]]}
  ff.bert.encoder.layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7d52f40], [5, 0x3b69360], [0, 0x3b7d060], [1, 0x3b7d060], [2, 0x3be9b20], [3, 0x7dd3700], [4, 0x7d54fe0], [5, 0x3b6b400]]}
  ff.bert.encoder.layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3a65b20]]}
  ff.bert.encoder.layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7bbcac0]]}
  ff.bert.encoder.layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3daec20], [3, 0x7e5ba00], [4, 0x7d67e00], [5, 0x3b8de00], [0, 0x3bcee20], [1, 0x3cfc040], [2, 0x3defc40], [3, 0x7e9ca20]]}
  ff.bert.encoder.layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7da8e20], [5, 0x3bcee20], [0, 0x3c0fe40], [1, 0x3d3d060]]}
  ff.bert.encoder.layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3e30c60], [3, 0x7edda40], [4, 0x7dacf40], [5, 0x3bd2f40], [0, 0x3c13f60], [1, 0x3d41180], [2, 0x3e71c80], [3, 0x7f1ea60]]}
  ff.bert.encoder.layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7dedf60], [5, 0x3c13f60], [0, 0x3c54f80], [1, 0x3d821a0]]}
  ff.reciprocal_of_sqrt_of_head_size_6:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7f5fa80]]}
  ff.bert.encoder.layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x391b120], [1, 0x391b120]]}
  ff.bert.encoder.layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7af5120], [4, 0x7af5120], [5, 0x391b120], [0, 0x395c140]]}
  ff.bert.encoder.layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x395c140], [2, 0x391b960], [3, 0x7af9240], [4, 0x7af9240], [5, 0x391f240], [0, 0x3960260], [1, 0x399d160], [2, 0x395c980]]}
  ff.bert.encoder.layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7b3a260], [4, 0x7b3a260], [5, 0x3960260], [0, 0x39a1280]]}
  ff.bert.encoder.layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x38da100]]}
  ff.bert.encoder.layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3b85220]]}
  ff.bert.encoder.layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7bdba60], [4, 0x7bc95e0], [5, 0x3a809a0], [0, 0x3a811e0], [1, 0x3a811e0], [2, 0x3a83a80], [3, 0x7c5da80], [4, 0x7c4b600], [5, 0x3b029c0], [0, 0x3b03200], [1, 0x3b03200], [2, 0x3b05aa0], [3, 0x7cdfaa0], [4, 0x7ccd620], [5, 0x3b849e0], [0, 0x3b85220]]}
  ff.bert.encoder.layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3a70580], [0, 0x3a70dc0], [1, 0x3a70dc0], [2, 0x3a73660]]}
  ff.bert.encoder.layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3b7f100], [1, 0x3b7f100], [2, 0x3bebbc0], [3, 0x7dd57a0], [4, 0x7d57080], [5, 0x3b6d4a0], [0, 0x3c01120], [1, 0x3c01120], [2, 0x3c6dbe0], [3, 0x7e577c0], [4, 0x7dd90a0], [5, 0x3bef4c0], [0, 0x3c83140], [1, 0x3c83140], [2, 0x3cefc00], [3, 0x7ed97e0]]}
  ff.bert.encoder.layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7d52f40], [5, 0x3b69360], [0, 0x3b7d060], [1, 0x3b7d060], [2, 0x3be9b20], [3, 0x7dd3700], [4, 0x7d54fe0], [5, 0x3b6b400]]}
  ff.bert.encoder.layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3a65b20]]}
  ff.bert.encoder.layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7bbcac0]]}
  ff.bert.encoder.layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3daec20], [3, 0x7e5ba00], [4, 0x7d67e00], [5, 0x3b8de00], [0, 0x3bcee20], [1, 0x3cfc040], [2, 0x3defc40], [3, 0x7e9ca20]]}
  ff.bert.encoder.layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7da8e20], [5, 0x3bcee20], [0, 0x3c0fe40], [1, 0x3d3d060]]}
  ff.bert.encoder.layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3e30c60], [3, 0x7edda40], [4, 0x7dacf40], [5, 0x3bd2f40], [0, 0x3c13f60], [1, 0x3d41180], [2, 0x3e71c80], [3, 0x7f1ea60]]}
  ff.bert.encoder.layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7dedf60], [5, 0x3c13f60], [0, 0x3c54f80], [1, 0x3d821a0]]}
  ff.reciprocal_of_sqrt_of_head_size_7:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7f5fa80]]}
  ff.bert.encoder.layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x391b120], [1, 0x391b120]]}
  ff.bert.encoder.layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7af5120], [4, 0x7af5120], [5, 0x391b120], [0, 0x395c140]]}
  ff.bert.encoder.layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x395c140], [2, 0x391b960], [3, 0x7af9240], [4, 0x7af9240], [5, 0x391f240], [0, 0x3960260], [1, 0x399d160], [2, 0x395c980]]}
  ff.bert.encoder.layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7b3a260], [4, 0x7b3a260], [5, 0x3960260], [0, 0x39a1280]]}
  ff.bert.encoder.layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7ee9bc0]]}
  ff.bert.encoder.layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3c8d3a0]]}
  ff.bert.encoder.layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7d63b60], [4, 0x7d516e0], [5, 0x3c08aa0], [0, 0x3bf9700], [1, 0x3b89360], [2, 0x3b8bc00], [3, 0x7de5b80], [4, 0x7dd3700], [5, 0x3c8aac0], [0, 0x3c7b720], [1, 0x3c0b380], [2, 0x3c0dc20], [3, 0x7e67ba0], [4, 0x7e55720], [5, 0x3d0cae0], [0, 0x3cfd740]]}
  ff.bert.encoder.layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3bf8680], [0, 0x3be92e0], [1, 0x3b78f40], [2, 0x3b7b7e0]]}
  ff.bert.encoder.layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a66360], [1, 0x3a787e0], [2, 0x39f4760], [3, 0x7bbc2e0], [4, 0x7c4ef00], [5, 0x3a74f00], [0, 0x3ae8380], [1, 0x3afa800], [2, 0x3a76780], [3, 0x7c3e300], [4, 0x7cd0f20], [5, 0x3af6f20], [0, 0x3b6a3a0], [1, 0x3b7c820], [2, 0x3af87a0], [3, 0x7cc0320]]}
  ff.bert.encoder.layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c4adc0], [5, 0x3a70dc0], [0, 0x3a642c0], [1, 0x3a76740], [2, 0x39f26c0], [3, 0x7bba240], [4, 0x7c4ce60], [5, 0x3a72e60]]}
  ff.bert.encoder.layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38e02c0]]}
  ff.bert.encoder.layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da940]]}
  ff.bert.encoder.layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7b7b280], [5, 0x39a1280], [0, 0x39a1ac0], [1, 0x39a1ac0], [2, 0x3964bc0], [3, 0x7b3ebc0], [4, 0x7bbc2a0], [5, 0x39e22a0]]}
  ff.bert.encoder.layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x39e2ae0], [1, 0x39e2ae0], [2, 0x39a5be0], [3, 0x7b7fbe0]]}
  ff.bert.encoder.layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bfd2c0], [5, 0x3a232c0], [0, 0x39e6c00], [1, 0x39e6c00], [2, 0x39a9d00], [3, 0x7b83d00], [4, 0x7c3e2e0], [5, 0x3a642e0]]}
  ff.bert.encoder.layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a27c20], [1, 0x3a27c20], [2, 0x39ead20], [3, 0x7bc4d20]]}
  ff.reciprocal_of_sqrt_of_head_size_8:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3aa5300]]}
  ff.bert.encoder.layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7c7fb40], [5, 0x3aa5b40], [0, 0x3a2c580], [1, 0x3a2c580], [2, 0x39ef680], [3, 0x7bc9680], [4, 0x7cc0b60], [5, 0x3ae6b60]]}
  ff.bert.encoder.layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a6d5a0], [1, 0x3a6d5a0], [2, 0x3a306a0], [3, 0x7c0a6a0]]}
  ff.bert.encoder.layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d01b80], [5, 0x3b27b80], [0, 0x3a716c0], [1, 0x3a716c0], [2, 0x3a347c0], [3, 0x7c0e7c0], [4, 0x7d42ba0], [5, 0x3b68ba0]]}
  ff.bert.encoder.layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3ab26e0], [1, 0x3ab26e0], [2, 0x3a757e0], [3, 0x7c4f7e0]]}
  ff.bert.encoder.layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3a60160]]}
  ff.bert.encoder.layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7bc8560]]}
  ff.bert.encoder.layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x38da100], [0, 0x38ea520], [1, 0x38ea520], [2, 0x38ea520], [3, 0x7ac4520], [4, 0x7ac4520], [5, 0x395c120], [0, 0x396c540], [1, 0x396c540], [2, 0x396c540], [3, 0x7b46540], [4, 0x7b46540], [5, 0x39de140], [0, 0x39ee560], [1, 0x39ee560], [2, 0x39ee560]]}
  ff.bert.encoder.layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100]]}
  ff.bert.encoder.layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x38dfac0], [1, 0x38ef6a0], [2, 0x38dd220], [3, 0x7ac6e00], [4, 0x7ab8a80], [5, 0x38dea80], [0, 0x3961ae0], [1, 0x39716c0], [2, 0x395f240], [3, 0x7b48e20], [4, 0x7b3aaa0], [5, 0x3960aa0], [0, 0x39e3b00], [1, 0x39f36e0], [2, 0x39e1260], [3, 0x7bcae40]]}
  ff.bert.encoder.layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7ab4940], [5, 0x38da940], [0, 0x38dda20], [1, 0x38ed600], [2, 0x38db180], [3, 0x7ac4d60], [4, 0x7ab69e0], [5, 0x38dc9e0]]}
  ff.bert.encoder.layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7ab4100]]}
  ff.bert.encoder.layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x38da100]]}
  ff.bert.encoder.layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x399d9a0], [3, 0x7b3e380], [4, 0x7b3e380], [5, 0x3964380], [0, 0x39a53a0], [1, 0x39de9c0], [2, 0x39de9c0], [3, 0x7b7f3a0]]}
  ff.bert.encoder.layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7b7f3a0], [5, 0x39a53a0], [0, 0x39e63c0], [1, 0x3a1f9e0]]}
  ff.bert.encoder.layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3a1f9e0], [3, 0x7bc03c0], [4, 0x7b834c0], [5, 0x39a94c0], [0, 0x39ea4e0], [1, 0x3a23b00], [2, 0x3a60a00], [3, 0x7c013e0]]}
  ff.bert.encoder.layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7bc44e0], [5, 0x39ea4e0], [0, 0x3a2b500], [1, 0x3a64b20]]}
  ff.reciprocal_of_sqrt_of_head_size_9:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7c42400]]}
  ff.bert.encoder.layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3a68c40], [2, 0x3aa2260], [3, 0x7c42c40], [4, 0x7bc8e40], [5, 0x39eee40], [0, 0x3a2fe60], [1, 0x3aa9c60], [2, 0x3ae3280]]}
  ff.bert.encoder.layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7c83c60], [4, 0x7c09e60], [5, 0x3a2fe60], [0, 0x3a70e80]]}
  ff.bert.encoder.layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3aeac80], [2, 0x3b242a0], [3, 0x7c87d80], [4, 0x7c0df80], [5, 0x3a33f80], [0, 0x3a74fa0], [1, 0x3b2bca0], [2, 0x3b652c0]]}
  ff.bert.encoder.layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7cc8da0], [4, 0x7c4efa0], [5, 0x3a74fa0], [0, 0x3ab5fc0]]}
  ff.bert.encoder.layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3a60160]]}
  ff.bert.encoder.layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7bc8560]]}
  ff.bert.encoder.layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x38da100], [0, 0x38ea520], [1, 0x38ea520], [2, 0x38ea520], [3, 0x7ac4520], [4, 0x7ac4520], [5, 0x395c120], [0, 0x396c540], [1, 0x396c540], [2, 0x396c540], [3, 0x7b46540], [4, 0x7b46540], [5, 0x39de140], [0, 0x39ee560], [1, 0x39ee560], [2, 0x39ee560]]}
  ff.bert.encoder.layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100]]}
  ff.bert.encoder.layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x38dfac0], [1, 0x38ef6a0], [2, 0x38dd220], [3, 0x7ac6e00], [4, 0x7ab8a80], [5, 0x38dea80], [0, 0x3961ae0], [1, 0x39716c0], [2, 0x395f240], [3, 0x7b48e20], [4, 0x7b3aaa0], [5, 0x3960aa0], [0, 0x39e3b00], [1, 0x39f36e0], [2, 0x39e1260], [3, 0x7bcae40]]}
  ff.bert.encoder.layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7ab4940], [5, 0x38da940], [0, 0x38dda20], [1, 0x38ed600], [2, 0x38db180], [3, 0x7ac4d60], [4, 0x7ab69e0], [5, 0x38dc9e0]]}
  ff.bert.encoder.layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7ab4100]]}
  ff.bert.encoder.layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x38da100]]}
  ff.bert.encoder.layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x399d9a0], [3, 0x7b3e380], [4, 0x7b3e380], [5, 0x3964380], [0, 0x39a53a0], [1, 0x39de9c0], [2, 0x39de9c0], [3, 0x7b7f3a0]]}
  ff.bert.encoder.layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7b7f3a0], [5, 0x39a53a0], [0, 0x39e63c0], [1, 0x3a1f9e0]]}
  ff.bert.encoder.layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3a1f9e0], [3, 0x7bc03c0], [4, 0x7b834c0], [5, 0x39a94c0], [0, 0x39ea4e0], [1, 0x3a23b00], [2, 0x3a60a00], [3, 0x7c013e0]]}
  ff.bert.encoder.layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7bc44e0], [5, 0x39ea4e0], [0, 0x3a2b500], [1, 0x3a64b20]]}
  ff.reciprocal_of_sqrt_of_head_size_10:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7c42400]]}
  ff.bert.encoder.layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3a68c40], [2, 0x3aa2260], [3, 0x7c42c40], [4, 0x7bc8e40], [5, 0x39eee40], [0, 0x3a2fe60], [1, 0x3aa9c60], [2, 0x3ae3280]]}
  ff.bert.encoder.layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7c83c60], [4, 0x7c09e60], [5, 0x3a2fe60], [0, 0x3a70e80]]}
  ff.bert.encoder.layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3aeac80], [2, 0x3b242a0], [3, 0x7c87d80], [4, 0x7c0df80], [5, 0x3a33f80], [0, 0x3a74fa0], [1, 0x3b2bca0], [2, 0x3b652c0]]}
  ff.bert.encoder.layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7cc8da0], [4, 0x7c4efa0], [5, 0x3a74fa0], [0, 0x3ab5fc0]]}
  ff.bert.encoder.layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3a60160]]}
  ff.bert.encoder.layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7bc8560]]}
  ff.bert.encoder.layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x38da100], [0, 0x38ea520], [1, 0x38ea520], [2, 0x38ea520], [3, 0x7ac4520], [4, 0x7ac4520], [5, 0x395c120], [0, 0x396c540], [1, 0x396c540], [2, 0x396c540], [3, 0x7b46540], [4, 0x7b46540], [5, 0x39de140], [0, 0x39ee560], [1, 0x39ee560], [2, 0x39ee560]]}
  ff.bert.encoder.layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100]]}
  ff.bert.encoder.layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x38dfac0], [1, 0x38ef6a0], [2, 0x38dd220], [3, 0x7ac6e00], [4, 0x7ab8a80], [5, 0x38dea80], [0, 0x3961ae0], [1, 0x39716c0], [2, 0x395f240], [3, 0x7b48e20], [4, 0x7b3aaa0], [5, 0x3960aa0], [0, 0x39e3b00], [1, 0x39f36e0], [2, 0x39e1260], [3, 0x7bcae40]]}
  ff.bert.encoder.layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7ab4940], [5, 0x38da940], [0, 0x38dda20], [1, 0x38ed600], [2, 0x38db180], [3, 0x7ac4d60], [4, 0x7ab69e0], [5, 0x38dc9e0]]}
  ff.bert.encoder.layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7ab4100]]}
  ff.bert.encoder.layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x38da100]]}
  ff.bert.encoder.layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x399d9a0], [3, 0x7b3e380], [4, 0x7b3e380], [5, 0x3964380], [0, 0x39a53a0], [1, 0x39de9c0], [2, 0x39de9c0], [3, 0x7b7f3a0]]}
  ff.bert.encoder.layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7b7f3a0], [5, 0x39a53a0], [0, 0x39e63c0], [1, 0x3a1f9e0]]}
  ff.bert.encoder.layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3a1f9e0], [3, 0x7bc03c0], [4, 0x7b834c0], [5, 0x39a94c0], [0, 0x39ea4e0], [1, 0x3a23b00], [2, 0x3a60a00], [3, 0x7c013e0]]}
  ff.bert.encoder.layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7bc44e0], [5, 0x39ea4e0], [0, 0x3a2b500], [1, 0x3a64b20]]}
  ff.reciprocal_of_sqrt_of_head_size_11:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7c42400]]}
  ff.bert.encoder.layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3a68c40], [2, 0x3aa2260], [3, 0x7c42c40], [4, 0x7bc8e40], [5, 0x39eee40], [0, 0x3a2fe60], [1, 0x3aa9c60], [2, 0x3ae3280]]}
  ff.bert.encoder.layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7c83c60], [4, 0x7c09e60], [5, 0x3a2fe60], [0, 0x3a70e80]]}
  ff.bert.encoder.layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3aeac80], [2, 0x3b242a0], [3, 0x7c87d80], [4, 0x7c0df80], [5, 0x3a33f80], [0, 0x3a74fa0], [1, 0x3b2bca0], [2, 0x3b652c0]]}
  ff.bert.encoder.layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[3, 0x7cc8da0], [4, 0x7c4efa0], [5, 0x3a74fa0], [0, 0x3ab5fc0]]}
  ff.bert.encoder.layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3be8260]]}
  ff.bert.encoder.layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7d50660]]}
  ff.bert.encoder.layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3a62200], [0, 0x3a62a40], [1, 0x39f26a0], [2, 0x39f26a0], [3, 0x7c4c620], [4, 0x7c4c620], [5, 0x3ae4220], [0, 0x3ae4a60], [1, 0x3a746c0], [2, 0x3a746c0], [3, 0x7cce640], [4, 0x7cce640], [5, 0x3b66240], [0, 0x3b66a80], [1, 0x3af66e0], [2, 0x3af66e0]]}
  ff.bert.encoder.layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x39e2280], [2, 0x39e2280], [3, 0x7c3c200], [4, 0x7c3c200]]}
  ff.bert.encoder.layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x7ab61a0], [4, 0x7ab61a0], [5, 0x38dc1a0], [0, 0x38dc9e0], [1, 0x38de240], [2, 0x38de240], [3, 0x7b381c0], [4, 0x7b381c0], [5, 0x395e1c0], [0, 0x395ea00], [1, 0x3960260], [2, 0x3960260], [3, 0x7bba1e0], [4, 0x7bba1e0], [5, 0x39e01e0], [0, 0x39e0a20]]}
  ff.bert.encoder.layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x38da940], [1, 0x38dc1a0], [2, 0x38dc1a0]]}
  ff.bert.encoder.layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  ff.bert.encoder.layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100]]}

  # constant
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ec94a0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b822e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_11_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3b822e0]]}
  lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d5fbc0]]}
  lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ec9ce0]]}
  lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7d61ac0]]}
  lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3d1efc0]]}
  dc.input_tensor.norm_mha_0.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7ee6300], [5, 0x3d2bac0]]}
  lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7ef8780]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3d0eba0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3d1e780]]}
  lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3d15580]]}
  lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3c71d20]]}
  dc.input_tensor.norm_ff_0.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7f5b800], [4, 0x7e6b4e0]]}
  lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3d71c20]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3d05160]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3c714e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3caa360]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7cdd340]]}
  lc.input_tensor.attention_mask_s_brcst_m2_10_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f0f660]]}
  lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3b03340]]}
  lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3b44360]]}
  lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3cfb800]]}
  lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7d61ac0]]}
  dc.input_tensor.norm_mha_1.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7ee6300], [5, 0x3d2bac0]]}
  lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7ef8780]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3d0eba0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3d1e780]]}
  lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3d15580]]}
  lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3c71d20]]}
  dc.input_tensor.norm_ff_1.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7f5b800], [4, 0x7e6b4e0]]}
  lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3d71c20]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3d05160]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3c714e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3caa360]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7cdd340]]}
  lc.input_tensor.attention_mask_s_brcst_m2_9_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d71d20]]}
  lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3b03340]]}
  lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3b44360]]}
  lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3cfb800]]}
  lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7d61ac0]]}
  dc.input_tensor.norm_mha_2.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7ee6300], [5, 0x3d2bac0]]}
  lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[3, 0x7ef8780]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3d0eba0]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3d1e780]]}
  lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3d15580]]}
  lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3c71d20]]}
  dc.input_tensor.norm_ff_2.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7f5b800], [4, 0x7e6b4e0]]}
  lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3d71c20]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3d05160]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x3c714e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3caa360]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7cdd340]]}
  lc.input_tensor.attention_mask_s_brcst_m2_8_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3c08c80]]}
  lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x3b03340]]}
  lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3b44360]]}
  lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3cfb800]]}
  lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3c8fc40]]}
  dc.input_tensor.norm_mha_3.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x806e400], [5, 0x3eb3bc0]]}
  lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[3, 0x8090460]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3e16d20]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3e96ca0]]}
  lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3bfc7e0]]}
  lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3b79780]]}
  dc.input_tensor.norm_ff_3.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7d42340], [4, 0x7d63360]]}
  lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3b7a7c0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3bfe840]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3b78f40]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3ccfe00]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7e338e0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_7_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f9a120]]}
  lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3dfc7e0]]}
  lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3c93740]]}
  lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x39a1280]]}
  lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3b88300]]}
  dc.input_tensor.norm_mha_4.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3c07240], [1, 0x3b95640]]}
  lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3c06a00]]}
  lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7d4f640]]}
  lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3b87ac0]]}
  lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7dd2ec0]]}
  lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3be92e0]]}
  dc.input_tensor.norm_ff_4.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3b79f80], [1, 0x3b79f80]]}
  lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3a75700]]}
  lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x39e2ac0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3eb2ca0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7df2080]]}
  lc.input_tensor.attention_mask_s_brcst_m2_6_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x39a1280]]}
  lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3c18080]]}
  lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x391b120]]}
  lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x39de180]]}
  lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3b88300]]}
  dc.input_tensor.norm_mha_5.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3c07240], [1, 0x3b95640]]}
  lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3c06a00]]}
  lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7d4f640]]}
  lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3b87ac0]]}
  lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7dd2ec0]]}
  lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3be92e0]]}
  dc.input_tensor.norm_ff_5.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3b79f80], [1, 0x3b79f80]]}
  lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3a75700]]}
  lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x39e2ac0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3eb2ca0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7df2080]]}
  lc.input_tensor.attention_mask_s_brcst_m2_5_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3964380]]}
  lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3c18080]]}
  lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x391b120]]}
  lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x39de180]]}
  lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3b88300]]}
  dc.input_tensor.norm_mha_6.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3c07240], [1, 0x3b95640]]}
  lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[5, 0x3c06a00]]}
  lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7d4f640]]}
  lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3b87ac0]]}
  lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7dd2ec0]]}
  lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x3be92e0]]}
  dc.input_tensor.norm_ff_6.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x3b79f80], [1, 0x3b79f80]]}
  lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[1, 0x3a75700]]}
  lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x39e2ac0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3eb2ca0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7df2080]]}
  lc.input_tensor.attention_mask_s_brcst_m2_4_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7b3e380]]}
  lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x3c18080]]}
  lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x391b120]]}
  lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x39de180]]}
  lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3c90480]]}
  dc.input_tensor.norm_mha_7.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3d7f760], [1, 0x3c9d7c0]]}
  lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[5, 0x3d8eb00]]}
  lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7ed7740]]}
  lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bb9a00]]}
  lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39f1e80]]}
  dc.input_tensor.norm_ff_7.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100]]}
  lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab5180]]}
  lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38ed600]]}
  lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38dda20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7c7f300]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a2bd40]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3a2bd40]]}
  lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x39eee40]]}
  lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7bc8e40]]}
  lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d83bc0]]}
  lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7bc8da0]]}
  dc.input_tensor.norm_mha_8.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3a70580], [3, 0x7bd8980]]}
  lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3a70580]]}
  lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3a70580]]}
  lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7bc8560]]}
  lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7ac4520]]}
  lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x38da940]]}
  dc.input_tensor.norm_ff_8.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x38da940], [1, 0x38ea520]]}
  lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x38da100]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3aa1a20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7bc8600]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ba9bc0]]}
  lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x39ee600]]}
  lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3a2f620]]}
  lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3b6ccc0]]}
  lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7bc8da0]]}
  dc.input_tensor.norm_mha_9.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3a70580], [3, 0x7bd8980]]}
  lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3a70580]]}
  lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3a70580]]}
  lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7bc8560]]}
  lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7ac4520]]}
  lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x38da940]]}
  dc.input_tensor.norm_ff_9.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x38da940], [1, 0x38ea520]]}
  lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x38da100]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3aa1a20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7bc8600]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3ab6800]]}
  lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x39ee600]]}
  lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3a2f620]]}
  lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3b6ccc0]]}
  lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7bc8da0]]}
  dc.input_tensor.norm_mha_10.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[2, 0x3a70580], [3, 0x7bd8980]]}
  lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[1, 0x3a70580]]}
  lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[0, 0x3a70580]]}
  lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 8, loc: dram, dram: [[4, 0x7bc8560]]}
  lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[3, 0x7ac4520]]}
  lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x38da940]]}
  dc.input_tensor.norm_ff_10.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[0, 0x38da940], [1, 0x38ea520]]}
  lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[5, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 9, loc: dram, dram: [[2, 0x38da100]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[2, 0x3aa1a20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[4, 0x7bc8600]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3ab6800]]}
  lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[5, 0x39ee600]]}
  lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x3a2f620]]}
  lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[1, 0x3b6ccc0]]}
  lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7d50ea0]]}
  dc.input_tensor.norm_mha_11.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[2, 0x3b78700], [3, 0x7d60a80]]}
  lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[1, 0x3b78700]]}
  lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[0, 0x3be8aa0]]}
  lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 11, loc: dram, dram: [[4, 0x7d50660]]}
  lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ac4520]]}
  lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4940]]}
  dc.input_tensor.norm_ff_11.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38dd1e0], [2, 0x38ea520]]}
  lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38dd1e0]]}
  lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}

  # epoch_to_epoch
  e2e_norm_ff_3.dc.add.10_0:                                                                    {input: norm_ff_3.dc.add.10, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300c91c0], [0, 0x3012a9e0]]}
  e2e_attention_mask_s_brcst_m2_7_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_7_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3cd0640]]}
  e2e_attention_mask_s_brcst_m2_6_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_6_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_5_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_5_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_4_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_4_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x30000000]]}
  e2e_norm_ff_7.dc.add.10_0:                                                                    {input: norm_ff_7.dc.add.10, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3018c200], [0, 0x301eda20]]}
  e2e_attention_mask_s_brcst_m2_3_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_3_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f9a960]]}
  e2e_attention_mask_s_brcst_m2_2_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_2_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x300061a0]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x300061a0]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 10, loc: dram, dram: [[0, 0x300061a0]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 1
    input_count: 1
    mha_0_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.query.weight, ff.bert.encoder.layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.key.weight, ff.bert.encoder.layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_as: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_0_query, mha_0_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_div: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_0_as, ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    attention_mask_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_11_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_mask: {type: add, grid_loc: [3, 6], grid_size: [2, 1], inputs: [mha_0_as_div, attention_mask_s_brcst_m2_11_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_0_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [4, 1], grid_size: [2, 1], inputs: [mha_0_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_0_as_mask_mha_0_as_softmax.dc.subtract.1: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_0_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_mha_0_as_mask_mha_0_as_softmax.dc.subtract.1, mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_0_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 4], grid_size: [2, 2], inputs: [mha_0_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_0_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.exp.2, lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_0_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 0], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_0_as_softmax.dc.exp.2_mha_0_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_0_mha_0_as_softmax.dc.exp.2_mha_0_as_softmax.dc.multiply.5, mha_0_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_0_value: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.value.weight, ff.bert.encoder.layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_ac: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.multiply.5, mha_0_value],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_0_output: {type: matmul, grid_loc: [7, 3], grid_size: [2, 4], inputs: [mha_0_ac, ff.bert.encoder.layer.0.attention.output.dense.weight, ff.bert.encoder.layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_0: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [encoder_input, mha_0_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_10_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_9_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_8_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_7_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_6_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_5_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_4_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_1_temporal_epoch_0:
    target_device: 2
    input_count: 1
    norm_mha_0.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [add_mha_0, lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_0_norm_mha_0.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_mha_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_0_add_mha_0_norm_mha_0.dc.subtract.1, norm_mha_0.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_mha_0.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_0.dc.subtract.1, norm_mha_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.2, lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [norm_mha_0.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_0.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [norm_mha_0.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_0.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_0.dc.reciprocal.7, lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8, norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.8, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.9, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_0_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_0.dc.add.10, ff.bert.encoder.layer.0.intermediate.dense.weight, ff.bert.encoder.layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff0_gelu: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [ff_0_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_0.dc.add.10_add_ff_0: {type: nop, grid_loc: [3, 4], grid_size: [2, 1], inputs: [norm_mha_0.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2_temporal_epoch_0:
    target_device: 3
    input_count: 1
    ff_0_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff0_gelu, ff.bert.encoder.layer.0.output.dense.weight, ff.bert.encoder.layer.0.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_0: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_0.dc.add.10_add_ff_0, ff_0_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_0, lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_0_norm_ff_0.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_0_norm_ff_0.dc.subtract.1, norm_ff_0.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_0.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_0.dc.subtract.1, norm_ff_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.2, lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_0.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_0.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_0.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_0.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_0.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_0.dc.reciprocal.7, lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8, norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_0.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.8, ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_0.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.9, ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_3_temporal_epoch_0:
    target_device: 4
    input_count: 1
    mha_1_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.query.weight, ff.bert.encoder.layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_1_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.key.weight, ff.bert.encoder.layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_1_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_1_query, mha_1_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_1],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_1_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_1_as, ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_1_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_1_as_div, attention_mask_s_brcst_m2_10_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_1_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_1_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_1_as_mask_mha_1_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_1_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_1_as_mask_mha_1_as_softmax.dc.subtract.1, mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_1_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_1_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_1_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.exp.2, lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_1_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_1_as_softmax.dc.exp.2_mha_1_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_1_as_softmax.dc.exp.2_mha_1_as_softmax.dc.multiply.5, mha_1_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_1_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.value.weight, ff.bert.encoder.layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_1_value_mha_1_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_1_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_1_value_mha_1_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_1_value_mha_1_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.multiply.5, buffer_0_mha_1_value_mha_1_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_1_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_1_ac, ff.bert.encoder.layer.1.attention.output.dense.weight, ff.bert.encoder.layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_0.dc.add.10_add_mha_1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_0.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_1: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_0.dc.add.10_add_mha_1, mha_1_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_1, lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_1_norm_mha_1.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_1_norm_mha_1.dc.subtract.1, norm_mha_1.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_4_temporal_epoch_0:
    target_device: 5
    input_count: 1
    norm_mha_1.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_1.dc.subtract.1, norm_mha_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.2, lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_1.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_1.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_1.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_1.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_1.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_1.dc.reciprocal.7, lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8, norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.8, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.9, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_1_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_1.dc.add.10, ff.bert.encoder.layer.1.intermediate.dense.weight, ff.bert.encoder.layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff1_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_1_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_1.dc.add.10_add_ff_1: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_1.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_5_temporal_epoch_0:
    target_device: 6
    input_count: 1
    ff_1_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff1_gelu, ff.bert.encoder.layer.1.output.dense.weight, ff.bert.encoder.layer.1.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_1: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_1.dc.add.10_add_ff_1, ff_1_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_1, lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_1_norm_ff_1.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_1_norm_ff_1.dc.subtract.1, norm_ff_1.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_1.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_1.dc.subtract.1, norm_ff_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.2, lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_1.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_1.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_1.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_1.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_1.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_1.dc.reciprocal.7, lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8, norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.8, ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.9, ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_6_temporal_epoch_0:
    target_device: 7
    input_count: 1
    mha_2_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.query.weight, ff.bert.encoder.layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_2_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.key.weight, ff.bert.encoder.layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_2_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_2_query, mha_2_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_2],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_2_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_2_as, ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_2_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_2_as_div, attention_mask_s_brcst_m2_9_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_2_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_2_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_2_as_mask_mha_2_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_2_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_2_as_mask_mha_2_as_softmax.dc.subtract.1, mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_2_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_2_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_2_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.exp.2, lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_2_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_2_as_softmax.dc.exp.2_mha_2_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_2_as_softmax.dc.exp.2_mha_2_as_softmax.dc.multiply.5, mha_2_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_2_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.value.weight, ff.bert.encoder.layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_2_value_mha_2_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_2_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_2_value_mha_2_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_2_value_mha_2_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.multiply.5, buffer_0_mha_2_value_mha_2_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_2_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_2_ac, ff.bert.encoder.layer.2.attention.output.dense.weight, ff.bert.encoder.layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_1.dc.add.10_add_mha_2: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_1.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_2: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_1.dc.add.10_add_mha_2, mha_2_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_2, lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_2_norm_mha_2.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_2_norm_mha_2.dc.subtract.1, norm_mha_2.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_7_temporal_epoch_0:
    target_device: 8
    input_count: 1
    norm_mha_2.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_2.dc.subtract.1, norm_mha_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.2, lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_2.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_2.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_2.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_2.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_2.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_2.dc.reciprocal.7, lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8, norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.8, ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.9, ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_2_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_2.dc.add.10, ff.bert.encoder.layer.2.intermediate.dense.weight, ff.bert.encoder.layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff2_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_2_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_2.dc.add.10_add_ff_2: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_2.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_8_temporal_epoch_0:
    target_device: 9
    input_count: 1
    ff_2_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff2_gelu, ff.bert.encoder.layer.2.output.dense.weight, ff.bert.encoder.layer.2.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_2: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_2.dc.add.10_add_ff_2, ff_2_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_2, lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_2_norm_ff_2.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_2_norm_ff_2.dc.subtract.1, norm_ff_2.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_2.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_2.dc.subtract.1, norm_ff_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.2, lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_2.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_2.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_2.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_2.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_2.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_2.dc.reciprocal.7, lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8, norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_2.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.8, ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_2.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.9, ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_9_temporal_epoch_0:
    target_device: 10
    input_count: 1
    mha_3_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.query.weight, ff.bert.encoder.layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_3_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.key.weight, ff.bert.encoder.layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_3_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_3_query, mha_3_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_3],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_3_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_3_as, ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_3_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_3_as_div, attention_mask_s_brcst_m2_8_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_3_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_3_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_3_as_mask_mha_3_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_3_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_3_as_mask_mha_3_as_softmax.dc.subtract.1, mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_3_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_3_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_3_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.exp.2, lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_3_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_3_as_softmax.dc.exp.2_mha_3_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_3_as_softmax.dc.exp.2_mha_3_as_softmax.dc.multiply.5, mha_3_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_3_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.value.weight, ff.bert.encoder.layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_3_value_mha_3_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_3_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_3_value_mha_3_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_3_value_mha_3_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.multiply.5, buffer_0_mha_3_value_mha_3_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_3_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_3_ac, ff.bert.encoder.layer.3.attention.output.dense.weight, ff.bert.encoder.layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_2.dc.add.10_add_mha_3: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_2.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_3: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_2.dc.add.10_add_mha_3, mha_3_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_3, lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_3_norm_mha_3.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_3_norm_mha_3.dc.subtract.1, norm_mha_3.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_10_temporal_epoch_0:
    target_device: 11
    input_count: 1
    norm_mha_3.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_3.dc.subtract.1, norm_mha_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.2, lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_3.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_3.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_3.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_3.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_3.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_3.dc.reciprocal.7, lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8, norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.8, ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.9, ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_3_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_3.dc.add.10, ff.bert.encoder.layer.3.intermediate.dense.weight, ff.bert.encoder.layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff3_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_3_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_3.dc.add.10_add_ff_3: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_3.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_11_temporal_epoch_0:
    target_device: 0
    input_count: 1
    ff_3_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff3_gelu, ff.bert.encoder.layer.3.output.dense.weight, ff.bert.encoder.layer.3.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_3: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_3.dc.add.10_add_ff_3, ff_3_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_3, lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_3_norm_ff_3.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_3_norm_ff_3.dc.subtract.1, norm_ff_3.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_3.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_3.dc.subtract.1, norm_ff_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.2, lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_3.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_3.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_3.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_3.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_3.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_3.dc.reciprocal.7, lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8, norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_3.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.8, ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_3.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.9, ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_12_temporal_epoch_1:
    target_device: 1
    input_count: 1
    mha_4_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_norm_ff_3.dc.add.10_0, ff.bert.encoder.layer.4.attention.self.query.weight, ff.bert.encoder.layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_4_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e_norm_ff_3.dc.add.10_0, ff.bert.encoder.layer.4.attention.self.key.weight, ff.bert.encoder.layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_4_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_4_query, mha_4_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_4],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_4_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_4_as, ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_4_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_4_as_div, e2e_attention_mask_s_brcst_m2_7_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_4_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_4_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_4_as_mask_mha_4_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_4_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_4_as_mask_mha_4_as_softmax.dc.subtract.1, mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_4_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_4_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_4_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.exp.2, lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_4_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_4_as_softmax.dc.exp.2_mha_4_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_4_as_softmax.dc.exp.2_mha_4_as_softmax.dc.multiply.5, mha_4_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_4_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [e2e_norm_ff_3.dc.add.10_0, ff.bert.encoder.layer.4.attention.self.value.weight, ff.bert.encoder.layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_4_value_mha_4_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_4_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_4_value_mha_4_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_4_value_mha_4_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.multiply.5, buffer_0_mha_4_value_mha_4_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_4_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_4_ac, ff.bert.encoder.layer.4.attention.output.dense.weight, ff.bert.encoder.layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_3.dc.add.10_add_mha_4: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_norm_ff_3.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_4: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_3.dc.add.10_add_mha_4, mha_4_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_4, lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_4_norm_mha_4.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_4_norm_mha_4.dc.subtract.1, norm_mha_4.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_13_temporal_epoch_1:
    target_device: 2
    input_count: 1
    norm_mha_4.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_4.dc.subtract.1, norm_mha_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.2, lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_4.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_4.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_4.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_4.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_4.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_4.dc.reciprocal.7, lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8, norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_4.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.8, ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_4.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.9, ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_4_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_4.dc.add.10, ff.bert.encoder.layer.4.intermediate.dense.weight, ff.bert.encoder.layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff4_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_4_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_4.dc.add.10_add_ff_4: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_4.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_14_temporal_epoch_1:
    target_device: 3
    input_count: 1
    ff_4_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff4_gelu, ff.bert.encoder.layer.4.output.dense.weight, ff.bert.encoder.layer.4.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_4: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_4.dc.add.10_add_ff_4, ff_4_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_4, lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_4_norm_ff_4.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_4_norm_ff_4.dc.subtract.1, norm_ff_4.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_4.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_4.dc.subtract.1, norm_ff_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.2, lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_4.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_4.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_4.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_4.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_4.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_4.dc.reciprocal.7, lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8, norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_4.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.8, ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_4.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.9, ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_15_temporal_epoch_1:
    target_device: 4
    input_count: 1
    mha_5_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.query.weight, ff.bert.encoder.layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_5_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.key.weight, ff.bert.encoder.layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_5_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_5_query, mha_5_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_5],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_5_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_5_as, ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_5_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_5_as_div, e2e_attention_mask_s_brcst_m2_6_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_5_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_5_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_5_as_mask_mha_5_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_5_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_5_as_mask_mha_5_as_softmax.dc.subtract.1, mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_5_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_5_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_5_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.exp.2, lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_5_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_5_as_softmax.dc.exp.2_mha_5_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_5_as_softmax.dc.exp.2_mha_5_as_softmax.dc.multiply.5, mha_5_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_5_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.value.weight, ff.bert.encoder.layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_5_value_mha_5_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_5_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_5_value_mha_5_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_5_value_mha_5_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.multiply.5, buffer_0_mha_5_value_mha_5_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_5_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_5_ac, ff.bert.encoder.layer.5.attention.output.dense.weight, ff.bert.encoder.layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_4.dc.add.10_add_mha_5: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_4.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_5: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_4.dc.add.10_add_mha_5, mha_5_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_5, lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_5_norm_mha_5.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_5_norm_mha_5.dc.subtract.1, norm_mha_5.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_16_temporal_epoch_1:
    target_device: 5
    input_count: 1
    norm_mha_5.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_5.dc.subtract.1, norm_mha_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.2, lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_5.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_5.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_5.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_5.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_5.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_5.dc.reciprocal.7, lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8, norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_5.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.8, ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_5.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.9, ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_5_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_5.dc.add.10, ff.bert.encoder.layer.5.intermediate.dense.weight, ff.bert.encoder.layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff5_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_5_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_5.dc.add.10_add_ff_5: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_5.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_17_temporal_epoch_1:
    target_device: 6
    input_count: 1
    ff_5_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff5_gelu, ff.bert.encoder.layer.5.output.dense.weight, ff.bert.encoder.layer.5.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_5.dc.add.10_add_ff_5, ff_5_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_5, lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_5_norm_ff_5.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_5_norm_ff_5.dc.subtract.1, norm_ff_5.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_5.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_5.dc.subtract.1, norm_ff_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.2, lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_5.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_5.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_5.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_5.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_5.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_5.dc.reciprocal.7, lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8, norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_5.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.8, ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_5.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.9, ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_18_temporal_epoch_1:
    target_device: 7
    input_count: 1
    mha_6_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.query.weight, ff.bert.encoder.layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_6_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.key.weight, ff.bert.encoder.layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_6_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_6_query, mha_6_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_6],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_6_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_6_as, ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_6_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_6_as_div, e2e_attention_mask_s_brcst_m2_5_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_6_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_6_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_6_as_mask_mha_6_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_6_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_6_as_mask_mha_6_as_softmax.dc.subtract.1, mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_6_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_6_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_6_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.exp.2, lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_6_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_6_as_softmax.dc.exp.2_mha_6_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_6_as_softmax.dc.exp.2_mha_6_as_softmax.dc.multiply.5, mha_6_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_6_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.value.weight, ff.bert.encoder.layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_6_value_mha_6_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_6_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_6_value_mha_6_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_6_value_mha_6_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.multiply.5, buffer_0_mha_6_value_mha_6_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_6_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_6_ac, ff.bert.encoder.layer.6.attention.output.dense.weight, ff.bert.encoder.layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_5.dc.add.10_add_mha_6: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_5.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_6: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_5.dc.add.10_add_mha_6, mha_6_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_6, lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_6_norm_mha_6.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_6_norm_mha_6.dc.subtract.1, norm_mha_6.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_19_temporal_epoch_1:
    target_device: 8
    input_count: 1
    norm_mha_6.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_6.dc.subtract.1, norm_mha_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.2, lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_6.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_6.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_6.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_6.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_6.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_6.dc.reciprocal.7, lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8, norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_6.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.8, ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_6.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.9, ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_6_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_6.dc.add.10, ff.bert.encoder.layer.6.intermediate.dense.weight, ff.bert.encoder.layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff6_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_6_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_6.dc.add.10_add_ff_6: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_6.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_20_temporal_epoch_1:
    target_device: 9
    input_count: 1
    ff_6_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff6_gelu, ff.bert.encoder.layer.6.output.dense.weight, ff.bert.encoder.layer.6.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_6: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_6.dc.add.10_add_ff_6, ff_6_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_6, lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_6_norm_ff_6.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_6_norm_ff_6.dc.subtract.1, norm_ff_6.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_6.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_6.dc.subtract.1, norm_ff_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.2, lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_6.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_6.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_6.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_6.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_6.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_6.dc.reciprocal.7, lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8, norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_6.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.8, ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_6.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.9, ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_21_temporal_epoch_1:
    target_device: 10
    input_count: 1
    mha_7_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.query.weight, ff.bert.encoder.layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_7_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.key.weight, ff.bert.encoder.layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_7_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_7_query, mha_7_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_7],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_7_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_7_as, ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_7_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_7_as_div, e2e_attention_mask_s_brcst_m2_4_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_7_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_7_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_7_as_mask_mha_7_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_7_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_7_as_mask_mha_7_as_softmax.dc.subtract.1, mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_7_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_7_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_7_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.exp.2, lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_7_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_7_as_softmax.dc.exp.2_mha_7_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_7_as_softmax.dc.exp.2_mha_7_as_softmax.dc.multiply.5, mha_7_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_7_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.value.weight, ff.bert.encoder.layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_7_value_mha_7_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_7_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_7_value_mha_7_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_7_value_mha_7_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.multiply.5, buffer_0_mha_7_value_mha_7_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_7_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_7_ac, ff.bert.encoder.layer.7.attention.output.dense.weight, ff.bert.encoder.layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_6.dc.add.10_add_mha_7: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_6.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_7: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_6.dc.add.10_add_mha_7, mha_7_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_7, lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_7_norm_mha_7.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_7_norm_mha_7.dc.subtract.1, norm_mha_7.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_22_temporal_epoch_1:
    target_device: 11
    input_count: 1
    norm_mha_7.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_7.dc.subtract.1, norm_mha_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.2, lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_7.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_7.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_7.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_7.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_7.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_7.dc.reciprocal.7, lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8, norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_7.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.8, ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_7.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.9, ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_7_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_7.dc.add.10, ff.bert.encoder.layer.7.intermediate.dense.weight, ff.bert.encoder.layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff7_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_7_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_7.dc.add.10_add_ff_7: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_7.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_23_temporal_epoch_1:
    target_device: 0
    input_count: 1
    ff_7_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff7_gelu, ff.bert.encoder.layer.7.output.dense.weight, ff.bert.encoder.layer.7.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_7: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_7.dc.add.10_add_ff_7, ff_7_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_7, lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_7_norm_ff_7.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_7_norm_ff_7.dc.subtract.1, norm_ff_7.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_7.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_7.dc.subtract.1, norm_ff_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.2, lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_7.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_7.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_7.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_7.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_7.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_7.dc.reciprocal.7, lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8, norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_7.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.8, ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_7.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.9, ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_24_temporal_epoch_2:
    target_device: 1
    input_count: 1
    mha_8_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_norm_ff_7.dc.add.10_0, ff.bert.encoder.layer.8.attention.self.query.weight, ff.bert.encoder.layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_8_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e_norm_ff_7.dc.add.10_0, ff.bert.encoder.layer.8.attention.self.key.weight, ff.bert.encoder.layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_8_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_8_query, mha_8_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_8],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_8_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_8_as, ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_8_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_8_as_div, e2e_attention_mask_s_brcst_m2_3_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_8_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_8_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_8_as_mask_mha_8_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_8_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_8_as_mask_mha_8_as_softmax.dc.subtract.1, mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_8_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_8_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_8_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.exp.2, lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_8_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_8_as_softmax.dc.exp.2_mha_8_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_8_as_softmax.dc.exp.2_mha_8_as_softmax.dc.multiply.5, mha_8_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_8_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [e2e_norm_ff_7.dc.add.10_0, ff.bert.encoder.layer.8.attention.self.value.weight, ff.bert.encoder.layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_8_value_mha_8_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_8_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_8_value_mha_8_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_8_value_mha_8_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.multiply.5, buffer_0_mha_8_value_mha_8_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_8_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_8_ac, ff.bert.encoder.layer.8.attention.output.dense.weight, ff.bert.encoder.layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_7.dc.add.10_add_mha_8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_norm_ff_7.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_8: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_7.dc.add.10_add_mha_8, mha_8_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_8, lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_8_norm_mha_8.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_8_norm_mha_8.dc.subtract.1, norm_mha_8.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_25_temporal_epoch_2:
    target_device: 2
    input_count: 1
    norm_mha_8.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_8.dc.subtract.1, norm_mha_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.2, lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_8.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_8.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_8.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_8.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_8.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_8.dc.reciprocal.7, lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8, norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_8.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.8, ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_8.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.9, ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_8_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_8.dc.add.10, ff.bert.encoder.layer.8.intermediate.dense.weight, ff.bert.encoder.layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff8_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_8_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_8.dc.add.10_add_ff_8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_8.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_26_temporal_epoch_2:
    target_device: 3
    input_count: 1
    ff_8_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff8_gelu, ff.bert.encoder.layer.8.output.dense.weight, ff.bert.encoder.layer.8.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_8: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_8.dc.add.10_add_ff_8, ff_8_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_8, lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_8_norm_ff_8.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_8_norm_ff_8.dc.subtract.1, norm_ff_8.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_8.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_8.dc.subtract.1, norm_ff_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.2, lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_8.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_8.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_8.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_8.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_8.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_8.dc.reciprocal.7, lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8, norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_8.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.8, ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_8.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.9, ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_27_temporal_epoch_2:
    target_device: 4
    input_count: 1
    mha_9_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.query.weight, ff.bert.encoder.layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_9_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.key.weight, ff.bert.encoder.layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_9_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_9_query, mha_9_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_9],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_9_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_9_as, ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_9_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_9_as_div, e2e_attention_mask_s_brcst_m2_2_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_9_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_9_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_9_as_mask_mha_9_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_9_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_9_as_mask_mha_9_as_softmax.dc.subtract.1, mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_9_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_9_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_9_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.exp.2, lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_9_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_9_as_softmax.dc.exp.2_mha_9_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_9_as_softmax.dc.exp.2_mha_9_as_softmax.dc.multiply.5, mha_9_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_9_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.value.weight, ff.bert.encoder.layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_9_value_mha_9_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_9_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_9_value_mha_9_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_9_value_mha_9_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.multiply.5, buffer_0_mha_9_value_mha_9_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_9_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_9_ac, ff.bert.encoder.layer.9.attention.output.dense.weight, ff.bert.encoder.layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_8.dc.add.10_add_mha_9: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_8.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_9: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_8.dc.add.10_add_mha_9, mha_9_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_9, lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_9_norm_mha_9.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_9],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_9_norm_mha_9.dc.subtract.1, norm_mha_9.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_28_temporal_epoch_2:
    target_device: 5
    input_count: 1
    norm_mha_9.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_9.dc.subtract.1, norm_mha_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.2, lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_9.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_9.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_9.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_9.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_9.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_9.dc.reciprocal.7, lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8, norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_9.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.8, ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_9.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.9, ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_9_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_9.dc.add.10, ff.bert.encoder.layer.9.intermediate.dense.weight, ff.bert.encoder.layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff9_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_9_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_9.dc.add.10_add_ff_9: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_9.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_29_temporal_epoch_2:
    target_device: 6
    input_count: 1
    ff_9_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff9_gelu, ff.bert.encoder.layer.9.output.dense.weight, ff.bert.encoder.layer.9.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_9: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_9.dc.add.10_add_ff_9, ff_9_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_9, lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_9_norm_ff_9.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_9],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_9_norm_ff_9.dc.subtract.1, norm_ff_9.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_9.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_9.dc.subtract.1, norm_ff_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.2, lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_9.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_9.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_9.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_9.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_9.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_9.dc.reciprocal.7, lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8, norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_9.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.8, ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_9.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.9, ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_30_temporal_epoch_2:
    target_device: 7
    input_count: 1
    mha_10_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.query.weight, ff.bert.encoder.layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_10_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.key.weight, ff.bert.encoder.layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_10_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_10_query, mha_10_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_10],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_10_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_10_as, ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_10_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_10_as_div, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_10_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_10_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_10_as_mask_mha_10_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_10_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_10_as_mask_mha_10_as_softmax.dc.subtract.1, mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_10_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_10_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_10_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.exp.2, lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_10_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_10_as_softmax.dc.exp.2_mha_10_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_10_as_softmax.dc.exp.2_mha_10_as_softmax.dc.multiply.5, mha_10_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_10_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.value.weight, ff.bert.encoder.layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_10_value_mha_10_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_10_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_10_value_mha_10_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_10_value_mha_10_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.multiply.5, buffer_0_mha_10_value_mha_10_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_10_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_10_ac, ff.bert.encoder.layer.10.attention.output.dense.weight, ff.bert.encoder.layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_9.dc.add.10_add_mha_10: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_9.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_10: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_9.dc.add.10_add_mha_10, mha_10_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_10, lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_10_norm_mha_10.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_10_norm_mha_10.dc.subtract.1, norm_mha_10.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_31_temporal_epoch_2:
    target_device: 8
    input_count: 1
    norm_mha_10.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_10.dc.subtract.1, norm_mha_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.2, lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_10.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_10.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_10.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_10.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_10.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_10.dc.reciprocal.7, lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8, norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_10.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.8, ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_10.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.9, ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_10_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_10.dc.add.10, ff.bert.encoder.layer.10.intermediate.dense.weight, ff.bert.encoder.layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff10_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_10_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_10.dc.add.10_add_ff_10: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_10.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_32_temporal_epoch_2:
    target_device: 9
    input_count: 1
    ff_10_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff10_gelu, ff.bert.encoder.layer.10.output.dense.weight, ff.bert.encoder.layer.10.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_10: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_10.dc.add.10_add_ff_10, ff_10_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_10, lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_10_norm_ff_10.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_10_norm_ff_10.dc.subtract.1, norm_ff_10.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_10.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_10.dc.subtract.1, norm_ff_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.2, lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_10.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_10.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_10.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_10.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_10.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_10.dc.reciprocal.7, lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8, norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_10.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.8, ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_10.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.9, ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_33_temporal_epoch_2:
    target_device: 10
    input_count: 1
    mha_11_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.query.weight, ff.bert.encoder.layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_11_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.key.weight, ff.bert.encoder.layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_11_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_11_query, mha_11_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_11],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_11_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_11_as, ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_11_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_11_as_div, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_11_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_11_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_11_as_mask_mha_11_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_11_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_11_as_mask_mha_11_as_softmax.dc.subtract.1, mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_11_as_softmax.dc.exp.2: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [mha_11_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_11_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.exp.2, lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_11_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_11_as_softmax.dc.exp.2_mha_11_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_mha_11_as_softmax.dc.exp.2_mha_11_as_softmax.dc.multiply.5, mha_11_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_11_value: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.value.weight, ff.bert.encoder.layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_11_value_mha_11_ac: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_11_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_11_value_mha_11_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_11_value_mha_11_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.multiply.5, buffer_0_mha_11_value_mha_11_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_11_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_11_ac, ff.bert.encoder.layer.11.attention.output.dense.weight, ff.bert.encoder.layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_10.dc.add.10_add_mha_11: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_10.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_11: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_10.dc.add.10_add_mha_11, mha_11_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_11, lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_11_norm_mha_11.dc.subtract.1: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_11_norm_mha_11.dc.subtract.1, norm_mha_11.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_mha_11.dc.multiply.2: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_11.dc.subtract.1, norm_mha_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_34_temporal_epoch_2:
    target_device: 11
    input_count: 1
    norm_mha_11.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.2, lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_11.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_11.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_11.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_11.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_11.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_11.dc.reciprocal.7, lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8, norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_11.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.8, ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_11.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.9, ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_11_ff1: {type: matmul, grid_loc: [2, 4], grid_size: [6, 4], inputs: [norm_mha_11.dc.add.10, ff.bert.encoder.layer.11.intermediate.dense.weight, ff.bert.encoder.layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff11_gelu: {type: gelu, grid_loc: [5, 0], grid_size: [2, 4], inputs: [ff_11_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    ff_11_ff2: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [ff11_gelu, ff.bert.encoder.layer.11.output.dense.weight, ff.bert.encoder.layer.11.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_norm_mha_11.dc.add.10_add_ff_11: {type: nop, grid_loc: [3, 0], grid_size: [2, 1], inputs: [norm_mha_11.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_ff_11: {type: add, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_norm_mha_11.dc.add.10_add_ff_11, ff_11_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_35_temporal_epoch_2:
    target_device: 0
    input_count: 1
    norm_ff_11.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [add_ff_11, lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_11_norm_ff_11.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_ff_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_0_add_ff_11_norm_ff_11.dc.subtract.1, norm_ff_11.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_11.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_ff_11.dc.subtract.1, norm_ff_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.2, lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_11.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [norm_ff_11.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_11.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [norm_ff_11.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_ff_11.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_ff_11.dc.reciprocal.7, lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_ff_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_1_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8, norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_11.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.8, ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_11.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.9, ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    norm_ff_11.dc.add.10_output_nop_0: {type: nop, grid_loc: [3, 4], grid_size: [2, 1], inputs: [norm_ff_11.dc.add.10], untilize_output: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q2: 0, $lptr_q2: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e_norm_ff_3.dc.add.10_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {
               encoder_input: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               ff.bert.encoder.layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_0: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_11_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_10_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_9_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_8_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_7_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_6_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_5_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_4_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_1_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_0.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_2_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_0.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_3_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_1: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_4_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_1.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_5_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_1.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_6_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_2: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_7_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_2.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_8_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_2.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_9_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_3: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_10_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_3.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_11_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_3.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_norm_ff_7.dc.add.10_0]
    -   execute: {graph_name: fwd_12_temporal_epoch_1, queue_settings: {
               e2e_norm_ff_3.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_attention_mask_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_4: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_13_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_4.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_14_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_4.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_15_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_5: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_16_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_5.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_17_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_5.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_18_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_6: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_19_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_6.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_20_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_6.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_21_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_7: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_22_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_7.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_23_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_7.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_norm_ff_3.dc.add.10_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_24_temporal_epoch_2, queue_settings: {
               e2e_norm_ff_7.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_attention_mask_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_8: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_25_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_8.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_26_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_8.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_27_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_9: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_28_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_9.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_29_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_9.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_30_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_10: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_31_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_10.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_32_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_10.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_33_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_11: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_34_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_11.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_35_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_11.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_norm_ff_7.dc.add.10_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    - endloop


