# git checkout ea96a530
# pytest pybuda/test/backend/models/test_bert.py::test_multichip_wormhole_multi_encoder_split_concurrent[inference-Golden-chip8-enc12-large]

devices:
  arch: [wormhole, wormhole_b0]

queues:

  # input
  encoder_input:                                                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300061a0]]}
  attention_mask:                                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 12], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30000000]]}

  # output
  encoder11.output_norm_ff_11:                                                                  {input: norm_ff_11.dc.add.10_output_nop_0, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 8], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  ff.bert.encoder.layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3b74e60], [2, 0x3b44260], [3, 0x7d06480], [4, 0x7d06480], [5, 0x3b3c060], [0, 0x3b7d080], [1, 0x3bb5e80], [2, 0x3b85280]]}
  ff.bert.encoder.layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e19720], [1, 0x3ef5540], [2, 0x3e67a40], [3, 0x8063ac0]]}
  ff.bert.encoder.layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8021220], [5, 0x3d69380], [0, 0x3dd8700], [1, 0x3eb4520], [2, 0x3e26a20], [3, 0x8022aa0], [4, 0x8062240], [5, 0x3daa3a0]]}
  ff.bert.encoder.layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3dd45e0], [1, 0x3eb0400], [2, 0x3e22900], [3, 0x801e980]]}
  ff.reciprocal_of_sqrt_of_head_size_0:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x80209e0]]}
  ff.bert.encoder.layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f9c100], [4, 0x7f9e9a0], [5, 0x3d272e0], [0, 0x3d92d80], [1, 0x3e5efc0], [2, 0x3de10a0], [3, 0x7fdd120], [4, 0x7fdf9c0]]}
  ff.bert.encoder.layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d231c0], [0, 0x3d8ec60], [1, 0x3e5aea0], [2, 0x3ddcf80]]}
  ff.bert.encoder.layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f1a0c0], [4, 0x7f1c960], [5, 0x3ce21a0], [0, 0x3d4dc40], [1, 0x3e19e80], [2, 0x3d9bf60], [3, 0x7f5b0e0], [4, 0x7f5d980]]}
  ff.bert.encoder.layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3cde080], [0, 0x3d49b20], [1, 0x3e15d60], [2, 0x3d97e40]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7e44360]]}
  ff.bert.encoder.layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3b07400]]}
  ff.bert.encoder.layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3b1a0c0], [1, 0x3b89420], [2, 0x3c10560], [3, 0x7e54780], [4, 0x7dea560], [5, 0x3b17820], [0, 0x3b9c0e0], [1, 0x3c0b440], [2, 0x3c92580], [3, 0x7ed67a0], [4, 0x7e6c580], [5, 0x3b99840], [0, 0x3c1e100], [1, 0x3c8d460], [2, 0x3d145a0], [3, 0x7f587c0]]}
  ff.bert.encoder.layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7eee5a0], [5, 0x3c1b860], [0, 0x3ca0120], [1, 0x3d0f480]]}
  ff.bert.encoder.layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3deaae0], [3, 0x7febc80], [4, 0x7fff960], [5, 0x3d945a0], [0, 0x3d70d00], [1, 0x3d8c3c0], [2, 0x3e6cb00], [3, 0x806dca0], [4, 0x8081980], [5, 0x3e165c0], [0, 0x3df2d20], [1, 0x3e0e3e0], [2, 0x3eeeb20], [3, 0x80efcc0], [4, 0x81039a0], [5, 0x3e985e0]]}
  ff.bert.encoder.layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7ff7f00], [5, 0x3d8cb40], [0, 0x3d5d7c0], [1, 0x3d88a60], [2, 0x3dd5540], [3, 0x7fe62c0], [4, 0x7ff9fa0], [5, 0x3d8ebe0]]}
  ff.bert.encoder.layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3d600a0]]}
  ff.bert.encoder.layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3dda6c0]]}
  ff.bert.encoder.layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3c232a0], [2, 0x3c55700], [3, 0x7ea9500], [4, 0x7f17000], [5, 0x3d504c0], [0, 0x3c915e0], [1, 0x3c642c0], [2, 0x3c96720]]}
  ff.bert.encoder.layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3b85b00], [3, 0x7dd9900], [4, 0x7e47400], [5, 0x3c031e0]]}
  ff.bert.encoder.layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3b0ace0], [1, 0x3b1a8c0], [2, 0x3b89c20], [3, 0x7ddda20], [4, 0x7e4b520], [5, 0x3c07300], [0, 0x3b4bd00], [1, 0x3b5b8e0]]}
  ff.bert.encoder.layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3bcac40], [3, 0x7e1ea40], [4, 0x7e8c540], [5, 0x3c48320]]}
  ff.reciprocal_of_sqrt_of_head_size_1:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3b9c900]]}
  ff.bert.encoder.layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3c4c440], [0, 0x3b8d560], [1, 0x3b9d140], [2, 0x3bcf5a0], [3, 0x7e233a0], [4, 0x7e90ea0], [5, 0x3c8d460], [0, 0x3bce580]]}
  ff.bert.encoder.layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3bde160], [2, 0x3c105c0], [3, 0x7e643c0], [4, 0x7ed1ec0]]}
  ff.bert.encoder.layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3cce480], [0, 0x3c0f5a0], [1, 0x3be2280], [2, 0x3c146e0], [3, 0x7e684e0], [4, 0x7ed5fe0], [5, 0x3d0f4a0], [0, 0x3c505c0]]}
  ff.bert.encoder.layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7e432e0], [5, 0x3bff0c0], [0, 0x3b06bc0], [1, 0x3b167a0]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3c6c3a0]]}
  ff.bert.encoder.layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3bc29e0]]}
  ff.bert.encoder.layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7d2c640], [4, 0x7d9b9a0], [5, 0x3c4b380], [0, 0x3c7c7c0], [1, 0x3c92520], [2, 0x3bd2e00], [3, 0x7dae660], [4, 0x7e1d9c0], [5, 0x3ccd3a0], [0, 0x3cfe7e0], [1, 0x3d14540], [2, 0x3c54e20], [3, 0x7e30680], [4, 0x7e9f9e0], [5, 0x3d4f3c0], [0, 0x3d80800]]}
  ff.bert.encoder.layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3d96560], [2, 0x3cd6e40], [3, 0x7eb26a0], [4, 0x7f21a00]]}
  ff.bert.encoder.layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7cc9e20], [4, 0x7d59980], [5, 0x3b00a40], [0, 0x3b03b20], [1, 0x3b0c540], [2, 0x3a7a140], [3, 0x7d4be40], [4, 0x7ddb9a0], [5, 0x3b82a60], [0, 0x3b85b40], [1, 0x3b8e560], [2, 0x3afc160], [3, 0x7dcde60], [4, 0x7e5d9c0], [5, 0x3c04a80], [0, 0x3c07b60]]}
  ff.bert.encoder.layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7cc54a0], [4, 0x7d55000], [5, 0x3aedd40], [0, 0x3b00a00], [1, 0x3af6fa0], [2, 0x3a74780], [3, 0x7cc7540], [4, 0x7d570a0]]}
  ff.bert.encoder.layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3af0620]]}
  ff.bert.encoder.layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3afc120]]}
  ff.bert.encoder.layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3c232a0], [2, 0x3c55700], [3, 0x7ea9500], [4, 0x7f17000], [5, 0x3d504c0], [0, 0x3c915e0], [1, 0x3c642c0], [2, 0x3c96720]]}
  ff.bert.encoder.layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3b85b00], [3, 0x7dd9900], [4, 0x7e47400], [5, 0x3c031e0]]}
  ff.bert.encoder.layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3b0ace0], [1, 0x3b1a8c0], [2, 0x3b89c20], [3, 0x7ddda20], [4, 0x7e4b520], [5, 0x3c07300], [0, 0x3b4bd00], [1, 0x3b5b8e0]]}
  ff.bert.encoder.layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3bcac40], [3, 0x7e1ea40], [4, 0x7e8c540], [5, 0x3c48320]]}
  ff.reciprocal_of_sqrt_of_head_size_2:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3b9c900]]}
  ff.bert.encoder.layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3c4c440], [0, 0x3b8d560], [1, 0x3b9d140], [2, 0x3bcf5a0], [3, 0x7e233a0], [4, 0x7e90ea0], [5, 0x3c8d460], [0, 0x3bce580]]}
  ff.bert.encoder.layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3bde160], [2, 0x3c105c0], [3, 0x7e643c0], [4, 0x7ed1ec0]]}
  ff.bert.encoder.layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3cce480], [0, 0x3c0f5a0], [1, 0x3be2280], [2, 0x3c146e0], [3, 0x7e684e0], [4, 0x7ed5fe0], [5, 0x3d0f4a0], [0, 0x3c505c0]]}
  ff.bert.encoder.layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7e432e0], [5, 0x3bff0c0], [0, 0x3b06bc0], [1, 0x3b167a0]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bdfbc0]]}
  ff.bert.encoder.layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a777c0]]}
  ff.bert.encoder.layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a85340], [1, 0x3a72ec0], [2, 0x3a85340], [3, 0x7beffe0], [4, 0x7be0400], [5, 0x3a87be0], [0, 0x3b07360], [1, 0x3af4ee0], [2, 0x3b07360], [3, 0x7c72000], [4, 0x7c62420], [5, 0x3b09c00], [0, 0x3b89380], [1, 0x3b76f00], [2, 0x3b89380], [3, 0x7cf4020]]}
  ff.bert.encoder.layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ce4440], [5, 0x3b8bc20], [0, 0x3c0b3a0], [1, 0x3bf8f20]]}
  ff.bert.encoder.layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3efcfc0], [2, 0x3e6f4c0], [3, 0x806dde0], [4, 0x80bb8e0], [5, 0x3df3660], [0, 0x3e32e20], [1, 0x3f7efe0], [2, 0x3ef14e0], [3, 0x80efe00], [4, 0x813d900], [5, 0x3e75680], [0, 0x3eb4e40], [1, 0x4001000], [2, 0x3f73500], [3, 0x8171e20], [4, 0x81bf920]]}
  ff.bert.encoder.layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3deb3c0], [0, 0x3e1d840], [1, 0x3ef9660], [2, 0x3e6bb60], [3, 0x8067be0], [4, 0x80a3260], [5, 0x3ded460], [0, 0x3e1f8e0]]}
  ff.bert.encoder.layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e221c0]]}
  ff.bert.encoder.layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x80a83e0]]}
  ff.bert.encoder.layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3d965c0], [3, 0x7fda7e0], [4, 0x7efe9c0], [5, 0x3c2bc80], [0, 0x3cb0540], [1, 0x3d1f8a0], [2, 0x3dd75e0], [3, 0x801b800]]}
  ff.bert.encoder.layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7f3f9e0], [5, 0x3c6cca0], [0, 0x3cf1560], [1, 0x3d608c0]]}
  ff.bert.encoder.layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3e18600], [3, 0x805c820], [4, 0x7f43b00], [5, 0x3c70dc0], [0, 0x3cf5680], [1, 0x3d649e0], [2, 0x3e59620], [3, 0x809d840]]}
  ff.bert.encoder.layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7f84b20], [5, 0x3cb1de0], [0, 0x3d366a0], [1, 0x3da5a00]]}
  ff.reciprocal_of_sqrt_of_head_size_3:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7e43b20]]}
  ff.bert.encoder.layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3da9b20], [2, 0x3e9ae80], [3, 0x80df0a0], [4, 0x7f89480], [5, 0x3cb6740], [0, 0x3d3b000], [1, 0x3deab40], [2, 0x3edbea0]]}
  ff.bert.encoder.layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x81200c0], [4, 0x7fca4a0], [5, 0x3cf7760], [0, 0x3d7c020]]}
  ff.bert.encoder.layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3e2bb60], [2, 0x3f1cec0], [3, 0x81241e0], [4, 0x7fce5c0], [5, 0x3cfb880], [0, 0x3d80140], [1, 0x3e6cb80], [2, 0x3f5dee0]]}
  ff.bert.encoder.layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x8165200], [4, 0x800f5e0], [5, 0x3d3c8a0], [0, 0x3dc1160]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7fd5ea0]]}
  ff.bert.encoder.layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3e90400]]}
  ff.bert.encoder.layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3f70b40], [3, 0x8171ce0], [4, 0x81859c0], [5, 0x3f1a600], [0, 0x3e75580], [1, 0x3ea0820], [2, 0x3ff2b60], [3, 0x81f3d00], [4, 0x82079e0], [5, 0x3f9c620], [0, 0x3ef75a0], [1, 0x3f22840], [2, 0x4074b80], [3, 0x8275d20], [4, 0x8289a00], [5, 0x401e640]]}
  ff.bert.encoder.layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3f795c0], [1, 0x3fa4860], [2, 0x40f6ba0], [3, 0x82f7d40]]}
  ff.bert.encoder.layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7f58020], [5, 0x3d914e0], [0, 0x3cd2600], [1, 0x3ca52e0], [2, 0x3cd7740], [3, 0x7eead60], [4, 0x7fda040], [5, 0x3e13500], [0, 0x3d54620], [1, 0x3d27300], [2, 0x3d59760], [3, 0x7f6cd80], [4, 0x805c060], [5, 0x3e95520], [0, 0x3dd6640], [1, 0x3da9320]]}
  ff.bert.encoder.layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3ddb780], [3, 0x7feeda0], [4, 0x80de080], [5, 0x3f17540], [0, 0x3e58660], [1, 0x3e2b340], [2, 0x3ddd820], [3, 0x7ff0e40]]}
  ff.bert.encoder.layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x80e0960]]}
  ff.bert.encoder.layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3e5d7e0]]}
  ff.bert.encoder.layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3dd13e0], [0, 0x3e02820], [1, 0x3da6980], [2, 0x3ce7260], [3, 0x7ec2ac0], [4, 0x7f31e20], [5, 0x3e12400], [0, 0x3e43840]]}
  ff.bert.encoder.layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3de79a0], [2, 0x3d28280], [3, 0x7f03ae0], [4, 0x7f72e40]]}
  ff.bert.encoder.layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3bea360], [1, 0x3c0fca0], [2, 0x3b7e8e0], [3, 0x7ce8540], [4, 0x7d5a140], [5, 0x3c09b20], [0, 0x3c2b380], [1, 0x3c50cc0]]}
  ff.bert.encoder.layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3e84860], [1, 0x3debac0], [2, 0x3d2c3a0], [3, 0x7f07c00]]}
  ff.reciprocal_of_sqrt_of_head_size_4:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3e53c60]]}
  ff.bert.encoder.layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7f0bd20], [4, 0x7f777a0], [5, 0x3e544a0], [0, 0x3e891c0], [1, 0x3df0420], [2, 0x3d30d00], [3, 0x7f4cd40], [4, 0x7fb87c0]]}
  ff.bert.encoder.layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3e954c0], [0, 0x3eca1e0], [1, 0x3e31440], [2, 0x3d71d20]]}
  ff.bert.encoder.layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7f8dd60], [4, 0x7ff97e0], [5, 0x3e995e0], [0, 0x3ece300], [1, 0x3e35560], [2, 0x3d75e40], [3, 0x7fced80], [4, 0x803a800]]}
  ff.bert.encoder.layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100]]}
  ff.bert.encoder.layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7e4fe80]]}
  ff.bert.encoder.layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3c86aa0]]}
  ff.bert.encoder.layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3c89b80], [1, 0x3c10dc0], [2, 0x3b7e9c0], [3, 0x7e602a0], [4, 0x7ee0220], [5, 0x3c96ec0], [0, 0x3d0bba0], [1, 0x3c92de0], [2, 0x3c009e0], [3, 0x7ee22c0], [4, 0x7f62240], [5, 0x3d18ee0], [0, 0x3d8dbc0], [1, 0x3d14e00], [2, 0x3c82a00], [3, 0x7f642e0]]}
  ff.bert.encoder.layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7fe4260], [5, 0x3d9af00], [0, 0x3e0fbe0], [1, 0x3d96e20]]}
  ff.bert.encoder.layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7f58020], [5, 0x3d914e0], [0, 0x3cd2600], [1, 0x3ca52e0], [2, 0x3cd7740], [3, 0x7eead60], [4, 0x7fda040], [5, 0x3e13500], [0, 0x3d54620], [1, 0x3d27300], [2, 0x3d59760], [3, 0x7f6cd80], [4, 0x805c060], [5, 0x3e95520], [0, 0x3dd6640], [1, 0x3da9320]]}
  ff.bert.encoder.layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3ddb780], [3, 0x7feeda0], [4, 0x80de080], [5, 0x3f17540], [0, 0x3e58660], [1, 0x3e2b340], [2, 0x3ddd820], [3, 0x7ff0e40]]}
  ff.bert.encoder.layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x80e0960]]}
  ff.bert.encoder.layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3e5d7e0]]}
  ff.bert.encoder.layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c0b3a0], [3, 0x7d76040], [4, 0x7cf4860], [5, 0x3b9c040], [0, 0x3c1b7c0], [1, 0x3c09340], [2, 0x3c4c3c0], [3, 0x7db7060]]}
  ff.bert.encoder.layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d35880], [5, 0x3bdd060], [0, 0x3c5c7e0], [1, 0x3c4a360]]}
  ff.bert.encoder.layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c8d3e0], [3, 0x7df8080], [4, 0x7d399a0], [5, 0x3be1180], [0, 0x3c60900], [1, 0x3c4e480], [2, 0x3cce400], [3, 0x7e390a0]]}
  ff.bert.encoder.layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d7a9c0], [5, 0x3c221a0], [0, 0x3ca1920], [1, 0x3c8f4a0]]}
  ff.reciprocal_of_sqrt_of_head_size_5:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e7a0c0]]}
  ff.bert.encoder.layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c935c0], [2, 0x3d0fc60], [3, 0x7e7a900], [4, 0x7d7f320], [5, 0x3c26b00], [0, 0x3ca6280], [1, 0x3cd45e0], [2, 0x3d50c80]]}
  ff.bert.encoder.layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ebb920], [4, 0x7dc0340], [5, 0x3c67b20], [0, 0x3ce72a0]]}
  ff.bert.encoder.layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3d15600], [2, 0x3d91ca0], [3, 0x7ebfa40], [4, 0x7dc4460], [5, 0x3c6bc40], [0, 0x3ceb3c0], [1, 0x3d56620], [2, 0x3dd2cc0]]}
  ff.bert.encoder.layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f00a60], [4, 0x7e05480], [5, 0x3cacc60], [0, 0x3d2c3e0]]}
  ff.bert.encoder.layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3e9ffe0]]}
  ff.bert.encoder.layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x391b120]]}
  ff.bert.encoder.layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7b80420], [4, 0x7b80420], [5, 0x39b6000], [0, 0x39f7020], [1, 0x3a70e20], [2, 0x3a40220], [3, 0x7c02440], [4, 0x7c02440], [5, 0x3a38020], [0, 0x3a79040], [1, 0x3af2e40], [2, 0x3ac2240], [3, 0x7c84460], [4, 0x7c84460], [5, 0x3aba040], [0, 0x3afb060]]}
  ff.bert.encoder.layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x39a5be0], [0, 0x39e6c00], [1, 0x3a60a00], [2, 0x3a2fe00]]}
  ff.bert.encoder.layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3f9ef00], [3, 0x8169320], [4, 0x8013700], [5, 0x3d409c0], [0, 0x3dc5280], [1, 0x3eae3e0], [2, 0x4020f20], [3, 0x81eb340], [4, 0x8095720], [5, 0x3dc29e0], [0, 0x3e472a0], [1, 0x3f30400], [2, 0x40a2f40], [3, 0x826d360], [4, 0x8117740], [5, 0x3e44a00]]}
  ff.bert.encoder.layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3ec92c0], [1, 0x3fb2420], [2, 0x4124f60], [3, 0x82ef380], [4, 0x8199760], [5, 0x3ec6a20], [0, 0x3ecb360], [1, 0x3fb44c0]]}
  ff.bert.encoder.layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x38da100]]}
  ff.bert.encoder.layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7ab4100]]}
  ff.bert.encoder.layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x830ba20], [5, 0x40a0660], [0, 0x3f899e0], [1, 0x3fb4c80], [2, 0x4106fc0], [3, 0x8308160], [4, 0x834ca40], [5, 0x40e1680]]}
  ff.bert.encoder.layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3fcaa00], [1, 0x3ff5ca0], [2, 0x4147fe0], [3, 0x8349180]]}
  ff.bert.encoder.layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x838da60], [5, 0x41226a0], [0, 0x3fceb20], [1, 0x3ff9dc0], [2, 0x414c100], [3, 0x834d2a0], [4, 0x83cea80], [5, 0x41636c0]]}
  ff.bert.encoder.layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x400fb40], [1, 0x403ade0], [2, 0x418d120], [3, 0x838e2c0]]}
  ff.reciprocal_of_sqrt_of_head_size_6:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x41a46e0]]}
  ff.bert.encoder.layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x391b120], [1, 0x391b120]]}
  ff.bert.encoder.layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7af5120], [4, 0x7af5120], [5, 0x391b120], [0, 0x395c140]]}
  ff.bert.encoder.layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x395c140], [2, 0x391b960], [3, 0x7af9240], [4, 0x7af9240], [5, 0x391f240], [0, 0x3960260], [1, 0x399d160], [2, 0x395c980]]}
  ff.bert.encoder.layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7b3a260], [4, 0x7b3a260], [5, 0x3960260], [0, 0x39a1280]]}
  ff.bert.encoder.layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x38da100]]}
  ff.bert.encoder.layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x38dd1e0]]}
  ff.bert.encoder.layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x38da940], [3, 0x7ab4940], [4, 0x7ab4940], [5, 0x38ea520], [0, 0x38dda20], [1, 0x38ed600], [2, 0x395c960], [3, 0x7b36960], [4, 0x7b36960], [5, 0x396c540], [0, 0x395fa40], [1, 0x396f620], [2, 0x39de980], [3, 0x7bb8980], [4, 0x7bb8980], [5, 0x39ee560]]}
  ff.bert.encoder.layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x39e1a60], [1, 0x39f1640], [2, 0x3a609a0], [3, 0x7c3a9a0]]}
  ff.bert.encoder.layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x38da100], [0, 0x38da940], [1, 0x38de220], [2, 0x38de220], [3, 0x7ab8220], [4, 0x7ab8220], [5, 0x395c120], [0, 0x395c960], [1, 0x3960240], [2, 0x3960240], [3, 0x7b3a240], [4, 0x7b3a240], [5, 0x39de140], [0, 0x39de980], [1, 0x39e2260], [2, 0x39e2260]]}
  ff.bert.encoder.layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7bbc260], [4, 0x7bbc260], [5, 0x3a60160], [0, 0x3a609a0], [1, 0x3a64280], [2, 0x3a64280], [3, 0x7bbe300], [4, 0x7bbe300]]}
  ff.bert.encoder.layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3a62a40]]}
  ff.bert.encoder.layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3a69400]]}
  ff.bert.encoder.layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3d04a20], [3, 0x7fe6300], [4, 0x7ff4680], [5, 0x3dab320], [0, 0x3e20000], [1, 0x3da7240], [2, 0x3d45a40], [3, 0x8027320]]}
  ff.bert.encoder.layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x80356a0], [5, 0x3dec340], [0, 0x3e61020], [1, 0x3de8260]]}
  ff.bert.encoder.layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3d86a60], [3, 0x8068340], [4, 0x80397c0], [5, 0x3df0460], [0, 0x3e65140], [1, 0x3dec380], [2, 0x3dc7a80], [3, 0x80a9360]]}
  ff.bert.encoder.layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100]]}
  ff.reciprocal_of_sqrt_of_head_size_7:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x38da100]]}
  ff.bert.encoder.layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7ab8220], [4, 0x7ab8220], [5, 0x38da940], [0, 0x38db180], [1, 0x38dea60], [2, 0x38dea60], [3, 0x7af9240], [4, 0x7af9240]]}
  ff.bert.encoder.layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x391b960], [0, 0x391c1a0], [1, 0x391fa80], [2, 0x391fa80]]}
  ff.bert.encoder.layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7b3a260], [4, 0x7b3a260], [5, 0x391fa80], [0, 0x39202c0], [1, 0x3923ba0], [2, 0x3923ba0], [3, 0x7b7b280], [4, 0x7b7b280]]}
  ff.bert.encoder.layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3960aa0], [0, 0x39612e0], [1, 0x3964bc0], [2, 0x3964bc0]]}
  ff.bert.encoder.layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x38da100]]}
  ff.bert.encoder.layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x38dd1e0]]}
  ff.bert.encoder.layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x38da940], [3, 0x7ab4940], [4, 0x7ab4940], [5, 0x38ea520], [0, 0x38dda20], [1, 0x38ed600], [2, 0x395c960], [3, 0x7b36960], [4, 0x7b36960], [5, 0x396c540], [0, 0x395fa40], [1, 0x396f620], [2, 0x39de980], [3, 0x7bb8980], [4, 0x7bb8980], [5, 0x39ee560]]}
  ff.bert.encoder.layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x39e1a60], [1, 0x39f1640], [2, 0x3a609a0], [3, 0x7c3a9a0]]}
  ff.bert.encoder.layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e13ce0], [3, 0x7f04b80], [4, 0x7e095a0], [5, 0x3cb0d80], [0, 0x3d30500], [1, 0x3d97e80], [2, 0x3e95d00], [3, 0x7f86ba0], [4, 0x7e8b5c0], [5, 0x3d32da0], [0, 0x3db2520], [1, 0x3e19ea0], [2, 0x3f17d20], [3, 0x8008bc0], [4, 0x7f0d5e0], [5, 0x3db4dc0]]}
  ff.bert.encoder.layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e34540], [1, 0x3e9bec0], [2, 0x3f99d40], [3, 0x808abe0], [4, 0x7f8f600], [5, 0x3e36de0], [0, 0x3e365e0], [1, 0x3e9df60]]}
  ff.bert.encoder.layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da940]]}
  ff.bert.encoder.layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38dd1e0]]}
  ff.bert.encoder.layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x39de9c0], [2, 0x39addc0], [3, 0x7b3ebc0], [4, 0x7b3ebc0], [5, 0x3964bc0], [0, 0x39a5be0], [1, 0x3a1f9e0], [2, 0x39eede0]]}
  ff.bert.encoder.layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7b3aaa0], [4, 0x7b3aaa0], [5, 0x3960aa0], [0, 0x39a1ac0]]}
  ff.bert.encoder.layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x395c980], [2, 0x392bd80], [3, 0x7af9a80], [4, 0x7af9a80], [5, 0x391fa80], [0, 0x3960aa0], [1, 0x399d9a0], [2, 0x396cda0]]}
  ff.bert.encoder.layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7af5960], [4, 0x7af5960], [5, 0x391b960], [0, 0x395c980]]}
  ff.reciprocal_of_sqrt_of_head_size_8:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x395c140]]}
  ff.bert.encoder.layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100], [2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x391b120], [1, 0x391b120]]}
  ff.bert.encoder.layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c072c0], [2, 0x3bc6ae0], [3, 0x7d48520], [4, 0x7d4adc0]]}
  ff.bert.encoder.layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3cc7ae0], [1, 0x3d93d20], [2, 0x3d56e20], [3, 0x7ed8860], [4, 0x7edb100], [5, 0x3c9d060], [0, 0x3d08b00], [1, 0x3dd4d40]]}
  ff.bert.encoder.layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3d52d00], [3, 0x7ed4740], [4, 0x7ed6fe0], [5, 0x3c98f40]]}
  ff.bert.encoder.layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7ac4520]]}
  ff.bert.encoder.layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x38dda20]]}
  ff.bert.encoder.layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x38dda20], [2, 0x38ead60], [3, 0x7ab5180], [4, 0x7ad4940], [5, 0x38db180], [0, 0x38ede40], [1, 0x395fa40], [2, 0x396cd80], [3, 0x7b371a0], [4, 0x7b56960], [5, 0x395d1a0], [0, 0x396fe60], [1, 0x39e1a60], [2, 0x39eeda0], [3, 0x7bb91c0], [4, 0x7bd8980]]}
  ff.bert.encoder.layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x39df1c0], [0, 0x39f1e80], [1, 0x3a63a80], [2, 0x3a70dc0]]}
  ff.bert.encoder.layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x399d9a0], [3, 0x7b3e380], [4, 0x7b3e380], [5, 0x3964380], [0, 0x39a53a0], [1, 0x39de9c0], [2, 0x3a1f9c0], [3, 0x7bc03a0], [4, 0x7bc03a0], [5, 0x39e63a0], [0, 0x3a273c0], [1, 0x3a609e0], [2, 0x3aa19e0], [3, 0x7c423c0], [4, 0x7c423c0], [5, 0x3a683c0]]}
  ff.bert.encoder.layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3aa93e0], [1, 0x3ae2a00], [2, 0x3b23a00], [3, 0x7cc43e0], [4, 0x7cc43e0], [5, 0x3aea3e0], [0, 0x3aab480], [1, 0x3ae4aa0]]}
  ff.bert.encoder.layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3b262e0]]}
  ff.bert.encoder.layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7cc9560]]}
  ff.bert.encoder.layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7c3a9a0], [5, 0x3a70580], [0, 0x39f1e80], [1, 0x3a01a60], [2, 0x3a70dc0], [3, 0x7c4adc0], [4, 0x7c7b9c0], [5, 0x3ab15a0]]}
  ff.bert.encoder.layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3a32ea0], [1, 0x3a42a80], [2, 0x3ab1de0], [3, 0x7c8bde0]]}
  ff.bert.encoder.layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7cbc9e0], [5, 0x3af25c0], [0, 0x3a36fc0], [1, 0x3a46ba0], [2, 0x3ab5f00], [3, 0x7c8ff00], [4, 0x7cfda00], [5, 0x3b335e0]]}
  ff.bert.encoder.layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3a77fe0], [1, 0x3a87bc0], [2, 0x3af6f20], [3, 0x7cd0f20]]}
  ff.reciprocal_of_sqrt_of_head_size_9:                                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3b74600]]}
  ff.bert.encoder.layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7cd5040], [4, 0x7d3f260], [5, 0x3b74e40], [0, 0x3a7c940], [1, 0x3a8c520], [2, 0x3afb880], [3, 0x7d16060], [4, 0x7d80280]]}
  ff.bert.encoder.layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3bb5e60], [0, 0x3abd960], [1, 0x3acd540], [2, 0x3b3c8a0]]}
  ff.bert.encoder.layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7d57080], [4, 0x7dc12a0], [5, 0x3bb9f80], [0, 0x3ac1a80], [1, 0x3ad1660], [2, 0x3b409c0], [3, 0x7d980a0], [4, 0x7e022c0]]}
  ff.bert.encoder.layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3bfafa0], [0, 0x3b02aa0], [1, 0x3b12680], [2, 0x3b819e0]]}
  ff.bert.encoder.layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3a79820]]}
  ff.bert.encoder.layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7bc3cc0]]}
  ff.bert.encoder.layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7bc3cc0], [5, 0x3a736a0], [0, 0x3a64300], [1, 0x3a89c40], [2, 0x3a6a480], [3, 0x7bd40e0], [4, 0x7c45ce0], [5, 0x3af56c0], [0, 0x3ae6320], [1, 0x3b0bc60], [2, 0x3aec4a0], [3, 0x7c56100], [4, 0x7cc7d00], [5, 0x3b776e0], [0, 0x3b68340], [1, 0x3b8dc80]]}
  ff.bert.encoder.layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3b6e4c0], [3, 0x7cd8120], [4, 0x7d49d20], [5, 0x3bf9700]]}
  ff.bert.encoder.layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7bbc2a0], [5, 0x3964bc0], [0, 0x3965400], [1, 0x3968ce0], [2, 0x3968ce0], [3, 0x7bbcae0], [4, 0x7c3e2c0], [5, 0x39e6be0], [0, 0x39e7420], [1, 0x39ead00], [2, 0x39ead00], [3, 0x7c3eb00], [4, 0x7cc02e0], [5, 0x3a68c00], [0, 0x3a69440], [1, 0x3a6cd20]]}
  ff.bert.encoder.layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3a6cd20], [3, 0x7cc0b20], [4, 0x7d42300], [5, 0x3aeac20], [0, 0x3aeb460], [1, 0x3aeed40], [2, 0x3a6edc0], [3, 0x7cc2bc0]]}
  ff.bert.encoder.layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7d44be0]]}
  ff.bert.encoder.layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3af05e0]]}
  ff.bert.encoder.layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7c3a9a0], [5, 0x3a70580], [0, 0x39f1e80], [1, 0x3a01a60], [2, 0x3a70dc0], [3, 0x7c4adc0], [4, 0x7c7b9c0], [5, 0x3ab15a0]]}
  ff.bert.encoder.layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3a32ea0], [1, 0x3a42a80], [2, 0x3ab1de0], [3, 0x7c8bde0]]}
  ff.bert.encoder.layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7cbc9e0], [5, 0x3af25c0], [0, 0x3a36fc0], [1, 0x3a46ba0], [2, 0x3ab5f00], [3, 0x7c8ff00], [4, 0x7cfda00], [5, 0x3b335e0]]}
  ff.bert.encoder.layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3a77fe0], [1, 0x3a87bc0], [2, 0x3af6f20], [3, 0x7cd0f20]]}
  ff.reciprocal_of_sqrt_of_head_size_10:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3b74600]]}
  ff.bert.encoder.layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7cd5040], [4, 0x7d3f260], [5, 0x3b74e40], [0, 0x3a7c940], [1, 0x3a8c520], [2, 0x3afb880], [3, 0x7d16060], [4, 0x7d80280]]}
  ff.bert.encoder.layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3bb5e60], [0, 0x3abd960], [1, 0x3acd540], [2, 0x3b3c8a0]]}
  ff.bert.encoder.layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7d57080], [4, 0x7dc12a0], [5, 0x3bb9f80], [0, 0x3ac1a80], [1, 0x3ad1660], [2, 0x3b409c0], [3, 0x7d980a0], [4, 0x7e022c0]]}
  ff.bert.encoder.layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3bfafa0], [0, 0x3b02aa0], [1, 0x3b12680], [2, 0x3b819e0]]}
  ff.bert.encoder.layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38ed600]]}
  ff.bert.encoder.layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab7a20]]}
  ff.bert.encoder.layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38dda20], [0, 0x38eb5a0], [1, 0x38db9c0], [2, 0x38fda20], [3, 0x7ab8260], [4, 0x7ac7e40], [5, 0x395fa40], [0, 0x396d5c0], [1, 0x395d9e0], [2, 0x397fa40], [3, 0x7b3a280], [4, 0x7b49e60], [5, 0x39e1a60], [0, 0x39ef5e0], [1, 0x39dfa00], [2, 0x3a01a60]]}
  ff.bert.encoder.layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bbc2a0], [4, 0x7bcbe80], [5, 0x3a63a80], [0, 0x3a71600]]}
  ff.bert.encoder.layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c0d480], [2, 0x3bccca0], [3, 0x7d4e6e0], [4, 0x7d50f80], [5, 0x3b946c0], [0, 0x3bc3260], [1, 0x3c8f4a0], [2, 0x3c4ecc0], [3, 0x7dd0700], [4, 0x7dd2fa0], [5, 0x3c166e0], [0, 0x3c45280], [1, 0x3d114c0], [2, 0x3cd0ce0], [3, 0x7e52720], [4, 0x7e54fc0]]}
  ff.bert.encoder.layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b90580], [0, 0x3bbf120], [1, 0x3c0b3e0], [2, 0x3bcac00], [3, 0x7d4c640], [4, 0x7d4eee0], [5, 0x3b92620], [0, 0x3bc11c0]]}
  ff.bert.encoder.layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3bf6ea0]]}
  ff.bert.encoder.layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b7d080]]}
  ff.bert.encoder.layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7c3b1e0], [4, 0x7c5a9a0], [5, 0x39ef5e0], [0, 0x3a022a0], [1, 0x3a73ea0], [2, 0x3a811e0], [3, 0x7c7c200], [4, 0x7c9b9c0]]}
  ff.bert.encoder.layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3a30600], [0, 0x3a432c0], [1, 0x3ab4ec0], [2, 0x3ac2200]]}
  ff.bert.encoder.layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7cbd220], [4, 0x7cdc9e0], [5, 0x3a34720], [0, 0x3a473e0], [1, 0x3ab8fe0], [2, 0x3ac6320], [3, 0x7cfe240], [4, 0x7d1da00]]}
  ff.bert.encoder.layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3a75740], [0, 0x3a88400], [1, 0x3afa000], [2, 0x3b07340]]}
  ff.reciprocal_of_sqrt_of_head_size_11:                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7d5ea20]]}
  ff.bert.encoder.layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [2, 2], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3b0b460], [3, 0x7d3faa0], [4, 0x7d5f260], [5, 0x3a7a0a0], [0, 0x3a8cd60], [1, 0x3afe960], [2, 0x3b4c480], [3, 0x7d80ac0]]}
  ff.bert.encoder.layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7da0280], [5, 0x3abb0c0], [0, 0x3acdd80], [1, 0x3b3f980]]}
  ff.bert.encoder.layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3b8d4a0], [3, 0x7dc1ae0], [4, 0x7da43a0], [5, 0x3abf1e0], [0, 0x3ad1ea0], [1, 0x3b43aa0], [2, 0x3bce4c0], [3, 0x7e02b00]]}
  ff.bert.encoder.layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7de53c0], [5, 0x3b00200], [0, 0x3b12ec0], [1, 0x3b84ac0]]}
  ff.bert.encoder.layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7cd9980]]}
  ff.bert.encoder.layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3ab0e40]]}
  ff.bert.encoder.layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3aea460], [2, 0x3b36f40], [3, 0x7cc7d40], [4, 0x7ce9da0], [5, 0x3af05e0], [0, 0x3ac1260], [1, 0x3b6c480], [2, 0x3bb8f60], [3, 0x7d49d60], [4, 0x7d6bdc0], [5, 0x3b72600], [0, 0x3b43280], [1, 0x3bee4a0], [2, 0x3c3af80], [3, 0x7dcbd80], [4, 0x7dedde0]]}
  ff.bert.encoder.layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3bf4620], [0, 0x3bc52a0], [1, 0x3c704c0], [2, 0x3cbcfa0]]}
  ff.bert.encoder.layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [2, 8], t: 1, mblock: [8, 1], ublock: [8, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7e4dda0], [4, 0x7e6fe00], [5, 0x3c04a40], [0, 0x3bd56c0], [1, 0x3c808e0], [2, 0x3ccd3c0], [3, 0x7ecfdc0], [4, 0x7ef1e20], [5, 0x3c86a60], [0, 0x3c576e0], [1, 0x3d02900], [2, 0x3d4f3e0], [3, 0x7f51de0], [4, 0x7f73e40], [5, 0x3d08a80], [0, 0x3cd9700]]}
  ff.bert.encoder.layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3d84920], [2, 0x3dd1400], [3, 0x7fd3e00], [4, 0x7ff5e60], [5, 0x3d8aaa0], [0, 0x3d5b720], [1, 0x3d869c0], [2, 0x3dd34a0]]}
  ff.bert.encoder.layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a62260]]}
  ff.bert.encoder.layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bcf7a0]]}

  # constant
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d68b40]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x801e140]]}
  lc.input_tensor.attention_mask_s_brcst_m2_11_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3e220c0]]}
  lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3dd3da0]]}
  lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d68300]]}
  lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x80de860]]}
  lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7de94e0]]}
  dc.input_tensor.norm_mha_0.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3b04320], [0, 0x3b16fe0]]}
  lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3b88be0]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3c0fd20]]}
  lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7de9d20]]}
  lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3d5f860]]}
  lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3d8ab00]]}
  dc.input_tensor.norm_ff_0.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3dd75e0], [3, 0x7fe8360]]}
  lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7ffc040]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3d90c80]]}
  lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3d8b340]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3b8cd20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3bced60]]}
  lc.input_tensor.attention_mask_s_brcst_m2_10_1.0:                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f1c120]]}
  lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7e22b60]]}
  lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7e90660]]}
  lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7eea520]]}
  lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3e53420]]}
  dc.input_tensor.norm_mha_1.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3bbf900], [3, 0x7d29560]]}
  lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7d9b160]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3c4ab40]]}
  lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3c91ce0]]}
  lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3aefde0]]}
  lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3b02aa0]]}
  dc.input_tensor.norm_ff_1.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3af9040], [2, 0x3a76820]]}
  lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7cc95e0]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7d59140]]}
  lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3b032e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3b8cd20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3bced60]]}
  lc.input_tensor.attention_mask_s_brcst_m2_9_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f19880]]}
  lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7e22b60]]}
  lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7e90660]]}
  lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7eea520]]}
  lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3d0f420]]}
  dc.input_tensor.norm_mha_2.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a746e0], [0, 0x3a82260]]}
  lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a72680]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a84b00]]}
  lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7bdfbc0]]}
  lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e325e0]]}
  lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3df2e20]]}
  dc.input_tensor.norm_ff_2.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x806ad00], [4, 0x80b8800]]}
  lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3e6ec80]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3efc780]]}
  lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3df25e0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3e9a640]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x7f88c40]]}
  lc.input_tensor.attention_mask_s_brcst_m2_8_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x806a4c0]]}
  lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3cb5f00]]}
  lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3d3a7c0]]}
  lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3eadba0]]}
  lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7feb440]]}
  dc.input_tensor.norm_mha_3.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7ffc880], [5, 0x3d914c0]]}
  lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3d704c0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3d8bb80]]}
  lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3e74d40]]}
  lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x80e0120]]}
  lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3f195e0]]}
  dc.input_tensor.norm_ff_3.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3e5a700], [1, 0x3e2d3e0]]}
  lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3ddf8c0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7ff2ee0]]}
  lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[5, 0x3f19e20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7f76f60]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3e88980]]}
  lc.input_tensor.attention_mask_s_brcst_m2_7_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3e6e440]]}
  lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3defbe0]]}
  lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3d304c0]]}
  lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3a79900]]}
  dc.input_tensor.norm_mha_4.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3af3ec0], [2, 0x3a716a0]]}
  lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x3c10580]]}
  lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3b7e180]]}
  lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7edf9e0]]}
  lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x80e0120]]}
  lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3f195e0]]}
  dc.input_tensor.norm_ff_4.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3e5a700], [1, 0x3e2d3e0]]}
  lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3ddf8c0]]}
  lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7ff2ee0]]}
  lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[5, 0x3f19e20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7bdf380]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d7eae0]]}
  lc.input_tensor.attention_mask_s_brcst_m2_6_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3efbf40]]}
  lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3c262c0]]}
  lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3ca5a40]]}
  lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3d97640]]}
  lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e21980]]}
  dc.input_tensor.norm_mha_5.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x80a5300], [5, 0x3def500]]}
  lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8069c80]]}
  lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3e6dc00]]}
  lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3efb700]]}
  lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x4127000]]}
  lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x82f1420]]}
  dc.input_tensor.norm_ff_5.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[4, 0x819b800], [5, 0x3ec8ac0]]}
  lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x840faa0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x4013c60]]}
  lc.input_tensor.attention_mask_s_brcst_m2_5_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7b7fbe0]]}
  lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x403ef00]]}
  lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x391b120]]}
  lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x39de180]]}
  lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x38da100]]}
  dc.input_tensor.norm_mha_6.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100]]}
  lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x38dd1e0]]}
  lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3a62200]]}
  lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3a62a40]]}
  dc.input_tensor.norm_ff_6.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[1, 0x3a66320], [2, 0x3a66320]]}
  lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7bc03a0]]}
  lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[4, 0x7bc03a0]]}
  lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3a63280]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x38da100]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x38da940]]}
  lc.input_tensor.attention_mask_s_brcst_m2_4_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7b7fbe0]]}
  lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[1, 0x38de220]]}
  lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x38de220]]}
  lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7bbc2a0]]}
  lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x38da100]]}
  dc.input_tensor.norm_mha_7.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100]]}
  lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x38dd1e0]]}
  lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da100]]}
  lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100]]}
  dc.input_tensor.norm_ff_7.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100], [3, 0x7ab4100]]}
  lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100]]}
  lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38da940]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x392b540]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x395c140]]}
  lc.input_tensor.attention_mask_s_brcst_m2_3_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x391b120]]}
  lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7af5120]]}
  lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7af5120]]}
  lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3d934e0]]}
  lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x38da100]]}
  dc.input_tensor.norm_mha_8.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x38da940], [1, 0x38da940]]}
  lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x38ea520]]}
  lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7ab4940]]}
  lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x38da940]]}
  lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3b25aa0]]}
  lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7cc6480]]}
  dc.input_tensor.norm_ff_8.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[4, 0x7cc6480], [5, 0x3aec480]]}
  lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3aad520]]}
  lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[1, 0x3ae6b40]]}
  lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7cc6cc0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[4, 0x7d3ea20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0:                        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x3a7c100]]}
  lc.input_tensor.attention_mask_s_brcst_m2_2_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3cc72a0]]}
  lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[1, 0x3a8bce0]]}
  lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0:                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[2, 0x3afb040]]}
  lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[3, 0x7dd90c0]]}
  lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3a69400]]}
  dc.input_tensor.norm_mha_9.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[3, 0x7bc0be0], [4, 0x7bc0be0]]}
  lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[5, 0x3a72e60]]}
  lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x3a63ac0]]}
  lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[2, 0x3a69c40]]}
  lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[4, 0x7d443a0]]}
  lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0:                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3aeccc0]]}
  dc.input_tensor.norm_ff_9.4:                                                                  {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x3aed500], [1, 0x3af0de0]]}
  lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[2, 0x3a70e60]]}
  lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[3, 0x7cc4c60]]}
  lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[5, 0x3aed500]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[4, 0x7d3ea20]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x3a7c100]]}
  lc.input_tensor.attention_mask_s_brcst_m2_1_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3c98700]]}
  lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[1, 0x3a8bce0]]}
  lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[2, 0x3afb040]]}
  lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[3, 0x7dd90c0]]}
  lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab71e0]]}
  dc.input_tensor.norm_mha_10.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4940], [5, 0x38da940]]}
  lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38ead60]]}
  lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x38db180]]}
  lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab7a20]]}
  lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d474a0]]}
  lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3bbe8e0]]}
  dc.input_tensor.norm_ff_10.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d47ce0], [5, 0x3b8d4a0]]}
  lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7d47ce0]]}
  lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3bc62a0]]}
  lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3bbe0a0]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[3, 0x7d3f260]]}
  lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0:                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[5, 0x3a79860]]}
  lc.input_tensor.attention_mask_s_brcst_m2_0_1.0:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d474a0]]}
  lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0:                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x3a8c520]]}
  lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0:                                          {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[1, 0x3afe120]]}
  lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[2, 0x3c0f4e0]]}
  lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3aef560]]}
  dc.input_tensor.norm_mha_11.4:                                                                {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x3aadd60], [1, 0x3ae7380]]}
  lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[2, 0x3b36700]]}
  lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[3, 0x7cc7500]]}
  lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[5, 0x3aefda0]]}
  lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a61a20]]}
  lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a83a80]]}
  dc.input_tensor.norm_ff_11.4:                                                                 {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bcc6c0], [4, 0x7bdc2a0]]}
  lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0:                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a73ea0]]}
  lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a81a20]]}
  lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3a842c0]]}

  # epoch_to_epoch
  e2e_ff2_gelu_0:                                                                               {input: ff2_gelu, type: queue, entries: 1, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3018c200], [0, 0x301eda20], [0, 0x3024f240], [0, 0x302b0a60], [0, 0x30312280], [0, 0x30373aa0], [0, 0x303d52c0], [0, 0x30436ae0]]}
  e2e_buffer_0_norm_mha_2.dc.add.10_add_ff_2_0:                                                 {input: buffer_0_norm_mha_2.dc.add.10_add_ff_2, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300c91c0], [0, 0x3012a9e0]]}
  e2e_attention_mask_s_brcst_m2_8_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_8_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_7_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_7_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 5, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_6_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_6_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  e2e_norm_mha_5.dc.subtract.1_0:                                                               {input: norm_mha_5.dc.subtract.1, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x30498300], [0, 0x304f9b20]]}
  e2e_buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8_0:                             {input: buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3055b340], [0, 0x305bcb60]]}
  e2e_attention_mask_s_brcst_m2_5_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_5_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 3, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_4_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_4_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 6, loc: dram, dram: [[0, 0x30000000]]}
  e2e_norm_ff_7.dc.add.10_0:                                                                    {input: norm_ff_7.dc.add.10, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3061e380], [0, 0x3067fba0]]}
  e2e_attention_mask_s_brcst_m2_3_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_3_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x81f3e40]]}
  e2e_attention_mask_s_brcst_m2_2_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_2_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 4, loc: dram, dram: [[0, 0x30000000]]}
  e2e_attention_mask_s_brcst_m2_1_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_1_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 7, loc: dram, dram: [[0, 0x30000000]]}
  e2e_ff10_gelu_0:                                                                              {input: ff10_gelu, type: queue, entries: 1, grid_size: [2, 4], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x300c91c0], [0, 0x3012a9e0], [0, 0x3018c200], [0, 0x301eda20], [0, 0x3024f240], [0, 0x302b0a60], [0, 0x30312280], [0, 0x30373aa0]]}
  e2e_buffer_0_norm_mha_10.dc.add.10_add_ff_10_0:                                               {input: buffer_0_norm_mha_10.dc.add.10_add_ff_10, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x303d52c0], [0, 0x30436ae0]]}
  e2e_attention_mask_s_brcst_m2_0_1.lc1_0:                                                      {input: attention_mask_s_brcst_m2_0_1.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 2, loc: dram, dram: [[0, 0x300061a0]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 1
    input_count: 1
    mha_0_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.query.weight, ff.bert.encoder.layer.0.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.key.weight, ff.bert.encoder.layer.0.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_as: {type: matmul, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_0_query, mha_0_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_div: {type: multiply, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_0_as, ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    attention_mask_s_brcst_m2_11_1.lc1: {type: matmul, grid_loc: [3, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_11_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_0_as_mask: {type: add, grid_loc: [3, 6], grid_size: [2, 1], inputs: [mha_0_as_div, attention_mask_s_brcst_m2_11_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_0_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [4, 1], grid_size: [2, 1], inputs: [mha_0_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_0_as_mask_mha_0_as_softmax.dc.subtract.1: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_0_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [4, 3], grid_size: [2, 1], inputs: [buffer_0_mha_0_as_mask_mha_0_as_softmax.dc.subtract.1, mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_0_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 4], grid_size: [2, 2], inputs: [mha_0_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_0_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [5, 6], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.exp.2, lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_0_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 0], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_0_as_softmax.dc.exp.2_mha_0_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_0_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [6, 1], grid_size: [2, 1], inputs: [buffer_0_mha_0_as_softmax.dc.exp.2_mha_0_as_softmax.dc.multiply.5, mha_0_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_0_value: {type: matmul, grid_loc: [2, 0], grid_size: [2, 4], inputs: [encoder_input, ff.bert.encoder.layer.0.attention.self.value.weight, ff.bert.encoder.layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_0_ac: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [mha_0_as_softmax.dc.multiply.5, mha_0_value],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_0_output: {type: matmul, grid_loc: [7, 3], grid_size: [2, 4], inputs: [mha_0_ac, ff.bert.encoder.layer.0.attention.output.dense.weight, ff.bert.encoder.layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    add_mha_0: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [encoder_input, mha_0_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    attention_mask_s_brcst_m2_10_1.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_10_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_9_1.lc1: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_9_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_8_1.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_8_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_7_1.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_7_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_6_1.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_6_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_5_1.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_5_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_4_1.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_4_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_3_1.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_3_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_2_1.lc1: {type: matmul, grid_loc: [9, 1], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_2_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_1_1.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_1_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    attention_mask_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [lc.input_tensor.attention_mask_s_brcst_m2_0_1.0, attention_mask],
         t: 1, mblock: [1, 3], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_1_temporal_epoch_0:
    target_device: 2
    input_count: 1
    norm_mha_0.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [add_mha_0, lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_0_norm_mha_0.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_mha_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_0_add_mha_0_norm_mha_0.dc.subtract.1, norm_mha_0.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_mha_0.dc.multiply.2: {type: multiply, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_0.dc.subtract.1, norm_mha_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.2, lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_0.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [norm_mha_0.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_0.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [norm_mha_0.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_0.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_mha_0.dc.reciprocal.7, lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8: {type: nop, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [buffer_1_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_0.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_norm_mha_0.dc.subtract.1_norm_mha_0.dc.multiply.8, norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.8, ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_0.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_mha_0.dc.multiply.9, ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_0_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_0.dc.add.10, ff.bert.encoder.layer.0.intermediate.dense.weight, ff.bert.encoder.layer.0.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff0_gelu: {type: gelu, grid_loc: [5, 4], grid_size: [2, 4], inputs: [ff_0_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_0.dc.add.10_add_ff_0: {type: nop, grid_loc: [3, 4], grid_size: [2, 1], inputs: [norm_mha_0.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_2_temporal_epoch_0:
    target_device: 3
    input_count: 1
    ff_0_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff0_gelu, ff.bert.encoder.layer.0.output.dense.weight, ff.bert.encoder.layer.0.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_0: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_0.dc.add.10_add_ff_0, ff_0_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_0, lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_0_norm_ff_0.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_0_norm_ff_0.dc.subtract.1, norm_ff_0.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_0.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_0.dc.subtract.1, norm_ff_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.2, lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_0.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_0.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_0.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_0.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_0.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_0.dc.reciprocal.7, lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_0.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_0.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_0.dc.subtract.1_norm_ff_0.dc.multiply.8, norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_0.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.8, ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_0.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_0.dc.multiply.9, ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_3_temporal_epoch_0:
    target_device: 4
    input_count: 1
    mha_1_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.query.weight, ff.bert.encoder.layer.1.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_1_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.key.weight, ff.bert.encoder.layer.1.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_1_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_1_query, mha_1_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_1],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_1_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_1_as, ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_1_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_1_as_div, attention_mask_s_brcst_m2_10_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_1_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_1_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_1_as_mask_mha_1_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_1_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_1_as_mask_mha_1_as_softmax.dc.subtract.1, mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_1_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_1_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_1_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.exp.2, lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_1_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_1_as_softmax.dc.exp.2_mha_1_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_1_as_softmax.dc.exp.2_mha_1_as_softmax.dc.multiply.5, mha_1_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_1_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_0.dc.add.10, ff.bert.encoder.layer.1.attention.self.value.weight, ff.bert.encoder.layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_1_value_mha_1_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_1_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_1_value_mha_1_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_1_value_mha_1_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_1_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_1_as_softmax.dc.multiply.5, buffer_0_mha_1_value_mha_1_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_1_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_1_ac, ff.bert.encoder.layer.1.attention.output.dense.weight, ff.bert.encoder.layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_0.dc.add.10_add_mha_1: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_0.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_1: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_0.dc.add.10_add_mha_1, mha_1_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_1, lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_1_norm_mha_1.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_1_norm_mha_1.dc.subtract.1, norm_mha_1.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_4_temporal_epoch_0:
    target_device: 5
    input_count: 1
    norm_mha_1.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_1.dc.subtract.1, norm_mha_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.2, lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_1.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_1.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_1.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_1.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_1.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_1.dc.reciprocal.7, lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_1.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_1.dc.subtract.1_norm_mha_1.dc.multiply.8, norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.8, ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_1.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_1.dc.multiply.9, ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_1_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_1.dc.add.10, ff.bert.encoder.layer.1.intermediate.dense.weight, ff.bert.encoder.layer.1.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff1_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_1_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_1.dc.add.10_add_ff_1: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_1.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_5_temporal_epoch_0:
    target_device: 6
    input_count: 1
    ff_1_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff1_gelu, ff.bert.encoder.layer.1.output.dense.weight, ff.bert.encoder.layer.1.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_1: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_1.dc.add.10_add_ff_1, ff_1_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_1, lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_1_norm_ff_1.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_1_norm_ff_1.dc.subtract.1, norm_ff_1.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_1.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_1.dc.subtract.1, norm_ff_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.2, lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_1.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_1.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_1.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_1.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_1.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_1.dc.reciprocal.7, lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_1.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_1.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_1.dc.subtract.1_norm_ff_1.dc.multiply.8, norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.8, ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_1.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_1.dc.multiply.9, ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_6_temporal_epoch_0:
    target_device: 7
    input_count: 1
    mha_2_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.query.weight, ff.bert.encoder.layer.2.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_2_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.key.weight, ff.bert.encoder.layer.2.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_2_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_2_query, mha_2_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_2],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_2_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_2_as, ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_2_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_2_as_div, attention_mask_s_brcst_m2_9_1.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_2_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_2_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_2_as_mask_mha_2_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_2_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_2_as_mask_mha_2_as_softmax.dc.subtract.1, mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_2_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_2_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_2_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.exp.2, lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_2_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_2_as_softmax.dc.exp.2_mha_2_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_2_as_softmax.dc.exp.2_mha_2_as_softmax.dc.multiply.5, mha_2_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_2_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_1.dc.add.10, ff.bert.encoder.layer.2.attention.self.value.weight, ff.bert.encoder.layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_2_value_mha_2_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_2_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_2_value_mha_2_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_2_value_mha_2_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_2_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_2_as_softmax.dc.multiply.5, buffer_0_mha_2_value_mha_2_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_2_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_2_ac, ff.bert.encoder.layer.2.attention.output.dense.weight, ff.bert.encoder.layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_1.dc.add.10_add_mha_2: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_1.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_2: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_1.dc.add.10_add_mha_2, mha_2_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_2, lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_2_norm_mha_2.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_2_norm_mha_2.dc.subtract.1, norm_mha_2.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_7_temporal_epoch_0:
    target_device: 0
    input_count: 1
    norm_mha_2.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_2.dc.subtract.1, norm_mha_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.2, lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_2.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_2.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_2.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_2.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_2.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_2.dc.reciprocal.7, lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_2.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_2.dc.subtract.1_norm_mha_2.dc.multiply.8, norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.8, ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_2.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_2.dc.multiply.9, ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_2_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_2.dc.add.10, ff.bert.encoder.layer.2.intermediate.dense.weight, ff.bert.encoder.layer.2.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff2_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_2_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_2.dc.add.10_add_ff_2: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_2.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_8_temporal_epoch_1:
    target_device: 1
    input_count: 1
    ff_2_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_ff2_gelu_0, ff.bert.encoder.layer.2.output.dense.weight, ff.bert.encoder.layer.2.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_2: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_norm_mha_2.dc.add.10_add_ff_2_0, ff_2_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_2, lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_2_norm_ff_2.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_2_norm_ff_2.dc.subtract.1, norm_ff_2.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_2.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_2.dc.subtract.1, norm_ff_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.2, lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_2.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_2.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_2.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_2.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_2.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_2.dc.reciprocal.7, lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_2.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_2.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_2.dc.subtract.1_norm_ff_2.dc.multiply.8, norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_2.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.8, ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_2.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_2.dc.multiply.9, ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_9_temporal_epoch_1:
    target_device: 2
    input_count: 1
    mha_3_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.query.weight, ff.bert.encoder.layer.3.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_3_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.key.weight, ff.bert.encoder.layer.3.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_3_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_3_query, mha_3_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_3],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_3_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_3_as, ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_3_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_3_as_div, e2e_attention_mask_s_brcst_m2_8_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_3_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_3_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_3_as_mask_mha_3_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_3_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_3_as_mask_mha_3_as_softmax.dc.subtract.1, mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_3_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_3_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_3_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.exp.2, lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_3_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_3_as_softmax.dc.exp.2_mha_3_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_3_as_softmax.dc.exp.2_mha_3_as_softmax.dc.multiply.5, mha_3_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_3_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_2.dc.add.10, ff.bert.encoder.layer.3.attention.self.value.weight, ff.bert.encoder.layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_3_value_mha_3_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_3_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_3_value_mha_3_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_3_value_mha_3_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_3_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_3_as_softmax.dc.multiply.5, buffer_0_mha_3_value_mha_3_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_3_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_3_ac, ff.bert.encoder.layer.3.attention.output.dense.weight, ff.bert.encoder.layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_2.dc.add.10_add_mha_3: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_2.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_3: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_2.dc.add.10_add_mha_3, mha_3_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_3, lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_3_norm_mha_3.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_3_norm_mha_3.dc.subtract.1, norm_mha_3.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_10_temporal_epoch_1:
    target_device: 3
    input_count: 1
    norm_mha_3.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_3.dc.subtract.1, norm_mha_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.2, lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_3.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_3.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_3.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_3.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_3.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_3.dc.reciprocal.7, lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_3.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_3.dc.subtract.1_norm_mha_3.dc.multiply.8, norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.8, ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_3.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_3.dc.multiply.9, ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_3_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_3.dc.add.10, ff.bert.encoder.layer.3.intermediate.dense.weight, ff.bert.encoder.layer.3.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff3_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_3_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_3.dc.add.10_add_ff_3: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_3.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_11_temporal_epoch_1:
    target_device: 4
    input_count: 1
    ff_3_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff3_gelu, ff.bert.encoder.layer.3.output.dense.weight, ff.bert.encoder.layer.3.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_3: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_3.dc.add.10_add_ff_3, ff_3_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_3, lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_3_norm_ff_3.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_3],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_3_norm_ff_3.dc.subtract.1, norm_ff_3.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_3.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_3.dc.subtract.1, norm_ff_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.2, lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_3.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_3.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_3.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_3.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_3.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_3.dc.reciprocal.7, lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_3.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_3.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_3.dc.subtract.1_norm_ff_3.dc.multiply.8, norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_3.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.8, ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_3.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_3.dc.multiply.9, ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_12_temporal_epoch_1:
    target_device: 5
    input_count: 1
    mha_4_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_3.dc.add.10, ff.bert.encoder.layer.4.attention.self.query.weight, ff.bert.encoder.layer.4.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_4_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_3.dc.add.10, ff.bert.encoder.layer.4.attention.self.key.weight, ff.bert.encoder.layer.4.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_4_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_4_query, mha_4_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_4],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_4_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_4_as, ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_4_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_4_as_div, e2e_attention_mask_s_brcst_m2_7_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_4_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_4_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_4_as_mask_mha_4_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_4_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_4_as_mask_mha_4_as_softmax.dc.subtract.1, mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_4_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_4_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_4_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.exp.2, lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_4_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_4_as_softmax.dc.exp.2_mha_4_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_4_as_softmax.dc.exp.2_mha_4_as_softmax.dc.multiply.5, mha_4_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_4_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_3.dc.add.10, ff.bert.encoder.layer.4.attention.self.value.weight, ff.bert.encoder.layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_4_value_mha_4_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_4_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_4_value_mha_4_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_4_value_mha_4_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_4_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_4_as_softmax.dc.multiply.5, buffer_0_mha_4_value_mha_4_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_4_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_4_ac, ff.bert.encoder.layer.4.attention.output.dense.weight, ff.bert.encoder.layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_3.dc.add.10_add_mha_4: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_3.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_4: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_3.dc.add.10_add_mha_4, mha_4_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_4, lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_4_norm_mha_4.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_4_norm_mha_4.dc.subtract.1, norm_mha_4.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_13_temporal_epoch_1:
    target_device: 6
    input_count: 1
    norm_mha_4.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_4.dc.subtract.1, norm_mha_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.2, lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_4.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_4.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_4.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_4.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_4.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_4.dc.reciprocal.7, lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_4.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_4.dc.subtract.1_norm_mha_4.dc.multiply.8, norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_4.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.8, ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_4.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_4.dc.multiply.9, ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_4_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_4.dc.add.10, ff.bert.encoder.layer.4.intermediate.dense.weight, ff.bert.encoder.layer.4.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff4_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_4_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_4.dc.add.10_add_ff_4: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_4.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_14_temporal_epoch_1:
    target_device: 7
    input_count: 1
    ff_4_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff4_gelu, ff.bert.encoder.layer.4.output.dense.weight, ff.bert.encoder.layer.4.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_4: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_4.dc.add.10_add_ff_4, ff_4_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_4, lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_4_norm_ff_4.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_4],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_4_norm_ff_4.dc.subtract.1, norm_ff_4.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_4.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_4.dc.subtract.1, norm_ff_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.2, lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_4.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_4.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_4.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_4.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_4.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_4.dc.reciprocal.7, lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_4.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_4.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_4.dc.subtract.1_norm_ff_4.dc.multiply.8, norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_4.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.8, ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_4.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_4.dc.multiply.9, ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_15_temporal_epoch_1:
    target_device: 0
    input_count: 1
    mha_5_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.query.weight, ff.bert.encoder.layer.5.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_5_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.key.weight, ff.bert.encoder.layer.5.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_5_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_5_query, mha_5_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_5],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_5_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_5_as, ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_5_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_5_as_div, e2e_attention_mask_s_brcst_m2_6_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_5_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_5_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_5_as_mask_mha_5_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_5_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_5_as_mask_mha_5_as_softmax.dc.subtract.1, mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_5_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_5_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_5_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.exp.2, lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_5_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_5_as_softmax.dc.exp.2_mha_5_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_5_as_softmax.dc.exp.2_mha_5_as_softmax.dc.multiply.5, mha_5_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_5_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_4.dc.add.10, ff.bert.encoder.layer.5.attention.self.value.weight, ff.bert.encoder.layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_5_value_mha_5_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_5_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_5_value_mha_5_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_5_value_mha_5_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_5_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_5_as_softmax.dc.multiply.5, buffer_0_mha_5_value_mha_5_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_5_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_5_ac, ff.bert.encoder.layer.5.attention.output.dense.weight, ff.bert.encoder.layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_4.dc.add.10_add_mha_5: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_4.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_5: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_4.dc.add.10_add_mha_5, mha_5_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_5, lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_5_norm_mha_5.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_5_norm_mha_5.dc.subtract.1, norm_mha_5.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_16_temporal_epoch_2:
    target_device: 1
    input_count: 1
    norm_mha_5.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [e2e_norm_mha_5.dc.subtract.1_0, e2e_norm_mha_5.dc.subtract.1_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.2, lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_5.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_5.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_5.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_5.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_5.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_5.dc.reciprocal.7, lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_5.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8, norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_5.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.8, ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_5.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_5.dc.multiply.9, ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_5_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_5.dc.add.10, ff.bert.encoder.layer.5.intermediate.dense.weight, ff.bert.encoder.layer.5.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff5_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_5_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_5.dc.add.10_add_ff_5: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_5.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_17_temporal_epoch_2:
    target_device: 2
    input_count: 1
    ff_5_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff5_gelu, ff.bert.encoder.layer.5.output.dense.weight, ff.bert.encoder.layer.5.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_5: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_5.dc.add.10_add_ff_5, ff_5_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_5, lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_5_norm_ff_5.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_5],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_5_norm_ff_5.dc.subtract.1, norm_ff_5.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_5.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_5.dc.subtract.1, norm_ff_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.2, lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_5.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_5.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_5.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_5.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_5.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_5.dc.reciprocal.7, lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_5.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_5.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_5.dc.subtract.1_norm_ff_5.dc.multiply.8, norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_5.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.8, ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_5.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_5.dc.multiply.9, ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_18_temporal_epoch_2:
    target_device: 3
    input_count: 1
    mha_6_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.query.weight, ff.bert.encoder.layer.6.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_6_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.key.weight, ff.bert.encoder.layer.6.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_6_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_6_query, mha_6_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_6],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_6_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_6_as, ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_6_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_6_as_div, e2e_attention_mask_s_brcst_m2_5_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_6_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_6_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_6_as_mask_mha_6_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_6_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_6_as_mask_mha_6_as_softmax.dc.subtract.1, mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_6_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_6_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_6_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.exp.2, lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_6_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_6_as_softmax.dc.exp.2_mha_6_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_6_as_softmax.dc.exp.2_mha_6_as_softmax.dc.multiply.5, mha_6_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_6_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_5.dc.add.10, ff.bert.encoder.layer.6.attention.self.value.weight, ff.bert.encoder.layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_6_value_mha_6_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_6_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_6_value_mha_6_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_6_value_mha_6_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_6_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_6_as_softmax.dc.multiply.5, buffer_0_mha_6_value_mha_6_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_6_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_6_ac, ff.bert.encoder.layer.6.attention.output.dense.weight, ff.bert.encoder.layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_5.dc.add.10_add_mha_6: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_5.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_6: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_5.dc.add.10_add_mha_6, mha_6_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_6, lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_6_norm_mha_6.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_6_norm_mha_6.dc.subtract.1, norm_mha_6.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_19_temporal_epoch_2:
    target_device: 4
    input_count: 1
    norm_mha_6.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_6.dc.subtract.1, norm_mha_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.2, lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_6.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_6.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_6.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_6.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_6.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_6.dc.reciprocal.7, lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_6.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_6.dc.subtract.1_norm_mha_6.dc.multiply.8, norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_6.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.8, ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_6.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_6.dc.multiply.9, ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_6_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_6.dc.add.10, ff.bert.encoder.layer.6.intermediate.dense.weight, ff.bert.encoder.layer.6.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff6_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_6_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_6.dc.add.10_add_ff_6: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_6.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_20_temporal_epoch_2:
    target_device: 5
    input_count: 1
    ff_6_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff6_gelu, ff.bert.encoder.layer.6.output.dense.weight, ff.bert.encoder.layer.6.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_6: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_6.dc.add.10_add_ff_6, ff_6_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_6, lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_6_norm_ff_6.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_6_norm_ff_6.dc.subtract.1, norm_ff_6.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_6.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_6.dc.subtract.1, norm_ff_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.2, lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_6.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_6.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_6.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_6.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_6.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_6.dc.reciprocal.7, lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_6.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_6.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_6.dc.subtract.1_norm_ff_6.dc.multiply.8, norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_6.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.8, ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_6.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_6.dc.multiply.9, ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_21_temporal_epoch_2:
    target_device: 6
    input_count: 1
    mha_7_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.query.weight, ff.bert.encoder.layer.7.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_7_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.key.weight, ff.bert.encoder.layer.7.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_7_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_7_query, mha_7_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_7],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_7_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_7_as, ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_7_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_7_as_div, e2e_attention_mask_s_brcst_m2_4_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_7_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_7_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_7_as_mask_mha_7_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_7_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_7_as_mask_mha_7_as_softmax.dc.subtract.1, mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_7_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_7_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_7_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.exp.2, lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_7_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_7_as_softmax.dc.exp.2_mha_7_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_7_as_softmax.dc.exp.2_mha_7_as_softmax.dc.multiply.5, mha_7_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_7_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_6.dc.add.10, ff.bert.encoder.layer.7.attention.self.value.weight, ff.bert.encoder.layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_7_value_mha_7_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_7_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_7_value_mha_7_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_7_value_mha_7_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_7_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_7_as_softmax.dc.multiply.5, buffer_0_mha_7_value_mha_7_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_7_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_7_ac, ff.bert.encoder.layer.7.attention.output.dense.weight, ff.bert.encoder.layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_6.dc.add.10_add_mha_7: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_6.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_7: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_6.dc.add.10_add_mha_7, mha_7_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_7, lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_7_norm_mha_7.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_7_norm_mha_7.dc.subtract.1, norm_mha_7.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_22_temporal_epoch_2:
    target_device: 7
    input_count: 1
    norm_mha_7.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_7.dc.subtract.1, norm_mha_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.2, lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_7.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_7.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_7.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_7.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_7.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_7.dc.reciprocal.7, lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_7.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_7.dc.subtract.1_norm_mha_7.dc.multiply.8, norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_7.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.8, ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_7.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_7.dc.multiply.9, ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_7_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_7.dc.add.10, ff.bert.encoder.layer.7.intermediate.dense.weight, ff.bert.encoder.layer.7.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff7_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_7_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_7.dc.add.10_add_ff_7: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_7.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_23_temporal_epoch_2:
    target_device: 0
    input_count: 1
    ff_7_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff7_gelu, ff.bert.encoder.layer.7.output.dense.weight, ff.bert.encoder.layer.7.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_7: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_7.dc.add.10_add_ff_7, ff_7_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_7, lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_7_norm_ff_7.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_7],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_7_norm_ff_7.dc.subtract.1, norm_ff_7.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_7.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_7.dc.subtract.1, norm_ff_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.2, lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_7.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_7.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_7.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_7.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_7.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_7.dc.reciprocal.7, lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_7.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_7.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_7.dc.subtract.1_norm_ff_7.dc.multiply.8, norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_7.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.8, ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_7.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_7.dc.multiply.9, ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_24_temporal_epoch_3:
    target_device: 1
    input_count: 1
    mha_8_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [e2e_norm_ff_7.dc.add.10_0, ff.bert.encoder.layer.8.attention.self.query.weight, ff.bert.encoder.layer.8.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_8_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [e2e_norm_ff_7.dc.add.10_0, ff.bert.encoder.layer.8.attention.self.key.weight, ff.bert.encoder.layer.8.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_8_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_8_query, mha_8_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_8],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_8_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_8_as, ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_8_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_8_as_div, e2e_attention_mask_s_brcst_m2_3_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_8_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_8_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_8_as_mask_mha_8_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_8_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_8_as_mask_mha_8_as_softmax.dc.subtract.1, mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_8_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_8_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_8_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.exp.2, lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_8_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_8_as_softmax.dc.exp.2_mha_8_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_8_as_softmax.dc.exp.2_mha_8_as_softmax.dc.multiply.5, mha_8_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_8_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [e2e_norm_ff_7.dc.add.10_0, ff.bert.encoder.layer.8.attention.self.value.weight, ff.bert.encoder.layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_8_value_mha_8_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_8_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_8_value_mha_8_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_8_value_mha_8_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_8_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_8_as_softmax.dc.multiply.5, buffer_0_mha_8_value_mha_8_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_8_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_8_ac, ff.bert.encoder.layer.8.attention.output.dense.weight, ff.bert.encoder.layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_7.dc.add.10_add_mha_8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [e2e_norm_ff_7.dc.add.10_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_8: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_7.dc.add.10_add_mha_8, mha_8_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_8, lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_8_norm_mha_8.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_8_norm_mha_8.dc.subtract.1, norm_mha_8.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_25_temporal_epoch_3:
    target_device: 2
    input_count: 1
    norm_mha_8.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_8.dc.subtract.1, norm_mha_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.2, lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_8.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_8.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_8.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_8.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_8.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_8.dc.reciprocal.7, lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_8.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_8.dc.subtract.1_norm_mha_8.dc.multiply.8, norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_8.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.8, ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_8.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_8.dc.multiply.9, ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_8_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_8.dc.add.10, ff.bert.encoder.layer.8.intermediate.dense.weight, ff.bert.encoder.layer.8.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff8_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_8_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_8.dc.add.10_add_ff_8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_8.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_26_temporal_epoch_3:
    target_device: 3
    input_count: 1
    ff_8_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff8_gelu, ff.bert.encoder.layer.8.output.dense.weight, ff.bert.encoder.layer.8.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_8: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_8.dc.add.10_add_ff_8, ff_8_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_8, lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_8_norm_ff_8.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_8_norm_ff_8.dc.subtract.1, norm_ff_8.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_8.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_8.dc.subtract.1, norm_ff_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.2, lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_8.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_8.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_8.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_8.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_8.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_8.dc.reciprocal.7, lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_8.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_8.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_8.dc.subtract.1_norm_ff_8.dc.multiply.8, norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_8.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.8, ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_8.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_8.dc.multiply.9, ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_27_temporal_epoch_3:
    target_device: 4
    input_count: 1
    mha_9_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.query.weight, ff.bert.encoder.layer.9.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_9_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.key.weight, ff.bert.encoder.layer.9.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_9_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_9_query, mha_9_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_9],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_9_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_9_as, ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_9_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_9_as_div, e2e_attention_mask_s_brcst_m2_2_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_9_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_9_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_9_as_mask_mha_9_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_9_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_9_as_mask_mha_9_as_softmax.dc.subtract.1, mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_9_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_9_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_9_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.exp.2, lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_9_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_9_as_softmax.dc.exp.2_mha_9_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_9_as_softmax.dc.exp.2_mha_9_as_softmax.dc.multiply.5, mha_9_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_9_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_8.dc.add.10, ff.bert.encoder.layer.9.attention.self.value.weight, ff.bert.encoder.layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_9_value_mha_9_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_9_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_9_value_mha_9_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_9_value_mha_9_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_9_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_9_as_softmax.dc.multiply.5, buffer_0_mha_9_value_mha_9_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_9_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_9_ac, ff.bert.encoder.layer.9.attention.output.dense.weight, ff.bert.encoder.layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_8.dc.add.10_add_mha_9: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_8.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_9: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_8.dc.add.10_add_mha_9, mha_9_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_9, lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_9_norm_mha_9.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_9],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_9_norm_mha_9.dc.subtract.1, norm_mha_9.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_28_temporal_epoch_3:
    target_device: 5
    input_count: 1
    norm_mha_9.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_9.dc.subtract.1, norm_mha_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.2, lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_9.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_9.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_9.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_9.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_9.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_9.dc.reciprocal.7, lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_9.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_9.dc.subtract.1_norm_mha_9.dc.multiply.8, norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_9.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.8, ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_9.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_9.dc.multiply.9, ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_9_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_9.dc.add.10, ff.bert.encoder.layer.9.intermediate.dense.weight, ff.bert.encoder.layer.9.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff9_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_9_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_9.dc.add.10_add_ff_9: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_9.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_29_temporal_epoch_3:
    target_device: 6
    input_count: 1
    ff_9_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [ff9_gelu, ff.bert.encoder.layer.9.output.dense.weight, ff.bert.encoder.layer.9.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_9: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [buffer_0_norm_mha_9.dc.add.10_add_ff_9, ff_9_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_9, lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_9_norm_ff_9.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_9],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_9_norm_ff_9.dc.subtract.1, norm_ff_9.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_9.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_9.dc.subtract.1, norm_ff_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.2, lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_9.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_9.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_9.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_9.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_9.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_9.dc.reciprocal.7, lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_9.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_9.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_9.dc.subtract.1_norm_ff_9.dc.multiply.8, norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_9.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.8, ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_9.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_9.dc.multiply.9, ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_30_temporal_epoch_3:
    target_device: 7
    input_count: 1
    mha_10_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.query.weight, ff.bert.encoder.layer.10.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_10_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.key.weight, ff.bert.encoder.layer.10.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_10_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_10_query, mha_10_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_10],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_10_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_10_as, ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_10_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_10_as_div, e2e_attention_mask_s_brcst_m2_1_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_10_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_10_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_10_as_mask_mha_10_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_10_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_mha_10_as_mask_mha_10_as_softmax.dc.subtract.1, mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_10_as_softmax.dc.exp.2: {type: exp, grid_loc: [4, 3], grid_size: [2, 2], inputs: [mha_10_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_10_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.exp.2, lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_10_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [4, 6], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_10_as_softmax.dc.exp.2_mha_10_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 0], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [4, 7], grid_size: [2, 1], inputs: [buffer_0_mha_10_as_softmax.dc.exp.2_mha_10_as_softmax.dc.multiply.5, mha_10_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_10_value: {type: matmul, grid_loc: [6, 0], grid_size: [2, 4], inputs: [norm_ff_9.dc.add.10, ff.bert.encoder.layer.10.attention.self.value.weight, ff.bert.encoder.layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_10_value_mha_10_ac: {type: nop, grid_loc: [6, 4], grid_size: [2, 1], inputs: [mha_10_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_10_value_mha_10_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_10_value_mha_10_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_10_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_10_as_softmax.dc.multiply.5, buffer_0_mha_10_value_mha_10_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_10_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_10_ac, ff.bert.encoder.layer.10.attention.output.dense.weight, ff.bert.encoder.layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_9.dc.add.10_add_mha_10: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_9.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_10: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_9.dc.add.10_add_mha_10, mha_10_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_10, lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_10_norm_mha_10.dc.subtract.1: {type: nop, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_10_norm_mha_10.dc.subtract.1, norm_mha_10.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    buffer_1_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8: {type: nop, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_31_temporal_epoch_3:
    target_device: 0
    input_count: 1
    norm_mha_10.dc.multiply.2: {type: multiply, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_10.dc.subtract.1, norm_mha_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.2, lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_10.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_10.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_10.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_10.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_10.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_10.dc.reciprocal.7, lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [buffer_1_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_10.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_10.dc.subtract.1_norm_mha_10.dc.multiply.8, norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_10.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.8, ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_10.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_10.dc.multiply.9, ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_10_ff1: {type: matmul, grid_loc: [4, 0], grid_size: [6, 4], inputs: [norm_mha_10.dc.add.10, ff.bert.encoder.layer.10.intermediate.dense.weight, ff.bert.encoder.layer.10.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff10_gelu: {type: gelu, grid_loc: [4, 4], grid_size: [2, 4], inputs: [ff_10_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_norm_mha_10.dc.add.10_add_ff_10: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_mha_10.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_32_temporal_epoch_4:
    target_device: 1
    input_count: 1
    ff_10_ff2: {type: matmul, grid_loc: [0, 0], grid_size: [2, 8], inputs: [e2e_ff10_gelu_0, ff.bert.encoder.layer.10.output.dense.weight, ff.bert.encoder.layer.10.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    add_ff_10: {type: add, grid_loc: [2, 0], grid_size: [2, 1], inputs: [e2e_buffer_0_norm_mha_10.dc.add.10_add_ff_10_0, ff_10_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [add_ff_10, lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_10_norm_ff_10.dc.subtract.1: {type: nop, grid_loc: [2, 1], grid_size: [2, 1], inputs: [add_ff_10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.subtract.1: {type: subtract, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_add_ff_10_norm_ff_10.dc.subtract.1, norm_ff_10.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_10.dc.multiply.2: {type: multiply, grid_loc: [2, 6], grid_size: [2, 1], inputs: [norm_ff_10.dc.subtract.1, norm_ff_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.2, lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_10.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [2, 1], inputs: [norm_ff_10.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_10.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.sqrt.6: {type: sqrt, grid_loc: [4, 1], grid_size: [2, 1], inputs: [norm_ff_10.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 2], grid_size: [2, 1], inputs: [norm_ff_10.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [2, 1], inputs: [norm_ff_10.dc.reciprocal.7, lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8: {type: nop, grid_loc: [2, 4], grid_size: [2, 1], inputs: [norm_ff_10.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [buffer_1_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_10.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [2, 1], inputs: [buffer_0_norm_ff_10.dc.subtract.1_norm_ff_10.dc.multiply.8, norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_10.dc.multiply.9: {type: multiply, grid_loc: [4, 6], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.8, ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_10.dc.add.10: {type: add, grid_loc: [5, 5], grid_size: [2, 1], inputs: [norm_ff_10.dc.multiply.9, ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}

  fwd_33_temporal_epoch_4:
    target_device: 2
    input_count: 1
    mha_11_query: {type: matmul, grid_loc: [0, 0], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.query.weight, ff.bert.encoder.layer.11.attention.self.query.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_11_key: {type: matmul, grid_loc: [0, 4], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.key.weight, ff.bert.encoder.layer.11.attention.self.key.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    mha_11_as: {type: matmul, grid_loc: [2, 0], grid_size: [2, 1], inputs: [mha_11_query, mha_11_key],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.lc1: {type: matmul, grid_loc: [2, 1], grid_size: [1, 1], inputs: [lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0, ff.reciprocal_of_sqrt_of_head_size_11],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.lc1, lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0],
         t: 1, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    mha_11_as_div: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [mha_11_as, ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}, broadcast: {c: 12}]}
    mha_11_as_mask: {type: add, grid_loc: [2, 4], grid_size: [2, 1], inputs: [mha_11_as_div, e2e_attention_mask_s_brcst_m2_0_1.lc1_0],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}, broadcast: {r: 12}]}
    mha_11_as_softmax.dc.reduce_max.0: {type: reduce, grid_loc: [2, 6], grid_size: [2, 1], inputs: [mha_11_as_mask],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {dim: c, m_k: 1, type: max, u_kt: 12}}
    mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.reduce_max.0, lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 1}}
    buffer_0_mha_11_as_mask_mha_11_as_softmax.dc.subtract.1: {type: nop, grid_loc: [2, 7], grid_size: [2, 1], inputs: [mha_11_as_mask],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_as_softmax.dc.subtract.1: {type: subtract, grid_loc: [4, 6], grid_size: [2, 1], inputs: [buffer_0_mha_11_as_mask_mha_11_as_softmax.dc.subtract.1, mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_11_as_softmax.dc.exp.2: {type: exp, grid_loc: [6, 0], grid_size: [2, 2], inputs: [mha_11_as_softmax.dc.subtract.1],
         t: 16, mblock: [3, 3], ublock: [2, 2], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    mha_11_as_softmax.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.exp.2, lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {m_k: 1, u_kt: 12}}
    mha_11_as_softmax.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 3], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.reduce_sum.3.lc1],
         t: 16, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_mha_11_as_softmax.dc.exp.2_mha_11_as_softmax.dc.multiply.5: {type: nop, grid_loc: [4, 7], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.exp.2],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_as_softmax.dc.multiply.5: {type: multiply, grid_loc: [6, 4], grid_size: [2, 1], inputs: [buffer_0_mha_11_as_softmax.dc.exp.2_mha_11_as_softmax.dc.multiply.5, mha_11_as_softmax.dc.reciprocal.4],
         t: 16, mblock: [3, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}]}
    mha_11_value: {type: matmul, grid_loc: [4, 0], grid_size: [2, 4], inputs: [norm_ff_10.dc.add.10, ff.bert.encoder.layer.11.attention.self.value.weight, ff.bert.encoder.layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 4, u_kt: 8}}
    buffer_1_mha_11_value_mha_11_ac: {type: nop, grid_loc: [4, 4], grid_size: [2, 1], inputs: [mha_11_value],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_mha_11_value_mha_11_ac: {type: nop, grid_loc: [6, 5], grid_size: [2, 1], inputs: [buffer_1_mha_11_value_mha_11_ac],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    mha_11_ac: {type: matmul, grid_loc: [6, 6], grid_size: [2, 1], inputs: [mha_11_as_softmax.dc.multiply.5, buffer_0_mha_11_value_mha_11_ac],
         t: 16, mblock: [3, 1], ublock: [2, 2], buf_size_mb: 32, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 2, u_kt: 6}}
    mha_11_output: {type: matmul, grid_loc: [8, 0], grid_size: [2, 4], inputs: [mha_11_ac, ff.bert.encoder.layer.11.attention.output.dense.weight, ff.bert.encoder.layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 16, u_kt: 2}}
    buffer_0_norm_ff_10.dc.add.10_add_mha_11: {type: nop, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_10.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_mha_11: {type: add, grid_loc: [6, 7], grid_size: [2, 1], inputs: [buffer_0_norm_ff_10.dc.add.10_add_mha_11, mha_11_output],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [2, 1], inputs: [add_mha_11, lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_mha_11_norm_mha_11.dc.subtract.1: {type: nop, grid_loc: [8, 5], grid_size: [2, 1], inputs: [add_mha_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.subtract.1: {type: subtract, grid_loc: [8, 6], grid_size: [2, 1], inputs: [buffer_0_add_mha_11_norm_mha_11.dc.subtract.1, norm_mha_11.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_mha_11.dc.multiply.2: {type: multiply, grid_loc: [8, 7], grid_size: [2, 1], inputs: [norm_mha_11.dc.subtract.1, norm_mha_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_34_temporal_epoch_4:
    target_device: 3
    input_count: 1
    norm_mha_11.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.2, lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_mha_11.dc.add.5: {type: add, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_mha_11.dc.reduce_avg.3.lc1, dc.input_tensor.norm_mha_11.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.sqrt.6: {type: sqrt, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_mha_11.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_mha_11.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [norm_mha_11.dc.reciprocal.7, lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [norm_mha_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8: {type: nop, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_1_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_mha_11.dc.multiply.8: {type: multiply, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0_norm_mha_11.dc.subtract.1_norm_mha_11.dc.multiply.8, norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_11.dc.multiply.9: {type: multiply, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.8, ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_mha_11.dc.add.10: {type: add, grid_loc: [2, 3], grid_size: [2, 1], inputs: [norm_mha_11.dc.multiply.9, ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff_11_ff1: {type: matmul, grid_loc: [2, 4], grid_size: [6, 4], inputs: [norm_mha_11.dc.add.10, ff.bert.encoder.layer.11.intermediate.dense.weight, ff.bert.encoder.layer.11.intermediate.dense.bias],
         t: 1, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    ff11_gelu: {type: gelu, grid_loc: [5, 0], grid_size: [2, 4], inputs: [ff_11_ff1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    ff_11_ff2: {type: matmul, grid_loc: [8, 0], grid_size: [2, 8], inputs: [ff11_gelu, ff.bert.encoder.layer.11.output.dense.weight, ff.bert.encoder.layer.11.output.dense.bias],
         t: 1, mblock: [3, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, m_k: 16, u_kt: 8}}
    buffer_0_norm_mha_11.dc.add.10_add_ff_11: {type: nop, grid_loc: [3, 0], grid_size: [2, 1], inputs: [norm_mha_11.dc.add.10],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_ff_11: {type: add, grid_loc: [3, 2], grid_size: [2, 1], inputs: [buffer_0_norm_mha_11.dc.add.10_add_ff_11, ff_11_ff2],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_35_temporal_epoch_4:
    target_device: 0
    input_count: 1
    norm_ff_11.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [add_ff_11, lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    buffer_0_add_ff_11_norm_ff_11.dc.subtract.1: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [add_ff_11],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.subtract.1: {type: subtract, grid_loc: [0, 2], grid_size: [2, 1], inputs: [buffer_0_add_ff_11_norm_ff_11.dc.subtract.1, norm_ff_11.dc.reduce_avg.0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    norm_ff_11.dc.multiply.2: {type: multiply, grid_loc: [0, 3], grid_size: [2, 1], inputs: [norm_ff_11.dc.subtract.1, norm_ff_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.2, lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 1, u_kt: 32}}
    norm_ff_11.dc.add.5: {type: add, grid_loc: [0, 7], grid_size: [2, 1], inputs: [norm_ff_11.dc.reduce_avg.3.lc1, dc.input_tensor.norm_ff_11.4],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.sqrt.6: {type: sqrt, grid_loc: [2, 0], grid_size: [2, 1], inputs: [norm_ff_11.dc.add.5],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.reciprocal.7: {type: reciprocal, grid_loc: [2, 1], grid_size: [2, 1], inputs: [norm_ff_11.dc.sqrt.6],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [2, 1], inputs: [norm_ff_11.dc.reciprocal.7, lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [3, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    buffer_1_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8: {type: nop, grid_loc: [0, 4], grid_size: [2, 1], inputs: [norm_ff_11.dc.subtract.1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8: {type: nop, grid_loc: [0, 6], grid_size: [2, 1], inputs: [buffer_1_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    norm_ff_11.dc.multiply.8: {type: multiply, grid_loc: [2, 3], grid_size: [2, 1], inputs: [buffer_0_norm_ff_11.dc.subtract.1_norm_ff_11.dc.multiply.8, norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_11.dc.multiply.9: {type: multiply, grid_loc: [2, 5], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.8, ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, ff.bert.encoder.layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    norm_ff_11.dc.add.10: {type: add, grid_loc: [2, 7], grid_size: [2, 1], inputs: [norm_ff_11.dc.multiply.9, ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 12}]}
    norm_ff_11.dc.add.10_output_nop_0: {type: nop, grid_loc: [3, 4], grid_size: [2, 1], inputs: [norm_ff_11.dc.add.10], untilize_output: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_36_temporal_epoch_4:
    target_device: 4
    input_count: 1

  fwd_37_temporal_epoch_4:
    target_device: 5
    input_count: 1

  fwd_38_temporal_epoch_4:
    target_device: 6
    input_count: 1

  fwd_39_temporal_epoch_4:
    target_device: 7
    input_count: 1


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0, $gptr_q1: 0, $lptr_q1: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $gptr_q4: 0, $lptr_q3: 0, $lptr_q4: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e_ff2_gelu_0, e2e_buffer_0_norm_mha_2.dc.add.10_add_ff_2_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {
               encoder_input: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               ff.bert.encoder.layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_0: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_0_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_11_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_0_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_0_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_10_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_9_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_8_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_7_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_6_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_5_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_4_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_3_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_2_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_1_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.attention_mask_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_1_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_0.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_0.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_0.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_2_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_0.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_0.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_3_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_1: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_1_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_1_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_1_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_4_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_1.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_1.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_1.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_5_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_1.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_1.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_6_temporal_epoch_0, queue_settings: {
               ff.bert.encoder.layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_2: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_2_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_2_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_2_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_2.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_7_temporal_epoch_0, queue_settings: {
               lc.input_tensor.norm_mha_2.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_2.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_2.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_norm_mha_5.dc.subtract.1_0, e2e_buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8_0]
    -   execute: {graph_name: fwd_8_temporal_epoch_1, queue_settings: {
               e2e_ff2_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_buffer_0_norm_mha_2.dc.add.10_add_ff_2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_2.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_2.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_9_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_8_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_3: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_3_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_3_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_3_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_3.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_10_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_3.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_3.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_3.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_11_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_3.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_3.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_12_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_7_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_4: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_4_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_4_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_4_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_4.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_13_temporal_epoch_1, queue_settings: {
               lc.input_tensor.norm_mha_4.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_4.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_4.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_14_temporal_epoch_1, queue_settings: {
               ff.bert.encoder.layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_4.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_4.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_15_temporal_epoch_1, queue_settings: {
               e2e_attention_mask_s_brcst_m2_6_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               ff.bert.encoder.layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_5: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_5_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_5_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_5_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_5.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_ff2_gelu_0, e2e_buffer_0_norm_mha_2.dc.add.10_add_ff_2_0, e2e_attention_mask_s_brcst_m2_8_1.lc1_0, e2e_attention_mask_s_brcst_m2_7_1.lc1_0, e2e_attention_mask_s_brcst_m2_6_1.lc1_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_norm_ff_7.dc.add.10_0]
    -   execute: {graph_name: fwd_16_temporal_epoch_2, queue_settings: {
               e2e_norm_mha_5.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.norm_mha_5.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_5.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_5.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_17_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_5.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_5.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_18_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_5_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_6: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_6_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_6_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_6_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_6.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_19_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_6.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_6.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_6.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_20_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_6.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_6.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_21_temporal_epoch_2, queue_settings: {
               e2e_attention_mask_s_brcst_m2_4_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               ff.bert.encoder.layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_7: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_7_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_7_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_7_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_7.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_22_temporal_epoch_2, queue_settings: {
               lc.input_tensor.norm_mha_7.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_7.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_7.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_23_temporal_epoch_2, queue_settings: {
               ff.bert.encoder.layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_7.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_7.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_norm_mha_5.dc.subtract.1_0, e2e_buffer_1_norm_mha_5.dc.subtract.1_norm_mha_5.dc.multiply.8_0, e2e_attention_mask_s_brcst_m2_5_1.lc1_0, e2e_attention_mask_s_brcst_m2_4_1.lc1_0]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_ff10_gelu_0, e2e_buffer_0_norm_mha_10.dc.add.10_add_ff_10_0]
    -   execute: {graph_name: fwd_24_temporal_epoch_3, queue_settings: {
               e2e_norm_ff_7.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_attention_mask_s_brcst_m2_3_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_8: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_8_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_8_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_8_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_8.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_25_temporal_epoch_3, queue_settings: {
               lc.input_tensor.norm_mha_8.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_8.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_8.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_26_temporal_epoch_3, queue_settings: {
               ff.bert.encoder.layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_8.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_8.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_27_temporal_epoch_3, queue_settings: {
               e2e_attention_mask_s_brcst_m2_2_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_9: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_9_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_9_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_9_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_9.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_28_temporal_epoch_3, queue_settings: {
               lc.input_tensor.norm_mha_9.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_9.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_9.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_29_temporal_epoch_3, queue_settings: {
               ff.bert.encoder.layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_9.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_9.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_30_temporal_epoch_3, queue_settings: {
               e2e_attention_mask_s_brcst_m2_1_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               ff.bert.encoder.layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_10: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_10_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_10_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_10_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_10.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_31_temporal_epoch_3, queue_settings: {
               lc.input_tensor.norm_mha_10.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_10.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_10.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_norm_ff_7.dc.add.10_0, e2e_attention_mask_s_brcst_m2_3_1.lc1_0, e2e_attention_mask_s_brcst_m2_2_1.lc1_0, e2e_attention_mask_s_brcst_m2_1_1.lc1_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_32_temporal_epoch_4, queue_settings: {
               e2e_ff10_gelu_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_buffer_0_norm_mha_10.dc.add.10_add_ff_10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               ff.bert.encoder.layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_10.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_10.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_33_temporal_epoch_4, queue_settings: {
               e2e_attention_mask_s_brcst_m2_0_1.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               ff.bert.encoder.layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m2_0_1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.reciprocal_of_sqrt_of_head_size_11: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.reciprocal_of_sqrt_of_head_size_11_s_brcst_m1_0_2.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_11_as_softmax.dc.reduce_max.0_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.mha_11_as_softmax.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_11.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_34_temporal_epoch_4, queue_settings: {
               lc.input_tensor.norm_mha_11.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_mha_11.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_mha_11.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_35_temporal_epoch_4, queue_settings: {
               lc.input_tensor.norm_ff_11.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_11.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.norm_ff_11.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.norm_ff_11.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.ff.bert.encoder.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               ff.bert.encoder.layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_36_temporal_epoch_4}
    -   execute: {graph_name: fwd_37_temporal_epoch_4}
    -   execute: {graph_name: fwd_38_temporal_epoch_4}
    -   execute: {graph_name: fwd_39_temporal_epoch_4}
    -   deallocate_queue: [e2e_ff10_gelu_0, e2e_buffer_0_norm_mha_10.dc.add.10_add_ff_10_0, e2e_attention_mask_s_brcst_m2_0_1.lc1_0]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    - endloop


