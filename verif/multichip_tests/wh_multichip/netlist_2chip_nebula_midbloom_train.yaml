# git checkout 26688b3
# tinybloom.py --with-pybuda -n 2 -d silicon --no_tt_opt

devices:
  arch: [wormhole, wormhole_b0]

queues:

  # input
  hidden_states:                                                                                                                                                                                               {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [32, 32], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[0, 0x30810400]]}

  # output
  transformer.output_layernorm_167:                                                                                                                                                                            {input: layernorm_167.dc.add.10_output_nop_0, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [16, 8], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: host, host: [0x0]}
  output_grad_hidden_states:                                                                                                                                                                                   {input: bw_in0_hidden_states_combine_add_0_output_nop_0, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [16, 8], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: host, host: [0x800020]}

  # parameter
  layers.0.input_layernorm.weight:                                                                                                                                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3fb2e80]]}
  layers.0.input_layernorm.bias:                                                                                                                                                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x38db140]]}
  layers.0.self_attention.query.weight:                                                                                                                                                                        {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x451ac20], [1, 0x4557360], [2, 0x455b440], [1, 0x4597b80], [2, 0x459bc60], [1, 0x45d83a0], [2, 0x45dc480], [1, 0x4618bc0], [2, 0x461cca0], [1, 0x46593e0], [2, 0x465d4c0], [1, 0x4699c00], [2, 0x469dce0], [1, 0x46da420], [2, 0x46de500], [1, 0x471ac40]]}
  layers.0.self_attention.query.bias:                                                                                                                                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x471ed20], [2, 0x4726e40], [2, 0x472ef60], [2, 0x4737080]]}
  layers.0.self_attention.key.weight:                                                                                                                                                                          {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3fd32a0], [2, 0x4013ac0], [2, 0x40542e0], [2, 0x4094b00], [2, 0x40d5320], [2, 0x4115b40], [2, 0x4156360], [2, 0x4196b80], [2, 0x41d73a0], [2, 0x4217bc0], [2, 0x42583e0], [2, 0x4298c00], [2, 0x42d9420], [2, 0x4319c40], [2, 0x435a460], [2, 0x439ac80]]}
  layers.0.self_attention.key.bias:                                                                                                                                                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x473f1a0], [2, 0x47472c0], [2, 0x474f3e0], [2, 0x4757500]]}
  layers.0.self_attention.value.weight:                                                                                                                                                                        {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x4762660], [2, 0x47a2e80], [2, 0x47e36a0], [2, 0x4823ec0], [2, 0x48646e0], [2, 0x48a4f00], [2, 0x48e5720], [2, 0x4925f40], [2, 0x4966760], [2, 0x49a6f80], [2, 0x49e77a0], [2, 0x4a27fc0], [2, 0x4a687e0], [2, 0x4aa9000], [2, 0x4ae9820], [2, 0x4b2a040]]}
  layers.0.self_attention.value.bias:                                                                                                                                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x475c4a0], [2, 0x475f620], [1, 0x47645c0], [2, 0x4767740]]}
  layers.0.self_attention.dense.weight:                                                                                                                                                                        {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x455e560], [1, 0x4595a60], [2, 0x459ed80], [1, 0x45d6280], [2, 0x45df5a0], [1, 0x4616aa0], [2, 0x461fdc0], [1, 0x46572c0], [2, 0x46605e0], [1, 0x4697ae0], [2, 0x46a0e00], [1, 0x46d8300], [2, 0x46e1620], [1, 0x4718b20], [2, 0x4721e40], [1, 0x4759340]]}
  layers.0.self_attention.dense.bias:                                                                                                                                                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x4aff640], [5, 0x4b07760], [5, 0x4b0f880], [5, 0x4b179a0]]}
  layers.0.post_attention_layernorm.weight:                                                                                                                                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x47718e0]]}
  layers.0.post_attention_layernorm.bias:                                                                                                                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x478eb80]]}
  layers.0.mlp.dense_h_to_4h.weight:                                                                                                                                                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [8, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x4792d40], [2, 0x4894d60], [2, 0x4996d80], [2, 0x4a98da0], [2, 0x4b9adc0], [2, 0x4c9cde0], [2, 0x4d9ee00], [2, 0x4ea0e20], [2, 0x4fa2e40], [2, 0x50a4e60], [2, 0x51a6e80], [2, 0x52a8ea0], [2, 0x53aaec0], [2, 0x54acee0], [2, 0x55aef00], [2, 0x56b0f20]]}
  layers.0.mlp.dense_h_to_4h.bias:                                                                                                                                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x48351c0], [1, 0x48453e0], [1, 0x4855600], [1, 0x4865820], [1, 0x4875a40], [1, 0x4885c60], [1, 0x4895e80], [1, 0x48a60a0]]}
  layers.0.mlp.dense_4h_to_h.weight:                                                                                                                                                                           {input: HOST, type: ram, entries: 1, grid_size: [4, 4], t: 1, mblock: [16, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x47bf0c0], [1, 0x48c10e0], [1, 0x49c3100], [1, 0x4ac5120], [1, 0x4bc7140], [1, 0x4cc9160], [1, 0x4dcb180], [1, 0x4ecd1a0], [1, 0x4fcf1c0], [1, 0x50d11e0], [1, 0x51d3200], [1, 0x52d5220], [1, 0x53d7240], [1, 0x54d9260], [1, 0x55db280], [1, 0x56dd2a0]]}
  layers.0.mlp.dense_4h_to_h.bias:                                                                                                                                                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4814d40], [1, 0x481ce60], [1, 0x4824f80], [1, 0x482d0a0]]}
  final_layernorm.weight:                                                                                                                                                                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x47f38e0]]}
  final_layernorm.bias:                                                                                                                                                                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f6e540]]}

  # constant
  lc.input_tensor.layernorm_112.dc.reduce_avg.0.0:                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f8e960]]}
  lc.input_tensor.layernorm_112.dc.reduce_avg.3.0:                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f8f9a0]]}
  dc.input_tensor.layernorm_112.4:                                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f909e0]]}
  lc.input_tensor.layernorm_112.dc.reciprocal.7_s_brcst_m1_1_0.0:                                                                                                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3fb0e00]]}
  lc.input_tensor.layers.0.input_layernorm.weight_s_brcst_m2_0_0.0:                                                                                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3fb1e40]]}
  lc.input_tensor.layers.0.input_layernorm.bias_s_brcst_m2_0_0.0:                                                                                                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x4519be0]]}
  input_1_multiply_129_fork_clone422_tile_bcast_tile_bcast:                                                                                                                                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x410b860]]}
  input_0_add_130_tile_bcast:                                                                                                                                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 4], t: 16, mblock: [1, 2], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x410c8a0], [1, 0x418d8c0], [1, 0x420e8e0], [1, 0x428f900]]}
  input_1_add_132:                                                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x4310920], [1, 0x4351140], [1, 0x4391960], [1, 0x43d2180], [2, 0x43db4a0], [1, 0x44129a0], [2, 0x441bcc0], [1, 0x44531c0], [2, 0x445c4e0], [1, 0x44939e0], [2, 0x449cd00], [1, 0x44d4200], [2, 0x44dd520], [1, 0x4514a20], [2, 0x451dd40], [1, 0x4555240]]}
  lc.input_tensor.softmax_133.dc.reduce_sum.1.0:                                                                                                                                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x475b460]]}
  lc.input_tensor.layernorm_153.dc.reduce_avg.0.0:                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x4afe600]]}
  lc.input_tensor.layernorm_153.dc.reduce_avg.3.0:                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x476c6e0]]}
  dc.input_tensor.layernorm_153.4:                                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x476d720]]}
  lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_1_0.0:                                                                                                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x476f860]]}
  lc.input_tensor.layers.0.post_attention_layernorm.weight_s_brcst_m2_0_0.0:                                                                                                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x47708a0]]}
  lc.input_tensor.layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.0:                                                                                                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x478db40]]}
  lc.input_tensor.layernorm_167.dc.reduce_avg.0.0:                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4813d00]]}
  lc.input_tensor.layernorm_167.dc.reduce_avg.3.0:                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x47be080]]}
  dc.input_tensor.layernorm_167.4:                                                                                                                                                                             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x479dc60]]}
  lc.input_tensor.layernorm_167.dc.reciprocal.7_s_brcst_m1_1_0.0:                                                                                                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x479cc20]]}
  lc.input_tensor.final_layernorm.weight_s_brcst_m2_0_0.0:                                                                                                                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x479bbe0]]}
  lc.input_tensor.final_layernorm.bias_s_brcst_m2_0_0.0:                                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x479aba0]]}
  lc.input_tensor.bw_in2_layernorm_167_layernorm_bw_0.dc.reduce_sum.0.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x4799b60]]}
  lc.input_tensor.bw_in1_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x47f28a0]]}
  lc.input_tensor.layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.0:                                                                                                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x47d1440]]}
  lc.input_tensor.final_layernorm.weight_s_brcst_m2_1_0.0:                                                                                                                                                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x4791d00]]}
  lc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x47d0400]]}
  lc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.3.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x47cf3c0]]}
  dc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.6:                                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x38da100], [2, 0x38fa520], [5, 0x391a920], [2, 0x393ad40], [5, 0x395b140], [2, 0x397b560], [5, 0x399b960], [2, 0x39bbd80], [5, 0x39dc180], [2, 0x39fc5a0], [5, 0x3a1c9a0], [2, 0x3a3cdc0], [5, 0x3a5d1c0], [2, 0x3a7d5e0], [5, 0x3a9d9e0], [2, 0x3abde00]]}
  lc.input_tensor.bw_in1_add_164_brcst_reduce_sum_0.0:                                                                                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x4518ba0]]}
  lc.input_tensor.bw_in1_add_158_brcst_reduce_sum_0.0:                                                                                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x38da100]]}
  lc.input_tensor.bw_in2_layernorm_153_layernorm_bw_0.dc.reduce_sum.0.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3ce2300]]}
  lc.input_tensor.bw_in1_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3ce3340]]}
  lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.0:                                                                                                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x3d02720]]}
  lc.input_tensor.layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.0:                                                                                                                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x3d03760]]}
  lc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3afe620]]}
  lc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.3.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3aff660]]}
  dc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.6:                                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3d047a0], [2, 0x3d047a0], [1, 0x3d44fc0], [2, 0x3d44fc0], [1, 0x3d857e0], [2, 0x3d857e0], [1, 0x3dc6000], [2, 0x3dc6000], [1, 0x3e06820], [2, 0x3e06820], [1, 0x3e47040], [2, 0x3e47040], [1, 0x3e87860], [2, 0x3e87860], [1, 0x3ec8080], [2, 0x3ec8080]]}
  lc.input_tensor.bw_in1_add_150_brcst_reduce_sum_0.0:                                                                                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3b006a0]]}
  lc.input_tensor.bw_in1_add_139_brcst_reduce_sum_0.0:                                                                                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x3ce2300]]}
  lc.input_tensor.bw_in0_softmax_133_softmax_bw_0.dc.reduce_sum.1.0:                                                                                                                                           {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f29c00]]}
  input_1_multiply_129_tile_bcast_tile_bcast:                                                                                                                                                                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3f088a0]]}
  lc.input_tensor.bw_in1_add_123_brcst_reduce_sum_0.0:                                                                                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x3f088a0]]}
  lc.input_tensor.bw_in1_add_117_brcst_reduce_sum_0.0:                                                                                                                                                         {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3f29d00]]}
  lc.input_tensor.bw_in2_layernorm_112_layernorm_bw_0.dc.reduce_sum.0.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f2ac40]]}
  lc.input_tensor.bw_in1_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f4c0a0]]}
  lc.input_tensor.layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.0:                                                                                                                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x43119e0]]}
  lc.input_tensor.layers.0.input_layernorm.weight_s_brcst_m2_1_0.0:                                                                                                                                            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f6d500]]}
  lc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x4312a20]]}
  lc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.3.0:                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x4313a60]]}
  dc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.6:                                                                                                                                                       {input: HOST, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x4314aa0], [1, 0x4353260], [2, 0x43552c0], [1, 0x4393a80], [2, 0x4395ae0], [1, 0x43d42a0], [2, 0x43d6300], [1, 0x4414ac0], [2, 0x4416b20], [1, 0x44552e0], [2, 0x4457340], [1, 0x4495b00], [2, 0x4497b60], [1, 0x44d6320], [2, 0x44d8380], [1, 0x4516b40]]}

  # epoch_to_epoch
  e2e_matmul_115_0:                                                                                                                                                                                            {input: matmul_115_serialized_to_e2e_matmul_115_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x5737d00]]}
  e2e_matmul_121_0:                                                                                                                                                                                            {input: matmul_121_serialized_to_e2e_matmul_121_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x4f27cc0]]}
  e2e_layernorm_112.dc.add.10_0:                                                                                                                                                                               {input: layernorm_112.dc.add.10, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x57b2f40], [2, 0x57f3760], [2, 0x5833f80], [2, 0x58747a0], [2, 0x58b4fc0], [2, 0x58f57e0], [2, 0x5936000], [2, 0x5976820], [2, 0x59b7040], [2, 0x59f7860], [2, 0x5a38080], [2, 0x5a788a0], [2, 0x5ab90c0], [2, 0x5af98e0], [2, 0x5b3a100], [2, 0x5b7a920]]}
  e2e_softmax_133.dc.multiply.3_0:                                                                                                                                                                             {input: softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x93651a0]]}
  e2e_matmul_137_0:                                                                                                                                                                                            {input: matmul_137_serialized_to_e2e_matmul_137_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x98028c0]]}
  e2e_layernorm_153.dc.multiply.9_0:                                                                                                                                                                           {input: layernorm_153.dc.multiply.9_serialized_to_e2e_layernorm_153.dc.multiply.9_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x9c0a8e0]]}
  e2e_layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.lc1_0:                                                                                                                                             {input: layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[0, 0x354a8820]]}
  e2e_matmul_156_0:                                                                                                                                                                                            {input: matmul_156_serialized_to_e2e_matmul_156_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0xa012900]]}
  e2e_add_152_0:                                                                                                                                                                                               {input: add_152_serialized_to_e2e_add_152_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x348d1900]]}
  e2e_layernorm_167.dc.subtract.1_0:                                                                                                                                                                           {input: layernorm_167.dc.subtract.1_serialized_to_e2e_layernorm_167.dc.subtract.1_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x93651a0]]}
  e2e_layernorm_167.dc.multiply.8_0:                                                                                                                                                                           {input: layernorm_167.dc.multiply.8_serialized_to_e2e_layernorm_167.dc.multiply.8_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x344c98e0]]}
  e2e_layernorm_167.dc.reciprocal.7_0:                                                                                                                                                                         {input: layernorm_167.dc.reciprocal.7, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x32061040]]}
  e2e_layernorm_167.dc.multiply.8_1:                                                                                                                                                                           {input: layernorm_167.dc.multiply.8_serialized_to_e2e_layernorm_167.dc.multiply.8_1_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x340c18c0]]}
  e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_2:                                                                                                                                                     {input: bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x98028c0], [2, 0x98430e0], [2, 0x9883900], [2, 0x98c4120], [2, 0x9904940], [2, 0x9945160], [2, 0x9985980], [2, 0x99c61a0], [2, 0x9a069c0], [2, 0x9a471e0], [2, 0x9a87a00], [2, 0x9ac8220], [2, 0x9b08a40], [2, 0x9b49260], [2, 0x9b89a80], [2, 0x9bca2a0]]}
  e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_1:                                                                                                                                                     {input: bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x976d1c0], [3, 0x97ad9e0], [3, 0x97ee200], [3, 0x982ea20], [3, 0x986f240], [3, 0x98afa60], [3, 0x98f0280], [3, 0x9930aa0], [3, 0x99712c0], [3, 0x99b1ae0], [3, 0x99f2300], [3, 0x9a32b20], [3, 0x9a73340], [3, 0x9ab3b60], [3, 0x9af4380], [3, 0x9b34ba0]]}
  e2e_gelu_159_0:                                                                                                                                                                                              {input: gelu_159_serialized_to_e2e_gelu_159_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x330a18a0]]}
  e2e_matmul_156_1:                                                                                                                                                                                            {input: matmul_156, type: queue, entries: 1, grid_size: [8, 8], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x50e6ae0], [1, 0x5127300], [1, 0x5167b20], [1, 0x51a8340], [1, 0x51e8b60], [1, 0x5229380], [1, 0x5269ba0], [1, 0x52aa3c0], [1, 0x52eabe0], [1, 0x532b400], [1, 0x536bc20], [1, 0x53ac440], [1, 0x53ecc60], [1, 0x542d480], [1, 0x546dca0], [1, 0x54ae4c0], [1, 0x54eece0], [1, 0x552f500], [1, 0x556fd20], [1, 0x55b0540], [1, 0x55f0d60], [1, 0x5631580], [1, 0x5671da0], [1, 0x56b25c0], [1, 0x56f2de0], [1, 0x5733600], [1, 0x5773e20], [1, 0x57b4640], [1, 0x57f4e60], [1, 0x5835680], [1, 0x5875ea0], [1, 0x58b66c0], [1, 0x58f6ee0], [1, 0x5937700], [1, 0x5977f20], [1, 0x59b8740], [1, 0x59f8f60], [1, 0x5a39780], [1, 0x5a79fa0], [1, 0x5aba7c0], [1, 0x5afafe0], [1, 0x5b3b800], [1, 0x5b7c020], [1, 0x5bbc840], [1, 0x5bfd060], [1, 0x5c3d880], [1, 0x5c7e0a0], [1, 0x5cbe8c0], [1, 0x5cff0e0], [1, 0x5d3f900], [1, 0x5d80120], [1, 0x5dc0940], [1, 0x5e01160], [1, 0x5e41980], [1, 0x5e821a0], [1, 0x5ec29c0], [1, 0x5f031e0], [1, 0x5f43a00], [1, 0x5f84220], [1, 0x5fc4a40], [1, 0x6005260], [1, 0x6045a80], [1, 0x60862a0], [1, 0x60c6ac0]]}
  e2e_bw_in0_matmul_162_matmul_1_0:                                                                                                                                                                            {input: bw_in0_matmul_162_matmul_1, type: queue, entries: 1, grid_size: [8, 8], t: 1, mblock: [2, 4], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x9c0aac0], [2, 0x9c4b2e0], [2, 0x9c8bb00], [2, 0x9ccc320], [2, 0x9d0cb40], [2, 0x9d4d360], [2, 0x9d8db80], [2, 0x9dce3a0], [2, 0x9e0ebc0], [2, 0x9e4f3e0], [2, 0x9e8fc00], [2, 0x9ed0420], [2, 0x9f10c40], [2, 0x9f51460], [2, 0x9f91c80], [2, 0x9fd24a0], [2, 0xa012cc0], [2, 0xa0534e0], [2, 0xa093d00], [2, 0xa0d4520], [2, 0xa114d40], [2, 0xa155560], [2, 0xa195d80], [2, 0xa1d65a0], [2, 0xa216dc0], [2, 0xa2575e0], [2, 0xa297e00], [2, 0xa2d8620], [2, 0xa318e40], [2, 0xa359660], [2, 0xa399e80], [2, 0xa3da6a0], [2, 0xa41aec0], [2, 0xa45b6e0], [2, 0xa49bf00], [2, 0xa4dc720], [2, 0xa51cf40], [2, 0xa55d760], [2, 0xa59df80], [2, 0xa5de7a0], [2, 0xa61efc0], [2, 0xa65f7e0], [2, 0xa6a0000], [2, 0xa6e0820], [2, 0xa721040], [2, 0xa761860], [2, 0xa7a2080], [2, 0xa7e28a0], [2, 0xa8230c0], [2, 0xa8638e0], [2, 0xa8a4100], [2, 0xa8e4920], [2, 0xa925140], [2, 0xa965960], [2, 0xa9a6180], [2, 0xa9e69a0], [2, 0xaa271c0], [2, 0xaa679e0], [2, 0xaaa8200], [2, 0xaae8a20], [2, 0xab29240], [2, 0xab69a60], [2, 0xabaa280], [2, 0xabeaaa0]]}
  e2e_bw_in0_gelu_159_multiply_1_0:                                                                                                                                                                            {input: bw_in0_gelu_159_multiply_1_serialized_to_e2e_bw_in0_gelu_159_multiply_1_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x8997b40]]}
  e2e_layernorm_153.dc.add.10_0:                                                                                                                                                                               {input: layernorm_153.dc.add.10_serialized_to_e2e_layernorm_153.dc.add.10_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x32c99880]]}
  e2e_bw_in0_matmul_156_matmul_1_0:                                                                                                                                                                            {input: bw_in0_matmul_156_matmul_1, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x8e44fe0], [5, 0x8e85800], [5, 0x8ec6020], [5, 0x8f06840], [5, 0x8f47060], [5, 0x8f87880], [5, 0x8fc80a0], [5, 0x90088c0], [5, 0x90490e0], [5, 0x9089900], [5, 0x90ca120], [5, 0x910a940], [5, 0x914b160], [5, 0x918b980], [5, 0x91cc1a0], [5, 0x920c9c0]]}
  e2e_layernorm_153.dc.multiply.8_0:                                                                                                                                                                           {input: layernorm_153.dc.multiply.8, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4cde8e0], [1, 0x4d1f100], [1, 0x4d5f920], [1, 0x4da0140], [1, 0x4de0960], [1, 0x4e21180], [1, 0x4e619a0], [1, 0x4ea21c0], [1, 0x4ee29e0], [1, 0x4f23200], [1, 0x4f63a20], [1, 0x4fa4240], [1, 0x4fe4a60], [1, 0x5025280], [1, 0x5065aa0], [1, 0x50a62c0]]}
  e2e_layernorm_153.dc.reciprocal.7_0:                                                                                                                                                                         {input: layernorm_153.dc.reciprocal.7, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x4cbe4c0]]}
  e2e_layernorm_153.dc.multiply.8_1:                                                                                                                                                                           {input: layernorm_153.dc.multiply.8_serialized_to_e2e_layernorm_153.dc.multiply.8_1_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x532fce0]]}
  e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_0:                                                                                                                                                          {input: bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_serialized_to_e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x8a1cba0]]}
  e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0:                                                                                                                                                     {input: bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_serialized_to_e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[4, 0x8ad4140]]}
  e2e_layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                                                                                                                                                      {input: layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x8e24bc0]]}
  e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0:                                                                                                                                                     {input: bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x9b753c0], [3, 0x9bb5be0], [3, 0x9bf6400], [3, 0x9c36c20], [3, 0x9c77440], [3, 0x9cb7c60], [3, 0x9cf8480], [3, 0x9d38ca0], [3, 0x9d794c0], [3, 0x9db9ce0], [3, 0x9dfa500], [3, 0x9e3ad20], [3, 0x9e7b540], [3, 0x9ebbd60], [3, 0x9efc580], [3, 0x9f3cda0]]}
  e2e_matmul_144_0:                                                                                                                                                                                            {input: matmul_144, type: queue, entries: 1, grid_size: [8, 2], t: 16, mblock: [2, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x4b1fac0], [5, 0x4b602e0], [5, 0x4ba0b00], [5, 0x4be1320], [5, 0x4c21b40], [5, 0x4c62360], [5, 0x4ca2b80], [5, 0x4ce33a0], [5, 0x4d23bc0], [5, 0x4d643e0], [5, 0x4da4c00], [5, 0x4de5420], [5, 0x4e25c40], [5, 0x4e66460], [5, 0x4ea6c80], [5, 0x4ee74a0]]}
  e2e_bw_in0_matmul_148_matmul_1_0:                                                                                                                                                                            {input: bw_in0_matmul_148_matmul_1_serialized_to_e2e_bw_in0_matmul_148_matmul_1_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[4, 0x8edc160]]}
  e2e_matmul_137_1:                                                                                                                                                                                            {input: matmul_137, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x48b62c0], [1, 0x48f6ae0], [1, 0x4937300], [1, 0x4977b20], [1, 0x49b8340], [1, 0x49f8b60], [1, 0x4a39380], [1, 0x4a79ba0], [1, 0x4aba3c0], [1, 0x4afabe0], [1, 0x4b3b400], [1, 0x4b7bc20], [1, 0x4bbc440], [1, 0x4bfcc60], [1, 0x4c3d480], [1, 0x4c7dca0]]}
  e2e_softmax_133.dc.multiply.3_1:                                                                                                                                                                             {input: softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_1_0, type: queue, entries: 1, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x4f72880]]}
  e2e_layernorm_112.dc.add.10_1:                                                                                                                                                                               {input: layernorm_112.dc.add.10_serialized_to_e2e_layernorm_112.dc.add.10_1_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x4b6a860]]}
  e2e_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0:                                                                                                                                                         {input: bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_serialized_to_e2e_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x348d1900]]}
  e2e_softmax_133.dc.multiply.3_3:                                                                                                                                                                             {input: softmax_133.dc.multiply.3, type: queue, entries: 1, grid_size: [4, 4], t: 16, mblock: [4, 2], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x499c9a0], [5, 0x4da49c0], [5, 0x51ac9e0], [5, 0x55b4a00], [5, 0x59bca20], [5, 0x5dc4a40], [5, 0x61cca60], [5, 0x65d4a80], [5, 0x69dcaa0], [5, 0x6de4ac0], [5, 0x71ecae0], [5, 0x75f4b00], [5, 0x79fcb20], [5, 0x7e04b40], [5, 0x820cb60], [5, 0x8614b80]]}
  e2e_matmul_115_1:                                                                                                                                                                                            {input: matmul_115, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x38da100], [1, 0x391a920], [1, 0x395b140], [1, 0x399b960], [1, 0x39dc180], [1, 0x3a1c9a0], [1, 0x3a5d1c0], [1, 0x3a9d9e0], [1, 0x3ade200], [1, 0x3b1ea20], [1, 0x3b5f240], [1, 0x3b9fa60], [1, 0x3be0280], [1, 0x3c20aa0], [1, 0x3c612c0], [1, 0x3ca1ae0]]}
  e2e_bw_in1_matmul_128_matmul_1_0:                                                                                                                                                                            {input: bw_in1_matmul_128_matmul_1, type: queue, entries: 1, grid_size: [2, 8], t: 16, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x8a1cba0], [5, 0x8a5d3c0], [5, 0x8a9dbe0], [5, 0x8ade400], [5, 0x8b1ec20], [5, 0x8b5f440], [5, 0x8b9fc60], [5, 0x8be0480], [5, 0x8c20ca0], [5, 0x8c614c0], [5, 0x8ca1ce0], [5, 0x8ce2500], [5, 0x8d22d20], [5, 0x8d63540], [5, 0x8da3d60], [5, 0x8de4580]]}
  e2e_layernorm_112.dc.add.10_2:                                                                                                                                                                               {input: layernorm_112.dc.add.10, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x38da100], [2, 0x391a920], [2, 0x395b140], [2, 0x399b960], [2, 0x39dc180], [2, 0x3a1c9a0], [2, 0x3a5d1c0], [2, 0x3a9d9e0], [2, 0x3ade200], [2, 0x3b1ea20], [2, 0x3b5f240], [2, 0x3b9fa60], [2, 0x3be0280], [2, 0x3c20aa0], [2, 0x3c612c0], [2, 0x3ca1ae0]]}
  e2e_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0:                                                                                                                                                    {input: bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_serialized_to_e2e_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[4, 0x8ad4140]]}
  e2e_bw_in0_matmul_128_matmul_1_0:                                                                                                                                                                            {input: bw_in0_matmul_128_matmul_1_serialized_to_e2e_bw_in0_matmul_128_matmul_1_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x8997b40]]}
  e2e_bw_in0_matmul_137_matmul_1_0:                                                                                                                                                                            {input: bw_in0_matmul_137_matmul_1, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x972cb80], [3, 0x976d3a0], [3, 0x97adbc0], [3, 0x97ee3e0], [3, 0x982ec00], [3, 0x986f420], [3, 0x98afc40], [3, 0x98f0460], [3, 0x9930c80], [3, 0x99714a0], [3, 0x99b1cc0], [3, 0x99f24e0], [3, 0x9a32d00], [3, 0x9a73520], [3, 0x9ab3d40], [3, 0x9af4560]]}
  e2e_bw_in0_matmul_121_matmul_1_0:                                                                                                                                                                            {input: bw_in0_matmul_121_matmul_1, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x98028c0], [2, 0x98430e0], [2, 0x9883900], [2, 0x98c4120], [2, 0x9904940], [2, 0x9945160], [2, 0x9985980], [2, 0x99c61a0], [2, 0x9a069c0], [2, 0x9a471e0], [2, 0x9a87a00], [2, 0x9ac8220], [2, 0x9b08a40], [2, 0x9b49260], [2, 0x9b89a80], [2, 0x9bca2a0]]}
  e2e_bw_in0_reshape_113_combine_add_1_0:                                                                                                                                                                      {input: bw_in0_reshape_113_combine_add_1, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x9c0aac0], [2, 0x9c4b2e0], [2, 0x9c8bb00], [2, 0x9ccc320], [2, 0x9d0cb40], [2, 0x9d4d360], [2, 0x9d8db80], [2, 0x9dce3a0], [2, 0x9e0ebc0], [2, 0x9e4f3e0], [2, 0x9e8fc00], [2, 0x9ed0420], [2, 0x9f10c40], [2, 0x9f51460], [2, 0x9f91c80], [2, 0x9fd24a0]]}
  e2e_layernorm_112.dc.multiply.8_0:                                                                                                                                                                           {input: layernorm_112.dc.multiply.8, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x38da100], [1, 0x391a920], [1, 0x395b140], [1, 0x399b960], [1, 0x39dc180], [1, 0x3a1c9a0], [1, 0x3a5d1c0], [1, 0x3a9d9e0], [1, 0x3ade200], [1, 0x3b1ea20], [1, 0x3b5f240], [1, 0x3b9fa60], [1, 0x3be0280], [1, 0x3c20aa0], [1, 0x3c612c0], [1, 0x3ca1ae0]]}
  e2e_bw_in1_layernorm_112_layernorm_bw_0.dc.multiply.0_0:                                                                                                                                                     {input: bw_in1_layernorm_112_layernorm_bw_0.dc.multiply.0, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x9b34d80], [3, 0x9b755a0], [3, 0x9bb5dc0], [3, 0x9bf65e0], [3, 0x9c36e00], [3, 0x9c77620], [3, 0x9cb7e40], [3, 0x9cf8660], [3, 0x9d38e80], [3, 0x9d796a0], [3, 0x9db9ec0], [3, 0x9dfa6e0], [3, 0x9e3af00], [3, 0x9e7b720], [3, 0x9ebbf40], [3, 0x9efc760]]}
  e2e_layernorm_112.dc.reciprocal.7_0:                                                                                                                                                                         {input: layernorm_112.dc.reciprocal.7, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}
  e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0:                                                                                                                                                     {input: bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x9fbdde0], [3, 0x9ffe600], [3, 0xa03ee20], [3, 0xa07f640], [3, 0xa0bfe60], [3, 0xa100680], [3, 0xa140ea0], [3, 0xa1816c0], [3, 0xa1c1ee0], [3, 0xa202700], [3, 0xa242f20], [3, 0xa283740], [3, 0xa2c3f60], [3, 0xa304780], [3, 0xa344fa0], [3, 0xa3857c0]]}
  e2e_layernorm_112.dc.multiply.8_1:                                                                                                                                                                           {input: layernorm_112.dc.multiply.8_serialized_to_e2e_layernorm_112.dc.multiply.8_1_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x32891860]]}
  e2e_layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0:                                                                                                                                                      {input: layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.lc1, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [16, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[0, 0x354a8820]]}
  e2e_bw_in0_add_152_combine_add_0_0:                                                                                                                                                                          {input: bw_in0_add_152_combine_add_0, type: queue, entries: 1, grid_size: [4, 4], t: 1, mblock: [4, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x93651a0], [3, 0x93a59c0], [3, 0x93e61e0], [3, 0x9426a00], [3, 0x9467220], [3, 0x94a7a40], [3, 0x94e8260], [3, 0x9528a80], [3, 0x95692a0], [3, 0x95a9ac0], [3, 0x95ea2e0], [3, 0x962ab00], [3, 0x966b320], [3, 0x96abb40], [3, 0x96ec360], [3, 0x9f7d5c0]]}
  e2e_bw_in0_hidden_states_combine_add_0_0:                                                                                                                                                                    {input: bw_in0_hidden_states_combine_add_0_serialized_to_e2e_bw_in0_hidden_states_combine_add_0_0_0, type: queue, entries: 1, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x38951920]]}

  # loss
  loss_transformer.output_layernorm_167:                                                                                                                                                                       {input: HOST, type: queue, entries: 2, grid_size: [1, 1], t: 1, mblock: [32, 32], ublock: [1, 1], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[0, 0x30428620]]}

  # buffering
  layernorm_112.dc.multiply.8_serialized_to_layernorm_112.dc.multiply.9_0_to_layernorm_112.dc.multiply.9_0_serialized_dram_queue:                                                                              {input: layernorm_112.dc.multiply.8_serialized_to_layernorm_112.dc.multiply.9_0, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  layers.0.input_layernorm.weight_s_brcst_m2_0_0.lc1_serialized_to_layernorm_112.dc.multiply.9_1_to_layernorm_112.dc.multiply.9_1_serialized_dram_queue:                                                       {input: layers.0.input_layernorm.weight_s_brcst_m2_0_0.lc1_serialized_to_layernorm_112.dc.multiply.9_1, type: queue, entries: 2, grid_size: [1, 1], t: 8, mblock: [1, 1], ublock: [1, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}
  add_132_serialized_to_softmax_133.dc.exp.0_0_to_softmax_133.dc.exp.0_0_serialized_dram_queue:                                                                                                                {input: add_132_serialized_to_softmax_133.dc.exp.0_0, type: queue, entries: 2, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[3, 0x7af4920]]}
  add_152_serialized_to_layernorm_153.dc.subtract.1_0_to_layernorm_153.dc.subtract.1_0_serialized_dram_queue:                                                                                                  {input: add_152_serialized_to_layernorm_153.dc.subtract.1_0, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x8187b20]]}
  layernorm_153.dc.reduce_avg.0.lc1_serialized_to_layernorm_153.dc.subtract.1_1_to_layernorm_153.dc.subtract.1_1_serialized_dram_queue:                                                                        {input: layernorm_153.dc.reduce_avg.0.lc1_serialized_to_layernorm_153.dc.subtract.1_1, type: queue, entries: 2, grid_size: [1, 1], t: 16, mblock: [1, 1], ublock: [2, 1], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x8147300]]}
  layernorm_153.dc.add.10_serialized_to_matmul_156_0_to_matmul_156_0_serialized_dram_queue:                                                                                                                    {input: layernorm_153.dc.add.10_serialized_to_matmul_156_0, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x5bbb140]]}
  gelu_159_serialized_to_matmul_162_0_to_matmul_162_0_serialized_dram_queue:                                                                                                                                   {input: gelu_159_serialized_to_matmul_162_0, type: queue, entries: 2, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x61072e0]]}
  layernorm_167.dc.add.10_serialized_to_layernorm_167.dc.add.10_output_nop_0_0_to_layernorm_167.dc.add.10_output_nop_0_0_serialized_dram_queue:                                                                {input: layernorm_167.dc.add.10_serialized_to_layernorm_167.dc.add.10_output_nop_0_0, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x63cb160]]}
  bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_0_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_0_serialized_dram_queue:                {input: bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_0, type: queue, entries: 2, grid_size: [1, 1], t: 16, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x7ab4100]]}
  bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.4_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_1_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_1_serialized_dram_queue:                      {input: bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.4_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_1, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[4, 0x7ab4100]]}
  bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8_0_to_bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8_0_serialized_dram_queue:            {input: bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8_0, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x785fb00]]}
  layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0_to_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0_serialized_dram_queue:             {input: layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0, type: queue, entries: 2, grid_size: [1, 1], t: 16, mblock: [1, 1], ublock: [2, 1], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x781f2e0]]}
  bw_in1_matmul_162_transpose_0_serialized_to_bw_in1_matmul_162_matmul_1_0_to_bw_in1_matmul_162_matmul_1_0_serialized_dram_queue:                                                                              {input: bw_in1_matmul_162_transpose_0_serialized_to_bw_in1_matmul_162_matmul_1_0, type: queue, entries: 2, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x57df2c0]]}
  bw_in0_gelu_159_gelu_derivative_0_serialized_to_bw_in0_gelu_159_multiply_1_0_to_bw_in0_gelu_159_multiply_1_0_serialized_dram_queue:                                                                          {input: bw_in0_gelu_159_gelu_derivative_0_serialized_to_bw_in0_gelu_159_multiply_1_0, type: queue, entries: 2, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x5b3fd20]]}
  bw_in0_matmul_156_matmul_1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0_serialized_dram_queue:                                   {input: bw_in0_matmul_156_matmul_1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x8ff28a0]]}
  layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.lc1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_1_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_1_serialized_dram_queue:  {input: layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.lc1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_1, type: queue, entries: 2, grid_size: [1, 1], t: 8, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x9324980]]}
  bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.9_serialized_to_bw_in0_add_152_combine_add_0_1_to_bw_in0_add_152_combine_add_0_1_serialized_dram_queue:                                                      {input: bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.9_serialized_to_bw_in0_add_152_combine_add_0_1, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x8b14960]]}
  softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_1_0_serialized_to_bw_in1_matmul_144_transpose_0_0_to_bw_in1_matmul_144_transpose_0_0_serialized_dram_queue:                            {input: softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_1_0_serialized_to_bw_in1_matmul_144_transpose_0_0, type: queue, entries: 2, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x6bdb180]]}
  bw_in1_matmul_144_matmul_1_serialized_to_bw_in0_matmul_137_matmul_1_0_to_bw_in0_matmul_137_matmul_1_0_serialized_dram_queue:                                                                                 {input: bw_in1_matmul_144_matmul_1_serialized_to_bw_in0_matmul_137_matmul_1_0, type: queue, entries: 2, grid_size: [1, 1], t: 512, mblock: [1, 1], ublock: [2, 1], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x8304940]]}
  bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.multiply.0_0_to_bw_in0_softmax_133_softmax_bw_0.dc.multiply.0_0_serialized_dram_queue:                                           {input: bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.multiply.0_0, type: queue, entries: 2, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x838fd60]]}
  bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0_to_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0_serialized_dram_queue:                                           {input: bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0, type: queue, entries: 2, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[4, 0x8ad4140]]}
  bw_in0_multiply_129_multiply_0_serialized_to_bw_in0_matmul_128_matmul_1_0_to_bw_in0_matmul_128_matmul_1_0_serialized_dram_queue:                                                                             {input: bw_in0_multiply_129_multiply_0_serialized_to_bw_in0_matmul_128_matmul_1_0, type: queue, entries: 2, grid_size: [1, 1], t: 2048, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x806fb20]]}
  bw_in1_matmul_128_matmul_1_serialized_to_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0_to_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0_serialized_dram_queue:                                 {input: bw_in1_matmul_128_matmul_1_serialized_to_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0, type: queue, entries: 2, grid_size: [1, 1], t: 256, mblock: [1, 1], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[4, 0x82c4120]]}
  bw_in0_matmul_115_matmul_1_serialized_to_bw_in0_reshape_113_combine_add_1_1_to_bw_in0_reshape_113_combine_add_1_1_serialized_dram_queue:                                                                     {input: bw_in0_matmul_115_matmul_1_serialized_to_bw_in0_reshape_113_combine_add_1_1, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x7b7fd40]]}
  bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2_0_to_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2_0_serialized_dram_queue:            {input: bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2_0, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: c, df: Float32, target_device: 0, loc: dram, dram: [[4, 0x82c4120]]}
  bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.7_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8_1_to_bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8_1_serialized_dram_queue:            {input: bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.7_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8_1, type: queue, entries: 2, grid_size: [1, 1], t: 128, mblock: [1, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[3, 0x7af4920]]}

  # grad_accumulator
  grad_acc_final_layernorm.bias:                                                                                                                                                                               {input: bw_in2_layernorm_167_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x38da100]]}
  grad_acc_final_layernorm.weight:                                                                                                                                                                             {input: bw_in1_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x47d2480]]}
  grad_acc_layers.0.mlp.dense_4h_to_h.bias:                                                                                                                                                                    {input: bw_in1_add_164_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x47aefa0]]}
  grad_acc_layers.0.mlp.dense_4h_to_h.weight:                                                                                                                                                                  {input: bw_in1_matmul_162_matmul_1, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [16, 1], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[5, 0x3ade200], [5, 0x3b5f220], [5, 0x3be0240], [5, 0x3c61260], [5, 0x3ce2280], [5, 0x3d632a0], [5, 0x3de42c0], [5, 0x3e652e0], [5, 0x3ee6300], [5, 0x3f67320], [5, 0x3fe8340], [5, 0x4069360], [5, 0x40ea380], [5, 0x416b3a0], [5, 0x41ec3c0], [5, 0x426d3e0], [5, 0x42ee400], [5, 0x436f420], [5, 0x43f0440], [5, 0x4471460], [5, 0x44f2480], [5, 0x45734a0], [5, 0x45f44c0], [5, 0x46754e0], [5, 0x46f6500], [5, 0x4777520], [5, 0x47f8540], [5, 0x4879560], [5, 0x48fa580], [5, 0x497b5a0], [5, 0x49fc5c0], [5, 0x4a7d5e0]]}
  grad_acc_layers.0.mlp.dense_h_to_4h.bias:                                                                                                                                                                    {input: bw_in1_add_158_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 16], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x38fb560], [5, 0x393bd80]]}
  grad_acc_layers.0.mlp.dense_h_to_4h.weight:                                                                                                                                                                  {input: bw_in1_matmul_156_matmul_1, type: ram, entries: 1, grid_size: [4, 8], t: 1, mblock: [4, 4], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[5, 0x397c5a0], [5, 0x39fd5c0], [5, 0x3a7e5e0], [5, 0x3aff600], [5, 0x3b80620], [5, 0x3c01640], [5, 0x3c82660], [5, 0x3d03680], [5, 0x3d846a0], [5, 0x3e056c0], [5, 0x3e866e0], [5, 0x3f07700], [5, 0x3f88720], [5, 0x4009740], [5, 0x408a760], [5, 0x410b780], [5, 0x418c7a0], [5, 0x420d7c0], [5, 0x428e7e0], [5, 0x430f800], [5, 0x4390820], [5, 0x4411840], [5, 0x4492860], [5, 0x4513880], [5, 0x45948a0], [5, 0x46158c0], [5, 0x46968e0], [5, 0x4717900], [5, 0x4798920], [5, 0x4819940], [5, 0x489a960], [5, 0x491b980]]}
  grad_acc_layers.0.post_attention_layernorm.bias:                                                                                                                                                             {input: bw_in2_layernorm_153_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x3ce2300]]}
  grad_acc_layers.0.post_attention_layernorm.weight:                                                                                                                                                           {input: bw_in1_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3ce4380]]}
  grad_acc_layers.0.self_attention.dense.bias:                                                                                                                                                                 {input: bw_in1_add_150_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3b016e0]]}
  grad_acc_layers.0.self_attention.dense.weight:                                                                                                                                                               {input: bw_in1_matmul_148_matmul_1, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3b21b00], [2, 0x3ba2b20], [2, 0x3c23b40], [2, 0x3ca4b60], [2, 0x3d25b80], [2, 0x3da6ba0], [2, 0x3e27bc0], [2, 0x3ea8be0]]}
  grad_acc_layers.0.self_attention.value.bias:                                                                                                                                                                 {input: bw_in1_add_139_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x3ce3340]]}
  grad_acc_layers.0.self_attention.value.weight:                                                                                                                                                               {input: bw_in1_matmul_137_matmul_1, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[1, 0x3d03760], [1, 0x3d84780], [1, 0x3e057a0], [1, 0x3e867c0], [1, 0x3f077e0], [1, 0x3f88800], [1, 0x4009820], [1, 0x408a840]]}
  grad_acc_layers.0.self_attention.key.bias:                                                                                                                                                                   {input: bw_in1_add_123_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3f098e0]]}
  grad_acc_layers.0.self_attention.key.weight:                                                                                                                                                                 {input: bw_in1_matmul_121_matmul_1, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[2, 0x3f098e0], [2, 0x3f8a900], [2, 0x400b920], [2, 0x408c940], [2, 0x410d960], [2, 0x418e980], [2, 0x420f9a0], [2, 0x42909c0]]}
  grad_acc_layers.0.self_attention.query.bias:                                                                                                                                                                 {input: bw_in1_add_117_brcst_reduce_sum_0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3f2ad40]]}
  grad_acc_layers.0.self_attention.query.weight:                                                                                                                                                               {input: bw_in1_matmul_115_matmul_1, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [2, 4], ublock_order: r, df: Float32, target_device: 0, loc: dram, dram: [[1, 0x3f4b160], [1, 0x3fcc180], [1, 0x404d1a0], [1, 0x40ce1c0], [1, 0x414f1e0], [1, 0x41d0200], [1, 0x4251220], [1, 0x42d2240]]}
  grad_acc_layers.0.input_layernorm.bias:                                                                                                                                                                      {input: bw_in2_layernorm_112_layernorm_bw_0.dc.reduce_sum.0.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f2bc80]]}
  grad_acc_layers.0.input_layernorm.weight:                                                                                                                                                                    {input: bw_in1_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.lc1, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, df: Float32, target_device: 1, loc: dram, dram: [[2, 0x3f4d0e0]]}

graphs:
  fwd_0_temporal_epoch_0:
    target_device: 1
    input_count: 1
    layernorm_112.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [hidden_states, lc.input_tensor.layernorm_112.dc.reduce_avg.0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    layernorm_112.dc.subtract.1: {type: subtract, grid_loc: [0, 1], grid_size: [4, 4], inputs: [hidden_states, layernorm_112.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layernorm_112.dc.multiply.2: {type: multiply, grid_loc: [4, 0], grid_size: [4, 4], inputs: [layernorm_112.dc.subtract.1, layernorm_112.dc.subtract.1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_112.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_112.dc.multiply.2, lc.input_tensor.layernorm_112.dc.reduce_avg.3.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    layernorm_112.dc.add.5: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_112.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_112.4],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_112.dc.sqrt.6: {type: sqrt, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_112.dc.add.5],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_112.dc.reciprocal.7: {type: reciprocal, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_112.dc.sqrt.6],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_112.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_112.dc.reciprocal.7, lc.input_tensor.layernorm_112.dc.reciprocal.7_s_brcst_m1_1_0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_112.dc.multiply.8: {type: multiply, grid_loc: [4, 4], grid_size: [4, 4], inputs: [layernorm_112.dc.subtract.1, layernorm_112.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layers.0.input_layernorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layers.0.input_layernorm.weight_s_brcst_m2_0_0.0, layers.0.input_layernorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_112.dc.multiply.8_serialized_to_layernorm_112.dc.multiply.9_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_112.dc.multiply.8],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    layers.0.input_layernorm.weight_s_brcst_m2_0_0.lc1_serialized_to_layernorm_112.dc.multiply.9_1: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layers.0.input_layernorm.weight_s_brcst_m2_0_0.lc1],
         t: 8, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8]}
    matmul_115_serialized_to_e2e_matmul_115_0_0: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [matmul_115],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    matmul_121_serialized_to_e2e_matmul_121_0_0: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [matmul_121],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    layernorm_112.dc.add.10_serialized_to_e2e_layernorm_112.dc.add.10_1_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_112.dc.add.10],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    layernorm_112.dc.multiply.8_serialized_to_e2e_layernorm_112.dc.multiply.8_1_0: {type: nop, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_112.dc.multiply.8],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}

  fwd_1_temporal_epoch_0:
    target_device: 0
    input_count: 1
    layernorm_112.dc.multiply.9: {type: multiply, grid_loc: [0, 0], grid_size: [4, 4], inputs: [layernorm_112.dc.multiply.8_serialized_to_layernorm_112.dc.multiply.9_0_to_layernorm_112.dc.multiply.9_0_serialized_dram_queue, layers.0.input_layernorm.weight_s_brcst_m2_0_0.lc1_serialized_to_layernorm_112.dc.multiply.9_1_to_layernorm_112.dc.multiply.9_1_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, broadcast: {r: 32}], input_0_tms: [vstack: 16, hstack: 8]}
    layers.0.input_layernorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layers.0.input_layernorm.bias_s_brcst_m2_0_0.0, layers.0.input_layernorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_112.dc.add.10: {type: add, grid_loc: [1, 4], grid_size: [4, 4], inputs: [layernorm_112.dc.multiply.9, layers.0.input_layernorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    matmul_115: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [layernorm_112.dc.add.10, layers.0.self_attention.query.weight, layers.0.self_attention.query.bias],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 32}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    matmul_121: {type: matmul, grid_loc: [5, 4], grid_size: [4, 4], inputs: [layernorm_112.dc.add.10, layers.0.self_attention.key.weight, layers.0.self_attention.key.bias],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 32}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}

  fwd_2_temporal_epoch_1:
    target_device: 1
    input_count: 1
    matmul_128: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_matmul_115_0, e2e_matmul_121_0],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16, hslice: 16, transpose], input_0_tms: [hstack: 8, vstack: 16, hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    input_1_multiply_129_fork_clone422_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [input_1_multiply_129_fork_clone422_tile_bcast_tile_bcast],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_129_fork_clone422_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [input_1_multiply_129_fork_clone422_tile_bcast_tile_bcast_splt_brcst_1_0],
         t: 16, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    multiply_129: {type: multiply, grid_loc: [1, 4], grid_size: [4, 4], inputs: [matmul_128, input_1_multiply_129_fork_clone422_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    add_130: {type: add, grid_loc: [4, 0], grid_size: [4, 4], inputs: [input_0_add_130_tile_bcast, multiply_129],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {r: 32}]}
    add_132: {type: add, grid_loc: [5, 4], grid_size: [4, 4], inputs: [add_130, input_1_add_132],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 16}]}
    add_132_serialized_to_softmax_133.dc.exp.0_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_132],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_0_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [softmax_133.dc.multiply.3],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    matmul_137_serialized_to_e2e_matmul_137_0_0: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [matmul_137],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_1_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [softmax_133.dc.multiply.3],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}

  fwd_3_temporal_epoch_1:
    target_device: 0
    input_count: 1
    softmax_133.dc.exp.0: {type: exp, grid_loc: [0, 0], grid_size: [4, 4], inputs: [add_132_serialized_to_softmax_133.dc.exp.0_0_to_softmax_133.dc.exp.0_0_serialized_dram_queue],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 16, hstack: 8],
         attributes: {approximate_mode: false}}
    lc.input_tensor.softmax_133.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.softmax_133.dc.reduce_sum.1.0],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    softmax_133.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [softmax_133.dc.exp.0, lc.input_tensor.softmax_133.dc.reduce_sum.1.0_splt_brcst_1_0],
         t: 16, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    softmax_133.dc.reciprocal.2: {type: reciprocal, grid_loc: [0, 6], grid_size: [1, 1], inputs: [softmax_133.dc.reduce_sum.1.lc1],
         t: 16, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_133.dc.multiply.3: {type: multiply, grid_loc: [1, 4], grid_size: [4, 4], inputs: [softmax_133.dc.exp.0, softmax_133.dc.reciprocal.2],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    matmul_137: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [e2e_layernorm_112.dc.add.10_0, layers.0.self_attention.value.weight, layers.0.self_attention.value.bias],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 32}],
         attributes: {bias: true, m_k: 32, u_kt: 1}}

  fwd_4_temporal_epoch_2:
    target_device: 1
    input_count: 1
    matmul_144: {type: matmul, grid_loc: [0, 0], grid_size: [8, 2], inputs: [e2e_softmax_133.dc.multiply.3_0, e2e_matmul_137_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16, hslice: 16], input_0_tms: [vstack: 16, hstack: 8],
         attributes: {m_k: 4, u_kt: 8}}
    matmul_148: {type: matmul, grid_loc: [0, 2], grid_size: [4, 4], inputs: [matmul_144, layers.0.self_attention.dense.weight, layers.0.self_attention.dense.bias],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 32}], input_0_tms: [hstack: 16],
         attributes: {bias: true, m_k: 32, u_kt: 1}}
    add_152: {type: add, grid_loc: [4, 2], grid_size: [4, 4], inputs: [hidden_states, matmul_148],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_153.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [add_152, lc.input_tensor.layernorm_153.dc.reduce_avg.0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    add_152_serialized_to_layernorm_153.dc.subtract.1_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_152],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    layernorm_153.dc.reduce_avg.0.lc1_serialized_to_layernorm_153.dc.subtract.1_1: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_153.dc.reduce_avg.0.lc1],
         t: 16, mblock: [1, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16]}
    layernorm_153.dc.multiply.9_serialized_to_e2e_layernorm_153.dc.multiply.9_0_0: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_153.dc.multiply.9],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    add_152_serialized_to_e2e_add_152_0_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_152],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    layernorm_153.dc.multiply.8_serialized_to_e2e_layernorm_153.dc.multiply.8_1_0: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_153.dc.multiply.8],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}

  fwd_5_temporal_epoch_2:
    target_device: 0
    input_count: 1
    layernorm_153.dc.subtract.1: {type: subtract, grid_loc: [0, 0], grid_size: [4, 4], inputs: [add_152_serialized_to_layernorm_153.dc.subtract.1_0_to_layernorm_153.dc.subtract.1_0_serialized_dram_queue, layernorm_153.dc.reduce_avg.0.lc1_serialized_to_layernorm_153.dc.subtract.1_1_to_layernorm_153.dc.subtract.1_1_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [vstack: 16, broadcast: {c: 32}], input_0_tms: [vstack: 16, hstack: 8]}
    layernorm_153.dc.multiply.2: {type: multiply, grid_loc: [0, 4], grid_size: [4, 4], inputs: [layernorm_153.dc.subtract.1, layernorm_153.dc.subtract.1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_153.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_153.dc.multiply.2, lc.input_tensor.layernorm_153.dc.reduce_avg.3.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    layernorm_153.dc.add.5: {type: add, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_153.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_153.4],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_153.dc.sqrt.6: {type: sqrt, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_153.dc.add.5],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_153.dc.reciprocal.7: {type: reciprocal, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_153.dc.sqrt.6],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_153.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_153.dc.reciprocal.7, lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_1_0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_153.dc.multiply.8: {type: multiply, grid_loc: [5, 0], grid_size: [4, 4], inputs: [layernorm_153.dc.subtract.1, layernorm_153.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}
    layers.0.post_attention_layernorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layers.0.post_attention_layernorm.weight_s_brcst_m2_0_0.0, layers.0.post_attention_layernorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_153.dc.multiply.9: {type: multiply, grid_loc: [5, 4], grid_size: [4, 4], inputs: [layernorm_153.dc.multiply.8, layers.0.post_attention_layernorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.0, layers.0.post_attention_layernorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  fwd_6_temporal_epoch_3:
    target_device: 1
    input_count: 1
    layernorm_153.dc.add.10: {type: add, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_layernorm_153.dc.multiply.9_0, e2e_layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.lc1_0],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}], input_0_tms: [vstack: 16, hstack: 8]}
    layernorm_153.dc.add.10_serialized_to_matmul_156_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_153.dc.add.10],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    matmul_156_serialized_to_e2e_matmul_156_0_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [matmul_156],
         t: 512, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 32]}
    layernorm_153.dc.add.10_serialized_to_e2e_layernorm_153.dc.add.10_0_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_153.dc.add.10],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}

  fwd_7_temporal_epoch_3:
    target_device: 0
    input_count: 1
    matmul_156: {type: matmul, grid_loc: [0, 0], grid_size: [8, 8], inputs: [layernorm_153.dc.add.10_serialized_to_matmul_156_0_to_matmul_156_0_serialized_dram_queue, layers.0.mlp.dense_h_to_4h.weight, layers.0.mlp.dense_h_to_4h.bias],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 32}], input_0_tms: [vstack: 16, hstack: 8],
         attributes: {bias: true, m_k: 32, u_kt: 1}}

  fwd_8_temporal_epoch_4:
    target_device: 1
    input_count: 1
    gelu_159: {type: gelu, grid_loc: [0, 0], grid_size: [8, 8], inputs: [e2e_matmul_156_0],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 32, vstack: 16],
         attributes: {approximate_mode: true}}
    gelu_159_serialized_to_matmul_162_0: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [gelu_159],
         t: 512, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 32, vslice: 16]}
    layernorm_167.dc.subtract.1_serialized_to_e2e_layernorm_167.dc.subtract.1_0_0: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_167.dc.subtract.1],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    gelu_159_serialized_to_e2e_gelu_159_0_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [gelu_159],
         t: 512, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 32, vslice: 16]}

  fwd_9_temporal_epoch_4:
    target_device: 0
    input_count: 1
    matmul_162: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [gelu_159_serialized_to_matmul_162_0_to_matmul_162_0_serialized_dram_queue, layers.0.mlp.dense_4h_to_h.weight, layers.0.mlp.dense_4h_to_h.bias],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 32}], input_0_tms: [vstack: 16, hstack: 32],
         attributes: {bias: true, m_k: 64, u_kt: 2}}
    add_166: {type: add, grid_loc: [0, 4], grid_size: [4, 4], inputs: [e2e_add_152_0, matmul_162],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 16, hstack: 8]}
    layernorm_167.dc.reduce_avg.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_166, lc.input_tensor.layernorm_167.dc.reduce_avg.0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    layernorm_167.dc.subtract.1: {type: subtract, grid_loc: [4, 1], grid_size: [4, 4], inputs: [add_166, layernorm_167.dc.reduce_avg.0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}]}

  fwd_10_temporal_epoch_5:
    target_device: 1
    input_count: 1
    layernorm_167.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_layernorm_167.dc.subtract.1_0, e2e_layernorm_167.dc.subtract.1_0],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [vstack: 16, hstack: 8], input_0_tms: [vstack: 16, hstack: 8]}
    layernorm_167.dc.reduce_avg.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_167.dc.multiply.2, lc.input_tensor.layernorm_167.dc.reduce_avg.3.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    layernorm_167.dc.add.5: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_167.dc.reduce_avg.3.lc1, dc.input_tensor.layernorm_167.4],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_167.dc.sqrt.6: {type: sqrt, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_167.dc.add.5],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layernorm_167.dc.reciprocal.7: {type: reciprocal, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_167.dc.sqrt.6],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_167.dc.reciprocal.7_s_brcst_m1_1_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_167.dc.reciprocal.7, lc.input_tensor.layernorm_167.dc.reciprocal.7_s_brcst_m1_1_0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_167.dc.multiply.8: {type: multiply, grid_loc: [2, 4], grid_size: [4, 4], inputs: [e2e_layernorm_167.dc.subtract.1_0, layernorm_167.dc.reciprocal.7_s_brcst_m1_1_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}], input_0_tms: [vstack: 16, hstack: 8]}
    final_layernorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [lc.input_tensor.final_layernorm.weight_s_brcst_m2_0_0.0, final_layernorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_167.dc.multiply.9: {type: multiply, grid_loc: [4, 0], grid_size: [4, 4], inputs: [layernorm_167.dc.multiply.8, final_layernorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    final_layernorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.final_layernorm.bias_s_brcst_m2_0_0.0, final_layernorm.bias],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layernorm_167.dc.add.10: {type: add, grid_loc: [6, 4], grid_size: [4, 4], inputs: [layernorm_167.dc.multiply.9, final_layernorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    layernorm_167.dc.add.10_serialized_to_layernorm_167.dc.add.10_output_nop_0_0: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_167.dc.add.10],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    layernorm_167.dc.multiply.8_serialized_to_e2e_layernorm_167.dc.multiply.8_0_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_167.dc.multiply.8],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    layernorm_167.dc.multiply.8_serialized_to_e2e_layernorm_167.dc.multiply.8_1_0: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_167.dc.multiply.8],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}

  fwd_11_temporal_epoch_5:
    target_device: 0
    input_count: 1
    layernorm_167.dc.add.10_output_nop_0: {type: nop, grid_loc: [0, 0], grid_size: [4, 4], inputs: [layernorm_167.dc.add.10_serialized_to_layernorm_167.dc.add.10_output_nop_0_0_to_layernorm_167.dc.add.10_output_nop_0_0_serialized_dram_queue], untilize_output: true,
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8, vstack: 16]}

  bwd_12_temporal_epoch_6:
    target_device: 0
    input_count: 1
    layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_167.dc.reciprocal.7_0, lc.input_tensor.layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    final_layernorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [lc.input_tensor.final_layernorm.weight_s_brcst_m2_1_0.0, final_layernorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 2], grid_size: [4, 4], inputs: [loss_transformer.output_layernorm_167, final_layernorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [4, 0], grid_size: [4, 4], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0, e2e_layernorm_167.dc.multiply.8_1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16]}
    bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [4, 4], grid_size: [4, 4], inputs: [e2e_layernorm_167.dc.multiply.8_1, bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}], input_0_tms: [hstack: 8, vstack: 16]}
    bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_0: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1],
         t: 16, mblock: [1, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16]}
    bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.4_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_1: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.4],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8_0: {type: nop, grid_loc: [1, 1], grid_size: [1, 1], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.lc1],
         t: 16, mblock: [1, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16]}

  bwd_13_temporal_epoch_6:
    target_device: 1
    input_count: 1
    bw_in2_layernorm_167_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_167_layernorm_bw_0.dc.reduce_sum.0.0, loss_transformer.output_layernorm_167], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_167_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [0, 0], grid_size: [4, 4], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_0_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_0_serialized_dram_queue, bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.4_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_1_to_bw_in0_layernorm_167_layernorm_bw_0.dc.add.5_1_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16], input_0_tms: [vstack: 16, broadcast: {c: 32}]}
    bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [0, 4], grid_size: [4, 4], inputs: [dc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.6, bw_in0_layernorm_167_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [4, 0], grid_size: [4, 4], inputs: [bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8_0_to_bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8_0_serialized_dram_queue, bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8, vstack: 16]}
    bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [4, 4], grid_size: [4, 4], inputs: [layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.lc1_serialized_to_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0_to_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0_serialized_dram_queue, bw_in0_layernorm_167_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 16, broadcast: {c: 32}]}

  bwd_14_temporal_epoch_7:
    target_device: 0
    input_count: 1
    bw_in1_layernorm_167_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_layernorm_167.dc.multiply.8_0, loss_transformer.output_layernorm_167],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8, vstack: 16]}
    bw_in1_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_167_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}

  bwd_15_temporal_epoch_7:
    target_device: 1
    input_count: 1
    bw_in0_matmul_162_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 8], inputs: [e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_1, layers.0.mlp.dense_4h_to_h.weight],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 32, u_kt: 1}}

  bwd_16_temporal_epoch_8:
    target_device: 0
    input_count: 1
    bw_in1_add_164_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_164_brcst_reduce_sum_0.0, e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_2], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_matmul_162_transpose_0: {type: nop, grid_loc: [0, 0], grid_size: [8, 8], inputs: [e2e_gelu_159_0],
         t: 1, mblock: [8, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 16, hstack: 32, transpose]}
    bw_in1_matmul_162_transpose_0_serialized_to_bw_in1_matmul_162_matmul_1_0: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [bw_in1_matmul_162_transpose_0],
         t: 512, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 64]}

  bwd_17_temporal_epoch_8:
    target_device: 1
    input_count: 1
    bw_in1_matmul_162_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 8], inputs: [bw_in1_matmul_162_transpose_0_serialized_to_bw_in1_matmul_162_matmul_1_0_to_bw_in1_matmul_162_matmul_1_0_serialized_dram_queue, e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_2], gradient_op: true,
         t: 1, mblock: [16, 1], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 64, hstack: 8],
         attributes: {m_k: 32, u_kt: 1}}

  bwd_18_temporal_epoch_9:
    target_device: 0
    input_count: 1
    bw_in0_gelu_159_gelu_derivative_0: {type: gelu_derivative, grid_loc: [0, 0], grid_size: [8, 8], inputs: [e2e_matmul_156_1],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    bw_in0_gelu_159_gelu_derivative_0_serialized_to_bw_in0_gelu_159_multiply_1_0: {type: nop, grid_loc: [8, 0], grid_size: [1, 1], inputs: [bw_in0_gelu_159_gelu_derivative_0],
         t: 512, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 32]}
    bw_in0_gelu_159_multiply_1_serialized_to_e2e_bw_in0_gelu_159_multiply_1_0_0: {type: nop, grid_loc: [8, 1], grid_size: [1, 1], inputs: [bw_in0_gelu_159_multiply_1],
         t: 512, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 32]}

  bwd_19_temporal_epoch_9:
    target_device: 1
    input_count: 1
    bw_in0_gelu_159_multiply_1: {type: multiply, grid_loc: [0, 0], grid_size: [8, 8], inputs: [bw_in0_gelu_159_gelu_derivative_0_serialized_to_bw_in0_gelu_159_multiply_1_0_to_bw_in0_gelu_159_multiply_1_0_serialized_dram_queue, e2e_bw_in0_matmul_162_matmul_1_0],
         t: 1, mblock: [2, 4], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 32, vstack: 16]}

  bwd_20_temporal_epoch_10:
    target_device: 0
    input_count: 1
    bw_in1_add_158_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 2], inputs: [lc.input_tensor.bw_in1_add_158_brcst_reduce_sum_0.0, e2e_bw_in0_gelu_159_multiply_1_0], gradient_op: true,
         t: 1, mblock: [1, 16], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 32, vstack: 16], input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 32, u_kt: 1}}
    bw_in0_matmul_156_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_bw_in0_gelu_159_multiply_1_0, layers.0.mlp.dense_h_to_4h.weight],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 32, vstack: 16],
         attributes: {m_k: 32, u_kt: 4}}
    bw_in1_matmul_156_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 4], inputs: [e2e_layernorm_153.dc.add.10_0],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 16, hstack: 8, transpose]}
    bw_in1_matmul_156_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [4, 8], inputs: [bw_in1_matmul_156_transpose_0, e2e_bw_in0_gelu_159_multiply_1_0], gradient_op: true,
         t: 1, mblock: [4, 4], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 32, vstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [e2e_layernorm_153.dc.reciprocal.7_0, lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.0, layers.0.post_attention_layernorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    bw_in0_matmul_156_matmul_1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [bw_in0_matmul_156_matmul_1],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.lc1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_1: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.lc1],
         t: 8, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_serialized_to_e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_0_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_153_layernorm_bw_0.dc.add.5],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_serialized_to_e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0_0: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}

  bwd_21_temporal_epoch_10:
    target_device: 1
    input_count: 1
    bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [0, 0], grid_size: [4, 4], inputs: [bw_in0_matmul_156_matmul_1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0_serialized_dram_queue, layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.lc1_serialized_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_1_to_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_1_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, broadcast: {r: 32}], input_0_tms: [hstack: 8, vstack: 16]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [1, 4], grid_size: [4, 4], inputs: [bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0, e2e_layernorm_153.dc.multiply.8_1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [vstack: 16, hstack: 8]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [4, 0], grid_size: [4, 4], inputs: [e2e_layernorm_153.dc.multiply.8_1, bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}], input_0_tms: [vstack: 16, hstack: 8]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [5, 4], grid_size: [4, 4], inputs: [bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}

  bwd_22_temporal_epoch_11:
    target_device: 0
    input_count: 1
    bw_in2_layernorm_153_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_153_layernorm_bw_0.dc.reduce_sum.0.0, e2e_bw_in0_matmul_156_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_layernorm_153_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [4, 4], grid_size: [4, 4], inputs: [e2e_layernorm_153.dc.multiply.8_0, e2e_bw_in0_matmul_156_matmul_1_0],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    bw_in1_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.0, bw_in1_layernorm_153_layernorm_bw_0.dc.multiply.0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [0, 0], grid_size: [4, 4], inputs: [dc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.6, e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_0],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [0, 4], grid_size: [4, 4], inputs: [e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0, bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.7],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8, vstack: 16]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [4, 0], grid_size: [4, 4], inputs: [e2e_layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, bw_in0_layernorm_153_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.9_serialized_to_bw_in0_add_152_combine_add_0_1: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.9],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    bw_in0_matmul_148_matmul_1_serialized_to_e2e_bw_in0_matmul_148_matmul_1_0_0: {type: nop, grid_loc: [8, 3], grid_size: [1, 1], inputs: [bw_in0_matmul_148_matmul_1],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}

  bwd_23_temporal_epoch_11:
    target_device: 1
    input_count: 1
    bw_in0_add_152_combine_add_0: {type: add, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0, bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.9_serialized_to_bw_in0_add_152_combine_add_0_1_to_bw_in0_add_152_combine_add_0_1_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16]}
    bw_in1_add_150_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_150_brcst_reduce_sum_0.0, bw_in0_add_152_combine_add_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_matmul_148_matmul_1: {type: matmul, grid_loc: [0, 4], grid_size: [4, 4], inputs: [bw_in0_add_152_combine_add_0, layers.0.self_attention.dense.weight],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_matmul_148_transpose_0: {type: nop, grid_loc: [4, 0], grid_size: [4, 4], inputs: [e2e_matmul_144_0],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [transpose, vstack: 16]}
    bw_in1_matmul_148_matmul_1: {type: matmul, grid_loc: [5, 4], grid_size: [2, 4], inputs: [bw_in1_matmul_148_transpose_0, bw_in0_add_152_combine_add_0], gradient_op: true,
         t: 1, mblock: [8, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 16, u_kt: 2}}

  bwd_24_temporal_epoch_12:
    target_device: 0
    input_count: 1
    bw_in0_matmul_144_matmul_1: {type: matmul, grid_loc: [4, 0], grid_size: [4, 4], inputs: [e2e_bw_in0_matmul_148_matmul_1_0, e2e_matmul_137_1],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hslice: 16, transpose], input_0_tms: [hstack: 8, vstack: 16, hslice: 16],
         attributes: {m_k: 1, u_kt: 2}}
    bw_in1_matmul_144_transpose_0: {type: nop, grid_loc: [0, 0], grid_size: [4, 4], inputs: [softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_1_0_serialized_to_bw_in1_matmul_144_transpose_0_0_to_bw_in1_matmul_144_transpose_0_0_serialized_dram_queue],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 16, hstack: 8, transpose]}
    bw_in1_matmul_144_matmul_1: {type: matmul, grid_loc: [0, 4], grid_size: [8, 2], inputs: [bw_in1_matmul_144_transpose_0, e2e_bw_in0_matmul_148_matmul_1_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16, hslice: 16],
         attributes: {m_k: 4, u_kt: 8}}
    bw_in1_matmul_144_matmul_1_serialized_to_bw_in0_matmul_137_matmul_1_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [bw_in1_matmul_144_matmul_1],
         t: 512, mblock: [1, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 2, vslice: 16]}

  bwd_25_temporal_epoch_12:
    target_device: 1
    input_count: 1
    softmax_133.dc.multiply.3_serialized_to_e2e_softmax_133.dc.multiply.3_1_0_serialized_to_bw_in1_matmul_144_transpose_0_0: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [e2e_softmax_133.dc.multiply.3_1],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    bw_in1_add_139_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_139_brcst_reduce_sum_0.0, bw_in1_matmul_144_matmul_1], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16], input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
#     bw_in1_matmul_144_matmul_1_serialized_to_bw_in0_matmul_137_matmul_1_0: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_144_matmul_1],
#          t: 512, mblock: [1, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
#          input_0_tms: [hslice: 2, vslice: 16]}

     ## MANUALLY INSERTED TO WORK AROUND THE PREFETCH/"EXTRA" STREAM LIMIT THAT WE WERE OTHERWISE EXCEEDING JUST FROM THE NETLIST ALONE (probably because of the serialization of pushing it over the top)
    custom_single_core_datacopy: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_144_matmul_1],
         t: 16, mblock: [16, 2], ublock: [2, 1], buf_size_mb: 32, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: []}

    bw_in0_matmul_137_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [4, 4], inputs: [bw_in1_matmul_144_matmul_1_serialized_to_bw_in0_matmul_137_matmul_1_0_to_bw_in0_matmul_137_matmul_1_0_serialized_dram_queue, layers.0.self_attention.value.weight],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [vstack: 16, hstack: 2, hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_matmul_137_transpose_0: {type: nop, grid_loc: [0, 4], grid_size: [4, 4], inputs: [e2e_layernorm_112.dc.add.10_1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vstack: 16, hstack: 8, transpose]}
    bw_in1_matmul_137_matmul_1: {type: matmul, grid_loc: [4, 1], grid_size: [2, 4], inputs: [bw_in1_matmul_137_transpose_0, custom_single_core_datacopy], gradient_op: true, #bw_in1_matmul_144_matmul_1], gradient_op: true,
         t: 1, mblock: [8, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.multiply.0_0: {type: nop, grid_loc: [5, 5], grid_size: [1, 1], inputs: [bw_in0_matmul_144_matmul_1],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    bw_in0_softmax_133_softmax_bw_0.dc.multiply.0: {type: multiply, grid_loc: [6, 0], grid_size: [4, 4], inputs: [bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.multiply.0_0_to_bw_in0_softmax_133_softmax_bw_0.dc.multiply.0_0_serialized_dram_queue, e2e_softmax_133.dc.multiply.3_1],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [vstack: 16, hstack: 8], input_0_tms: [vstack: 16, hstack: 8]}
    lc.input_tensor.bw_in0_softmax_133_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0: {type: nop, grid_loc: [4, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in0_softmax_133_softmax_bw_0.dc.reduce_sum.1.0],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    bw_in0_softmax_133_softmax_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [bw_in0_softmax_133_softmax_bw_0.dc.multiply.0, lc.input_tensor.bw_in0_softmax_133_softmax_bw_0.dc.reduce_sum.1.0_splt_brcst_1_0],
         t: 16, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [bw_in0_matmul_144_matmul_1],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    bw_in0_softmax_133_softmax_bw_0.dc.subtract.2: {type: subtract, grid_loc: [6, 4], grid_size: [4, 4], inputs: [bw_in0_matmul_144_matmul_1_serialized_to_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0_to_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0_serialized_dram_queue, bw_in0_softmax_133_softmax_bw_0.dc.reduce_sum.1.lc1],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}], input_0_tms: [hstack: 8, vstack: 16]}
    bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_serialized_to_e2e_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0_0: {type: nop, grid_loc: [5, 7], grid_size: [1, 1], inputs: [bw_in0_softmax_133_softmax_bw_0.dc.subtract.2],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}

  bwd_26_temporal_epoch_13:
    target_device: 0
    input_count: 1
    bw_in0_softmax_133_softmax_bw_0.dc.multiply.3: {type: multiply, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0, e2e_softmax_133.dc.multiply.3_3],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8, vstack: 16]}
    input_1_multiply_129_tile_bcast_tile_bcast_splt_brcst_1_0: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [input_1_multiply_129_tile_bcast_tile_bcast],
         t: 16, mblock: [1, 1], ublock: [1, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {z: 16}]}
    input_1_multiply_129_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [input_1_multiply_129_tile_bcast_tile_bcast_splt_brcst_1_0],
         t: 16, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_multiply_129_multiply_0: {type: multiply, grid_loc: [1, 4], grid_size: [4, 4], inputs: [bw_in0_softmax_133_softmax_bw_0.dc.multiply.3, input_1_multiply_129_tile_bcast_tile_bcast_splt_brcst_1_0_splt_brcst_3_0],
         t: 16, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}
    bw_in0_multiply_129_multiply_0_serialized_to_bw_in0_matmul_128_matmul_1_0: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_multiply_129_multiply_0],
         t: 2048, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    bw_in1_matmul_128_transpose_0: {type: nop, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e_matmul_115_1],
         t: 16, mblock: [1, 8], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 16, transpose]}
    bw_in1_matmul_128_matmul_1: {type: matmul, grid_loc: [5, 0], grid_size: [2, 8], inputs: [bw_in1_matmul_128_transpose_0, bw_in0_multiply_129_multiply_0],
         t: 16, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 32, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 4, u_kt: 8}}
    bw_in1_matmul_128_matmul_1_serialized_to_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0: {type: nop, grid_loc: [4, 0], grid_size: [1, 1], inputs: [bw_in1_matmul_128_matmul_1],
         t: 256, mblock: [1, 1], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 2, hslice: 8]}
    bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_serialized_to_e2e_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0_0: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    bw_in0_matmul_128_matmul_1_serialized_to_e2e_bw_in0_matmul_128_matmul_1_0_0: {type: nop, grid_loc: [4, 1], grid_size: [1, 1], inputs: [bw_in0_matmul_128_matmul_1],
         t: 512, mblock: [1, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 2]}

  bwd_27_temporal_epoch_13:
    target_device: 1
    input_count: 1
    bw_in0_matmul_128_matmul_1: {type: matmul, grid_loc: [0, 0], grid_size: [8, 2], inputs: [bw_in0_multiply_129_multiply_0_serialized_to_bw_in0_matmul_128_matmul_1_0_to_bw_in0_matmul_128_matmul_1_0_serialized_dram_queue, e2e_matmul_121_0],
         t: 16, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 32, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16, hslice: 16], input_0_tms: [vstack: 16, hstack: 8],
         attributes: {m_k: 4, u_kt: 8}}
    bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0: {type: nop, grid_loc: [0, 2], grid_size: [4, 4], inputs: [bw_in1_matmul_128_matmul_1_serialized_to_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0_to_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8, vstack: 2, transpose, hstack: 16]}
    bw_in0_matmul_121_matmul_1: {type: matmul, grid_loc: [4, 2], grid_size: [4, 4], inputs: [bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0, layers.0.self_attention.key.weight],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose],
         attributes: {m_k: 16, u_kt: 2}}

  bwd_28_temporal_epoch_14:
    target_device: 0
    input_count: 1
    bw_in1_add_123_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_123_brcst_reduce_sum_0.0, e2e_bw_in1_matmul_128_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose, hstack: 16], input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_matmul_121_transpose_0: {type: nop, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_layernorm_112.dc.add.10_2],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_121_matmul_1: {type: matmul, grid_loc: [1, 4], grid_size: [2, 4], inputs: [bw_in1_matmul_121_transpose_0, e2e_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0], gradient_op: true,
         t: 1, mblock: [8, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_add_117_brcst_reduce_sum_0.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_add_117_brcst_reduce_sum_0.0, e2e_bw_in0_matmul_128_matmul_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2, vstack: 16, hstack: 16], input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_matmul_115_matmul_1: {type: matmul, grid_loc: [3, 4], grid_size: [4, 4], inputs: [e2e_bw_in0_matmul_128_matmul_1_0, layers.0.self_attention.query.weight],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [transpose], input_0_tms: [hstack: 2, vstack: 16, hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_matmul_115_transpose_0: {type: nop, grid_loc: [4, 0], grid_size: [4, 4], inputs: [e2e_layernorm_112.dc.add.10_2],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [transpose]}
    bw_in1_matmul_115_matmul_1: {type: matmul, grid_loc: [7, 4], grid_size: [2, 4], inputs: [bw_in1_matmul_115_transpose_0, e2e_bw_in0_matmul_128_matmul_1_0], gradient_op: true,
         t: 1, mblock: [8, 2], ublock: [2, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 2, vstack: 16, hstack: 16],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_matmul_115_matmul_1_serialized_to_bw_in0_reshape_113_combine_add_1_1: {type: nop, grid_loc: [0, 7], grid_size: [1, 1], inputs: [bw_in0_matmul_115_matmul_1],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [e2e_layernorm_112.dc.reciprocal.7_0, lc.input_tensor.layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}

  bwd_29_temporal_epoch_14:
    target_device: 1
    input_count: 1
    bw_in0_reshape_113_combine_add_0: {type: add, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_bw_in0_matmul_137_matmul_1_0, e2e_bw_in0_matmul_121_matmul_1_0],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    bw_in0_reshape_113_combine_add_1: {type: add, grid_loc: [0, 4], grid_size: [4, 4], inputs: [bw_in0_reshape_113_combine_add_0, bw_in0_matmul_115_matmul_1_serialized_to_bw_in0_reshape_113_combine_add_1_1_to_bw_in0_reshape_113_combine_add_1_1_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16]}
    bw_in1_layernorm_112_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [4, 0], grid_size: [4, 4], inputs: [e2e_layernorm_112.dc.multiply.8_0, bw_in0_reshape_113_combine_add_1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    layers.0.input_layernorm.weight_s_brcst_m2_1_0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layers.0.input_layernorm.weight_s_brcst_m2_1_0.0, layers.0.input_layernorm.weight],
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         attributes: {m_k: 1, u_kt: 1}}
    bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0: {type: multiply, grid_loc: [5, 4], grid_size: [4, 4], inputs: [bw_in0_reshape_113_combine_add_1, layers.0.input_layernorm.weight_s_brcst_m2_1_0.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}]}

  bwd_30_temporal_epoch_15:
    target_device: 0
    input_count: 1
    bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0, lc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2: {type: multiply, grid_loc: [0, 0], grid_size: [4, 4], inputs: [bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2_0_to_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2_0_serialized_dram_queue, e2e_layernorm_112.dc.multiply.8_1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [vstack: 16, hstack: 8], input_0_tms: [vstack: 16, hstack: 8]}
    bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.3.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2, lc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.3.0],
         t: 1, mblock: [16, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.4: {type: multiply, grid_loc: [1, 4], grid_size: [4, 4], inputs: [e2e_layernorm_112.dc.multiply.8_1, bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.3.lc1],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}], input_0_tms: [vstack: 16, hstack: 8]}
    bw_in0_layernorm_112_layernorm_bw_0.dc.add.5: {type: add, grid_loc: [4, 0], grid_size: [4, 4], inputs: [bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.lc1, bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.4],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.7: {type: multiply, grid_loc: [5, 4], grid_size: [4, 4], inputs: [dc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.6, bw_in0_layernorm_112_layernorm_bw_0.dc.add.5],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}

  bwd_31_temporal_epoch_15:
    target_device: 1
    input_count: 1
    bw_in2_layernorm_112_layernorm_bw_0.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in2_layernorm_112_layernorm_bw_0.dc.reduce_sum.0.0, e2e_bw_in0_reshape_113_combine_add_1_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in1_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.bw_in1_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.0, e2e_bw_in1_layernorm_112_layernorm_bw_0.dc.multiply.0_0], gradient_op: true,
         t: 1, mblock: [1, 8], ublock: [1, 4], buf_size_mb: 1, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}],
         attributes: {m_k: 16, u_kt: 2}}
    bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.2_0: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hslice: 8, vslice: 16]}
    bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.7_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8_1: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.7],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}
    bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8: {type: subtract, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0, bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.7_serialized_to_bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8_1_to_bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8_1_serialized_dram_queue],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_1_tms: [hstack: 8, vstack: 16]}
    bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.9: {type: multiply, grid_loc: [0, 4], grid_size: [4, 4], inputs: [e2e_layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, bw_in0_layernorm_112_layernorm_bw_0.dc.subtract.8],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [broadcast: {c: 32}]}
    bw_in0_hidden_states_combine_add_0: {type: add, grid_loc: [4, 2], grid_size: [4, 4], inputs: [e2e_bw_in0_add_152_combine_add_0_0, bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.9],
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32, Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3}
    bw_in0_hidden_states_combine_add_0_serialized_to_e2e_bw_in0_hidden_states_combine_add_0_0_0: {type: nop, grid_loc: [5, 0], grid_size: [1, 1], inputs: [bw_in0_hidden_states_combine_add_0],
         t: 128, mblock: [1, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [vslice: 16, hslice: 8]}

  bwd_32_temporal_epoch_16:
    target_device: 0
    input_count: 1
    bw_in0_hidden_states_combine_add_0_output_nop_0: {type: nop, grid_loc: [0, 0], grid_size: [4, 4], inputs: [e2e_bw_in0_hidden_states_combine_add_0_0], untilize_output: true,
         t: 1, mblock: [4, 2], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float32], out_df: Float32, intermed_df: Float32, acc_df: Float32, math_fidelity: HiFi3,
         input_0_tms: [hstack: 8, vstack: 16]}

  bwd_33_temporal_epoch_16:
    target_device: 1
    input_count: 1


programs:
  - run_fwd:
    - param: [$p_loop_count]
    - var: {$c_zero: 0, $gptr_q4: 0, $lptr_q4: 0, $gptr_q5: 0, $lptr_q5: 0, $c_microbatch_size: 1, $gptr_q6: 0, $gptr_q1: 0, $lptr_q1: 0, $lptr_q6: 0, $gptr_q7: 0, $c_one: 1, $lptr_q7: 0}
    - staticvar: {$gptr_q0_shadow: 0, $gptr_q2_shadow: 0, $gptr_q0: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q3: 0, $lptr_q0: 0, $lptr_q3: 0}
    - varinst: [$gptr_q2, set, $gptr_q2_shadow]
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_matmul_115_0, e2e_layernorm_112.dc.add.10_0]
    -   execute: {graph_name: fwd_0_temporal_epoch_0, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.layernorm_112.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_112.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_112.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_112.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layers.0.input_layernorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.input_layernorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_1_temporal_epoch_0, queue_settings: {
               lc.input_tensor.layers.0.input_layernorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.input_layernorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.self_attention.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.self_attention.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.self_attention.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.self_attention.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_softmax_133.dc.multiply.3_0, e2e_matmul_137_0]
    -   execute: {graph_name: fwd_2_temporal_epoch_1, queue_settings: {
               e2e_matmul_115_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_matmul_121_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               input_1_multiply_129_fork_clone422_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_0_add_130_tile_bcast: {prologue: false, epilogue: false, zero: False, rd_ptr_autoinc: 0, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               input_1_add_132: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_3_temporal_epoch_1, queue_settings: {
               e2e_layernorm_112.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.softmax_133.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.self_attention.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.self_attention.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_115_0, e2e_layernorm_112.dc.add.10_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_layernorm_153.dc.multiply.9_0, e2e_layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.lc1_0, e2e_add_152_0]
    -   execute: {graph_name: fwd_4_temporal_epoch_2, queue_settings: {
               hidden_states: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_softmax_133.dc.multiply.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_matmul_137_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layers.0.self_attention.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.self_attention.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_153.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_5_temporal_epoch_2, queue_settings: {
               lc.input_tensor.layernorm_153.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_153.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layers.0.post_attention_layernorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.post_attention_layernorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layers.0.post_attention_layernorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_softmax_133.dc.multiply.3_0, e2e_matmul_137_0]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_matmul_156_0]
    -   execute: {graph_name: fwd_6_temporal_epoch_3, queue_settings: {
               e2e_layernorm_153.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5}} }
    -   execute: {graph_name: fwd_7_temporal_epoch_3, queue_settings: {
               layers.0.mlp.dense_h_to_4h.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.mlp.dense_h_to_4h.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_layernorm_153.dc.multiply.9_0, e2e_layers.0.post_attention_layernorm.bias_s_brcst_m2_0_0.lc1_0]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_layernorm_167.dc.subtract.1_0]
    -   execute: {graph_name: fwd_8_temporal_epoch_4, queue_settings: {
               e2e_matmul_156_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6}} }
    -   execute: {graph_name: fwd_9_temporal_epoch_4, queue_settings: {
               e2e_add_152_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layers.0.mlp.dense_4h_to_h.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.mlp.dense_4h_to_h.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_167.dc.reduce_avg.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_matmul_156_0, e2e_add_152_0]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: fwd_10_temporal_epoch_5, queue_settings: {
               e2e_layernorm_167.dc.subtract.1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               lc.input_tensor.layernorm_167.dc.reduce_avg.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_167.4: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_167.dc.reciprocal.7_s_brcst_m1_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.final_layernorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               final_layernorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.final_layernorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               final_layernorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_11_temporal_epoch_5}
    -   deallocate_queue: [e2e_layernorm_167.dc.subtract.1_0]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 2]
    - endloop

  - run_bwd:
    - param: [$p_zero_grad, $p_loop_count]
    - var: {$v_zero_grad: 0, $c_microbatch_size: 1, $c_one: 1, $c_zero: 0, $gptr_q4: 0, $lptr_q6: 0, $gptr_q10: 0, $lptr_q21: 0, $lptr_q20: 0, $gptr_q21: 0, $gptr_q20: 0, $lptr_q14: 0, $lptr_q8: 0, $gptr_q16: 0, $lptr_q4: 0, $lptr_q18: 0, $gptr_q18: 0, $gptr_q8: 0, $lptr_q16: 0, $gptr_q6: 0, $lptr_q12: 0, $gptr_q14: 0, $gptr_q12: 0, $lptr_q10: 0}
    - staticvar: {$gptr_q0_shadow: 0, $gptr_q0: 0, $gptr_q1: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q3: 0, $gptr_q5: 0, $lptr_q5: 0, $gptr_q7: 0, $lptr_q19: 0, $gptr_q19: 0, $gptr_q15: 0, $lptr_q17: 0, $gptr_q2: 0, $gptr_q3: 0, $gptr_q17: 0, $lptr_q15: 0, $lptr_q2: 0, $lptr_q13: 0, $gptr_q13: 0, $gptr_q9: 0, $lptr_q11: 0, $gptr_q11: 0, $lptr_q9: 0, $lptr_q7: 0}
    - varinst: [$gptr_q0, set, $gptr_q0_shadow]
    - varinst: [$v_zero_grad, set, $p_zero_grad]
    - loop: $p_loop_count
    -   allocate_queue: [e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_1, e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_2, e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0]
    -   execute: {graph_name: bwd_12_temporal_epoch_6, queue_settings: {
               loss_transformer.output_layernorm_167: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               e2e_layernorm_167.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               e2e_layernorm_167.dc.multiply.8_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               final_layernorm.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_167.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.final_layernorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_13_temporal_epoch_6, queue_settings: {
               loss_transformer.output_layernorm_167: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               lc.input_tensor.bw_in2_layernorm_167_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_167_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_final_layernorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q0_shadow, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 4]
    -   allocate_queue: [e2e_bw_in0_matmul_162_matmul_1_0]
    -   execute: {graph_name: bwd_14_temporal_epoch_7, queue_settings: {
               loss_transformer.output_layernorm_167: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_167.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               lc.input_tensor.bw_in1_layernorm_167_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_final_layernorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_15_temporal_epoch_7, queue_settings: {
               e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               layers.0.mlp.dense_4h_to_h.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_1]
    -   varinst: [$gptr_q2, incwrap, $c_microbatch_size, 4]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 4]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q4, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: bwd_16_temporal_epoch_8, queue_settings: {
               e2e_gelu_159_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_2: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               lc.input_tensor.bw_in1_add_164_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layers.0.mlp.dense_4h_to_h.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_17_temporal_epoch_8, queue_settings: {
               e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_2: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               grad_acc_layers.0.mlp.dense_4h_to_h.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_2]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q6, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_bw_in0_gelu_159_multiply_1_0]
    -   execute: {graph_name: bwd_18_temporal_epoch_9, queue_settings: {
               e2e_matmul_156_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7}} }
    -   execute: {graph_name: bwd_19_temporal_epoch_9, queue_settings: {
               e2e_bw_in0_matmul_162_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8}} }
    -   deallocate_queue: [e2e_bw_in0_matmul_162_matmul_1_0]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q8, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_bw_in0_matmul_156_matmul_1_0, e2e_layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_0]
    -   execute: {graph_name: bwd_20_temporal_epoch_10, queue_settings: {
               e2e_layernorm_153.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_layernorm_153.dc.add.10_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               e2e_bw_in0_gelu_159_multiply_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               layers.0.post_attention_layernorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layers.0.mlp.dense_h_to_4h.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_158_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layers.0.post_attention_layernorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layers.0.mlp.dense_h_to_4h.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.mlp.dense_h_to_4h.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_21_temporal_epoch_10, queue_settings: {
               e2e_layernorm_153.dc.multiply.8_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               lc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_gelu_159_multiply_1_0]
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_bw_in0_matmul_148_matmul_1_0, e2e_bw_in0_add_152_combine_add_0_0]
    -   execute: {graph_name: bwd_22_temporal_epoch_11, queue_settings: {
               e2e_layernorm_153.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_matmul_156_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               lc.input_tensor.bw_in2_layernorm_153_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_153_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_153_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layers.0.post_attention_layernorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.post_attention_layernorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_23_temporal_epoch_11, queue_settings: {
               e2e_matmul_144_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               layers.0.self_attention.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_150_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.dense.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.dense.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_matmul_156_matmul_1_0, e2e_layernorm_153.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0, e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_layernorm_153_layernorm_bw_0.dc.add.5_0, e2e_bw_in0_layernorm_167_layernorm_bw_0.dc.multiply.9_0]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q12, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0, e2e_bw_in0_matmul_137_matmul_1_0]
    -   execute: {graph_name: bwd_24_temporal_epoch_12, queue_settings: {
               e2e_matmul_137_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_bw_in0_matmul_148_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14}} }
    -   execute: {graph_name: bwd_25_temporal_epoch_12, queue_settings: {
               e2e_layernorm_112.dc.add.10_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               e2e_softmax_133.dc.multiply.3_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               layers.0.self_attention.value.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_139_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_softmax_133_softmax_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.value.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.value.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_matmul_148_matmul_1_0]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q14, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_bw_in0_matmul_128_matmul_1_0, e2e_bw_in1_matmul_128_matmul_1_0, e2e_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0, e2e_bw_in0_matmul_121_matmul_1_0]
    -   execute: {graph_name: bwd_26_temporal_epoch_13, queue_settings: {
               e2e_matmul_115_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_softmax_133.dc.multiply.3_3: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               e2e_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               input_1_multiply_129_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_27_temporal_epoch_13, queue_settings: {
               e2e_matmul_121_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               layers.0.self_attention.key.weight: {prologue: false, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_softmax_133_softmax_bw_0.dc.subtract.2_0]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q16, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_reshape_113_combine_add_1_0, e2e_bw_in1_layernorm_112_layernorm_bw_0.dc.multiply.0_0, e2e_layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0]
    -   execute: {graph_name: bwd_28_temporal_epoch_14, queue_settings: {
               e2e_layernorm_112.dc.reciprocal.7_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_layernorm_112.dc.add.10_2: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_bw_in0_matmul_128_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_bw_in1_matmul_128_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               layers.0.self_attention.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_123_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_add_117_brcst_reduce_sum_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.key.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.key.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.query.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.self_attention.query.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_29_temporal_epoch_14, queue_settings: {
               e2e_layernorm_112.dc.multiply.8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               e2e_bw_in0_matmul_137_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e_bw_in0_matmul_121_matmul_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               layers.0.input_layernorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layers.0.input_layernorm.weight_s_brcst_m2_1_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_matmul_128_matmul_1_0, e2e_bw_in1_matmul_128_matmul_1_0, e2e_bw_in0_matmul_121_add_123_unsqueeze3_170_squeeze_0_0, e2e_bw_in0_matmul_137_matmul_1_0, e2e_bw_in0_matmul_121_matmul_1_0]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q18, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 2]
    -   allocate_queue: [e2e_bw_in0_hidden_states_combine_add_0_0]
    -   execute: {graph_name: bwd_30_temporal_epoch_15, queue_settings: {
               e2e_layernorm_112.dc.multiply.8_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               lc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.dc.reduce_sum.3.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.bw_in0_layernorm_112_layernorm_bw_0.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: bwd_31_temporal_epoch_15, queue_settings: {
               e2e_bw_in0_add_152_combine_add_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_bw_in0_reshape_113_combine_add_1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_bw_in1_layernorm_112_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               lc.input_tensor.bw_in2_layernorm_112_layernorm_bw_0.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.bw_in1_layernorm_112_layernorm_bw_0.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               grad_acc_layers.0.input_layernorm.bias: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               grad_acc_layers.0.input_layernorm.weight: {prologue: true, epilogue: true, zero: $v_zero_grad, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e_bw_in0_layernorm_112_layernorm_bw_0.dc.multiply.0_0, e2e_bw_in0_add_152_combine_add_0_0, e2e_bw_in0_reshape_113_combine_add_1_0, e2e_bw_in1_layernorm_112_layernorm_bw_0.dc.multiply.0_0, e2e_layernorm_112.dc.reciprocal.7_s_brcst_m1_0_0.lc1_0]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q20, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 2]
    -   varinst: [$gptr_q20, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 2]
    -   execute: {graph_name: bwd_32_temporal_epoch_16, queue_settings: {
               e2e_bw_in0_hidden_states_combine_add_0_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21}} }
    -   execute: {graph_name: bwd_33_temporal_epoch_16}
    -   deallocate_queue: [e2e_bw_in0_hidden_states_combine_add_0_0]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 2]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 2]
    -   varinst: [$v_zero_grad, set, 0]
    - endloop

  - run_opt:
    - var: {$c_microbatch_size: 1, $c_one: 1, $c_zero: 0}


