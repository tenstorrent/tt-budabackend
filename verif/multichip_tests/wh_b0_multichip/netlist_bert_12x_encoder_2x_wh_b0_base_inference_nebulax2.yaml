# git checkout aba67f98c
# pytest pybuda/test/backend/models/test_bert.py::test_pt_encoder[inference-Wormhole_B0-chip2-enc12-base]

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                                                      {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [4, 24], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30020820]]}
  attention_mask:                                                               {input: HOST, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x30000000]]}

  # output
  bert_encoder.output_layernorm_635:                                            {input: layernorm_635.dc.add.14, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: host, host: [0x0]}

  # parameter
  layer.0.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x850e320], [4, 0x85ec9e0]]}
  layer.0.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x431ad00], [1, 0x439bcc0]]}
  layer.0.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4212c00], [3, 0x85a0740]]}
  layer.0.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x867ee00], [5, 0x43dccc0]]}
  layer.0.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8684fa0], [5, 0x43e2e60]]}
  layer.0.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x43216e0], [1, 0x43a26a0]]}
  layer.0.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x42bd640], [3, 0x86333a0]]}
  layer.0.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x87173c0], [5, 0x4475280]]}
  layer.0.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4351b20]]}
  layer.0.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8720660]]}
  layer.0.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x447ed40], [0, 0x432a180], [1, 0x43db520], [2, 0x435de40], [3, 0x86c9100], [4, 0x872c980], [5, 0x4511160], [0, 0x43bc5a0]]}
  layer.0.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x446d940], [2, 0x43f0260], [3, 0x875b520], [4, 0x87beda0], [5, 0x45a3580], [0, 0x444e9c0], [1, 0x4473ae0], [2, 0x43f6400]]}
  layer.0.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x87616c0], [4, 0x87c4f40], [5, 0x45a9720], [0, 0x4454b60], [1, 0x4479c80], [2, 0x43fc5a0], [3, 0x87f3ae0], [4, 0x8857360]]}
  layer.0.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x463bb40], [0, 0x44e6f80], [1, 0x450c0a0], [2, 0x448e9c0], [3, 0x8885f00], [4, 0x88e9780], [5, 0x463d3c0], [0, 0x44e8800]]}
  layer.0.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8388a20]]}
  layer.0.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4108960]]}
  layer.1.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x413f6e0], [2, 0x3f1d060]]}
  layer.1.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x82b6680], [4, 0x8394d40]]}
  layer.1.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x41847e0], [0, 0x4114c80]]}
  layer.1.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x41d1b00], [2, 0x3faf480]]}
  layer.1.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x82c72e0], [4, 0x82f3de0]]}
  layer.1.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4207840], [0, 0x41b3be0]]}
  layer.1.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40ac300], [2, 0x407b700]]}
  layer.1.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8359700], [4, 0x8386200]]}
  layer.1.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4145100]]}
  layer.1.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x836bbe0]]}
  layer.1.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x83986e0], [5, 0x4214c00], [0, 0x41f1380], [1, 0x4151420], [2, 0x41165a0], [3, 0x8377f00], [4, 0x842ab00], [5, 0x42a7020]]}
  layer.1.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42837a0], [1, 0x41e3840], [2, 0x41a89c0], [3, 0x840a320], [4, 0x84bcf20], [5, 0x4339440], [0, 0x4289940], [1, 0x41e99e0]]}
  layer.1.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41aeb60], [3, 0x84104c0], [4, 0x84c30c0], [5, 0x433f5e0], [0, 0x428fae0], [1, 0x41efb80], [2, 0x4240f80], [3, 0x84a28e0]]}
  layer.1.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x85554e0], [5, 0x43d1a00], [0, 0x4321f00], [1, 0x4281fa0], [2, 0x42d33a0], [3, 0x8534d00], [4, 0x8556d60], [5, 0x43d3280]]}
  layer.1.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fe0800]]}
  layer.1.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81032e0]]}
  layer.2.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81c62e0], [5, 0x409ff20]]}
  layer.2.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4119520], [1, 0x3fecb20]]}
  layer.2.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ec0120], [3, 0x810f600]]}
  layer.2.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8258700], [5, 0x4132340]]}
  layer.2.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x839aee0], [5, 0x4216c00]]}
  layer.2.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x41a70a0], [1, 0x41d7ca0]]}
  layer.2.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3fb5620], [3, 0x82bd060]]}
  layer.2.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x417de00], [0, 0x41027c0]]}
  layer.2.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x41dde60]]}
  layer.2.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4049ae0]]}
  layer.2.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8351520], [4, 0x842fbe0], [5, 0x42aa0a0], [0, 0x41ea180], [1, 0x41deec0], [2, 0x4055e00], [3, 0x83e3940], [4, 0x84c2000]]}
  layer.2.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x433c4c0], [0, 0x427c5a0], [1, 0x42712e0], [2, 0x40e8220], [3, 0x8475d60], [4, 0x8554420], [5, 0x4342660], [0, 0x4282740]]}
  layer.2.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4277480], [2, 0x40ee3c0], [3, 0x847bf00], [4, 0x855a5c0], [5, 0x4348800], [0, 0x42888e0], [1, 0x43098a0], [2, 0x41807e0]]}
  layer.2.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x86c57c0], [4, 0x871d560], [5, 0x447b420], [0, 0x43280c0], [1, 0x43d9460], [2, 0x43502a0], [3, 0x86c7040], [4, 0x871ede0]]}
  layer.2.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x472d060]]}
  layer.2.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8bd8a00]]}
  layer.3.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47b1900], [0, 0x4672aa0]]}
  layer.3.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4779b60], [2, 0x4739380]]}
  layer.3.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8b759c0], [4, 0x8be4d20]]}
  layer.3.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4843d20], [0, 0x4704ec0]]}
  layer.3.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4849ec0], [0, 0x470b060]]}
  layer.3.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4780540], [2, 0x473fd60]]}
  layer.3.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8c20400], [4, 0x8c77980]]}
  layer.3.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x48dc2e0], [0, 0x479d480]]}
  layer.3.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4169100]]}
  layer.3.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ff55a0]]}
  layer.3.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f56680], [3, 0x81a2aa0], [4, 0x825f920], [5, 0x4175420], [0, 0x4120f80], [1, 0x40018c0], [2, 0x3fe8aa0], [3, 0x8234ec0]]}
  layer.3.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x835f8a0], [4, 0x838c3a0], [5, 0x420e220], [0, 0x41ea9a0], [1, 0x413ef60], [2, 0x410fbc0], [3, 0x8365a40], [4, 0x8392540]]}
  layer.3.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x87a1f40], [4, 0x87e3760], [5, 0x462e040], [0, 0x445e660], [1, 0x44c8060], [2, 0x45b9be0], [3, 0x8834360], [4, 0x8875b80]]}
  layer.3.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46c0460], [0, 0x44f0a80], [1, 0x455a480], [2, 0x464c000], [3, 0x88c6780], [4, 0x8907fa0], [5, 0x46c1ce0], [0, 0x44f2300]]}
  layer.3.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x467e4a0]]}
  layer.3.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x890b8c0]]}
  layer.4.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4435c40], [2, 0x45277c0]]}
  layer.4.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x44f43c0], [1, 0x455cd80]]}
  layer.4.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x468a7c0], [3, 0x88c9080]]}
  layer.4.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8917be0], [5, 0x46c5600]]}
  layer.4.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x891dd80], [5, 0x46cb7a0]]}
  layer.4.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x44fada0], [1, 0x4563760]]}
  layer.4.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4735200], [3, 0x895bce0]]}
  layer.4.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x89b01a0], [5, 0x475dbc0]]}
  layer.4.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x44ea080]]}
  layer.4.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x44c0e60]]}
  layer.4.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8887fc0], [4, 0x88eb840], [5, 0x463f480], [0, 0x44f63a0], [1, 0x450e9a0], [2, 0x44cd180], [3, 0x891a3e0], [4, 0x897dc60]]}
  layer.4.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x46d18a0], [0, 0x45887c0], [1, 0x45a0dc0], [2, 0x455f5a0], [3, 0x89ac800], [4, 0x8a10080], [5, 0x46d7a40], [0, 0x458e960]]}
  layer.4.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x45a6f60], [2, 0x4565740], [3, 0x89b29a0], [4, 0x8a16220], [5, 0x46ddbe0], [0, 0x4594b00], [1, 0x4639380], [2, 0x45f7b60]]}
  layer.4.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8a44dc0], [4, 0x8aa8640], [5, 0x4770000], [0, 0x4626f20], [1, 0x46cb7a0], [2, 0x4689f80], [3, 0x8a46640], [4, 0x8aa9ec0]]}
  layer.4.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x46593c0]]}
  layer.4.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x468d8a0]]}
  layer.5.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8a49f60], [4, 0x8aabf80]]}
  layer.5.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47a2ce0], [0, 0x46656e0]]}
  layer.5.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x46ce0a0], [2, 0x4699bc0]]}
  layer.5.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8adc380], [4, 0x8b3e3a0]]}
  layer.5.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8ae2520], [4, 0x8b44540]]}
  layer.5.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47a96c0], [0, 0x466c0c0]]}
  layer.5.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8009460], [4, 0x809e100]]}
  layer.5.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7cf27a0], [4, 0x7d06ca0]]}
  layer.5.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x42b4440]]}
  layer.5.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8536dc0]]}
  layer.5.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8589200], [5, 0x43d5340], [0, 0x4324800], [1, 0x42c0760], [2, 0x42d5ca0], [3, 0x85430e0], [4, 0x861b620], [5, 0x4467760]]}
  layer.5.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x43b6c20], [1, 0x4352b80], [2, 0x43680c0], [3, 0x85d5500], [4, 0x86ada40], [5, 0x44f9b80], [0, 0x43bcdc0], [1, 0x4358d20]]}
  layer.5.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x436e260], [3, 0x85db6a0], [4, 0x86b3be0], [5, 0x44ffd20], [0, 0x43c2f60], [1, 0x435eec0], [2, 0x4400680], [3, 0x866dac0]]}
  layer.5.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8746000], [5, 0x4592140], [0, 0x4455380], [1, 0x43f12e0], [2, 0x4492aa0], [3, 0x86ffee0], [4, 0x8747880], [5, 0x45939c0]]}
  layer.5.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4423780]]}
  layer.5.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8703800]]}
  layer.6.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x874b1a0], [5, 0x4595a80]]}
  layer.6.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4457c80], [1, 0x442faa0]]}
  layer.6.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44953a0], [3, 0x870fb20]]}
  layer.6.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x87dd5c0], [5, 0x4627ea0]]}
  layer.6.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a0bd00], [2, 0x3924400]]}
  layer.6.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b5a2a0], [4, 0x7bf4080]]}
  layer.6.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39998e0], [0, 0x3a0c540]]}
  layer.6.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a9e120], [2, 0x39b6820]]}
  layer.6.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7c2ae40]]}
  layer.6.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3aa0a00]]}
  layer.6.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d0ce40], [5, 0x3b47b60], [0, 0x3b9e0c0], [1, 0x3c2dc20], [2, 0x3aa4b60], [3, 0x7cfa9e0], [4, 0x7d9f260], [5, 0x3bd9f80]]}
  layer.6.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3c304e0], [1, 0x3cc0040], [2, 0x3b36f80], [3, 0x7d8ce00], [4, 0x7e31680], [5, 0x3c6c3a0], [0, 0x3c36680], [1, 0x3cc61e0]]}
  layer.6.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3b3d120], [3, 0x7d92fa0], [4, 0x7e37820], [5, 0x3c72540], [0, 0x3c3c820], [1, 0x3ccc380], [2, 0x3bcf540], [3, 0x7e253c0]]}
  layer.6.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ec9c40], [5, 0x3d04960], [0, 0x3ccec40], [1, 0x3d5e7a0], [2, 0x3c61960], [3, 0x7eb77e0], [4, 0x7ecb4c0], [5, 0x3d061e0]]}
  layer.6.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3c63a20]]}
  layer.6.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7f5f160]]}
  layer.7.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3d99e80], [0, 0x3cd1540]]}
  layer.7.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3d91480], [2, 0x3c6fd40]]}
  layer.7.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ebb940], [4, 0x7f6b480]]}
  layer.7.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3e2c2a0], [0, 0x3d63960]]}
  layer.7.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3e32440], [0, 0x3d69b00]]}
  layer.7.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a37780], [1, 0x3af8f20]]}
  layer.7.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7b46520], [5, 0x396c520]]}
  layer.7.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a2f540], [1, 0x3af2540]]}
  layer.7.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7c09560]]}
  layer.7.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3a009e0]]}
  layer.7.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3aa6360], [2, 0x39bd200], [3, 0x7b6cfa0], [4, 0x7c37160], [5, 0x3a2cd80], [0, 0x3aacd20], [1, 0x3b38780], [2, 0x3a4f620]]}
  layer.7.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7bff3c0], [4, 0x7cc9580], [5, 0x3abf1a0], [0, 0x3b3f140], [1, 0x3bcaba0], [2, 0x3ae1a40], [3, 0x7c05560], [4, 0x7ccf720]]}
  layer.7.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3ac5340], [0, 0x3b452e0], [1, 0x3bd0d40], [2, 0x3ae7be0], [3, 0x7c0b700], [4, 0x7cd58c0], [5, 0x3b57760], [0, 0x3bd7700]]}
  layer.7.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c63160], [2, 0x3b7a000], [3, 0x7c9db20], [4, 0x7d67ce0], [5, 0x3be9b80], [0, 0x3c69b20], [1, 0x3c649e0], [2, 0x3b7b880]]}
  layer.7.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ab4100]]}
  layer.7.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x38da100]]}
  layer.8.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x396c520], [1, 0x396c520]]}
  layer.8.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da940], [3, 0x7ac0420]]}
  layer.8.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4940], [5, 0x38e6420]]}
  layer.8.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x39fe940], [1, 0x39fe940]]}
  layer.8.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x38da100], [1, 0x38da100]]}
  layer.8.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a04ae0], [2, 0x38e1320]]}
  layer.8.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ac6e00], [4, 0x7b5f380]]}
  layer.8.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3979080], [0, 0x3a05320]]}
  layer.8.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39180e0]]}
  layer.8.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b60c80]]}
  layer.8.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x38da100], [3, 0x7ab4100], [4, 0x7ab4100], [5, 0x38da100], [0, 0x399d120], [1, 0x3a60120], [2, 0x396c520], [3, 0x7b46520]]}
  layer.8.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x39ff9c0], [3, 0x7c15880], [4, 0x7bd99c0], [5, 0x3a0cd00], [0, 0x3a3d920], [1, 0x3aff0c0], [2, 0x3a05b60], [3, 0x7c1ba20]]}
  layer.8.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bdfb60], [5, 0x3a12ea0], [0, 0x3a43ac0], [1, 0x3b05260], [2, 0x3a0bd00], [3, 0x7c21bc0], [4, 0x7c71f80], [5, 0x3aa52c0]]}
  layer.8.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3ad5ee0], [1, 0x3b97680], [2, 0x3a9e120], [3, 0x7cb3fe0], [4, 0x7d043a0], [5, 0x3b376e0], [0, 0x3ad7760], [1, 0x3b98f00]]}
  layer.8.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ce6480]]}
  layer.8.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b3b000]]}
  layer.9.attention.self.query.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3adb080], [1, 0x3b9afc0]]}
  layer.9.attention.self.query.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x81c3180], [5, 0x3fad2c0]]}
  layer.9.attention.self.key.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7eccd40], [5, 0x3d07a60]]}
  layer.9.attention.self.key.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3f0eb60], [0, 0x3ea20c0]]}
  layer.9.attention.self.value.weight:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3f14d00], [0, 0x3ea8260]]}
  layer.9.attention.self.value.bias:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f76520], [2, 0x3db25e0]]}
  layer.9.attention.output.dense.weight:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x80b3ea0], [4, 0x8130d60]]}
  layer.9.attention.output.dense.bias:                                          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fa7120], [0, 0x3f3a680]]}
  layer.9.attention.output.LayerNorm.weight:                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8146b00]]}
  layer.9.attention.output.LayerNorm.bias:                                      {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3fb3460]]}
  layer.9.intermediate.dense.weight:                                            {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3db2d60], [0, 0x3e2dbc0], [1, 0x3ea0800], [2, 0x3d43a20], [3, 0x7e80b80], [4, 0x7f64ba0], [5, 0x3e45180], [0, 0x3ebffe0]]}
  layer.9.intermediate.dense.bias:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3f32c20], [2, 0x3dd5e40], [3, 0x7f12fa0], [4, 0x7ff6fc0], [5, 0x3ed75a0], [0, 0x3f52400], [1, 0x3f38dc0], [2, 0x3ddbfe0]]}
  layer.9.output.dense.weight:                                                  {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7f19140], [4, 0x7ffd160], [5, 0x3edd740], [0, 0x3f585a0], [1, 0x3f3ef60], [2, 0x3de2180], [3, 0x7fab560], [4, 0x808f580]]}
  layer.9.output.dense.bias:                                                    {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f6fb60], [0, 0x3fea9c0], [1, 0x3fd1380], [2, 0x3e745a0], [3, 0x803d980], [4, 0x81219a0], [5, 0x3f713e0], [0, 0x3fec240]]}
  layer.9.output.LayerNorm.weight:                                              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ea6a40]]}
  layer.9.output.LayerNorm.bias:                                                {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81252c0]]}
  layer.10.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f74d00], [0, 0x3fee300]]}
  layer.10.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fd3c80], [2, 0x3eb2d60]]}
  layer.10.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8040280], [4, 0x81315e0]]}
  layer.10.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4007120], [0, 0x4080720]]}
  layer.10.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400d2c0], [0, 0x40868c0]]}
  layer.10.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fda660], [2, 0x3eb9740]]}
  layer.10.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3d06a80], [1, 0x3d3d800]]}
  layer.10.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7da0320], [5, 0x3bf1de0]]}
  layer.10.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c9f0c0]]}
  layer.10.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7cadfc0]]}
  layer.10.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3f428c0], [1, 0x3f7d740], [2, 0x3de9be0], [3, 0x8152e20], [4, 0x81c9b60], [5, 0x3fbf780], [0, 0x3fd4ce0], [1, 0x400fb60]]}
  layer.10.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3e7c000], [3, 0x81e5240], [4, 0x825bf80], [5, 0x4051ba0], [0, 0x4067100], [1, 0x40a1f80], [2, 0x3e821a0], [3, 0x81eb3e0]]}
  layer.10.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8262120], [5, 0x4057d40], [0, 0x406d2a0], [1, 0x40a8120], [2, 0x3e88340], [3, 0x81f1580], [4, 0x82f4540], [5, 0x40ea160]]}
  layer.10.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x40ff6c0], [1, 0x413a540], [2, 0x3f1a760], [3, 0x82839a0], [4, 0x8386960], [5, 0x417c580], [0, 0x4100f40], [1, 0x413bdc0]]}
  layer.10.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3dfbf20]]}
  layer.10.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3d08b40]]}
  layer.11.attention.self.query.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f68420], [4, 0x7ffe920]]}
  layer.11.attention.self.query.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ec50a0], [0, 0x3e08240]]}
  layer.11.attention.self.key.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3e2aac0], [2, 0x3d14e60]]}
  layer.11.attention.self.key.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ffa840], [4, 0x8090d40]]}
  layer.11.attention.self.value.weight:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3d97e60], [2, 0x3c76720]]}
  layer.11.attention.self.value.bias:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8096ee0], [5, 0x3ecba80]]}
  layer.11.attention.output.dense.weight:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [12, 3], ublock: [2, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e0ec20], [1, 0x3ed5500]]}
  layer.11.attention.output.dense.bias:                                         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3da7ac0], [3, 0x8002a80]]}
  layer.11.attention.output.LayerNorm.weight:                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3f02840]]}
  layer.11.attention.output.LayerNorm.bias:                                     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f699c0]]}
  layer.11.intermediate.dense.weight:                                           {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [8, 3], ublock: [3, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7da8560], [5, 0x3bf87c0], [0, 0x3c74660], [1, 0x3cab3e0], [2, 0x3b8a4c0], [3, 0x7cba2e0], [4, 0x7e3a980], [5, 0x3c8abe0]]}
  layer.11.intermediate.dense.bias:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3b7d100], [3, 0x7c9fbe0], [4, 0x7d9a180], [5, 0x3bebc40], [0, 0x3c6d440], [1, 0x3c68300], [2, 0x3b832a0], [3, 0x7ca5d80]]}
  layer.11.output.dense.weight:                                                 {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [6, 3], ublock: [16, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3c1c8e0], [3, 0x7d4c700], [4, 0x7eccda0], [5, 0x3d1d000], [0, 0x3d98ea0], [1, 0x3dcfc20], [2, 0x3caed00], [3, 0x7ddeb20]]}
  layer.11.output.dense.bias:                                                   {input: HOST, type: ram, entries: 1, grid_size: [1, 8], t: 1, mblock: [1, 3], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f5f1c0], [5, 0x3daf420], [0, 0x3e2b2c0], [1, 0x3e62040], [2, 0x3d41120], [3, 0x7e70f40], [4, 0x7f60a40], [5, 0x3db0ca0]]}
  layer.11.output.LayerNorm.weight:                                             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e944e0]]}
  layer.11.output.LayerNorm.bias:                                               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 6], ublock: [1, 4], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e74860]]}

  # constant
  input_1_multiply_16_tile_bcast_tile_bcast:                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4320ea0]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x43a1e60]]}
  dc.input_tensor.softmax_18.2:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x42a5020]]}
  lc.input_tensor.softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8632b60]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4327880]]}
  dc.input_tensor.layernorm_38.1:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x43a8840]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x434fa60]]}
  dc.input_tensor.layernorm_38.6:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x43dac20]]}
  dc.input_tensor.layernorm_38.8:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x447cca0]]}
  lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4329940]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x43dace0]]}
  lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x86c88c0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x450d920]]}
  dc.input_tensor.layernorm_52.1:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4490240]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8887780]]}
  dc.input_tensor.layernorm_52.6:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x842d300]]}
  dc.input_tensor.layernorm_52.8:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x413d640]]}
  lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f1c820]]}
  lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x82b5e40]]}
  lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4183fa0]]}
  input_1_multiply_69_tile_bcast_tile_bcast:                                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x82bc820]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41b33a0]]}
  dc.input_tensor.softmax_71.2:                                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4093ce0]]}
  lc.input_tensor.softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.0:                  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x407aec0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x420d9e0]]}
  dc.input_tensor.layernorm_91.1:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41b9d80]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x413e720]]}
  dc.input_tensor.layernorm_91.6:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x410db20]]}
  dc.input_tensor.layernorm_91.8:                                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x82f1d40]]}
  lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42143c0]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41f0b40]]}
  lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4115d60]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4323780]]}
  dc.input_tensor.layernorm_105.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4283820]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42d4c20]]}
  dc.input_tensor.layernorm_105.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f52540]]}
  dc.input_tensor.layernorm_105.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81c4240]]}
  lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x409f6e0]]}
  lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4118ce0]]}
  lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3ebf8e0]]}
  input_1_multiply_122_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x411f6c0]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ff2cc0]]}
  dc.input_tensor.softmax_124.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80eacc0]]}
  lc.input_tensor.softmax_124.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81a1a20]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x42a9020]]}
  dc.input_tensor.layernorm_144.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x41ad240]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x41dde40]]}
  dc.input_tensor.layernorm_144.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4047a40]]}
  dc.input_tensor.layernorm_144.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x834f480]]}
  lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x842f3a0]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x42a9860]]}
  lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x41de680]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x472c820]]}
  dc.input_tensor.layernorm_158.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47720c0]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8b74940]]}
  dc.input_tensor.layernorm_158.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8bd6960]]}
  dc.input_tensor.layernorm_158.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47af860]]}
  lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x4672260]]}
  lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4779320]]}
  lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8b75180]]}
  input_1_multiply_175_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x477fd00]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x473f520]]}
  dc.input_tensor.softmax_177.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8c07de0]]}
  lc.input_tensor.softmax_177.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8c77140]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x825e8a0]]}
  dc.input_tensor.layernorm_197.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41384e0]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x411ff00]]}
  dc.input_tensor.layernorm_197.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3ff3500]]}
  dc.input_tensor.layernorm_197.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3f545e0]]}
  lc.input_tensor.layernorm_197.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x81a2260]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x825f0e0]]}
  lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4120740]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x455bd00]]}
  dc.input_tensor.layernorm_211.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x464d880]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88c8000]]}
  dc.input_tensor.layernorm_211.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8909820]]}
  dc.input_tensor.layernorm_211.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46c3560]]}
  lc.input_tensor.layernorm_211.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x44f3b80]]}
  lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x455c540]]}
  lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x88c8840]]}
  input_1_multiply_228_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x44fa560]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4562f20]]}
  dc.input_tensor.softmax_230.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x471cbe0]]}
  lc.input_tensor.softmax_230.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x895b4a0]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4494320]]}
  dc.input_tensor.layernorm_250.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x85585e0]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x47866e0]]}
  dc.input_tensor.layernorm_250.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4745f00]]}
  dc.input_tensor.layernorm_250.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8cb2820]]}
  lc.input_tensor.layernorm_250.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4778ae0]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x463ec40]]}
  lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x450e160]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x4771880]]}
  dc.input_tensor.layernorm_264.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x46287a0]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x46cd020]]}
  dc.input_tensor.layernorm_264.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x468b800]]}
  dc.input_tensor.layernorm_264.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8a47ec0]]}
  lc.input_tensor.layernorm_264.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8aab740]]}
  lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x88eb000]]}
  lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x46cd860]]}
  input_1_multiply_281_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x47a8e80]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x466b880]]}
  dc.input_tensor.softmax_283.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x47604c0]]}
  lc.input_tensor.softmax_283.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x472bfe0]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b47320]]}
  dc.input_tensor.layernorm_303.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3b6d4a0]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3c2d3e0]]}
  dc.input_tensor.layernorm_303.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3aa2ac0]]}
  dc.input_tensor.layernorm_303.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7cf8940]]}
  lc.input_tensor.layernorm_303.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x43d4b00]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4323fc0]]}
  lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x42d5460]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4456c00]]}
  dc.input_tensor.layernorm_317.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x43f2b60]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8536580]]}
  dc.input_tensor.layernorm_317.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x8701760]]}
  dc.input_tensor.layernorm_317.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8749100]]}
  lc.input_tensor.layernorm_317.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4595240]]}
  lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4457440]]}
  lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4494b60]]}
  input_1_multiply_334_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x445de20]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x803fa40]]}
  dc.input_tensor.softmax_336.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x39812c0]]}
  lc.input_tensor.softmax_336.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a0bd00]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b60440]]}
  dc.input_tensor.layernorm_356.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7bfa220]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a2bd00]]}
  dc.input_tensor.layernorm_356.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a9e960]]}
  dc.input_tensor.layernorm_356.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3aa42c0]]}
  lc.input_tensor.layernorm_356.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x39bc9c0]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7bf3840]]}
  lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3a2c540]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3cd04c0]]}
  dc.input_tensor.layernorm_370.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3d60020]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3c631e0]]}
  dc.input_tensor.layernorm_370.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7eb9060]]}
  dc.input_tensor.layernorm_370.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3aa0a20]]}
  lc.input_tensor.layernorm_370.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3cd0d00]]}
  lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3d90c40]]}
  lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7ebb100]]}
  input_1_multiply_387_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3d97620]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3c75ee0]]}
  dc.input_tensor.softmax_389.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f4dd60]]}
  lc.input_tensor.softmax_389.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ffd8a0]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x39fe940]]}
  dc.input_tensor.layernorm_409.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7bd8940]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bd8940]]}
  dc.input_tensor.layernorm_409.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x39fe940]]}
  dc.input_tensor.layernorm_409.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3a356e0]]}
  lc.input_tensor.layernorm_409.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3af86e0]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x39ff180]]}
  lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7bd9180]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7c9f3a0]]}
  dc.input_tensor.layernorm_423.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7d69560]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3beb400]]}
  dc.input_tensor.layernorm_423.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c6b3a0]]}
  dc.input_tensor.layernorm_423.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c66260]]}
  lc.input_tensor.layernorm_423.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a04ae0]]}
  lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38da100]]}
  lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7ab4100]]}
  input_1_multiply_440_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38e0ae0]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7ac65c0]]}
  dc.input_tensor.softmax_442.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7b46d60]]}
  lc.input_tensor.softmax_442.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3978840]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a0ac80]]}
  dc.input_tensor.layernorm_462.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x38e74c0]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b59220]]}
  dc.input_tensor.layernorm_462.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7bf17a0]]}
  dc.input_tensor.layernorm_462.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x397f220]]}
  lc.input_tensor.layernorm_462.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3a0b4c0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3a0b4c0]]}
  lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7b59a60]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3a9f9a0]]}
  dc.input_tensor.layernorm_476.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7cb5860]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d05c20]]}
  dc.input_tensor.layernorm_476.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3b38f60]]}
  dc.input_tensor.layernorm_476.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3ad8fe0]]}
  lc.input_tensor.layernorm_476.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3b9a780]]}
  lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3aa01e0]]}
  lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7d06460]]}
  input_1_multiply_493_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f75ce0]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3db1da0]]}
  dc.input_tensor.softmax_495.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x809b880]]}
  lc.input_tensor.softmax_495.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8130520]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f7c6c0]]}
  dc.input_tensor.layernorm_515.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3db8780]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x81462c0]]}
  dc.input_tensor.layernorm_515.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3dafd00]]}
  dc.input_tensor.layernorm_515.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3f40820]]}
  lc.input_tensor.layernorm_515.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f7cf00]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3de93a0]]}
  lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x81c9320]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fd2c00]]}
  dc.input_tensor.layernorm_529.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3e75e20]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x803f200]]}
  dc.input_tensor.layernorm_529.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x8123220]]}
  dc.input_tensor.layernorm_529.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3f72c60]]}
  lc.input_tensor.layernorm_529.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3fedac0]]}
  lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0:             {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fd3440]]}
  lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0:               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f64360]]}
  input_1_multiply_546_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3fd9e20]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3eb8f00]]}
  dc.input_tensor.softmax_548.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x80d26a0]]}
  lc.input_tensor.softmax_548.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x81c3a00]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c735e0]]}
  dc.input_tensor.layernorm_568.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3c6e4a0]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3b89440]]}
  dc.input_tensor.layernorm_568.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7cabf20]]}
  dc.input_tensor.layernorm_568.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7da64c0]]}
  lc.input_tensor.layernorm_568.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3bf7f80]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3c73e20]]}
  lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3b89c80]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3f1bfe0]]}
  dc.input_tensor.layernorm_582.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8285220]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x83881e0]]}
  dc.input_tensor.layernorm_582.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x80009e0]]}
  dc.input_tensor.layernorm_582.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x7f66380]]}
  lc.input_tensor.layernorm_582.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x7ffe0e0]]}
  lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ec4860]]}
  lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3e2a280]]}
  input_1_multiply_599_tile_bcast_tile_bcast:                                   {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ecb240]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3e0e3e0]]}
  dc.input_tensor.softmax_601.2:                                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3ebcee0]]}
  lc.input_tensor.softmax_601.dc.reciprocal.4_s_brcst_m1_0_0.0:                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3da7280]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x809d080]]}
  dc.input_tensor.layernorm_621.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x3ed1c20]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3ea1040]]}
  dc.input_tensor.layernorm_621.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x3f67920]]}
  dc.input_tensor.layernorm_621.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x3dadc60]]}
  lc.input_tensor.layernorm_621.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 1, loc: dram, dram: [[3, 0x8008c20]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0:  {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x809d8c0]]}
  lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x3ea1880]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.0.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e2cb40]]}
  dc.input_tensor.layernorm_635.1:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x3e638c0]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.5.0:                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3d429a0]]}
  dc.input_tensor.layernorm_635.6:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x7e727c0]]}
  dc.input_tensor.layernorm_635.8:                                              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [2, 1], ublock: [2, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x7f622c0]]}
  lc.input_tensor.layernorm_635.dc.reciprocal.11_s_brcst_m1_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x3db2520]]}
  lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0:            {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x3e2d380]]}
  lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0:              {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x3d431e0]]}

  # epoch_to_epoch
  e2e_layernorm_105.dc.add.14_0:                                                {input: layernorm_105.dc.add.14, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x38da100]]}
  e2e_softmax_124.dc.multiply.5_0:                                              {input: softmax_124.dc.multiply.5, type: queue, entries: 4, grid_size: [1, 1], t: 12, mblock: [2, 1], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x38da100]]}
  e2e_layernorm_250.dc.subtract.3_0:                                            {input: layernorm_250.dc.subtract.3, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[4, 0x8d09da0]]}
  e2e_buffer_0_layernorm_250.dc.subtract.3_layernorm_250.dc.multiply.12_0:      {input: buffer_0_layernorm_250.dc.subtract.3_layernorm_250.dc.multiply.12, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[5, 0x48e2480]]}
  e2e_layernorm_356.dc.add.14_0:                                                {input: layernorm_356.dc.add.14, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[0, 0x47a3620]]}
  e2e_layernorm_462.dc.add.14_0:                                                {input: layernorm_462.dc.add.14, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[1, 0x4786f20]]}
  e2e_layernorm_568.dc.add.14_0:                                                {input: layernorm_568.dc.add.14, type: queue, entries: 4, grid_size: [1, 1], t: 1, mblock: [2, 6], ublock: [2, 4], ublock_order: c, df: Float16_b, target_device: 1, loc: dram, dram: [[2, 0x4747fa0]]}

graphs:
  fwd_0_0:
    target_device: 1
    input_count: 2
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [input_1, layer.0.attention.self.query.weight, layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_8: {type: matmul, grid_loc: [0, 2], grid_size: [1, 2], inputs: [input_1, layer.0.attention.self.key.weight, layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_14: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_16: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [matmul_14, input_1_multiply_16_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_17: {type: add, grid_loc: [1, 0], grid_size: [1, 1], inputs: [multiply_16, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_18.dc.exp.0: {type: exp, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_17],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [softmax_18.dc.exp.0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_18.dc.add.3: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_18.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_18.dc.reciprocal.4: {type: reciprocal, grid_loc: [1, 4], grid_size: [1, 1], inputs: [softmax_18.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [softmax_18.dc.reciprocal.4, lc.input_tensor.softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_18.dc.multiply.5: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [softmax_18.dc.exp.0, softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_22: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [input_1, layer.0.attention.self.value.weight, layer.0.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_29: {type: matmul, grid_loc: [1, 7], grid_size: [1, 1], inputs: [softmax_18.dc.multiply.5, matmul_22],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [2, 0], grid_size: [1, 2], inputs: [matmul_29, layer.0.attention.output.dense.weight, layer.0.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    add_37: {type: add, grid_loc: [2, 2], grid_size: [1, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_38.dc.multiply.2: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_38.dc.subtract.3: {type: subtract, grid_loc: [2, 5], grid_size: [1, 1], inputs: [add_37, layernorm_38.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.3, layernorm_38.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_38.dc.multiply.7: {type: multiply, grid_loc: [3, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.add.9: {type: add, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.7, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.sqrt.10: {type: sqrt, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_38.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_38.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_38.dc.reciprocal.11, lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_38.dc.subtract.3_buffer_0_layernorm_38.dc.subtract.3_layernorm_38.dc.multiply.12: {type: nop, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_38.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_38.dc.subtract.3_layernorm_38.dc.multiply.12: {type: nop, grid_loc: [2, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_38.dc.subtract.3_buffer_0_layernorm_38.dc.subtract.3_layernorm_38.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.multiply.12: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_38.dc.subtract.3_layernorm_38.dc.multiply.12, layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_38.dc.multiply.13: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.12, layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_38.dc.add.14: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.13, layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_41: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [layernorm_38.dc.add.14, layer.0.intermediate.dense.weight, layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_44: {type: gelu, grid_loc: [4, 4], grid_size: [1, 2], inputs: [matmul_41],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_47: {type: matmul, grid_loc: [6, 0], grid_size: [1, 8], inputs: [gelu_44, layer.0.output.dense.weight, layer.0.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_38.dc.add.14_add_51: {type: nop, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_38.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_47, buffer_0_layernorm_38.dc.add.14_add_51],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_52.dc.multiply.2: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_52.dc.subtract.3: {type: subtract, grid_loc: [7, 2], grid_size: [1, 1], inputs: [add_51, layernorm_52.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.3, layernorm_52.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_52.dc.multiply.7: {type: multiply, grid_loc: [7, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.add.9: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.7, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.sqrt.10: {type: sqrt, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_52.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reciprocal.11: {type: reciprocal, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_52.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_52.dc.reciprocal.11, lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_52.dc.subtract.3_buffer_0_layernorm_52.dc.subtract.3_layernorm_52.dc.multiply.12: {type: nop, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_52.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_52.dc.subtract.3_layernorm_52.dc.multiply.12: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_52.dc.subtract.3_buffer_0_layernorm_52.dc.subtract.3_layernorm_52.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.multiply.12: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_52.dc.subtract.3_layernorm_52.dc.multiply.12, layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.multiply.13: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.12, layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.0.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_52.dc.add.14: {type: add, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.13, layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_55: {type: matmul, grid_loc: [9, 1], grid_size: [1, 2], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.query.weight, layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_61: {type: matmul, grid_loc: [9, 3], grid_size: [1, 2], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.key.weight, layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_67: {type: matmul, grid_loc: [9, 5], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_69: {type: multiply, grid_loc: [9, 6], grid_size: [1, 1], inputs: [matmul_67, input_1_multiply_69_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_70: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [multiply_69, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}

  fwd_0_1:
    target_device: 0
    input_count: 2
    softmax_71.dc.exp.0: {type: exp, grid_loc: [0, 0], grid_size: [1, 1], inputs: [add_70],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [softmax_71.dc.exp.0, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_71.dc.add.3: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_71.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_71.dc.reciprocal.4: {type: reciprocal, grid_loc: [0, 3], grid_size: [1, 1], inputs: [softmax_71.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [softmax_71.dc.reciprocal.4, lc.input_tensor.softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_71.dc.multiply.5: {type: multiply, grid_loc: [0, 5], grid_size: [1, 1], inputs: [softmax_71.dc.exp.0, softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_75: {type: matmul, grid_loc: [0, 6], grid_size: [1, 2], inputs: [layernorm_52.dc.add.14, layer.1.attention.self.value.weight, layer.1.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_82: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [softmax_71.dc.multiply.5, matmul_75],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [1, 1], grid_size: [1, 2], inputs: [matmul_82, layer.1.attention.output.dense.weight, layer.1.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_52.dc.add.14_add_90: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_52.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_90: {type: add, grid_loc: [1, 4], grid_size: [1, 1], inputs: [matmul_86, buffer_0_layernorm_52.dc.add.14_add_90],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_91.dc.multiply.2: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_91.dc.subtract.3: {type: subtract, grid_loc: [1, 7], grid_size: [1, 1], inputs: [add_90, layernorm_91.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.3, layernorm_91.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [2, 3], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_91.dc.multiply.7: {type: multiply, grid_loc: [2, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.add.9: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.7, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.sqrt.10: {type: sqrt, grid_loc: [2, 6], grid_size: [1, 1], inputs: [layernorm_91.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reciprocal.11: {type: reciprocal, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_91.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_91.dc.reciprocal.11, lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_91.dc.subtract.3_buffer_0_layernorm_91.dc.subtract.3_layernorm_91.dc.multiply.12: {type: nop, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_91.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_91.dc.subtract.3_layernorm_91.dc.multiply.12: {type: nop, grid_loc: [2, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_91.dc.subtract.3_buffer_0_layernorm_91.dc.subtract.3_layernorm_91.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.multiply.12: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_91.dc.subtract.3_layernorm_91.dc.multiply.12, layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_91.dc.multiply.13: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.12, layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [3, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_91.dc.add.14: {type: add, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.13, layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_94: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [layernorm_91.dc.add.14, layer.1.intermediate.dense.weight, layer.1.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_97: {type: gelu, grid_loc: [3, 6], grid_size: [1, 2], inputs: [matmul_94],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_100: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_97, layer.1.output.dense.weight, layer.1.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_91.dc.add.14_add_104: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_91.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_104: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [matmul_100, buffer_0_layernorm_91.dc.add.14_add_104],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_105.dc.multiply.2: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_105.dc.subtract.3: {type: subtract, grid_loc: [6, 4], grid_size: [1, 1], inputs: [add_104, layernorm_105.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.3, layernorm_105.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 0], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_105.dc.multiply.7: {type: multiply, grid_loc: [7, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.add.9: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.7, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.sqrt.10: {type: sqrt, grid_loc: [7, 3], grid_size: [1, 1], inputs: [layernorm_105.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reciprocal.11: {type: reciprocal, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_105.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_105.dc.reciprocal.11, lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_105.dc.subtract.3_buffer_0_layernorm_105.dc.subtract.3_layernorm_105.dc.multiply.12: {type: nop, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_105.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_105.dc.subtract.3_layernorm_105.dc.multiply.12: {type: nop, grid_loc: [6, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_105.dc.subtract.3_buffer_0_layernorm_105.dc.subtract.3_layernorm_105.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.multiply.12: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_105.dc.subtract.3_layernorm_105.dc.multiply.12, layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_105.dc.multiply.13: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.12, layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.1.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_105.dc.add.14: {type: add, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.13, layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_108: {type: matmul, grid_loc: [8, 3], grid_size: [1, 2], inputs: [layernorm_105.dc.add.14, layer.2.attention.self.query.weight, layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_114: {type: matmul, grid_loc: [8, 5], grid_size: [1, 2], inputs: [layernorm_105.dc.add.14, layer.2.attention.self.key.weight, layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_120: {type: matmul, grid_loc: [8, 7], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_122: {type: multiply, grid_loc: [9, 0], grid_size: [1, 1], inputs: [matmul_120, input_1_multiply_122_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_123: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [multiply_122, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_124.dc.exp.0: {type: exp, grid_loc: [9, 2], grid_size: [1, 1], inputs: [add_123],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [softmax_124.dc.exp.0, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_124.dc.add.3: {type: add, grid_loc: [9, 4], grid_size: [1, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_124.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_124.dc.reciprocal.4: {type: reciprocal, grid_loc: [9, 5], grid_size: [1, 1], inputs: [softmax_124.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_124.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [softmax_124.dc.reciprocal.4, lc.input_tensor.softmax_124.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_124.dc.multiply.5: {type: multiply, grid_loc: [9, 7], grid_size: [1, 1], inputs: [softmax_124.dc.exp.0, softmax_124.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}

  fwd_0_2:
    target_device: 1
    input_count: 2
    matmul_128: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [e2e_layernorm_105.dc.add.14_0, layer.2.attention.self.value.weight, layer.2.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_135: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [e2e_softmax_124.dc.multiply.5_0, matmul_128],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [0, 3], grid_size: [1, 2], inputs: [matmul_135, layer.2.attention.output.dense.weight, layer.2.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_105.dc.add.14_add_143: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [e2e_layernorm_105.dc.add.14_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_143: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [matmul_139, buffer_0_layernorm_105.dc.add.14_add_143],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_144.dc.multiply.2: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.1, layernorm_144.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_144.dc.subtract.3: {type: subtract, grid_loc: [1, 1], grid_size: [1, 1], inputs: [add_143, layernorm_144.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_144.dc.subtract.3, layernorm_144.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [1, 5], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_144.dc.multiply.7: {type: multiply, grid_loc: [1, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.6, layernorm_144.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.add.9: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.7, dc.input_tensor.layernorm_144.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.sqrt.10: {type: sqrt, grid_loc: [2, 0], grid_size: [1, 1], inputs: [layernorm_144.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reciprocal.11: {type: reciprocal, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_144.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [2, 2], grid_size: [1, 1], inputs: [layernorm_144.dc.reciprocal.11, lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_144.dc.subtract.3_buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_144.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12: {type: nop, grid_loc: [1, 3], grid_size: [1, 1], inputs: [buffer_0_layernorm_144.dc.subtract.3_buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.multiply.12: {type: multiply, grid_loc: [2, 3], grid_size: [1, 1], inputs: [buffer_0_layernorm_144.dc.subtract.3_layernorm_144.dc.multiply.12, layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_144.dc.multiply.13: {type: multiply, grid_loc: [2, 5], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.12, layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_144.dc.add.14: {type: add, grid_loc: [2, 7], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.13, layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_147: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layernorm_144.dc.add.14, layer.2.intermediate.dense.weight, layer.2.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_150: {type: gelu, grid_loc: [4, 0], grid_size: [1, 2], inputs: [matmul_147],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_153: {type: matmul, grid_loc: [5, 0], grid_size: [1, 8], inputs: [gelu_150, layer.2.output.dense.weight, layer.2.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_144.dc.add.14_add_157: {type: nop, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_144.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_157: {type: add, grid_loc: [4, 3], grid_size: [1, 1], inputs: [matmul_153, buffer_0_layernorm_144.dc.add.14_add_157],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 4], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_158.dc.multiply.2: {type: multiply, grid_loc: [4, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_158.dc.subtract.3: {type: subtract, grid_loc: [4, 6], grid_size: [1, 1], inputs: [add_157, layernorm_158.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_158.dc.subtract.3, layernorm_158.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [6, 2], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_158.dc.multiply.7: {type: multiply, grid_loc: [6, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.add.9: {type: add, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.7, dc.input_tensor.layernorm_158.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.sqrt.10: {type: sqrt, grid_loc: [6, 5], grid_size: [1, 1], inputs: [layernorm_158.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reciprocal.11: {type: reciprocal, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_158.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_158.dc.reciprocal.11, lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_158.dc.subtract.3_buffer_0_layernorm_158.dc.subtract.3_layernorm_158.dc.multiply.12: {type: nop, grid_loc: [4, 7], grid_size: [1, 1], inputs: [layernorm_158.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_158.dc.subtract.3_layernorm_158.dc.multiply.12: {type: nop, grid_loc: [6, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_158.dc.subtract.3_buffer_0_layernorm_158.dc.subtract.3_layernorm_158.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.multiply.12: {type: multiply, grid_loc: [7, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_158.dc.subtract.3_layernorm_158.dc.multiply.12, layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_158.dc.multiply.13: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.12, layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [7, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.2.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_158.dc.add.14: {type: add, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.13, layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_161: {type: matmul, grid_loc: [7, 5], grid_size: [1, 2], inputs: [layernorm_158.dc.add.14, layer.3.attention.self.query.weight, layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_167: {type: matmul, grid_loc: [8, 0], grid_size: [1, 2], inputs: [layernorm_158.dc.add.14, layer.3.attention.self.key.weight, layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_173: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_175: {type: multiply, grid_loc: [8, 2], grid_size: [1, 1], inputs: [matmul_173, input_1_multiply_175_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_176: {type: add, grid_loc: [8, 3], grid_size: [1, 1], inputs: [multiply_175, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_177.dc.exp.0: {type: exp, grid_loc: [8, 4], grid_size: [1, 1], inputs: [add_176],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [softmax_177.dc.exp.0, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_177.dc.add.3: {type: add, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_177.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_177.dc.reciprocal.4: {type: reciprocal, grid_loc: [8, 7], grid_size: [1, 1], inputs: [softmax_177.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_177.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [softmax_177.dc.reciprocal.4, lc.input_tensor.softmax_177.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_177.dc.multiply.5: {type: multiply, grid_loc: [9, 1], grid_size: [1, 1], inputs: [softmax_177.dc.exp.0, softmax_177.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_181: {type: matmul, grid_loc: [9, 2], grid_size: [1, 2], inputs: [layernorm_158.dc.add.14, layer.3.attention.self.value.weight, layer.3.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_188: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [softmax_177.dc.multiply.5, matmul_181],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_192: {type: matmul, grid_loc: [9, 5], grid_size: [1, 2], inputs: [matmul_188, layer.3.attention.output.dense.weight, layer.3.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_158.dc.add.14_add_196: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_158.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_3:
    target_device: 0
    input_count: 2
    add_196: {type: add, grid_loc: [0, 0], grid_size: [1, 1], inputs: [matmul_192, buffer_0_layernorm_158.dc.add.14_add_196],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_197.dc.multiply.2: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.1, layernorm_197.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_197.dc.subtract.3: {type: subtract, grid_loc: [0, 3], grid_size: [1, 1], inputs: [add_196, layernorm_197.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.multiply.4: {type: multiply, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_197.dc.subtract.3, layernorm_197.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 7], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.4, lc.input_tensor.layernorm_197.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_197.dc.multiply.7: {type: multiply, grid_loc: [1, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.6, layernorm_197.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.add.9: {type: add, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.7, dc.input_tensor.layernorm_197.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.sqrt.10: {type: sqrt, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_197.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reciprocal.11: {type: reciprocal, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_197.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_197.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [layernorm_197.dc.reciprocal.11, lc.input_tensor.layernorm_197.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_197.dc.subtract.3_buffer_0_layernorm_197.dc.subtract.3_layernorm_197.dc.multiply.12: {type: nop, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_197.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_197.dc.subtract.3_layernorm_197.dc.multiply.12: {type: nop, grid_loc: [0, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_197.dc.subtract.3_buffer_0_layernorm_197.dc.subtract.3_layernorm_197.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.multiply.12: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_197.dc.subtract.3_layernorm_197.dc.multiply.12, layernorm_197.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_197.dc.multiply.13: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.12, layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [2, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_197.dc.add.14: {type: add, grid_loc: [2, 1], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.13, layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_200: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [layernorm_197.dc.add.14, layer.3.intermediate.dense.weight, layer.3.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_203: {type: gelu, grid_loc: [2, 2], grid_size: [1, 2], inputs: [matmul_200],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_206: {type: matmul, grid_loc: [4, 0], grid_size: [1, 8], inputs: [gelu_203, layer.3.output.dense.weight, layer.3.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_197.dc.add.14_add_210: {type: nop, grid_loc: [2, 4], grid_size: [1, 1], inputs: [layernorm_197.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_210: {type: add, grid_loc: [2, 5], grid_size: [1, 1], inputs: [matmul_206, buffer_0_layernorm_197.dc.add.14_add_210],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [2, 6], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_211.dc.multiply.2: {type: multiply, grid_loc: [2, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.1, layernorm_211.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_211.dc.subtract.3: {type: subtract, grid_loc: [5, 0], grid_size: [1, 1], inputs: [add_210, layernorm_211.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.multiply.4: {type: multiply, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_211.dc.subtract.3, layernorm_211.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 4], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.4, lc.input_tensor.layernorm_211.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_211.dc.multiply.7: {type: multiply, grid_loc: [5, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.6, layernorm_211.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.add.9: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.7, dc.input_tensor.layernorm_211.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.sqrt.10: {type: sqrt, grid_loc: [5, 7], grid_size: [1, 1], inputs: [layernorm_211.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reciprocal.11: {type: reciprocal, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_211.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_211.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 1], grid_size: [1, 1], inputs: [layernorm_211.dc.reciprocal.11, lc.input_tensor.layernorm_211.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_211.dc.subtract.3_buffer_0_layernorm_211.dc.subtract.3_layernorm_211.dc.multiply.12: {type: nop, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_211.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_211.dc.subtract.3_layernorm_211.dc.multiply.12: {type: nop, grid_loc: [5, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_211.dc.subtract.3_buffer_0_layernorm_211.dc.subtract.3_layernorm_211.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.multiply.12: {type: multiply, grid_loc: [6, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_211.dc.subtract.3_layernorm_211.dc.multiply.12, layernorm_211.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_211.dc.multiply.13: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.12, layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.3.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_211.dc.add.14: {type: add, grid_loc: [6, 6], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.13, layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_214: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [layernorm_211.dc.add.14, layer.4.attention.self.query.weight, layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_220: {type: matmul, grid_loc: [7, 2], grid_size: [1, 2], inputs: [layernorm_211.dc.add.14, layer.4.attention.self.key.weight, layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_226: {type: matmul, grid_loc: [6, 7], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_228: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [matmul_226, input_1_multiply_228_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_229: {type: add, grid_loc: [7, 5], grid_size: [1, 1], inputs: [multiply_228, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_230.dc.exp.0: {type: exp, grid_loc: [7, 6], grid_size: [1, 1], inputs: [add_229],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [softmax_230.dc.exp.0, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_230.dc.add.3: {type: add, grid_loc: [8, 0], grid_size: [1, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_230.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_230.dc.reciprocal.4: {type: reciprocal, grid_loc: [8, 1], grid_size: [1, 1], inputs: [softmax_230.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_230.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 2], grid_size: [1, 1], inputs: [softmax_230.dc.reciprocal.4, lc.input_tensor.softmax_230.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_230.dc.multiply.5: {type: multiply, grid_loc: [8, 3], grid_size: [1, 1], inputs: [softmax_230.dc.exp.0, softmax_230.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_234: {type: matmul, grid_loc: [8, 4], grid_size: [1, 2], inputs: [layernorm_211.dc.add.14, layer.4.attention.self.value.weight, layer.4.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_241: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [softmax_230.dc.multiply.5, matmul_234],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_245: {type: matmul, grid_loc: [9, 0], grid_size: [1, 2], inputs: [matmul_241, layer.4.attention.output.dense.weight, layer.4.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_211.dc.add.14_add_249: {type: nop, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_211.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_249: {type: add, grid_loc: [9, 2], grid_size: [1, 1], inputs: [matmul_245, buffer_0_layernorm_211.dc.add.14_add_249],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_250.dc.multiply.2: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_250.1, layernorm_250.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_250.dc.subtract.3: {type: subtract, grid_loc: [9, 5], grid_size: [1, 1], inputs: [add_249, layernorm_250.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_250.dc.subtract.3_buffer_0_layernorm_250.dc.subtract.3_layernorm_250.dc.multiply.12: {type: nop, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_250.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_250.dc.subtract.3_layernorm_250.dc.multiply.12: {type: nop, grid_loc: [9, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_250.dc.subtract.3_buffer_0_layernorm_250.dc.subtract.3_layernorm_250.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_4:
    target_device: 1
    input_count: 2
    layernorm_250.dc.multiply.4: {type: multiply, grid_loc: [0, 0], grid_size: [1, 1], inputs: [e2e_layernorm_250.dc.subtract.3_0, e2e_layernorm_250.dc.subtract.3_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [0, 1], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.4, lc.input_tensor.layernorm_250.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_250.dc.multiply.7: {type: multiply, grid_loc: [0, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_250.6, layernorm_250.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.add.9: {type: add, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.7, dc.input_tensor.layernorm_250.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.sqrt.10: {type: sqrt, grid_loc: [0, 4], grid_size: [1, 1], inputs: [layernorm_250.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reciprocal.11: {type: reciprocal, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_250.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_250.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [1, 1], inputs: [layernorm_250.dc.reciprocal.11, lc.input_tensor.layernorm_250.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_250.dc.multiply.12: {type: multiply, grid_loc: [0, 7], grid_size: [1, 1], inputs: [e2e_buffer_0_layernorm_250.dc.subtract.3_layernorm_250.dc.multiply.12_0, layernorm_250.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_250.dc.multiply.13: {type: multiply, grid_loc: [1, 1], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.12, layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_250.dc.add.14: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.13, layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_253: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [layernorm_250.dc.add.14, layer.4.intermediate.dense.weight, layer.4.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_256: {type: gelu, grid_loc: [1, 4], grid_size: [1, 2], inputs: [matmul_253],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_259: {type: matmul, grid_loc: [3, 0], grid_size: [1, 8], inputs: [gelu_256, layer.4.output.dense.weight, layer.4.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_250.dc.add.14_add_263: {type: nop, grid_loc: [1, 6], grid_size: [1, 1], inputs: [layernorm_250.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_263: {type: add, grid_loc: [1, 7], grid_size: [1, 1], inputs: [matmul_259, buffer_0_layernorm_250.dc.add.14_add_263],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_264.dc.multiply.2: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_264.1, layernorm_264.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_264.dc.subtract.3: {type: subtract, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_263, layernorm_264.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.multiply.4: {type: multiply, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_264.dc.subtract.3, layernorm_264.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 6], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.4, lc.input_tensor.layernorm_264.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_264.dc.multiply.7: {type: multiply, grid_loc: [4, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_264.6, layernorm_264.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.add.9: {type: add, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.7, dc.input_tensor.layernorm_264.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.sqrt.10: {type: sqrt, grid_loc: [5, 1], grid_size: [1, 1], inputs: [layernorm_264.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reciprocal.11: {type: reciprocal, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_264.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_264.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [5, 3], grid_size: [1, 1], inputs: [layernorm_264.dc.reciprocal.11, lc.input_tensor.layernorm_264.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_264.dc.subtract.3_buffer_0_layernorm_264.dc.subtract.3_layernorm_264.dc.multiply.12: {type: nop, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_264.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_264.dc.subtract.3_layernorm_264.dc.multiply.12: {type: nop, grid_loc: [4, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_264.dc.subtract.3_buffer_0_layernorm_264.dc.subtract.3_layernorm_264.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.multiply.12: {type: multiply, grid_loc: [5, 4], grid_size: [1, 1], inputs: [buffer_0_layernorm_264.dc.subtract.3_layernorm_264.dc.multiply.12, layernorm_264.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_264.dc.multiply.13: {type: multiply, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.12, layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.4.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_264.dc.add.14: {type: add, grid_loc: [6, 0], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.13, layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_267: {type: matmul, grid_loc: [6, 1], grid_size: [1, 2], inputs: [layernorm_264.dc.add.14, layer.5.attention.self.query.weight, layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_273: {type: matmul, grid_loc: [6, 3], grid_size: [1, 2], inputs: [layernorm_264.dc.add.14, layer.5.attention.self.key.weight, layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_279: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_281: {type: multiply, grid_loc: [6, 6], grid_size: [1, 1], inputs: [matmul_279, input_1_multiply_281_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_282: {type: add, grid_loc: [6, 7], grid_size: [1, 1], inputs: [multiply_281, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_283.dc.exp.0: {type: exp, grid_loc: [7, 0], grid_size: [1, 1], inputs: [add_282],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [softmax_283.dc.exp.0, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_283.dc.add.3: {type: add, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_283.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_283.dc.reciprocal.4: {type: reciprocal, grid_loc: [7, 3], grid_size: [1, 1], inputs: [softmax_283.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_283.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [7, 4], grid_size: [1, 1], inputs: [softmax_283.dc.reciprocal.4, lc.input_tensor.softmax_283.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_283.dc.multiply.5: {type: multiply, grid_loc: [7, 5], grid_size: [1, 1], inputs: [softmax_283.dc.exp.0, softmax_283.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_287: {type: matmul, grid_loc: [7, 6], grid_size: [1, 2], inputs: [layernorm_264.dc.add.14, layer.5.attention.self.value.weight, layer.5.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_294: {type: matmul, grid_loc: [8, 0], grid_size: [1, 1], inputs: [softmax_283.dc.multiply.5, matmul_287],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_298: {type: matmul, grid_loc: [8, 1], grid_size: [1, 2], inputs: [matmul_294, layer.5.attention.output.dense.weight, layer.5.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_264.dc.add.14_add_302: {type: nop, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_264.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_302: {type: add, grid_loc: [8, 4], grid_size: [1, 1], inputs: [matmul_298, buffer_0_layernorm_264.dc.add.14_add_302],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_303.dc.multiply.2: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_303.1, layernorm_303.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_303.dc.subtract.3: {type: subtract, grid_loc: [8, 7], grid_size: [1, 1], inputs: [add_302, layernorm_303.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.multiply.4: {type: multiply, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_303.dc.subtract.3, layernorm_303.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.4, lc.input_tensor.layernorm_303.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_303.dc.multiply.7: {type: multiply, grid_loc: [9, 4], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_303.6, layernorm_303.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.add.9: {type: add, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.7, dc.input_tensor.layernorm_303.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.sqrt.10: {type: sqrt, grid_loc: [9, 6], grid_size: [1, 1], inputs: [layernorm_303.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reciprocal.11: {type: reciprocal, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_303.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    buffer_0_layernorm_303.dc.subtract.3_buffer_0_layernorm_303.dc.subtract.3_layernorm_303.dc.multiply.12: {type: nop, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_303.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_303.dc.subtract.3_layernorm_303.dc.multiply.12: {type: nop, grid_loc: [9, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_303.dc.subtract.3_buffer_0_layernorm_303.dc.subtract.3_layernorm_303.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}

  fwd_0_5:
    target_device: 0
    input_count: 2
    layernorm_303.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 1], inputs: [layernorm_303.dc.reciprocal.11, lc.input_tensor.layernorm_303.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_303.dc.multiply.12: {type: multiply, grid_loc: [0, 1], grid_size: [1, 1], inputs: [buffer_0_layernorm_303.dc.subtract.3_layernorm_303.dc.multiply.12, layernorm_303.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_303.dc.multiply.13: {type: multiply, grid_loc: [0, 3], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.12, layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_303.dc.add.14: {type: add, grid_loc: [0, 5], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.13, layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_306: {type: matmul, grid_loc: [1, 0], grid_size: [1, 8], inputs: [layernorm_303.dc.add.14, layer.5.intermediate.dense.weight, layer.5.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_309: {type: gelu, grid_loc: [0, 6], grid_size: [1, 2], inputs: [matmul_306],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_312: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_309, layer.5.output.dense.weight, layer.5.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_303.dc.add.14_add_316: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_303.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_316: {type: add, grid_loc: [3, 1], grid_size: [1, 1], inputs: [matmul_312, buffer_0_layernorm_303.dc.add.14_add_316],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_317.dc.multiply.2: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_317.1, layernorm_317.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_317.dc.subtract.3: {type: subtract, grid_loc: [3, 4], grid_size: [1, 1], inputs: [add_316, layernorm_317.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.multiply.4: {type: multiply, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_317.dc.subtract.3, layernorm_317.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [4, 0], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.4, lc.input_tensor.layernorm_317.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_317.dc.multiply.7: {type: multiply, grid_loc: [4, 1], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_317.6, layernorm_317.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.add.9: {type: add, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.7, dc.input_tensor.layernorm_317.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.sqrt.10: {type: sqrt, grid_loc: [4, 3], grid_size: [1, 1], inputs: [layernorm_317.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reciprocal.11: {type: reciprocal, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_317.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_317.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [4, 5], grid_size: [1, 1], inputs: [layernorm_317.dc.reciprocal.11, lc.input_tensor.layernorm_317.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_317.dc.subtract.3_buffer_0_layernorm_317.dc.subtract.3_layernorm_317.dc.multiply.12: {type: nop, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_317.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_317.dc.subtract.3_layernorm_317.dc.multiply.12: {type: nop, grid_loc: [3, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_317.dc.subtract.3_buffer_0_layernorm_317.dc.subtract.3_layernorm_317.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.multiply.12: {type: multiply, grid_loc: [4, 6], grid_size: [1, 1], inputs: [buffer_0_layernorm_317.dc.subtract.3_layernorm_317.dc.multiply.12, layernorm_317.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_317.dc.multiply.13: {type: multiply, grid_loc: [5, 0], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.12, layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [5, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.5.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_317.dc.add.14: {type: add, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.13, layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_320: {type: matmul, grid_loc: [5, 3], grid_size: [1, 2], inputs: [layernorm_317.dc.add.14, layer.6.attention.self.query.weight, layer.6.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_326: {type: matmul, grid_loc: [5, 5], grid_size: [1, 2], inputs: [layernorm_317.dc.add.14, layer.6.attention.self.key.weight, layer.6.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_332: {type: matmul, grid_loc: [5, 7], grid_size: [1, 1], inputs: [matmul_320, matmul_326],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_334: {type: multiply, grid_loc: [6, 0], grid_size: [1, 1], inputs: [matmul_332, input_1_multiply_334_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_335: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [multiply_334, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_336.dc.exp.0: {type: exp, grid_loc: [6, 2], grid_size: [1, 1], inputs: [add_335],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [softmax_336.dc.exp.0, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_336.dc.add.3: {type: add, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_336.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_336.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 5], grid_size: [1, 1], inputs: [softmax_336.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_336.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 6], grid_size: [1, 1], inputs: [softmax_336.dc.reciprocal.4, lc.input_tensor.softmax_336.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_336.dc.multiply.5: {type: multiply, grid_loc: [6, 7], grid_size: [1, 1], inputs: [softmax_336.dc.exp.0, softmax_336.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_340: {type: matmul, grid_loc: [7, 0], grid_size: [1, 2], inputs: [layernorm_317.dc.add.14, layer.6.attention.self.value.weight, layer.6.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_347: {type: matmul, grid_loc: [7, 2], grid_size: [1, 1], inputs: [softmax_336.dc.multiply.5, matmul_340],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_351: {type: matmul, grid_loc: [7, 3], grid_size: [1, 2], inputs: [matmul_347, layer.6.attention.output.dense.weight, layer.6.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_317.dc.add.14_add_355: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_317.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_355: {type: add, grid_loc: [7, 6], grid_size: [1, 1], inputs: [matmul_351, buffer_0_layernorm_317.dc.add.14_add_355],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_356.dc.multiply.2: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_356.1, layernorm_356.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_356.dc.subtract.3: {type: subtract, grid_loc: [8, 1], grid_size: [1, 1], inputs: [add_355, layernorm_356.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_356.dc.subtract.3, layernorm_356.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [8, 5], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.4, lc.input_tensor.layernorm_356.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_356.dc.multiply.7: {type: multiply, grid_loc: [8, 6], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_356.6, layernorm_356.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.add.9: {type: add, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.7, dc.input_tensor.layernorm_356.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.sqrt.10: {type: sqrt, grid_loc: [9, 0], grid_size: [1, 1], inputs: [layernorm_356.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reciprocal.11: {type: reciprocal, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_356.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_356.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [9, 2], grid_size: [1, 1], inputs: [layernorm_356.dc.reciprocal.11, lc.input_tensor.layernorm_356.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_356.dc.subtract.3_buffer_0_layernorm_356.dc.subtract.3_layernorm_356.dc.multiply.12: {type: nop, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_356.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_356.dc.subtract.3_layernorm_356.dc.multiply.12: {type: nop, grid_loc: [8, 3], grid_size: [1, 1], inputs: [buffer_0_layernorm_356.dc.subtract.3_buffer_0_layernorm_356.dc.subtract.3_layernorm_356.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.multiply.12: {type: multiply, grid_loc: [9, 3], grid_size: [1, 1], inputs: [buffer_0_layernorm_356.dc.subtract.3_layernorm_356.dc.multiply.12, layernorm_356.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 4], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_356.dc.multiply.13: {type: multiply, grid_loc: [9, 5], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.12, layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_356.dc.add.14: {type: add, grid_loc: [9, 7], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.13, layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_0_6:
    target_device: 1
    input_count: 2
    matmul_359: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_layernorm_356.dc.add.14_0, layer.6.intermediate.dense.weight, layer.6.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_362: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_359],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_365: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_362, layer.6.output.dense.weight, layer.6.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_356.dc.add.14_add_369: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_layernorm_356.dc.add.14_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_369: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_365, buffer_0_layernorm_356.dc.add.14_add_369],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_370.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_370.1, layernorm_370.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_370.dc.subtract.3: {type: subtract, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_369, layernorm_370.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.multiply.4: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_370.dc.subtract.3, layernorm_370.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.4, lc.input_tensor.layernorm_370.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_370.dc.multiply.7: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_370.6, layernorm_370.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.add.9: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.7, dc.input_tensor.layernorm_370.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.sqrt.10: {type: sqrt, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_370.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_370.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_370.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_370.dc.reciprocal.11, lc.input_tensor.layernorm_370.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_370.dc.subtract.3_buffer_0_layernorm_370.dc.subtract.3_layernorm_370.dc.multiply.12: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_370.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_370.dc.subtract.3_layernorm_370.dc.multiply.12: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_370.dc.subtract.3_buffer_0_layernorm_370.dc.subtract.3_layernorm_370.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.multiply.12: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_370.dc.subtract.3_layernorm_370.dc.multiply.12, layernorm_370.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_370.dc.multiply.13: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.12, layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.6.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_370.dc.add.14: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.13, layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_373: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [layernorm_370.dc.add.14, layer.7.attention.self.query.weight, layer.7.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_379: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [layernorm_370.dc.add.14, layer.7.attention.self.key.weight, layer.7.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_385: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_373, matmul_379],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_387: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_385, input_1_multiply_387_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_388: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [multiply_387, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_389.dc.exp.0: {type: exp, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_388],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [softmax_389.dc.exp.0, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_389.dc.add.3: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_389.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_389.dc.reciprocal.4: {type: reciprocal, grid_loc: [5, 7], grid_size: [1, 1], inputs: [softmax_389.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_389.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_389.dc.reciprocal.4, lc.input_tensor.softmax_389.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_389.dc.multiply.5: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [softmax_389.dc.exp.0, softmax_389.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_393: {type: matmul, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_370.dc.add.14, layer.7.attention.self.value.weight, layer.7.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_400: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_389.dc.multiply.5, matmul_393],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_404: {type: matmul, grid_loc: [6, 5], grid_size: [1, 2], inputs: [matmul_400, layer.7.attention.output.dense.weight, layer.7.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_370.dc.add.14_add_408: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_370.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_408: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_404, buffer_0_layernorm_370.dc.add.14_add_408],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_409.dc.multiply.2: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_409.1, layernorm_409.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_409.dc.subtract.3: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_408, layernorm_409.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.multiply.4: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_409.dc.subtract.3, layernorm_409.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.4, lc.input_tensor.layernorm_409.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_409.dc.multiply.7: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_409.6, layernorm_409.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.add.9: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.7, dc.input_tensor.layernorm_409.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.sqrt.10: {type: sqrt, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_409.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reciprocal.11: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_409.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_409.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_409.dc.reciprocal.11, lc.input_tensor.layernorm_409.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_409.dc.subtract.3_buffer_0_layernorm_409.dc.subtract.3_layernorm_409.dc.multiply.12: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_409.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_409.dc.subtract.3_layernorm_409.dc.multiply.12: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_409.dc.subtract.3_buffer_0_layernorm_409.dc.subtract.3_layernorm_409.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.multiply.12: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_409.dc.subtract.3_layernorm_409.dc.multiply.12, layernorm_409.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_409.dc.multiply.13: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.12, layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_409.dc.add.14: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.13, layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_0_7:
    target_device: 0
    input_count: 2
    matmul_412: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layernorm_409.dc.add.14, layer.7.intermediate.dense.weight, layer.7.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_415: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_412],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_418: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_415, layer.7.output.dense.weight, layer.7.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_409.dc.add.14_add_422: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_409.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_422: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_418, buffer_0_layernorm_409.dc.add.14_add_422],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_423.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_423.1, layernorm_423.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_423.dc.subtract.3: {type: subtract, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_422, layernorm_423.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.multiply.4: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_423.dc.subtract.3, layernorm_423.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.4, lc.input_tensor.layernorm_423.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_423.dc.multiply.7: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_423.6, layernorm_423.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.add.9: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.7, dc.input_tensor.layernorm_423.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.sqrt.10: {type: sqrt, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_423.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_423.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_423.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_423.dc.reciprocal.11, lc.input_tensor.layernorm_423.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_423.dc.subtract.3_buffer_0_layernorm_423.dc.subtract.3_layernorm_423.dc.multiply.12: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_423.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_423.dc.subtract.3_layernorm_423.dc.multiply.12: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_423.dc.subtract.3_buffer_0_layernorm_423.dc.subtract.3_layernorm_423.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.multiply.12: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_423.dc.subtract.3_layernorm_423.dc.multiply.12, layernorm_423.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_423.dc.multiply.13: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.12, layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.7.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_423.dc.add.14: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.13, layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_426: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [layernorm_423.dc.add.14, layer.8.attention.self.query.weight, layer.8.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_432: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [layernorm_423.dc.add.14, layer.8.attention.self.key.weight, layer.8.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_438: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_426, matmul_432],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_440: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_438, input_1_multiply_440_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_441: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [multiply_440, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_442.dc.exp.0: {type: exp, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_441],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [softmax_442.dc.exp.0, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_442.dc.add.3: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_442.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_442.dc.reciprocal.4: {type: reciprocal, grid_loc: [5, 7], grid_size: [1, 1], inputs: [softmax_442.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_442.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_442.dc.reciprocal.4, lc.input_tensor.softmax_442.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_442.dc.multiply.5: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [softmax_442.dc.exp.0, softmax_442.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_446: {type: matmul, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_423.dc.add.14, layer.8.attention.self.value.weight, layer.8.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_453: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_442.dc.multiply.5, matmul_446],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_457: {type: matmul, grid_loc: [6, 5], grid_size: [1, 2], inputs: [matmul_453, layer.8.attention.output.dense.weight, layer.8.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_423.dc.add.14_add_461: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_423.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_461: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_457, buffer_0_layernorm_423.dc.add.14_add_461],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_462.dc.multiply.2: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_462.1, layernorm_462.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_462.dc.subtract.3: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_461, layernorm_462.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.multiply.4: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_462.dc.subtract.3, layernorm_462.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.4, lc.input_tensor.layernorm_462.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_462.dc.multiply.7: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_462.6, layernorm_462.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.add.9: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.7, dc.input_tensor.layernorm_462.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.sqrt.10: {type: sqrt, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_462.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reciprocal.11: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_462.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_462.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_462.dc.reciprocal.11, lc.input_tensor.layernorm_462.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_462.dc.subtract.3_buffer_0_layernorm_462.dc.subtract.3_layernorm_462.dc.multiply.12: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_462.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_462.dc.subtract.3_layernorm_462.dc.multiply.12: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_462.dc.subtract.3_buffer_0_layernorm_462.dc.subtract.3_layernorm_462.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.multiply.12: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_462.dc.subtract.3_layernorm_462.dc.multiply.12, layernorm_462.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_462.dc.multiply.13: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.12, layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_462.dc.add.14: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.13, layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_0_8:
    target_device: 1
    input_count: 2
    matmul_465: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_layernorm_462.dc.add.14_0, layer.8.intermediate.dense.weight, layer.8.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_468: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_465],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_471: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_468, layer.8.output.dense.weight, layer.8.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_462.dc.add.14_add_475: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_layernorm_462.dc.add.14_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_475: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_471, buffer_0_layernorm_462.dc.add.14_add_475],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_476.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_476.1, layernorm_476.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_476.dc.subtract.3: {type: subtract, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_475, layernorm_476.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.multiply.4: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_476.dc.subtract.3, layernorm_476.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.4, lc.input_tensor.layernorm_476.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_476.dc.multiply.7: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_476.6, layernorm_476.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.add.9: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.7, dc.input_tensor.layernorm_476.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.sqrt.10: {type: sqrt, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_476.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_476.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_476.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_476.dc.reciprocal.11, lc.input_tensor.layernorm_476.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_476.dc.subtract.3_buffer_0_layernorm_476.dc.subtract.3_layernorm_476.dc.multiply.12: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_476.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_476.dc.subtract.3_layernorm_476.dc.multiply.12: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_476.dc.subtract.3_buffer_0_layernorm_476.dc.subtract.3_layernorm_476.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.multiply.12: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_476.dc.subtract.3_layernorm_476.dc.multiply.12, layernorm_476.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_476.dc.multiply.13: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.12, layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.8.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_476.dc.add.14: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.13, layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_479: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [layernorm_476.dc.add.14, layer.9.attention.self.query.weight, layer.9.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_485: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [layernorm_476.dc.add.14, layer.9.attention.self.key.weight, layer.9.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_491: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_479, matmul_485],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_493: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_491, input_1_multiply_493_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_494: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [multiply_493, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_495.dc.exp.0: {type: exp, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_494],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [softmax_495.dc.exp.0, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_495.dc.add.3: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_495.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_495.dc.reciprocal.4: {type: reciprocal, grid_loc: [5, 7], grid_size: [1, 1], inputs: [softmax_495.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_495.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_495.dc.reciprocal.4, lc.input_tensor.softmax_495.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_495.dc.multiply.5: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [softmax_495.dc.exp.0, softmax_495.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_499: {type: matmul, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_476.dc.add.14, layer.9.attention.self.value.weight, layer.9.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_506: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_495.dc.multiply.5, matmul_499],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_510: {type: matmul, grid_loc: [6, 5], grid_size: [1, 2], inputs: [matmul_506, layer.9.attention.output.dense.weight, layer.9.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_476.dc.add.14_add_514: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_476.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_514: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_510, buffer_0_layernorm_476.dc.add.14_add_514],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_515.dc.multiply.2: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_515.1, layernorm_515.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_515.dc.subtract.3: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_514, layernorm_515.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.multiply.4: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_515.dc.subtract.3, layernorm_515.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.4, lc.input_tensor.layernorm_515.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_515.dc.multiply.7: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_515.6, layernorm_515.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.add.9: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.7, dc.input_tensor.layernorm_515.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.sqrt.10: {type: sqrt, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_515.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reciprocal.11: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_515.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_515.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_515.dc.reciprocal.11, lc.input_tensor.layernorm_515.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_515.dc.subtract.3_buffer_0_layernorm_515.dc.subtract.3_layernorm_515.dc.multiply.12: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_515.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_515.dc.subtract.3_layernorm_515.dc.multiply.12: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_515.dc.subtract.3_buffer_0_layernorm_515.dc.subtract.3_layernorm_515.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.multiply.12: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_515.dc.subtract.3_layernorm_515.dc.multiply.12, layernorm_515.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_515.dc.multiply.13: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.12, layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_515.dc.add.14: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.13, layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_0_9:
    target_device: 0
    input_count: 2
    matmul_518: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layernorm_515.dc.add.14, layer.9.intermediate.dense.weight, layer.9.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_521: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_518],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_524: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_521, layer.9.output.dense.weight, layer.9.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_515.dc.add.14_add_528: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [layernorm_515.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_528: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_524, buffer_0_layernorm_515.dc.add.14_add_528],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_529.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_529.1, layernorm_529.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_529.dc.subtract.3: {type: subtract, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_528, layernorm_529.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.multiply.4: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_529.dc.subtract.3, layernorm_529.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.4, lc.input_tensor.layernorm_529.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_529.dc.multiply.7: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_529.6, layernorm_529.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.add.9: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.7, dc.input_tensor.layernorm_529.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.sqrt.10: {type: sqrt, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_529.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_529.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_529.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_529.dc.reciprocal.11, lc.input_tensor.layernorm_529.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_529.dc.subtract.3_buffer_0_layernorm_529.dc.subtract.3_layernorm_529.dc.multiply.12: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_529.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_529.dc.subtract.3_layernorm_529.dc.multiply.12: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_529.dc.subtract.3_buffer_0_layernorm_529.dc.subtract.3_layernorm_529.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.multiply.12: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_529.dc.subtract.3_layernorm_529.dc.multiply.12, layernorm_529.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_529.dc.multiply.13: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.12, layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.9.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_529.dc.add.14: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.13, layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_532: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [layernorm_529.dc.add.14, layer.10.attention.self.query.weight, layer.10.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_538: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [layernorm_529.dc.add.14, layer.10.attention.self.key.weight, layer.10.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_544: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_532, matmul_538],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_546: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_544, input_1_multiply_546_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_547: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [multiply_546, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_548.dc.exp.0: {type: exp, grid_loc: [5, 4], grid_size: [1, 1], inputs: [add_547],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [5, 5], grid_size: [1, 1], inputs: [softmax_548.dc.exp.0, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_548.dc.add.3: {type: add, grid_loc: [5, 6], grid_size: [1, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_548.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_548.dc.reciprocal.4: {type: reciprocal, grid_loc: [5, 7], grid_size: [1, 1], inputs: [softmax_548.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_548.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_548.dc.reciprocal.4, lc.input_tensor.softmax_548.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_548.dc.multiply.5: {type: multiply, grid_loc: [6, 1], grid_size: [1, 1], inputs: [softmax_548.dc.exp.0, softmax_548.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_552: {type: matmul, grid_loc: [6, 2], grid_size: [1, 2], inputs: [layernorm_529.dc.add.14, layer.10.attention.self.value.weight, layer.10.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_559: {type: matmul, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_548.dc.multiply.5, matmul_552],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_563: {type: matmul, grid_loc: [6, 5], grid_size: [1, 2], inputs: [matmul_559, layer.10.attention.output.dense.weight, layer.10.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_529.dc.add.14_add_567: {type: nop, grid_loc: [6, 7], grid_size: [1, 1], inputs: [layernorm_529.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_567: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_563, buffer_0_layernorm_529.dc.add.14_add_567],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_568.dc.multiply.2: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_568.1, layernorm_568.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_568.dc.subtract.3: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_567, layernorm_568.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.multiply.4: {type: multiply, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_568.dc.subtract.3, layernorm_568.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 7], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.4, lc.input_tensor.layernorm_568.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_568.dc.multiply.7: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_568.6, layernorm_568.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.add.9: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.7, dc.input_tensor.layernorm_568.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.sqrt.10: {type: sqrt, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_568.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reciprocal.11: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_568.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_568.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_568.dc.reciprocal.11, lc.input_tensor.layernorm_568.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_568.dc.subtract.3_buffer_0_layernorm_568.dc.subtract.3_layernorm_568.dc.multiply.12: {type: nop, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_568.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_568.dc.subtract.3_layernorm_568.dc.multiply.12: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_568.dc.subtract.3_buffer_0_layernorm_568.dc.subtract.3_layernorm_568.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.multiply.12: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_568.dc.subtract.3_layernorm_568.dc.multiply.12, layernorm_568.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_568.dc.multiply.13: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.12, layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_568.dc.add.14: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.13, layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_0_10:
    target_device: 1
    input_count: 2
    matmul_571: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [e2e_layernorm_568.dc.add.14_0, layer.10.intermediate.dense.weight, layer.10.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_574: {type: gelu, grid_loc: [1, 0], grid_size: [1, 2], inputs: [matmul_571],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_577: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_574, layer.10.output.dense.weight, layer.10.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_568.dc.add.14_add_581: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e_layernorm_568.dc.add.14_0],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_581: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_577, buffer_0_layernorm_568.dc.add.14_add_581],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_582.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_582.1, layernorm_582.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_582.dc.subtract.3: {type: subtract, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_581, layernorm_582.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.multiply.4: {type: multiply, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_582.dc.subtract.3, layernorm_582.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 2], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.4, lc.input_tensor.layernorm_582.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_582.dc.multiply.7: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_582.6, layernorm_582.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.add.9: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.7, dc.input_tensor.layernorm_582.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.sqrt.10: {type: sqrt, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_582.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_582.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_582.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_582.dc.reciprocal.11, lc.input_tensor.layernorm_582.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_582.dc.subtract.3_buffer_0_layernorm_582.dc.subtract.3_layernorm_582.dc.multiply.12: {type: nop, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_582.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_582.dc.subtract.3_layernorm_582.dc.multiply.12: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_582.dc.subtract.3_buffer_0_layernorm_582.dc.subtract.3_layernorm_582.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.multiply.12: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_582.dc.subtract.3_layernorm_582.dc.multiply.12, layernorm_582.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_582.dc.multiply.13: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.12, layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.10.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_582.dc.add.14: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.13, layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    matmul_585: {type: matmul, grid_loc: [4, 5], grid_size: [1, 2], inputs: [layernorm_582.dc.add.14, layer.11.attention.self.query.weight, layer.11.attention.self.query.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_591: {type: matmul, grid_loc: [5, 0], grid_size: [1, 2], inputs: [layernorm_582.dc.add.14, layer.11.attention.self.key.weight, layer.11.attention.self.key.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_597: {type: matmul, grid_loc: [4, 7], grid_size: [1, 1], inputs: [matmul_585, matmul_591],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [transpose, vslice: 12], input_0_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    multiply_599: {type: multiply, grid_loc: [5, 2], grid_size: [1, 1], inputs: [matmul_597, input_1_multiply_599_tile_bcast_tile_bcast],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}, broadcast: {r: 4}, broadcast: {c: 4}],
         attributes: {single_tile: [1]}}
    add_600: {type: add, grid_loc: [5, 3], grid_size: [1, 1], inputs: [multiply_599, attention_mask],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}]}
    softmax_601.dc.exp.0: {type: exp, grid_loc: [5, 7], grid_size: [1, 1], inputs: [add_600],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [6, 0], grid_size: [1, 1], inputs: [softmax_601.dc.exp.0, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}, broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 4}}
    softmax_601.dc.add.3: {type: add, grid_loc: [6, 1], grid_size: [1, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_601.2],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    softmax_601.dc.reciprocal.4: {type: reciprocal, grid_loc: [6, 2], grid_size: [1, 1], inputs: [softmax_601.dc.add.3],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    softmax_601.dc.reciprocal.4_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [6, 3], grid_size: [1, 1], inputs: [softmax_601.dc.reciprocal.4, lc.input_tensor.softmax_601.dc.reciprocal.4_s_brcst_m1_0_0.0],
         t: 12, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {z: 12}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 1}}
    softmax_601.dc.multiply.5: {type: multiply, grid_loc: [6, 4], grid_size: [1, 1], inputs: [softmax_601.dc.exp.0, softmax_601.dc.reciprocal.4_s_brcst_m1_0_0.lc1],
         t: 12, mblock: [2, 1], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [128, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 4}]}
    matmul_605: {type: matmul, grid_loc: [5, 4], grid_size: [1, 2], inputs: [layernorm_582.dc.add.14, layer.11.attention.self.value.weight, layer.11.attention.self.value.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    matmul_612: {type: matmul, grid_loc: [6, 5], grid_size: [1, 1], inputs: [softmax_601.dc.multiply.5, matmul_605],
         t: 12, mblock: [2, 1], ublock: [2, 2], buf_size_mb: 24, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [hslice: 12],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 4}}
    matmul_616: {type: matmul, grid_loc: [6, 6], grid_size: [1, 2], inputs: [matmul_612, layer.11.attention.output.dense.weight, layer.11.attention.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}], input_0_tms: [hstack: 12],
         attributes: {bias: true, m_k: 12, min_buffer_input: 0, u_kt: 2}}
    buffer_0_layernorm_582.dc.add.14_add_620: {type: nop, grid_loc: [5, 6], grid_size: [1, 1], inputs: [layernorm_582.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_620: {type: add, grid_loc: [7, 0], grid_size: [1, 1], inputs: [matmul_616, buffer_0_layernorm_582.dc.add.14_add_620],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [7, 1], grid_size: [1, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_621.dc.multiply.2: {type: multiply, grid_loc: [7, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_621.1, layernorm_621.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_621.dc.subtract.3: {type: subtract, grid_loc: [7, 3], grid_size: [1, 1], inputs: [add_620, layernorm_621.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.multiply.4: {type: multiply, grid_loc: [7, 4], grid_size: [1, 1], inputs: [layernorm_621.dc.subtract.3, layernorm_621.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [7, 6], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.4, lc.input_tensor.layernorm_621.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_621.dc.multiply.7: {type: multiply, grid_loc: [8, 0], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_621.6, layernorm_621.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.add.9: {type: add, grid_loc: [8, 1], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.7, dc.input_tensor.layernorm_621.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.sqrt.10: {type: sqrt, grid_loc: [8, 2], grid_size: [1, 1], inputs: [layernorm_621.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reciprocal.11: {type: reciprocal, grid_loc: [8, 3], grid_size: [1, 1], inputs: [layernorm_621.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_621.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [8, 4], grid_size: [1, 1], inputs: [layernorm_621.dc.reciprocal.11, lc.input_tensor.layernorm_621.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_621.dc.subtract.3_buffer_0_layernorm_621.dc.subtract.3_layernorm_621.dc.multiply.12: {type: nop, grid_loc: [7, 5], grid_size: [1, 1], inputs: [layernorm_621.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_621.dc.subtract.3_layernorm_621.dc.multiply.12: {type: nop, grid_loc: [7, 7], grid_size: [1, 1], inputs: [buffer_0_layernorm_621.dc.subtract.3_buffer_0_layernorm_621.dc.subtract.3_layernorm_621.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.multiply.12: {type: multiply, grid_loc: [8, 5], grid_size: [1, 1], inputs: [buffer_0_layernorm_621.dc.subtract.3_layernorm_621.dc.multiply.12, layernorm_621.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [8, 6], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_621.dc.multiply.13: {type: multiply, grid_loc: [8, 7], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.12, layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [9, 0], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.attention.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_621.dc.add.14: {type: add, grid_loc: [9, 1], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.13, layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}

  fwd_0_11:
    target_device: 0
    input_count: 2
    matmul_624: {type: matmul, grid_loc: [0, 0], grid_size: [1, 8], inputs: [layernorm_621.dc.add.14, layer.11.intermediate.dense.weight, layer.11.intermediate.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 8, min_buffer_input: 0, u_kt: 3}}
    gelu_627: {type: gelu, grid_loc: [1, 1], grid_size: [1, 2], inputs: [matmul_624],
         t: 1, mblock: [2, 12], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    matmul_630: {type: matmul, grid_loc: [2, 0], grid_size: [1, 8], inputs: [gelu_627, layer.11.output.dense.weight, layer.11.output.dense.bias],
         t: 1, mblock: [2, 3], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 4}],
         attributes: {bias: true, m_k: 6, min_buffer_input: 0, u_kt: 16}}
    buffer_0_layernorm_621.dc.add.14_add_634: {type: nop, grid_loc: [1, 0], grid_size: [1, 1], inputs: [layernorm_621.dc.add.14],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [96], ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_634: {type: add, grid_loc: [1, 3], grid_size: [1, 1], inputs: [matmul_630, buffer_0_layernorm_621.dc.add.14_add_634],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [1, 4], grid_size: [1, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_sum.0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_635.dc.multiply.2: {type: multiply, grid_loc: [1, 5], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_635.1, layernorm_635.dc.reduce_sum.0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layernorm_635.dc.subtract.3: {type: subtract, grid_loc: [1, 6], grid_size: [1, 1], inputs: [add_634, layernorm_635.dc.multiply.2],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [280, 0], ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.multiply.4: {type: multiply, grid_loc: [1, 7], grid_size: [1, 1], inputs: [layernorm_635.dc.subtract.3, layernorm_635.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [3, 1], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.4, lc.input_tensor.layernorm_635.dc.reduce_sum.5.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 24}],
         attributes: {m_k: 1, min_buffer_input: 0, single_tile: [1], u_kt: 24}}
    layernorm_635.dc.multiply.7: {type: multiply, grid_loc: [3, 3], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_635.6, layernorm_635.dc.reduce_sum.5.lc1],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.add.9: {type: add, grid_loc: [3, 4], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.7, dc.input_tensor.layernorm_635.8],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.sqrt.10: {type: sqrt, grid_loc: [3, 5], grid_size: [1, 1], inputs: [layernorm_635.dc.add.9],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reciprocal.11: {type: reciprocal, grid_loc: [3, 6], grid_size: [1, 1], inputs: [layernorm_635.dc.sqrt.10],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: c, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: false}}
    layernorm_635.dc.reciprocal.11_s_brcst_m1_0_0.lc1: {type: matmul, grid_loc: [3, 7], grid_size: [1, 1], inputs: [layernorm_635.dc.reciprocal.11, lc.input_tensor.layernorm_635.dc.reciprocal.11_s_brcst_m1_0_0.0],
         t: 1, mblock: [2, 1], ublock: [2, 1], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    buffer_0_layernorm_635.dc.subtract.3_buffer_0_layernorm_635.dc.subtract.3_layernorm_635.dc.multiply.12: {type: nop, grid_loc: [3, 0], grid_size: [1, 1], inputs: [layernorm_635.dc.subtract.3],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [296], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    buffer_0_layernorm_635.dc.subtract.3_layernorm_635.dc.multiply.12: {type: nop, grid_loc: [3, 2], grid_size: [1, 1], inputs: [buffer_0_layernorm_635.dc.subtract.3_buffer_0_layernorm_635.dc.subtract.3_layernorm_635.dc.multiply.12],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, input_buf_min_size_tiles: [184], ublock_order: r, in_df: [Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.multiply.12: {type: multiply, grid_loc: [4, 0], grid_size: [1, 1], inputs: [buffer_0_layernorm_635.dc.subtract.3_layernorm_635.dc.multiply.12, layernorm_635.dc.reciprocal.11_s_brcst_m1_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 24}]}
    layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 1], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.weight],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_635.dc.multiply.13: {type: multiply, grid_loc: [4, 2], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.12, layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.lc1],
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}
    layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [4, 3], grid_size: [1, 1], inputs: [lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0, layer.11.output.LayerNorm.bias],
         t: 1, mblock: [1, 6], ublock: [1, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 1}}
    layernorm_635.dc.add.14: {type: add, grid_loc: [4, 4], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.13, layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.lc1], untilize_output: true,
         t: 1, mblock: [2, 6], ublock: [2, 4], buf_size_mb: 2, ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 4}]}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$c_microbatch_size: 2, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q4_shadow: 0, $gptr_q6_shadow: 0, $gptr_q0: 0, $lptr_q0: 0, $lptr_q1: 0, $lptr_q2: 0, $gptr_q2: 0, $gptr_q3: 0, $lptr_q3: 0, $lptr_q4: 0, $gptr_q5: 0, $lptr_q11: 0, $gptr_q10: 0, $lptr_q10: 0, $lptr_q7: 0, $gptr_q11: 0, $lptr_q9: 0, $gptr_q2_shadow: 0, $gptr_q8: 0, $lptr_q5: 0, $gptr_q9: 0, $gptr_q1: 0, $lptr_q8: 0, $gptr_q1_shadow: 0, $gptr_q4: 0, $gptr_q7: 0, $lptr_q6: 0, $gptr_q8_shadow: 0, $gptr_q6: 0}
    - loop: $p_loop_count
    -   varinst: [$gptr_q8, set, $gptr_q8_shadow]
    -   varinst: [$gptr_q6, set, $gptr_q6_shadow]
    -   varinst: [$gptr_q4, set, $gptr_q4_shadow]
    -   varinst: [$gptr_q2, set, $gptr_q2_shadow]
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   execute: {graph_name: fwd_0_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.0.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.1.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e_layernorm_105.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               e2e_softmax_124.dc.multiply.5_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.2.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_177.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               lc.input_tensor.layernorm_197.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.3.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_230.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e_layernorm_250.dc.subtract.3_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               e2e_buffer_0_layernorm_250.dc.subtract.3_layernorm_250.dc.multiply.12_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               lc.input_tensor.layernorm_250.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.4.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_283.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               lc.input_tensor.layernorm_303.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.5.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_336.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e_layernorm_356.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.6.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_389.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.7.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_442.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_8, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e_layernorm_462.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.8.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_495.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.9.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_548.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 8]
    -   execute: {graph_name: fwd_0_10, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e_layernorm_568.dc.add.14_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.10.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599_tile_bcast_tile_bcast: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_601.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reciprocal.4_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.attention.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   execute: {graph_name: fwd_0_11, queue_settings: {
               layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reciprocal.11_s_brcst_m1_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.weight_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layer.11.output.LayerNorm.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q10, incwrap, $c_microbatch_size, 8]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 8]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 8]
    - endloop


