# Source: netlist_ffc2a4193e5846d3eb5db123fce17b18.yaml

devices:
  arch: blackhole

queues:
  # input
  input_activations: {input: HOST, type: queue, entries: 4, grid_size: [1, 3], t: 2, mblock: [3, 2], ublock: [1, 1], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x10000000], [2, 0x1000d840], [2,
        0x1001b080]]}
  # output
  output_activations: {input: ff_0_ff2.bias, type: queue, entries: 4, grid_size: [1, 1], t: 2, mblock: [3, 3], ublock: [1, 2], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x100288c0]]}
  # parameter
  ff.output.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 2, mblock: [1, 1], ublock: [1, 2], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x10000000], [3, 0x10001240], [
        3, 0x10002480]]}
  ff.intermediate.dense.bias: {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 2, mblock: [1, 1], ublock: [1, 8], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x100036c0], [3, 0x10007f00],
      [3, 0x1000c740]]}
  ff.intermediate.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [1, 3], t: 2, mblock: [2, 4], ublock: [3, 2], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x10000000], [4, 0x1001b040],
      [4, 0x10036080]]}
  ff.output.dense.weight: {input: HOST, type: ram, entries: 1, grid_size: [4, 1], t: 2, mblock: [6, 2], ublock: [1, 3], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x10010f80], [3, 0x100253c0],
      [3, 0x10039800], [3, 0x1004dc40]]}
  # constant
  lc.input_tensor.ff.output.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Bfp8_b, target_device: 0, loc: dram, dram: [
      [3, 0x10062080]]}
  lc.input_tensor.ff.intermediate.dense.bias_s_brcst_m2_0_0.0: {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], df: Bfp8_b, target_device: 0, loc: dram, dram: [
      [5, 0x10000000]]}

graphs:
  # forward
  fwd_0:
    target_device: 0
    input_count: 2
    ff.output.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [1, 0], grid_size: [1, 1], inputs: [lc.input_tensor.ff.output.dense.bias_s_brcst_m2_0_0.0, ff.output.dense.bias], t: 2, mblock: [1,
        2], ublock: [1, 3], input_buf_min_size_tiles: [2, 12], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: HiFi3, attributes: {
        m_k: 1, u_kt: 1}, input_0_tms: [broadcast: {z: 2}]}
    ff.intermediate.dense.bias_s_brcst_m2_0_0.lc1: {type: matmul, grid_loc: [0, 0], grid_size: [1, 2], inputs: [lc.input_tensor.ff.intermediate.dense.bias_s_brcst_m2_0_0.0, ff.intermediate.dense.bias],
      t: 2, mblock: [1, 12], ublock: [1, 1], input_buf_min_size_tiles: [2, 0], buf_size_mb: 2, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: HiFi3,
      attributes: {m_k: 1, u_kt: 1}, input_0_tms: [broadcast: {z: 2}]}
    ff_0_ff1: {type: matmul, grid_loc: [1, 1], grid_size: [1, 1], inputs: [input_activations, ff.intermediate.dense.weight], t: 2, mblock: [3, 12], ublock: [1, 2], input_buf_min_size_tiles: [18, 432], buf_size_mb: 2,
      ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: HiFi3, attributes: {m_k: 2, u_kt: 3}}
    ff_0_ff1.bias: {type: add, grid_loc: [0, 2], grid_size: [1, 1], inputs: [ff_0_ff1, ff.intermediate.dense.bias_s_brcst_m2_0_0.lc1], t: 2, mblock: [1, 12], ublock: [3, 2], input_buf_min_size_tiles: [
        12, 18], buf_size_mb: 2, ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 3}]}
    ff0_gelu: {type: gelu, grid_loc: [0, 3], grid_size: [1, 1], inputs: [ff_0_ff1.bias], t: 2, mblock: [1, 12], ublock: [3, 2], input_buf_min_size_tiles: [12], buf_size_mb: 2, ublock_order: c, in_df: [
        Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: HiFi3}
    ff_0_ff2: {type: matmul, grid_loc: [0, 4], grid_size: [1, 2], inputs: [ff0_gelu, ff.output.dense.weight], t: 2, mblock: [3, 3], ublock: [1, 1], input_buf_min_size_tiles: [6, 144], buf_size_mb: 2, ublock_order: r,
      in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: HiFi3, attributes: {m_k: 24, u_kt: 1}}
    ff_0_ff2.bias: {type: add, grid_loc: [0, 6], grid_size: [1, 1], inputs: [ff_0_ff2, ff.output.dense.bias_s_brcst_m2_0_0.lc1], untilize_output: false, t: 2, mblock: [3, 3], ublock: [1, 2], input_buf_min_size_tiles: [
        4, 4], buf_size_mb: 1, ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: HiFi3, input_1_tms: [broadcast: {r: 3}]}

programs:
  # forward
- run_fwd:
  - param: [$p_microbatch_count, $p_microbatch_size]
  - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
  - var: {$c_zero: 0, $c_wrap: 2}       # c_wrap = 2 - finally need to be equal to 2*entries == 2*microbatch_size*microbatch_count
  - varinst: [$c_wrap, mul, $c_wrap, $p_microbatch_size]     # c_wrap = 2*microbatch_size
  - varinst: [$c_wrap, mul, $c_wrap, $p_microbatch_count]     # c_wrap = 2*microbatch_size*microbatch_count
  - loop: $p_microbatch_count     #loop over number of microbatches that make a minibatch
  - execute: {graph_name: fwd_0, queue_settings: {lc.input_tensor.ff.output.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.output.dense.bias: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, lc.input_tensor.ff.intermediate.dense.bias_s_brcst_m2_0_0.0: {prologue: true, epilogue: false, zero: false,
          rd_ptr_local: $c_zero, rd_ptr_global: $c_zero}, ff.intermediate.dense.bias: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.intermediate.dense.weight: {
          prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}, ff.output.dense.weight: {prologue: true, epilogue: false, zero: false, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
        input_activations: {prologue: false, epilogue: false, zero: false, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0}}}
  - varinst: [$gptr_q0, incwrap, $p_microbatch_size, $c_wrap]
  - varinst: [$lptr_q0, incwrap, $p_microbatch_size, $c_wrap]
  - endloop

test-config:
  comparison-config:
    type: AllCloseHw
    atol: 0.01
    rtol: 0.15
    check_pcc: 0.99
    check_pct: 0.80
    verbosity: Concise
  stimulus-config:
    type: Uniform
    uniform_lower_bound: -1.0
    uniform_upper_bound: 1.0
    overrides:
      .*\.weight:
        type: Normal
        normal_mean: 0.0
        normal_stddev: 0.01
      .*\.bias:
        type: Constant
        constant_value: 1.0
        set_tile_rows: [1, 1]
      .*\.bias_s_brcst.*:
        type: Constant
        constant_value: 1.0
        set_tile_cols: [1, 1]
  io-config:
    inputs: [input_activations]
    outputs: [output_activations]
  test-args:
    minibatch_count: 2
    microbatch_count: 2
    microbatch_size: 2
