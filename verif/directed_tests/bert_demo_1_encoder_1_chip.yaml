# git checkout 9b802ba2a
# pytest run_squad_wh.py

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                            {input: HOST, type: queue, entries: 512, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: host, host: [[0, 0x0]]}
  attention_mask:                                     {input: HOST, type: queue, entries: 512, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0xd200020], [0, 0xd610040], [0, 0xda20060]]}

  # output
  bert_squad.output_add_57:                           {input: matmul_55_output_nop_0, type: queue, entries: 512, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0xde30080]]}

  # parameter
  encoder.layer.0.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4001b360], [3, 0x9845160]]}
  encoder.layer.0.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x42f11a0]]}
  encoder.layer.0.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x98c30a0], [4, 0x4011d6e0]]}
  encoder.layer.0.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x401649a0]]}
  encoder.layer.0.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x42ab180], [5, 0x401db080], [0, 0x401294e0], [1, 0x41eef00]]}
  encoder.layer.0.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x98bea80], [4, 0x401190c0]]}
  encoder.layer.0.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40efa80], [2, 0x400a7380], [3, 0x98d1180], [3, 0x4011e980]]}
  encoder.layer.0.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41ea8e0], [1, 0x4009b580]]}
  encoder.layer.0.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40000000]]}
  encoder.layer.0.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40df660]]}
  encoder.layer.0.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4008f4c0], [4, 0x982f5c0], [4, 0x4008c860], [5, 0x41bd940], [5, 0x400ed840], [0, 0x4009cc80], [1, 0x415e8c0], [1, 0x4000f560]]}
  encoder.layer.0.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40006940], [2, 0x40d6a40], [2, 0x40012740], [3, 0x983c540]]}
  encoder.layer.0.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x97b0520], [3, 0x400034a0], [4, 0x97a35a0], [4, 0x40000840], [5, 0x4131920], [5, 0x40061820], [0, 0x40010c60], [1, 0x40d28a0]]}
  encoder.layer.0.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40d0580], [1, 0x40004620], [2, 0x40d4720], [2, 0x40010420]]}
  encoder.layer.0.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x97a0100]]}
  encoder.layer.0.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40000000]]}
  lm_head.weight:                                     {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [16, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40000000], [2, 0x40d0100]]}
  lm_head.bias:                                       {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40d0100]]}

  # constant
  input_1_multiply_16:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x99171a0]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x400ed3a0]]}
  dc.input_tensor.softmax_18.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4009fba0], [2, 0x4135aa0]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40128ca0]]}
  dc.input_tensor.layernorm_38.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4249960], [5, 0x40179860]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40118880]]}
  dc.input_tensor.layernorm_38.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x98bb5e0]]}
  dc.input_tensor.layernorm_38.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4011b4e0]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40010420]]}
  dc.input_tensor.layernorm_52.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40d0100], [5, 0x40000000]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40000000]]}
  dc.input_tensor.layernorm_52.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x97a0100]]}
  dc.input_tensor.layernorm_52.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40000000]]}

  # epoch_to_epoch
  e2e__fused_op_8_0:                                  {input: _fused_op_8, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x402210a0], [0, 0x4016f500]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 256
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [input_1, encoder.layer.0.attention.self.query.weight, encoder.layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_8: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [input_1, encoder.layer.0.attention.self.key.weight, encoder.layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_14: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_14, input_1_multiply_16, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_22: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [input_1, encoder.layer.0.attention.self.value.weight, encoder.layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [84, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    buffer_0__fused_op_0__fused_op_2: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_0], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_1: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_18.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_0__fused_op_2, _fused_op_1],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_29: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_2, matmul_22],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_29, encoder.layer.0.attention.output.dense.weight, encoder.layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_37: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_3: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1, add_37], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_3, _fused_op_3], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_4: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_3, _fused_op_4, encoder.layer.0.attention.output.LayerNorm.weight, encoder.layer.0.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_41: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_5, encoder.layer.0.intermediate.dense.weight, encoder.layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_47: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_41, encoder.layer.0.output.dense.weight, encoder.layer.0.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_5_add_51: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_5],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_47, buffer_0__fused_op_5_add_51],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_6: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, add_51],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_6, _fused_op_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_8: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_6, _fused_op_7, encoder.layer.0.output.LayerNorm.weight, encoder.layer.0.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 256
    matmul_55: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e__fused_op_8_0, lm_head.weight, lm_head.bias],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [90, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    matmul_55_output_nop_0: {type: nop, grid_loc: [0, 1], grid_size: [2, 1], inputs: [matmul_55], untilize_output: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0], ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$gptr_q1: 0, $lptr_q1: 0, $c_microbatch_size: 256, $c_one: 1, $c_zero: 0}
    - staticvar: {$gptr_q0: 0, $lptr_q0: 0}
    - loop: $p_loop_count
    -   allocate_queue: [e2e__fused_op_8_0]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               encoder.layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 1024]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               e2e__fused_op_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               lm_head.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lm_head.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_8_0]
    -   varinst: [$gptr_q1, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 512]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16.0: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - add_17.0: { type: add, inputs: [dest, input2], mblock: [2, 1], ublock: [2, 4], output: dest}
        - softmax_18.dc.exp.0.0: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.add.3.0: { type: add, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.reciprocal.4.0: { type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: output}
  2: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.multiply.5.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [3, 3], ublock: [2, 4], output: output}
  3: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.2.0: { type: multiply, inputs: [input0, input1], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.subtract.3.0: { type: subtract, inputs: [input2, dest], mblock: [3, 8], ublock: [2, 4], output: output}
  4: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.7.0: { type: multiply, inputs: [input0, input1], mblock: [6, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.add.9.0: { type: add, inputs: [dest, input2], mblock: [6, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.10.0: { type: sqrt, inputs: [dest], mblock: [6, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.11.0: { type: reciprocal, inputs: [dest], mblock: [6, 1], ublock: [2, 1], output: output}
  5: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.12.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.13.0: { type: multiply, inputs: [dest, input2], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.14.0: { type: add, inputs: [dest, input3], mblock: [3, 8], ublock: [2, 4], output: output}
