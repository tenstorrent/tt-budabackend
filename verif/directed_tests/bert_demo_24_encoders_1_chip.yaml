# git checkout 9b802ba2a
# pytest run_squad_wh.py --num_encoders 24

devices:
  arch: wormhole_b0

queues:

  # input
  input_1:                                             {input: HOST, type: queue, entries: 512, grid_size: [1, 1], t: 1, mblock: [12, 32], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: host, host: [[0, 0x0]]}
  attention_mask:                                      {input: HOST, type: queue, entries: 512, grid_size: [1, 3], t: 1, mblock: [1, 1], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0xd200020], [0, 0xd610040], [0, 0xda20060]]}

  # output
  bert_squad.output_add_1276:                          {input: matmul_1274_output_nop_0, type: queue, entries: 512, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: host, host: [[0, 0xde30080]]}

  # parameter
  encoder.layer.0.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaad97e0], [3, 0x415628e0]]}
  encoder.layer.0.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40e6ba40]]}
  encoder.layer.0.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xab6f240], [4, 0x4131e7a0]]}
  encoder.layer.0.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x598c8c0]]}
  encoder.layer.0.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x418c7a00], [0, 0x4187f7c0], [1, 0x5684780], [1, 0x413aefc0]]}
  encoder.layer.0.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x565c280], [2, 0x41225b40]]}
  encoder.layer.0.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xabcd920], [3, 0x417285c0], [4, 0xb05dc60], [4, 0x418b3480]]}
  encoder.layer.0.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x56c96a0], [5, 0x4190da20]]}
  encoder.layer.0.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb04d840]]}
  encoder.layer.0.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56610e0]]}
  encoder.layer.0.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4122d600], [3, 0xac16de0], [3, 0x417fa600], [4, 0xb12fca0], [4, 0x419854c0], [5, 0x5759ce0], [5, 0x4199e060], [0, 0x41952040]]}
  encoder.layer.0.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x57b7fe0], [1, 0x414e2820], [2, 0x5671500], [2, 0x412b9620]]}
  encoder.layer.0.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaca2e00], [3, 0x41886620], [4, 0xb1bbcc0], [4, 0x41a114e0], [5, 0x57e5d00], [5, 0x41a2a080], [0, 0x419de060], [1, 0x57c0c00]]}
  encoder.layer.0.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x414eb440], [2, 0x567a120], [2, 0x412c2240], [3, 0xad2ee20]]}
  encoder.layer.0.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41683560]]}
  encoder.layer.0.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x417af980]]}
  encoder.layer.1.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x416bff40], [1, 0x5525ee0]]}
  encoder.layer.1.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x412d0ec0]]}
  encoder.layer.1.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x557e180], [2, 0x4117c4c0]]}
  encoder.layer.1.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4167a940]]}
  encoder.layer.1.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4174bf60], [1, 0x55b1f00], [1, 0x412d9ae0], [2, 0x560a1a0]]}
  encoder.layer.1.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x412084e0], [3, 0xabb02c0]]}
  encoder.layer.1.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41634920], [4, 0xaf769a0], [4, 0x417c2580], [5, 0x55c25e0]]}
  encoder.layer.1.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xafbd200], [4, 0x41808a20]]}
  encoder.layer.1.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4120cb00]]}
  encoder.layer.1.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xabb48e0]]}
  encoder.layer.1.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41693980], [4, 0xafc1820], [4, 0x4180d040], [5, 0x5623260], [5, 0x4183b9e0], [0, 0x417f37a0], [1, 0x55f8760], [1, 0x41322fa0]]}
  encoder.layer.1.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5653660], [2, 0x4121cf20], [3, 0xabc4d00], [3, 0x4171f9a0]]}
  encoder.layer.1.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4176e5e0], [4, 0xb0a3c80], [4, 0x418f94a0], [5, 0x56cdcc0], [5, 0x41912040], [0, 0x418c6020], [1, 0x572bfc0], [1, 0x41456800]]}
  encoder.layer.1.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x413e2ee0], [3, 0xadd9640], [3, 0x4193e600], [4, 0xb350880]]}
  encoder.layer.1.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x57989a0]]}
  encoder.layer.1.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x413e5200]]}
  encoder.layer.2.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaddb960], [3, 0x41940920]]}
  encoder.layer.2.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb352ba0]]}
  encoder.layer.2.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x59f6d00], [5, 0x41c3c200]]}
  encoder.layer.2.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41c735e0]]}
  encoder.layer.2.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xae67980], [3, 0x419cc940], [4, 0xb35b7c0], [4, 0x41c2b100]]}
  encoder.layer.2.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5a82d20], [5, 0x41cc8220]]}
  encoder.layer.2.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41c7c200], [1, 0x5ab79a0], [1, 0x41673d40], [2, 0x57c31e0]]}
  encoder.layer.2.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4140fa40], [3, 0xaead9a0]]}
  encoder.layer.2.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41912e80]]}
  encoder.layer.2.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb2a9500]]}
  encoder.layer.2.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41afed20], [5, 0x5872560], [5, 0x41ab9540], [0, 0x41a6d520], [1, 0x58ae440], [1, 0x4154ef80], [2, 0x567cc80], [2, 0x412c7a00]]}
  encoder.layer.2.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xad345e0], [3, 0x419232a0], [4, 0xb2b9920], [4, 0x41b8ad40]]}
  encoder.layer.2.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x58fe580], [5, 0x41b45560], [0, 0x41af9540], [1, 0x593a460], [1, 0x415dafa0], [2, 0x5708ca0], [2, 0x41353a20], [3, 0xad3d200]]}
  encoder.layer.2.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4192bec0], [4, 0xb2c2540], [4, 0x41b93960], [5, 0x598a5a0]]}
  encoder.layer.2.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xadc9220]]}
  encoder.layer.2.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4192e1e0]]}
  encoder.layer.3.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb2c4860], [4, 0x41b95c80]]}
  encoder.layer.3.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41bd1dc0]]}
  encoder.layer.3.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41be6d80], [1, 0x5a27ca0]]}
  encoder.layer.3.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41667800]]}
  encoder.layer.3.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41352ee0], [4, 0xaca5cc0], [4, 0x4147d040], [5, 0x5265ea0]]}
  encoder.layer.3.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41487cc0], [0, 0x41340800]]}
  encoder.layer.3.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5142980], [1, 0x40eecba0], [2, 0x5114bc0], [2, 0x40e83e20]]}
  encoder.layer.3.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa8b1a80], [3, 0x41398f00]]}
  encoder.layer.3.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa8b60a0]]}
  encoder.layer.3.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4139d520]]}
  encoder.layer.3.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xacec520], [4, 0x41524880], [5, 0x530d6e0], [5, 0x4148cb20], [0, 0x413482c0], [1, 0x51915c0], [1, 0x40f3b7e0], [2, 0x5163800]]}
  encoder.layer.3.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40ed2a60], [3, 0xa8c64c0], [3, 0x413ad940], [4, 0xad78540]]}
  encoder.layer.3.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x415b08a0], [5, 0x5399700], [5, 0x41518b40], [0, 0x413d42e0], [1, 0x521d5e0], [1, 0x40fc7800], [2, 0x51ef820], [2, 0x40edb680]]}
  encoder.layer.3.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa8cf0e0], [3, 0x413b6560], [4, 0xad81160], [4, 0x4163c8c0]]}
  encoder.layer.3.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa7b4d80]]}
  encoder.layer.3.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41298060]]}
  encoder.layer.4.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x50a6620], [5, 0x412c8440]]}
  encoder.layer.4.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41265360]]}
  encoder.layer.4.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5024000], [1, 0x40e14220]]}
  encoder.layer.4.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x50b56e0]]}
  encoder.layer.4.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x413aa7c0], [5, 0x5132640], [5, 0x41354460], [0, 0x4126df80]]}
  encoder.layer.4.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x50b10c0], [2, 0x40e0e760]]}
  encoder.layer.4.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40ea0240], [2, 0x50be300], [2, 0x40e135c0], [3, 0xa7c5620]]}
  encoder.layer.4.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x412c28a0], [4, 0xac15680]]}
  encoder.layer.4.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5104320]]}
  encoder.layer.4.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40e595e0]]}
  encoder.layer.4.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa80b640], [3, 0x412c6ec0], [4, 0xac19ca0], [4, 0x413f1020], [5, 0x51d9e80], [5, 0x413fbca0], [0, 0x412b47e0], [1, 0x50b6960]]}
  encoder.layer.4.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x51889a0], [1, 0x40f32bc0], [2, 0x515abe0], [2, 0x40ec9e40]]}
  encoder.layer.4.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xaea1e00], [4, 0x416e70e0], [5, 0x5444760], [5, 0x41631b00], [0, 0x415e4aa0], [1, 0x54501c0], [1, 0x411fb560], [2, 0x5481900]]}
  encoder.layer.4.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4114b340], [3, 0xaad74c0], [3, 0x415605c0], [4, 0xaf2de20]]}
  encoder.layer.4.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x550d920]]}
  encoder.layer.4.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4114d660]]}
  encoder.layer.5.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaa4b4a0], [3, 0x414d45a0]]}
  encoder.layer.5.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41773940]]}
  encoder.layer.5.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5531fa0], [5, 0x4171f340]]}
  encoder.layer.5.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41671300]]}
  encoder.layer.5.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xab65800], [3, 0x415ee900], [4, 0xaf30980], [4, 0x4177c560]]}
  encoder.layer.5.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x55bdfc0], [5, 0x417ab360]]}
  encoder.layer.5.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41679f20], [1, 0x54dfec0], [1, 0x4128aea0], [2, 0x5538160]]}
  encoder.layer.5.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41177ea0], [3, 0xabab820]]}
  encoder.layer.5.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5425f60]]}
  encoder.layer.5.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41606380]]}
  encoder.layer.5.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x414c1b20], [1, 0x52a9e40], [1, 0x41056cc0], [2, 0x527c080], [2, 0x40fc8ec0], [3, 0xa932c20], [3, 0x413b90c0], [4, 0xad86920]]}
  encoder.layer.5.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41642080], [5, 0x5436380], [5, 0x416167a0], [0, 0x4154db40]]}
  encoder.layer.5.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5335e60], [1, 0x410e2ce0], [2, 0x53080a0], [2, 0x41054ee0], [3, 0xa9bec40], [3, 0x414450e0], [4, 0xae12940], [4, 0x4164aca0]]}
  encoder.layer.5.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x543efa0], [5, 0x4161f3c0], [0, 0x41556760], [1, 0x53c1e80]]}
  encoder.layer.5.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x416d6cc0]]}
  encoder.layer.5.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x416216e0]]}
  encoder.layer.6.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41558a80], [1, 0x53c41a0]]}
  encoder.layer.6.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41c224e0]]}
  encoder.layer.6.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4116f540], [2, 0x53f58e0]]}
  encoder.layer.6.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41142720]]}
  encoder.layer.6.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x420c4f80], [5, 0x5e5d700], [5, 0x4220b100], [0, 0x4230b480]]}
  encoder.layer.6.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x619d2a0], [1, 0x41d31960]]}
  encoder.layer.6.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5e5da40], [2, 0x418ac520], [3, 0xb201880], [3, 0x41e8b860]]}
  encoder.layer.6.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb8f3a80], [4, 0x4210afa0]]}
  encoder.layer.6.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x418f2540]]}
  encoder.layer.6.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb2478a0]]}
  encoder.layer.6.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41ed1880], [4, 0xb8f80a0], [4, 0x4210f5c0], [5, 0x5ea3f60], [5, 0x422b2940], [0, 0x423b2cc0], [1, 0x61a2100], [1, 0x41d39420]]}
  encoder.layer.6.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5ea6f00], [2, 0x41902960], [3, 0xb257cc0], [3, 0x41f5d8a0]]}
  encoder.layer.6.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb9840c0], [4, 0x4219b5e0], [5, 0x5f2ff80], [5, 0x4233e960], [0, 0x4243ece0], [1, 0x622e120], [1, 0x41dc5440], [2, 0x5eafb20]]}
  encoder.layer.6.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4190b580], [3, 0xb2608e0], [3, 0x41f664c0], [4, 0xba100e0]]}
  encoder.layer.6.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6085220]]}
  encoder.layer.6.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41c30860]]}
  encoder.layer.7.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5d40ba0], [2, 0x418155a0]]}
  encoder.layer.7.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb1a23a0]]}
  encoder.layer.7.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41d8efe0], [4, 0xb77b600]]}
  encoder.layer.7.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41fea320]]}
  encoder.layer.7.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb7355e0], [4, 0x41fa4300], [5, 0x5d86860], [5, 0x42134620]]}
  encoder.layer.7.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5dccbc0], [2, 0x418a15c0]]}
  encoder.layer.7.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb1aafc0], [3, 0x41e1b000], [4, 0xb807620], [4, 0x41ff2f40]]}
  encoder.layer.7.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5dcd0c0], [5, 0x4217aac0]]}
  encoder.layer.7.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb1f0fe0]]}
  encoder.layer.7.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41e61020]]}
  encoder.layer.7.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb84d640], [4, 0x42038f60], [5, 0x5dd16e0], [5, 0x4217f0e0], [0, 0x4227f460], [1, 0x6111280], [1, 0x41ca5940], [2, 0x5dd1a20]]}
  encoder.layer.7.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbbfccc0], [4, 0x42367760], [5, 0x60e0360], [5, 0x4244c360]]}
  encoder.layer.7.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x424cfb60], [1, 0x6303600], [1, 0x41e97cc0], [2, 0x5f81fe0], [2, 0x4196dce0], [3, 0xb281640], [3, 0x41fb2e20], [4, 0xba58c60]]}
  encoder.layer.7.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x425f2ae0], [1, 0x64a9980], [1, 0x4203c560], [2, 0x6187860]]}
  encoder.layer.7.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42454f80]]}
  encoder.layer.7.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x425f4e00]]}
  encoder.layer.8.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x64abca0], [1, 0x4203e880]]}
  encoder.layer.8.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb487700]]}
  encoder.layer.8.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6189b80], [2, 0x41b73da0]]}
  encoder.layer.8.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x42156e80]]}
  encoder.layer.8.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41f687e0], [4, 0xba12400], [4, 0x42227e40], [5, 0x601d7c0]]}
  encoder.layer.8.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4242c1a0], [0, 0x424cb540]]}
  encoder.layer.8.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x62bd5e0], [1, 0x41e51ca0], [2, 0x5f3bfc0], [2, 0x41927cc0]]}
  encoder.layer.8.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb27d020], [3, 0x41fae800]]}
  encoder.layer.8.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x60c5000]]}
  encoder.layer.8.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42431000]]}
  encoder.layer.8.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4255bb80], [1, 0x638f620], [1, 0x41f23ce0], [2, 0x600e000], [2, 0x419f9d00], [3, 0xb30d660], [3, 0x4203ee40], [4, 0xbae4c80]]}
  encoder.layer.8.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x422d2b20], [5, 0x60d5420], [5, 0x42441420], [0, 0x425e7ba0]]}
  encoder.layer.8.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x641b640], [1, 0x41fafd00], [2, 0x609a020], [2, 0x41a85d20], [3, 0xb399680], [3, 0x420cae60], [4, 0xbb70ca0], [4, 0x422db740]]}
  encoder.layer.8.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60de040], [5, 0x4244a040], [0, 0x425f07c0], [1, 0x64a7660]]}
  encoder.layer.8.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5bf02e0]]}
  encoder.layer.8.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x417fc560]]}
  encoder.layer.9.attention.self.query.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41ecc0c0], [1, 0x5cc5e60]]}
  encoder.layer.9.attention.self.query.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41818e40]]}
  encoder.layer.9.attention.self.key.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x596ab80], [2, 0x4158f980]]}
  encoder.layer.9.attention.self.key.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaf8b860]]}
  encoder.layer.9.attention.self.value.weight:         {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41e75f40], [0, 0x41f580e0], [1, 0x5d51e80], [1, 0x41821a60]]}
  encoder.layer.9.attention.self.value.bias:           {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x59f6ba0], [2, 0x4161b9a0]]}
  encoder.layer.9.attention.output.dense.weight:       {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaf94480], [3, 0x41a8f180], [4, 0xb4924a0], [4, 0x41de1b80]]}
  encoder.layer.9.attention.output.dense.bias:         {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c04d20], [5, 0x41ebbf60]]}
  encoder.layer.9.attention.output.LayerNorm.weight:   {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41ad51a0]]}
  encoder.layer.9.attention.output.LayerNorm.bias:     {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41a1e0e0]]}
  encoder.layer.9.intermediate.dense.weight:           {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41c71120], [5, 0x5a87340], [5, 0x41ccc840], [0, 0x41cc2220], [1, 0x5afd9c0], [1, 0x416b9d60], [2, 0x5809200], [2, 0x41414060]]}
  encoder.layer.9.intermediate.dense.bias:             {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaeb1fc0], [3, 0x41a131a0], [4, 0xb3b1c00], [4, 0x41cfd140]]}
  encoder.layer.9.output.dense.weight:                 {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5b13360], [5, 0x41d58860], [0, 0x41d4e240], [1, 0x5b899e0], [1, 0x41745d80], [2, 0x5895220], [2, 0x414a0080], [3, 0xaebabe0]]}
  encoder.layer.9.output.dense.bias:                   {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41a1bdc0], [4, 0xb3ba820], [4, 0x41d05d60], [5, 0x5b9f380]]}
  encoder.layer.9.output.LayerNorm.weight:             {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaf46c00]]}
  encoder.layer.9.output.LayerNorm.bias:               {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb3a17e0]]}
  encoder.layer.10.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb3bcb40], [4, 0x41d08080]]}
  encoder.layer.10.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5ba16a0]]}
  encoder.layer.10.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41de50c0], [0, 0x41e3ba80]]}
  encoder.layer.10.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5c77220]]}
  encoder.layer.10.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41a2e500], [4, 0xb448b60], [4, 0x41d940a0], [5, 0x5baa2c0]]}
  encoder.layer.10.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41e710e0], [0, 0x41ec7aa0]]}
  encoder.layer.10.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5c7fe40], [1, 0x417d2e20], [2, 0x5924b60], [2, 0x41549960]]}
  encoder.layer.10.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41ddd560], [5, 0x5c00700]]}
  encoder.layer.10.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x417e3520]]}
  encoder.layer.10.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb0fcb00]]}
  encoder.layer.10.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41c0cb60], [4, 0xb5bbd80], [4, 0x41e8ba80], [5, 0x5c6b380], [5, 0x42019140], [0, 0x42138f00], [1, 0x5f5cdc0], [1, 0x41a8c800]]}
  encoder.layer.10.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5c1ff40], [2, 0x417f3940], [3, 0xb10cf20], [3, 0x41c98b80]]}
  encoder.layer.10.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb647da0], [4, 0x41f17aa0], [5, 0x5cf73a0], [5, 0x420a5160], [0, 0x421c4f20], [1, 0x5fe8de0], [1, 0x41b18820], [2, 0x5c28b60]]}
  encoder.layer.10.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb0fa7e0], [3, 0x41c0a840], [4, 0xb5b9a60], [4, 0x41e89760]]}
  encoder.layer.10.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x42250f40]]}
  encoder.layer.10.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x6074e00]]}
  encoder.layer.11.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41ba4840], [2, 0x5cb4b80]]}
  encoder.layer.11.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4180c980]]}
  encoder.layer.11.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb116380], [3, 0x41d02fc0]]}
  encoder.layer.11.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41ae9be0]]}
  encoder.layer.11.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5df96c0], [1, 0x418c92a0], [2, 0x59fba00], [2, 0x41623460]]}
  encoder.layer.11.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xafdd940], [3, 0x41ae55c0]]}
  encoder.layer.11.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb4db960], [4, 0x41e283e0], [5, 0x5c097c0], [5, 0x41eda9a0]]}
  encoder.layer.11.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41fb8d60], [1, 0x5e3f6e0]]}
  encoder.layer.11.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41e6e400]]}
  encoder.layer.11.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c4f7e0]]}
  encoder.layer.11.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41f209c0], [0, 0x41fbd380], [1, 0x5e43d00], [1, 0x4190fb00], [2, 0x5aa3240], [2, 0x416caca0], [3, 0xafe27a0], [3, 0x41af2800]]}
  encoder.layer.11.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb524e20], [4, 0x41e7e820], [5, 0x5c5fc00], [5, 0x41fac9e0]]}
  encoder.layer.11.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x420493a0], [1, 0x5ecfd20], [1, 0x4199bb20], [2, 0x5b2f260], [2, 0x41756cc0], [3, 0xb06e7c0], [3, 0x41b7e820], [4, 0xb52da40]]}
  encoder.layer.11.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41e87440], [5, 0x5c68820], [5, 0x41fb5600], [0, 0x420d53c0]]}
  encoder.layer.11.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9d33120]]}
  encoder.layer.11.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40652100]]}
  encoder.layer.12.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9e2d3a0], [4, 0x4073a860]]}
  encoder.layer.12.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x467cf80]]}
  encoder.layer.12.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x405bb3e0], [0, 0x40598180]]}
  encoder.layer.12.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x469de80]]}
  encoder.layer.12.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40552160], [1, 0x4657e60], [1, 0x40437be0], [2, 0x4549320]]}
  encoder.layer.12.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9eb93c0], [4, 0x407c6880]]}
  encoder.layer.12.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4685ba0], [5, 0x40647400], [0, 0x406241a0], [1, 0x46a6aa0]]}
  encoder.layer.12.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4047e440], [2, 0x458f7c0]]}
  encoder.layer.12.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4068d420]]}
  encoder.layer.12.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4066a1c0]]}
  encoder.layer.12.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9c584c0], [3, 0x4057f880], [4, 0x9d5aee0], [4, 0x4064e400], [5, 0x45d2520], [5, 0x404e4d80], [0, 0x40480120], [1, 0x4585e20]]}
  encoder.layer.12.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4413c80], [2, 0x403c7ca0], [3, 0x9c4d580], [3, 0x40574940]]}
  encoder.layer.12.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9cccba0], [4, 0x405c1ba0], [5, 0x44e4ce0], [5, 0x403f7540], [0, 0x403f38c0], [1, 0x44f6960], [1, 0x40359ae0], [2, 0x441c8a0]]}
  encoder.layer.12.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x403d08c0], [3, 0x9c561a0], [3, 0x4057d560], [4, 0x9d58bc0]]}
  encoder.layer.12.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x44a88c0]]}
  encoder.layer.12.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x403d2be0]]}
  encoder.layer.13.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x446a940], [1, 0x402cdac0]]}
  encoder.layer.13.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x403e8fa0]]}
  encoder.layer.13.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x44b8ce0], [2, 0x403e3000]]}
  encoder.layer.13.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9ce44e0]]}
  encoder.layer.13.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40570da0], [0, 0x4050c140], [1, 0x4611e40], [1, 0x403f1bc0]]}
  encoder.layer.13.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4544d00], [2, 0x4046f020]]}
  encoder.layer.13.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9ced100], [3, 0x4060c0e0], [4, 0x9de7380], [4, 0x406f4840]]}
  encoder.layer.13.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4678960], [5, 0x405b6dc0]]}
  encoder.layer.13.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9f22040]]}
  encoder.layer.13.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40884640]]}
  encoder.layer.13.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa0df2c0], [4, 0x4098e400], [5, 0x47ef3a0], [5, 0x407bdb80], [0, 0x407b72a0], [1, 0x47b2b00], [1, 0x404e4aa0], [2, 0x468e620]]}
  encoder.layer.13.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x405cd980], [3, 0x9f32460], [3, 0x40894a60], [4, 0xa16b2e0]]}
  encoder.layer.13.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40a1a420], [5, 0x487b3c0], [5, 0x40849ba0], [0, 0x408432c0], [1, 0x483eb20], [1, 0x40570ac0], [2, 0x471a640], [2, 0x405d65a0]]}
  encoder.layer.13.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9f3b080], [3, 0x4089d680], [4, 0xa173f00], [4, 0x40aa6440]]}
  encoder.layer.13.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9f3d3a0]]}
  encoder.layer.13.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4089f9a0]]}
  encoder.layer.14.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa176220], [4, 0x40aa8760]]}
  encoder.layer.14.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4993400]]}
  encoder.layer.14.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40961be0], [0, 0x40930b00]]}
  encoder.layer.14.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x492c360]]}
  encoder.layer.14.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9dbf180], [3, 0x406c4580], [4, 0x9ebe220], [4, 0x407ce340]]}
  encoder.layer.14.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x46cf060], [5, 0x4069d840]]}
  encoder.layer.14.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4067a5e0], [1, 0x46eff60], [1, 0x40482ee0], [2, 0x45ae200]]}
  encoder.layer.14.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x404abb60], [3, 0x9e051a0]]}
  encoder.layer.14.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4735f80]]}
  encoder.layer.14.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404c8f00]]}
  encoder.layer.14.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x45f4220], [2, 0x404b0180], [3, 0x9e097c0], [3, 0x4070ade0], [4, 0x9f65a60], [4, 0x40875b80], [5, 0x46d3ec0], [5, 0x406a5300]]}
  encoder.layer.14.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x406c0e40], [1, 0x47463a0], [1, 0x404d9320], [2, 0x4680240]]}
  encoder.layer.14.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4053c1a0], [3, 0x9e957e0], [3, 0x40796e00], [4, 0x9ff1a80], [4, 0x40901ba0], [5, 0x475fee0], [5, 0x40731320], [0, 0x406c9a60]]}
  encoder.layer.14.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x474efc0], [1, 0x404e1f40], [2, 0x4688e60], [2, 0x405c81c0]]}
  encoder.layer.14.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x97f14e0]]}
  encoder.layer.14.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x400ecca0]]}
  encoder.layer.15.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x98ac380], [4, 0x40137e80]]}
  encoder.layer.15.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41a31c0]]}
  encoder.layer.15.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40136d00], [0, 0x401c2d20]]}
  encoder.layer.15.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4242ba0]]}
  encoder.layer.15.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x400fd0c0], [4, 0x99383a0], [4, 0x401c3ea0], [5, 0x41abde0]]}
  encoder.layer.15.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x401c2d20], [0, 0x4024ed40]]}
  encoder.layer.15.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41fcb80], [1, 0x400a2d80], [2, 0x41ad080], [2, 0x400d4ba0]]}
  encoder.layer.15.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x400e95e0], [2, 0x41f3520]]}
  encoder.layer.15.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x401c7340]]}
  encoder.layer.15.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40253360]]}
  encoder.layer.15.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x424c000], [1, 0x400edc00], [2, 0x41f7b40], [2, 0x40135820], [3, 0x987d540], [3, 0x401a4900], [4, 0x997ec00], [4, 0x4020d360]]}
  encoder.layer.15.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41f52a0], [5, 0x401d7760], [0, 0x40263780], [1, 0x42d8020]]}
  encoder.layer.15.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40000840], [4, 0x9801920], [4, 0x40061820], [5, 0x40d0940], [5, 0x400034a0], [0, 0x4008f4c0], [1, 0x416c540], [1, 0x40012740]]}
  encoder.layer.15.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40000000], [2, 0x40d0100], [2, 0x40000000], [3, 0x97a0100]]}
  encoder.layer.15.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x415c120]]}
  encoder.layer.15.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40002320]]}
  encoder.layer.16.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40d2420], [2, 0x40002320]]}
  encoder.layer.16.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x97a2420]]}
  encoder.layer.16.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40000000], [1, 0x40d0100]]}
  encoder.layer.16.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x415e440]]}
  encoder.layer.16.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x400ed840], [5, 0x415c960], [5, 0x4008f4c0], [0, 0x4011b4e0]]}
  encoder.layer.16.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41f8560], [1, 0x4009e760]]}
  encoder.layer.16.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4167060], [2, 0x4008eb80], [3, 0x97ab4c0], [3, 0x400a6c80]]}
  encoder.layer.16.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x98a7d60], [4, 0x40133860]]}
  encoder.layer.16.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x43e8500]]}
  encoder.layer.16.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40320920]]}
  encoder.layer.16.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x9b2a600], [3, 0x404519c0], [4, 0x9b28300], [4, 0x403bc320], [5, 0x4362860], [5, 0x402db820], [0, 0x402d7f60], [1, 0x434f460]]}
  encoder.layer.16.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40228a60], [2, 0x43f8920], [2, 0x40330d40], [3, 0x9bb6620]]}
  encoder.layer.16.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x404dd9e0], [4, 0x9bb4320], [4, 0x40448340], [5, 0x43ee880], [5, 0x40367840], [0, 0x40363f80], [1, 0x43db480], [1, 0x40231680]]}
  encoder.layer.16.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4401540], [2, 0x40339960], [3, 0x9bbf240], [3, 0x40569a00]]}
  encoder.layer.16.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x402bd6a0]]}
  encoder.layer.16.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4403860]]}
  encoder.layer.17.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4033bc80], [3, 0x9bc1560]]}
  encoder.layer.17.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4056bd20]]}
  encoder.layer.17.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9c40b80], [4, 0x40535b80]]}
  encoder.layer.17.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x44dc0c0]]}
  encoder.layer.17.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4283b60], [2, 0x401c1840], [3, 0x9909560], [3, 0x40230920]]}
  encoder.layer.17.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9a0ac20], [4, 0x40299380]]}
  encoder.layer.17.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41fdec0], [5, 0x401e0380], [0, 0x4026c3a0], [1, 0x42e0c40]]}
  encoder.layer.17.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4018a040], [2, 0x42c9b80]]}
  encoder.layer.17.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x402263a0]]}
  encoder.layer.17.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40179c20]]}
  encoder.layer.17.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4018e660], [2, 0x42ce1a0], [2, 0x402080a0], [3, 0x99b0da0], [3, 0x402d8160], [4, 0x9a0fa80], [4, 0x402a0e40], [5, 0x4247380]]}
  encoder.layer.17.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x402367c0], [0, 0x402cc7e0], [1, 0x4341080], [1, 0x4021a680]]}
  encoder.layer.17.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x435a1c0], [2, 0x402940c0], [3, 0x9a3cdc0], [3, 0x40364180], [4, 0x9a9baa0], [4, 0x4032ce60], [5, 0x42d33a0], [5, 0x4023f3e0]]}
  encoder.layer.17.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x402d5400], [1, 0x4349ca0], [1, 0x402232a0], [2, 0x43e61e0]]}
  encoder.layer.17.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x402cb400]]}
  encoder.layer.17.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41254ac0]]}
  encoder.layer.18.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x49073e0], [5, 0x408d5bc0]]}
  encoder.layer.18.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4d74000]]}
  encoder.layer.18.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40b933e0], [2, 0x4e85a20]]}
  encoder.layer.18.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40b90aa0]]}
  encoder.layer.18.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4c63bc0], [5, 0x40e5d260], [0, 0x40e74660], [1, 0x4d7cc20]]}
  encoder.layer.18.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40c1f400], [2, 0x4f11a40]]}
  encoder.layer.18.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40b996c0], [3, 0xa463100], [3, 0x40e94be0], [4, 0xa709a80]]}
  encoder.layer.18.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4c5f5a0], [5, 0x40e58c40]]}
  encoder.layer.18.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40bdf6e0]]}
  encoder.layer.18.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa4a9120]]}
  encoder.layer.18.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40edac00], [4, 0xa74faa0], [4, 0x40e9b3c0], [5, 0x4caa420], [5, 0x40f04aa0], [0, 0x40f1bea0], [1, 0x4dc3480], [1, 0x40c26ec0]]}
  encoder.layer.18.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4f19500], [2, 0x40befb00], [3, 0xa4b9540], [3, 0x40f66c20]]}
  encoder.layer.18.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa7dbac0], [4, 0x40f273e0], [5, 0x4d36440], [5, 0x40f90ac0], [0, 0x40fa7ec0], [1, 0x4e4f4a0], [1, 0x40cb2ee0], [2, 0x4f22120]]}
  encoder.layer.18.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa645b00], [4, 0x40e76980], [5, 0x4c58c60], [5, 0x40e10900]]}
  encoder.layer.18.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40e14dc0]]}
  encoder.layer.18.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4d1d740]]}
  encoder.layer.19.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40aa6f80], [2, 0x4ddafc0]]}
  encoder.layer.19.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40b3d840]]}
  encoder.layer.19.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa390040], [3, 0x40d60f00]]}
  encoder.layer.19.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40b34c20]]}
  encoder.layer.19.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40b46460], [3, 0xa41c060], [3, 0x40decf20], [4, 0xa647e20]]}
  encoder.layer.19.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40e78ca0], [5, 0x4c5af80]]}
  encoder.layer.19.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40e12c20], [0, 0x40e25a20], [1, 0x4d2dfe0], [1, 0x40b4d3c0]]}
  encoder.layer.19.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4e81400], [2, 0x40b8c480]]}
  encoder.layer.19.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5085940]]}
  encoder.layer.19.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40d673e0]]}
  encoder.layer.19.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa691e00], [3, 0x411750e0], [4, 0xa9ca9a0], [4, 0x41118f20], [5, 0x4f241a0], [5, 0x411af380], [0, 0x4113ca80], [1, 0x4eee700]]}
  encoder.layer.19.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40d54da0], [2, 0x5095d60], [2, 0x40d77800], [3, 0xa71de20]]}
  encoder.layer.19.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41201100], [4, 0xaa569c0], [4, 0x411a4f40], [5, 0x4fb01c0], [5, 0x4123b3a0], [0, 0x411c8aa0], [1, 0x4f7a720], [1, 0x40d5d9c0]]}
  encoder.layer.19.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x509e980], [2, 0x40d80420], [3, 0xa726a40], [3, 0x4128d120]]}
  encoder.layer.19.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x40de99e0]]}
  encoder.layer.19.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x50a0ca0]]}
  encoder.layer.20.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40d82740], [3, 0xa728d60]]}
  encoder.layer.20.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4128f440]]}
  encoder.layer.20.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xaae3220], [4, 0x41292780]]}
  encoder.layer.20.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x509da00]]}
  encoder.layer.20.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41041880], [4, 0xa8f8120], [4, 0x41043a40], [5, 0x4e4ecc0]]}
  encoder.layer.20.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4edb4c0], [1, 0x40d3ef00]]}
  encoder.layer.20.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4fae140], [2, 0x40c84740], [3, 0xa54e180], [3, 0x40ffb860]]}
  encoder.layer.20.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa8f3b00], [4, 0x4103f420]]}
  encoder.layer.20.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40cca760]]}
  encoder.layer.20.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa5941a0]]}
  encoder.layer.20.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40bf8720], [3, 0xa4c2160], [3, 0x40f6f840], [4, 0xa867ae0], [4, 0x40fb3400], [5, 0x4dc2460], [5, 0x4101cae0], [0, 0x41033ee0]]}
  encoder.layer.20.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4110a320], [0, 0x41121720], [1, 0x4ee0320], [1, 0x40d469c0]]}
  encoder.layer.20.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4ff7600], [2, 0x40cdab80], [3, 0xa5a45c0], [3, 0x410878a0], [4, 0xa93e140], [4, 0x41089a60], [5, 0x4e94ce0], [5, 0x41112f40]]}
  encoder.layer.20.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4112a340], [1, 0x4ee8f40], [1, 0x40d4f5e0], [2, 0x5083620]]}
  encoder.layer.20.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4119ef60]]}
  encoder.layer.20.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4112c660]]}
  encoder.layer.21.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa53dea0], [4, 0x40dcfd00]]}
  encoder.layer.21.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x408f9ac0]]}
  encoder.layer.21.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa265b20], [4, 0x40b635e0]]}
  encoder.layer.21.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x49f6a80]]}
  encoder.layer.21.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x489ee00], [2, 0x40719360], [3, 0x9f950a0], [3, 0x409026e0]]}
  encoder.layer.21.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa2f1b40], [4, 0x40bef600]]}
  encoder.layer.21.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x49ff6a0], [5, 0x40ac0cc0], [0, 0x40af0800], [1, 0x49ff800]]}
  encoder.layer.21.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x406a8a40], [2, 0x48e4e20]]}
  encoder.layer.21.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40b06ce0]]}
  encoder.layer.21.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40b36820]]}
  encoder.layer.21.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4a45820], [1, 0x406ad060], [2, 0x48e9440], [2, 0x4076f7a0], [3, 0xa03c8e0], [3, 0x409a9f20], [4, 0xa2f69a0], [4, 0x40bf70c0]]}
  encoder.layer.21.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4a48b60], [5, 0x40b17100], [0, 0x40b46c40], [1, 0x4ad1840]]}
  encoder.layer.21.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40739080], [2, 0x4975460], [2, 0x407fb7c0], [3, 0xa0c8900], [3, 0x40a35f40], [4, 0xa3829c0], [4, 0x40c830e0], [5, 0x4a51780]]}
  encoder.layer.21.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40b1fd20], [0, 0x40b4f860], [1, 0x4ada460], [1, 0x407c50a0]]}
  encoder.layer.21.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40b34780]]}
  encoder.layer.21.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x499c020]]}
  encoder.layer.22.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x409edc00], [0, 0x409bcb20]]}
  encoder.layer.22.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4934f80]]}
  encoder.layer.22.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x405fd7a0], [2, 0x480b320]]}
  encoder.layer.22.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x406c7280]]}
  encoder.layer.22.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x49ac440], [5, 0x40a79c20], [0, 0x40a48b40], [1, 0x493dba0]]}
  encoder.layer.22.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x406897c0], [2, 0x4897340]]}
  encoder.layer.22.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x406cfea0], [3, 0x9f4e840], [3, 0x408b3aa0], [4, 0xa21fb00]]}
  encoder.layer.22.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40b5efc0], [5, 0x49f2460]]}
  encoder.layer.22.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4075f380]]}
  encoder.layer.22.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4be4020]]}
  encoder.layer.22.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40ca44c0], [0, 0x40c28a20], [1, 0x4bb6280], [1, 0x4089d0e0], [2, 0x4bd1120], [2, 0x40a13780], [3, 0xa273ae0], [3, 0x40be3d80]]}
  encoder.layer.22.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa4a9260], [4, 0x40dc4dc0], [5, 0x4bf4440], [5, 0x40d304e0]]}
  encoder.layer.22.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40cb4a40], [1, 0x4c422a0], [1, 0x40929100], [2, 0x4c5d140], [2, 0x40a9f7a0], [3, 0xa2ffb00], [3, 0x40c6fda0], [4, 0xa4b1e80]]}
  encoder.layer.22.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40dcd9e0], [5, 0x4bfd060], [5, 0x40d39100], [0, 0x40d40a60]]}
  encoder.layer.22.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40db49a0]]}
  encoder.layer.22.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4bff380]]}
  encoder.layer.23.attention.self.query.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40d3b420], [0, 0x40d42d80]]}
  encoder.layer.23.attention.self.query.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4cceb00]]}
  encoder.layer.23.attention.self.key.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40a16940], [2, 0x4d4a980]]}
  encoder.layer.23.attention.self.key.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40b2c000]]}
  encoder.layer.23.attention.self.value.weight:        {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [4, 4], ublock: [4, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4c0f7a0], [5, 0x40dc7440], [0, 0x40dceda0], [1, 0x4cd7720]]}
  encoder.layer.23.attention.self.value.bias:          {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40aa2960], [2, 0x4dd69a0]]}
  encoder.layer.23.attention.output.dense.weight:      {input: HOST, type: ram, entries: 1, grid_size: [2, 2], t: 1, mblock: [8, 4], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40be21c0], [1, 0x4b6cdc0], [1, 0x40853c20], [2, 0x4b7ace0]]}
  encoder.layer.23.attention.output.dense.bias:        {input: HOST, type: ram, entries: 1, grid_size: [1, 2], t: 1, mblock: [1, 4], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40b51b80], [1, 0x4adc780]]}
  encoder.layer.23.attention.output.LayerNorm.weight:  {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40d9b120]]}
  encoder.layer.23.attention.output.LayerNorm.bias:    {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4b697c0]]}
  encoder.layer.23.intermediate.dense.weight:          {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 8], ublock: [2, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40bae060], [0, 0x40b561a0], [1, 0x4ae0da0], [1, 0x407c7c00], [2, 0x4aeecc0], [2, 0x40975020], [3, 0xa1e1180], [3, 0x40b51420]]}
  encoder.layer.23.intermediate.dense.bias:            {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa49dea0], [4, 0x40dab540], [5, 0x4b79be0], [5, 0x40c3a080]]}
  encoder.layer.23.output.dense.weight:                {input: HOST, type: ram, entries: 1, grid_size: [2, 4], t: 1, mblock: [8, 2], ublock: [8, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4a01480], [2, 0x408877e0], [3, 0xa154920], [3, 0x40ac1f60], [4, 0xa40e9e0], [4, 0x40d0f100], [5, 0x4add7a0], [5, 0x40b22040]]}
  encoder.layer.23.output.dense.bias:                  {input: HOST, type: ram, entries: 1, grid_size: [1, 4], t: 1, mblock: [1, 2], ublock: [1, 4], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40a01040], [3, 0xa26d1a0], [3, 0x40bdd440], [4, 0xa4a6ac0]]}
  encoder.layer.23.output.LayerNorm.weight:            {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4bc0d00]]}
  encoder.layer.23.output.LayerNorm.bias:              {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 8], ublock: [1, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40a03360]]}
  lm_head.weight:                                      {input: HOST, type: ram, entries: 1, grid_size: [2, 1], t: 1, mblock: [16, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa26f4c0], [3, 0x40bdf760]]}
  lm_head.bias:                                        {input: HOST, type: ram, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa4a8de0]]}

  # constant
  input_1_multiply_16:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5798160]]}
  lc.input_tensor.softmax_18.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xabafe40]]}
  dc.input_tensor.softmax_18.2:                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41899060], [5, 0x56af280]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.0.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x418c57e0]]}
  dc.input_tensor.layernorm_38.1:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x56ca7a0], [1, 0x413f4fe0]]}
  lc.input_tensor.layernorm_38.dc.reduce_sum.5.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x56608a0]]}
  dc.input_tensor.layernorm_38.6:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4122a160]]}
  dc.input_tensor.layernorm_38.8:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xac13940]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.0.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41912640]]}
  dc.input_tensor.layernorm_52.1:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xb247ce0], [4, 0x41a9d500]]}
  lc.input_tensor.layernorm_52.dc.reduce_sum.5.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5871d20]]}
  dc.input_tensor.layernorm_52.6:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41ab60a0]]}
  dc.input_tensor.layernorm_52.8:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41a6a080]]}
  input_1_multiply_69:                                 {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xafbc9c0]]}
  lc.input_tensor.softmax_71.dc.reduce_sum.1.0:        {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x418085a0]]}
  dc.input_tensor.softmax_71.2:                        {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5608600], [5, 0x417bfda0]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.0.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5622a20]]}
  dc.input_tensor.layernorm_91.1:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x417da1c0], [0, 0x41791f80]]}
  lc.input_tensor.layernorm_91.dc.reduce_sum.5.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x55f7f20]]}
  dc.input_tensor.layernorm_91.6:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4131fb00]]}
  dc.input_tensor.layernorm_91.8:                      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x56501c0]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41c21ca0]]}
  dc.input_tensor.layernorm_105.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x59954e0], [5, 0x41bda9e0]]}
  lc.input_tensor.layernorm_105.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41c72da0]]}
  dc.input_tensor.layernorm_105.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5ab3cc0]]}
  dc.input_tensor.layernorm_105.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41670420]]}
  input_1_multiply_122:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5ab7160]]}
  lc.input_tensor.softmax_124.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x416738c0]]}
  dc.input_tensor.softmax_124.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x57a8dc0], [2, 0x413f5620]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41a12960]]}
  dc.input_tensor.layernorm_144.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41b85560], [1, 0x59c6480]]}
  lc.input_tensor.layernorm_144.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x567c440]]}
  dc.input_tensor.layernorm_144.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x412c4560]]}
  dc.input_tensor.layernorm_144.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xad31140]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41bd1580]]}
  dc.input_tensor.layernorm_158.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x584cc20], [1, 0x414ed760]]}
  lc.input_tensor.layernorm_158.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41666fc0]]}
  dc.input_tensor.layernorm_158.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5794cc0]]}
  dc.input_tensor.layernorm_158.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x413dfa40]]}
  input_1_multiply_175:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaf30140]]}
  lc.input_tensor.softmax_177.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5114740]]}
  dc.input_tensor.softmax_177.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40e69a00], [3, 0xa897660]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xacebce0]]}
  dc.input_tensor.layernorm_197.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x414c3060], [5, 0x52abec0]]}
  lc.input_tensor.layernorm_197.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4148c2e0]]}
  dc.input_tensor.layernorm_197.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41344e20]]}
  dc.input_tensor.layernorm_197.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40ee9700]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5425720]]}
  dc.input_tensor.layernorm_211.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x415a4b60], [0, 0x41460300]]}
  lc.input_tensor.layernorm_211.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x52a9600]]}
  dc.input_tensor.layernorm_211.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41053820]]}
  dc.input_tensor.layernorm_211.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x50b0020]]}
  input_1_multiply_228:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40e12d80]]}
  lc.input_tensor.softmax_230.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa7c51a0]]}
  dc.input_tensor.softmax_230.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x412a8480], [4, 0xabfb260]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x413f07e0]]}
  dc.input_tensor.layernorm_250.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5178660], [5, 0x4139a480]]}
  lc.input_tensor.layernorm_250.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x412b3fa0]]}
  dc.input_tensor.layernorm_250.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x50b34c0]]}
  dc.input_tensor.layernorm_250.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40ee6260]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41773100]]}
  dc.input_tensor.layernorm_264.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x54d0780], [5, 0x416bdb20]]}
  lc.input_tensor.layernorm_264.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41670ac0]]}
  dc.input_tensor.layernorm_264.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x54dc1e0]]}
  dc.input_tensor.layernorm_264.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41287580]]}
  input_1_multiply_281:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x54df680]]}
  lc.input_tensor.softmax_283.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4128aa20]]}
  dc.input_tensor.softmax_283.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x551dd40], [2, 0x4115da80]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4116ed00]]}
  dc.input_tensor.layernorm_303.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40f676a0], [3, 0xa8d1400]]}
  lc.input_tensor.layernorm_303.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x413b8880]]}
  dc.input_tensor.layernorm_303.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xad83480]]}
  dc.input_tensor.layernorm_303.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4163ebe0]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x527b840]]}
  dc.input_tensor.layernorm_317.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x53940c0], [2, 0x410e0f00]]}
  lc.input_tensor.layernorm_317.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xaa4ac60]]}
  dc.input_tensor.layernorm_317.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x414d1100]]}
  dc.input_tensor.layernorm_317.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xae9e960]]}
  input_1_multiply_334:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41e75700]]}
  lc.input_tensor.softmax_336.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb201400]]}
  dc.input_tensor.softmax_336.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x41e71440], [4, 0xb8d9660]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5ea3720]]}
  dc.input_tensor.layernorm_356.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x42251120], [0, 0x423514a0]]}
  lc.input_tensor.layernorm_356.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x61a18c0]]}
  dc.input_tensor.layernorm_356.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41d35f80]]}
  dc.input_tensor.layernorm_356.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x418a9080]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x42227600]]}
  dc.input_tensor.layernorm_370.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5fbbfa0], [5, 0x423ca980]]}
  lc.input_tensor.layernorm_370.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x424cad00]]}
  dc.input_tensor.layernorm_370.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41c40c80]]}
  dc.input_tensor.layernorm_370.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x42261360]]}
  input_1_multiply_387:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5dcc880]]}
  lc.input_tensor.softmax_389.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4217a640]]}
  dc.input_tensor.softmax_389.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x42264800], [1, 0x6095640]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4227ec20]]}
  dc.input_tensor.layernorm_409.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x60afa60], [1, 0x41c44120]]}
  lc.input_tensor.layernorm_409.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5dd11e0]]}
  dc.input_tensor.layernorm_409.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x418a5be0]]}
  dc.input_tensor.layernorm_409.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5ea3a60]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x41b73560]]}
  dc.input_tensor.layernorm_423.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb425ee0], [3, 0x4215faa0]]}
  lc.input_tensor.layernorm_423.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xbc058e0]]}
  dc.input_tensor.layernorm_423.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x42370380]]}
  dc.input_tensor.layernorm_423.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60e8f80]]}
  input_1_multiply_440:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41e51460]]}
  lc.input_tensor.softmax_442.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5f3bb40]]}
  dc.input_tensor.softmax_442.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4190d8a0], [3, 0xb262c00]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xba58420]]}
  dc.input_tensor.layernorm_462.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4226de60], [5, 0x60637e0]]}
  lc.input_tensor.layernorm_462.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x424307c0]]}
  dc.input_tensor.layernorm_462.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x62ba140]]}
  dc.input_tensor.layernorm_462.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x422cf680]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4203bd20]]}
  dc.input_tensor.layernorm_476.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x6126040], [2, 0x41b11d40]]}
  lc.input_tensor.layernorm_476.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb4256a0]]}
  dc.input_tensor.layernorm_476.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb48eb80]]}
  dc.input_tensor.layernorm_476.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41dda0c0]]}
  input_1_multiply_493:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41a8e940]]}
  lc.input_tensor.softmax_495.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb492020]]}
  dc.input_tensor.softmax_495.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xaf71440], [3, 0x41a74520]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41f9e100]]}
  dc.input_tensor.layernorm_515.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5d97ea0], [1, 0x41867a80]]}
  lc.input_tensor.layernorm_515.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x59fb1c0]]}
  dc.input_tensor.layernorm_515.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4161ffc0]]}
  dc.input_tensor.layernorm_515.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xafda4a0]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41de4880]]}
  dc.input_tensor.layernorm_529.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x41dda260], [1, 0x5c15a00]]}
  lc.input_tensor.layernorm_529.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x417d1da0]]}
  dc.input_tensor.layernorm_529.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5921240]]}
  dc.input_tensor.layernorm_529.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4152c0a0]]}
  input_1_multiply_546:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x417d25e0]]}
  lc.input_tensor.softmax_548.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x59246e0]]}
  dc.input_tensor.softmax_548.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4152f540], [3, 0xaf57020]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x5c6ab40]]}
  dc.input_tensor.layernorm_568.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41fb7920], [0, 0x420d76e0]]}
  lc.input_tensor.layernorm_568.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f5c580]]}
  dc.input_tensor.layernorm_568.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x41a89360]]}
  dc.input_tensor.layernorm_568.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x5c1caa0]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xb115b40]]}
  dc.input_tensor.layernorm_582.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x41ca17a0], [4, 0xb6d3dc0]]}
  lc.input_tensor.layernorm_582.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41fa3ac0]]}
  dc.input_tensor.layernorm_582.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5d833c0]]}
  dc.input_tensor.layernorm_582.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x42131180]]}
  input_1_multiply_599:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41e27ba0]]}
  lc.input_tensor.softmax_601.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x5c09340]]}
  dc.input_tensor.softmax_601.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41ec0580], [0, 0x41f9e940]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4190f2c0]]}
  dc.input_tensor.layernorm_621.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x5a41a20], [2, 0x41669480]]}
  lc.input_tensor.layernorm_621.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xafe1f60]]}
  dc.input_tensor.layernorm_621.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb4d84c0]]}
  dc.input_tensor.layernorm_621.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xb521980]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x5f5bd40]]}
  dc.input_tensor.layernorm_635.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x41a27b40], [2, 0x5bbb280]]}
  lc.input_tensor.layernorm_635.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x417e2ce0]]}
  dc.input_tensor.layernorm_635.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x54412c0]]}
  dc.input_tensor.layernorm_635.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40473640]]}
  input_1_multiply_652:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4047dc00]]}
  lc.input_tensor.softmax_654.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x458f340]]}
  dc.input_tensor.softmax_654.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40476ae0], [3, 0x9d43540]]}
  lc.input_tensor.layernorm_674.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40490f00]]}
  dc.input_tensor.layernorm_674.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9d5d960], [3, 0x40662d60]]}
  lc.input_tensor.layernorm_674.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9ebd9e0]]}
  dc.input_tensor.layernorm_674.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x407caea0]]}
  dc.input_tensor.layernorm_674.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x46cbbc0]]}
  lc.input_tensor.layernorm_688.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4064dbc0]]}
  dc.input_tensor.layernorm_688.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4570d00], [5, 0x40483560]]}
  lc.input_tensor.layernorm_688.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x4047f8e0]]}
  dc.input_tensor.layernorm_688.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4582980]]}
  dc.input_tensor.layernorm_688.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x403e5b00]]}
  input_1_multiply_705:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4060b8a0]]}
  lc.input_tensor.softmax_707.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x9de6f00]]}
  dc.input_tensor.softmax_707.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x406da420], [5, 0x465e540]]}
  lc.input_tensor.layernorm_727.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40662520]]}
  dc.input_tensor.layernorm_727.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40755a80], [1, 0x47512e0]]}
  lc.input_tensor.layernorm_727.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x404e4260]]}
  dc.input_tensor.layernorm_727.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x468b180]]}
  dc.input_tensor.layernorm_727.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x405ca4e0]]}
  lc.input_tensor.layernorm_741.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x407bd340]]}
  dc.input_tensor.layernorm_741.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x408cf2e0], [1, 0x48cab40]]}
  lc.input_tensor.layernorm_741.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x405fcae0]]}
  dc.input_tensor.layernorm_741.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x47a6660]]}
  dc.input_tensor.layernorm_741.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x406625c0]]}
  input_1_multiply_758:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x406c0600]]}
  lc.input_tensor.softmax_760.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40482a60]]}
  dc.input_tensor.softmax_760.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4593de0], [2, 0x40491740]]}
  lc.input_tensor.layernorm_780.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x4070a5a0]]}
  dc.input_tensor.layernorm_780.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9f04240], [4, 0x40814360]]}
  lc.input_tensor.layernorm_780.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x46d3680]]}
  dc.input_tensor.layernorm_780.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x406a1e60]]}
  dc.input_tensor.layernorm_780.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x46ecac0]]}
  lc.input_tensor.layernorm_794.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9f21800]]}
  dc.input_tensor.layernorm_794.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40822e20], [4, 0xa07daa0]]}
  lc.input_tensor.layernorm_794.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x4098dbc0]]}
  dc.input_tensor.layernorm_794.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x47ebf00]]}
  dc.input_tensor.layernorm_794.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x403f3860]]}
  input_1_multiply_811:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x400e8da0]]}
  lc.input_tensor.softmax_813.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41f30a0]]}
  dc.input_tensor.softmax_813.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4011abc0], [3, 0x9801900]]}
  lc.input_tensor.layernorm_833.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40134fe0]]}
  dc.input_tensor.layernorm_833.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x981bd20], [3, 0x401430e0]]}
  lc.input_tensor.layernorm_833.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x997e3c0]]}
  dc.input_tensor.layernorm_833.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40209ec0]]}
  dc.input_tensor.layernorm_833.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x41f1e00]]}
  lc.input_tensor.layernorm_847.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40000000]]}
  dc.input_tensor.layernorm_847.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x97a0100], [4, 0x40000000]]}
  lc.input_tensor.layernorm_847.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40d0100]]}
  dc.input_tensor.layernorm_847.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40000000]]}
  dc.input_tensor.layernorm_847.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x4008c020]]}
  input_1_multiply_864:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4008e340]]}
  lc.input_tensor.softmax_866.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x97ab040]]}
  dc.input_tensor.softmax_866.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x4008c860], [4, 0x988d940]]}
  lc.input_tensor.layernorm_886.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x41a2980]]}
  dc.input_tensor.layernorm_886.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x400d54e0], [0, 0x40161500]]}
  lc.input_tensor.layernorm_886.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x424b7c0]]}
  dc.input_tensor.layernorm_886.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x434bfc0]]}
  dc.input_tensor.layernorm_886.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x402255c0]]}
  lc.input_tensor.layernorm_900.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9c40340]]}
  dc.input_tensor.layernorm_900.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x404d4360], [5, 0x447a8a0]]}
  lc.input_tensor.layernorm_900.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x402d7720]]}
  dc.input_tensor.layernorm_900.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x403effa0]]}
  dc.input_tensor.layernorm_900.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x44674a0]]}
  input_1_multiply_917:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x403f6d00]]}
  lc.input_tensor.softmax_919.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x403f3440]]}
  dc.input_tensor.softmax_919.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x402b23c0], [1, 0x4326c60]]}
  lc.input_tensor.layernorm_939.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40207860]]}
  dc.input_tensor.layernorm_939.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x994f580], [3, 0x40276940]]}
  lc.input_tensor.layernorm_939.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9a0f240]]}
  dc.input_tensor.layernorm_939.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x4029d9a0]]}
  dc.input_tensor.layernorm_939.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4243ee0]]}
  lc.input_tensor.layernorm_953.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x403200e0]]}
  dc.input_tensor.layernorm_953.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9ac8de0], [3, 0x403f01a0]]}
  lc.input_tensor.layernorm_953.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x9b27ac0]]}
  dc.input_tensor.layernorm_953.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x403b8e80]]}
  dc.input_tensor.layernorm_953.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x435f3c0]]}
  input_1_multiply_970:                                {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa4628c0]]}
  lc.input_tensor.softmax_972.dc.reduce_sum.1.0:       {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40e94760]]}
  dc.input_tensor.softmax_972.2:                       {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa6ef660], [4, 0x40e7db00]]}
  lc.input_tensor.layernorm_992.dc.reduce_sum.0.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4ca9be0]]}
  dc.input_tensor.layernorm_992.1:                     {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40ea3280], [0, 0x40eba680]]}
  lc.input_tensor.layernorm_992.dc.reduce_sum.5.0:     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4dc2c40]]}
  dc.input_tensor.layernorm_992.6:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40c23a20]]}
  dc.input_tensor.layernorm_992.8:                     {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4f16060]]}
  lc.input_tensor.layernorm_1006.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa38f800]]}
  dc.input_tensor.layernorm_1006.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40cff6e0], [4, 0xa5e42e0]]}
  lc.input_tensor.layernorm_1006.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40e76140]]}
  dc.input_tensor.layernorm_1006.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4c557c0]]}
  dc.input_tensor.layernorm_1006.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x40e0d460]]}
  input_1_multiply_1023:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40e251e0]]}
  lc.input_tensor.softmax_1025.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4d2db60]]}
  dc.input_tensor.softmax_1025.2:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40b32fa0], [2, 0x4e66fe0]]}
  lc.input_tensor.layernorm_1045.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa462080]]}
  dc.input_tensor.layernorm_1045.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x40e32f40], [4, 0xa68de40]]}
  lc.input_tensor.layernorm_1045.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40e7d2c0]]}
  dc.input_tensor.layernorm_1045.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40e97f20]]}
  dc.input_tensor.layernorm_1045.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40d51900]]}
  lc.input_tensor.layernorm_1059.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xaae29e0]]}
  dc.input_tensor.layernorm_1059.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x41230f60], [5, 0x503c1e0]]}
  lc.input_tensor.layernorm_1059.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x412c73c0]]}
  dc.input_tensor.layernorm_1059.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4eeb260]]}
  dc.input_tensor.layernorm_1059.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5006740]]}
  input_1_multiply_1076:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x412c7c00]]}
  lc.input_tensor.softmax_1078.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x41264ee0]]}
  dc.input_tensor.softmax_1078.2:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x5009be0], [1, 0x40df9e00]]}
  lc.input_tensor.layernorm_1098.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4e4e480]]}
  dc.input_tensor.layernorm_1098.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x410a8b00], [0, 0x410bff00]]}
  lc.input_tensor.layernorm_1098.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4edfae0]]}
  dc.input_tensor.layernorm_1098.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40d43520]]}
  dc.input_tensor.layernorm_1098.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x4ff4160]]}
  lc.input_tensor.layernorm_1112.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40d66ba0]]}
  dc.input_tensor.layernorm_1112.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa6305e0], [3, 0x411138c0]]}
  lc.input_tensor.layernorm_1112.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa9ca160]]}
  dc.input_tensor.layernorm_1112.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x41115a80]]}
  dc.input_tensor.layernorm_1112.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4f20d00]]}
  input_1_multiply_1129:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40ac0480]]}
  lc.input_tensor.softmax_1131.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x40af0380]]}
  dc.input_tensor.softmax_1131.2:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x49e53e0], [1, 0x4068e620]]}
  lc.input_tensor.layernorm_1151.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9f94860]]}
  dc.input_tensor.layernorm_1151.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9fdb0c0], [3, 0x40948700]]}
  lc.input_tensor.layernorm_1151.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0xa2f6160]]}
  dc.input_tensor.layernorm_1151.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x40bf3c20]]}
  dc.input_tensor.layernorm_1151.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x4a456c0]]}
  lc.input_tensor.layernorm_1165.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x408b3260]]}
  dc.input_tensor.layernorm_1165.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x47a9b00], [2, 0x40665a60]]}
  lc.input_tensor.layernorm_1165.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9f4d7c0]]}
  dc.input_tensor.layernorm_1165.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x408afdc0]]}
  dc.input_tensor.layernorm_1165.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa202240]]}
  input_1_multiply_1182:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0x9f4e000]]}
  lc.input_tensor.softmax_1184.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x405fd320]]}
  dc.input_tensor.softmax_1184.2:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa2056e0], [4, 0x40b44ba0]]}
  lc.input_tensor.layernorm_1204.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x40abfc40]]}
  dc.input_tensor.layernorm_1204.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40a8eb60], [1, 0x4983bc0]]}
  lc.input_tensor.layernorm_1204.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4068dde0]]}
  dc.input_tensor.layernorm_1204.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x489b960]]}
  dc.input_tensor.layernorm_1204.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x40715ec0]]}
  lc.input_tensor.layernorm_1218.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x4cce2c0]]}
  dc.input_tensor.layernorm_1218.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x409b5120], [2, 0x4ce9160]]}
  lc.input_tensor.layernorm_1218.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x40b2b7c0]]}
  dc.input_tensor.layernorm_1218.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xa38bb20]]}
  dc.input_tensor.layernorm_1218.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40cfbdc0]]}
  input_1_multiply_1235:                               {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa38efc0]]}
  lc.input_tensor.softmax_1237.dc.reduce_sum.1.0:      {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40cff260]]}
  dc.input_tensor.softmax_1237.2:                      {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 16, mblock: [3, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa5c9ec0], [4, 0x40e5bd20]]}
  lc.input_tensor.layernorm_1257.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[1, 0x407c73c0]]}
  dc.input_tensor.layernorm_1257.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[2, 0x4a8d4a0], [2, 0x40913800]]}
  lc.input_tensor.layernorm_1257.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[3, 0xa1e0940]]}
  dc.input_tensor.layernorm_1257.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x40b4df80]]}
  dc.input_tensor.layernorm_1257.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xa49aa00]]}
  lc.input_tensor.layernorm_1271.dc.reduce_sum.0.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[4, 0x40db4160]]}
  dc.input_tensor.layernorm_1271.1:                    {input: HOST, type: queue, entries: 1, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[5, 0x4b82800], [5, 0x40c42ca0]]}
  lc.input_tensor.layernorm_1271.dc.reduce_sum.5.0:    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [1, 1], ublock: [1, 1], ublock_order: r, tile_dim: [32, 32], df: Float16_b, target_device: 0, loc: dram, dram: [[0, 0x40c281e0]]}
  dc.input_tensor.layernorm_1271.6:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x4bb2de0]]}
  dc.input_tensor.layernorm_1271.8:                    {input: HOST, type: queue, entries: 1, grid_size: [1, 1], t: 1, mblock: [6, 1], ublock: [2, 1], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x40899c40]]}

  # epoch_to_epoch
  e2e__fused_op_8_0:                                   {input: _fused_op_8, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x421c12c0], [4, 0xbc06120]]}
  e2e__fused_op_17_0:                                  {input: _fused_op_17, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x42373820], [5, 0x60ec420]]}
  e2e__fused_op_26_0:                                  {input: _fused_op_26, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x424653a0], [0, 0x42605220]]}
  e2e__fused_op_35_0:                                  {input: _fused_op_35, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6537cc0], [1, 0x420ca8a0]]}
  e2e__fused_op_44_0:                                  {input: _fused_op_44, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6215ba0], [2, 0x41bffdc0]]}
  e2e__fused_op_53_0:                                  {input: _fused_op_53, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb490320], [3, 0x421c12c0]]}
  e2e__fused_op_62_0:                                  {input: _fused_op_62, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc06120], [4, 0x42373820]]}
  e2e__fused_op_71_0:                                  {input: _fused_op_71, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60ec420], [5, 0x424653a0]]}
  e2e__fused_op_80_0:                                  {input: _fused_op_80, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x42605220], [1, 0x6537cc0]]}
  e2e__fused_op_89_0:                                  {input: _fused_op_89, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x420ca8a0], [2, 0x6215ba0]]}
  e2e__fused_op_98_0:                                  {input: _fused_op_98, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41bffdc0], [3, 0xb490320]]}
  e2e__fused_op_107_0:                                 {input: _fused_op_107, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x421c12c0], [4, 0xbc06120]]}
  e2e__fused_op_116_0:                                 {input: _fused_op_116, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x42373820], [5, 0x60ec420]]}
  e2e__fused_op_125_0:                                 {input: _fused_op_125, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x424653a0], [0, 0x42605220]]}
  e2e__fused_op_134_0:                                 {input: _fused_op_134, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x6537cc0], [1, 0x420ca8a0]]}
  e2e__fused_op_143_0:                                 {input: _fused_op_143, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x6215ba0], [2, 0x41bffdc0]]}
  e2e__fused_op_152_0:                                 {input: _fused_op_152, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0xb490320], [3, 0x421c12c0]]}
  e2e__fused_op_161_0:                                 {input: _fused_op_161, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0xbc06120], [4, 0x42373820]]}
  e2e__fused_op_170_0:                                 {input: _fused_op_170, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x60ec420], [5, 0x424653a0]]}
  e2e__fused_op_179_0:                                 {input: _fused_op_179, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[0, 0x42605220], [1, 0x6537cc0]]}
  e2e__fused_op_188_0:                                 {input: _fused_op_188, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[1, 0x420ca8a0], [2, 0x6215ba0]]}
  e2e__fused_op_197_0:                                 {input: _fused_op_197, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[2, 0x41bffdc0], [3, 0xb490320]]}
  e2e__fused_op_206_0:                                 {input: _fused_op_206, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[3, 0x421c12c0], [4, 0xbc06120]]}
  e2e__fused_op_215_0:                                 {input: _fused_op_215, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 8], ublock: [2, 4], ublock_order: c, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[4, 0x42373820], [5, 0x60ec420]]}
  e2e_matmul_1274_0:                                   {input: matmul_1274, type: queue, entries: 256, grid_size: [2, 1], t: 1, mblock: [3, 1], ublock: [2, 1], ublock_order: r, tile_dim: [32, 32], df: Bfp8_b, target_device: 0, loc: dram, dram: [[5, 0x424653a0], [0, 0x42605220]]}

graphs:
  fwd_0_0_temporal_epoch_0:
    target_device: 0
    input_count: 256
    buffer_0_input_1_matmul_2: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [input_1],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_2: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0_input_1_matmul_2, encoder.layer.0.attention.self.query.weight, encoder.layer.0.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_8: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0_input_1_matmul_2, encoder.layer.0.attention.self.key.weight, encoder.layer.0.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_14: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_2, matmul_8],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_0: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_14, input_1_multiply_16, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_18.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_0, lc.input_tensor.softmax_18.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_22: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0_input_1_matmul_2, encoder.layer.0.attention.self.value.weight, encoder.layer.0.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_0__fused_op_2: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_0], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_1: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_18.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_18.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_2: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_0__fused_op_2, _fused_op_1],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_29: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_2, matmul_22],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_33: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_29, encoder.layer.0.attention.output.dense.weight, encoder.layer.0.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_37: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_33, input_1],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_37, lc.input_tensor.layernorm_38.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_3: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_38.1, layernorm_38.dc.reduce_sum.0.lc1, add_37], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_38.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_3, _fused_op_3], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_38.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_38.dc.multiply.4, lc.input_tensor.layernorm_38.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_4: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_38.6, layernorm_38.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_38.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_5: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_3, _fused_op_4, encoder.layer.0.attention.output.LayerNorm.weight, encoder.layer.0.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_41: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_5, encoder.layer.0.intermediate.dense.weight, encoder.layer.0.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_47: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_41, encoder.layer.0.output.dense.weight, encoder.layer.0.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_5_add_51: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_5],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_51: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_47, buffer_0__fused_op_5_add_51],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_51, lc.input_tensor.layernorm_52.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_6: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_52.1, layernorm_52.dc.reduce_sum.0.lc1, add_51],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_52.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_6, _fused_op_6],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_52.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_52.dc.multiply.4, lc.input_tensor.layernorm_52.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_7: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_52.6, layernorm_52.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_52.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_8: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_6, _fused_op_7, encoder.layer.0.output.LayerNorm.weight, encoder.layer.0.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_1_temporal_epoch_1:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_8_matmul_55: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_8_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_55: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_8_matmul_55, encoder.layer.1.attention.self.query.weight, encoder.layer.1.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_61: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_8_matmul_55, encoder.layer.1.attention.self.key.weight, encoder.layer.1.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_67: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_55, matmul_61],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_9: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_67, input_1_multiply_69, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_71.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_9, lc.input_tensor.softmax_71.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_75: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_8_matmul_55, encoder.layer.1.attention.self.value.weight, encoder.layer.1.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_9__fused_op_11: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_9], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_10: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_71.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_71.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_11: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_9__fused_op_11, _fused_op_10],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_82: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_11, matmul_75],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_86: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_82, encoder.layer.1.attention.output.dense.weight, encoder.layer.1.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_90: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_86, e2e__fused_op_8_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_90, lc.input_tensor.layernorm_91.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_12: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_91.1, layernorm_91.dc.reduce_sum.0.lc1, add_90], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_91.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_12, _fused_op_12], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_91.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_91.dc.multiply.4, lc.input_tensor.layernorm_91.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_13: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_91.6, layernorm_91.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_91.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_14: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_12, _fused_op_13, encoder.layer.1.attention.output.LayerNorm.weight, encoder.layer.1.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_94: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_14, encoder.layer.1.intermediate.dense.weight, encoder.layer.1.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_100: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_94, encoder.layer.1.output.dense.weight, encoder.layer.1.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_14_add_104: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_14],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_104: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_100, buffer_0__fused_op_14_add_104],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_104, lc.input_tensor.layernorm_105.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_15: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_105.1, layernorm_105.dc.reduce_sum.0.lc1, add_104],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_105.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_15, _fused_op_15],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_105.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_105.dc.multiply.4, lc.input_tensor.layernorm_105.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_16: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_105.6, layernorm_105.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_105.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_17: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_15, _fused_op_16, encoder.layer.1.output.LayerNorm.weight, encoder.layer.1.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_2_temporal_epoch_2:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_17_matmul_108: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_17_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_108: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_17_matmul_108, encoder.layer.2.attention.self.query.weight, encoder.layer.2.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_114: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_17_matmul_108, encoder.layer.2.attention.self.key.weight, encoder.layer.2.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_120: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_108, matmul_114],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_18: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_120, input_1_multiply_122, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_124.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_18, lc.input_tensor.softmax_124.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_128: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_17_matmul_108, encoder.layer.2.attention.self.value.weight, encoder.layer.2.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_18__fused_op_20: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_18], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_19: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_124.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_124.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_20: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_18__fused_op_20, _fused_op_19],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_135: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_20, matmul_128],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_139: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_135, encoder.layer.2.attention.output.dense.weight, encoder.layer.2.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_143: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_139, e2e__fused_op_17_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_143, lc.input_tensor.layernorm_144.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_21: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_144.1, layernorm_144.dc.reduce_sum.0.lc1, add_143], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_144.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_21, _fused_op_21], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_144.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_144.dc.multiply.4, lc.input_tensor.layernorm_144.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_22: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_144.6, layernorm_144.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_144.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_23: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_21, _fused_op_22, encoder.layer.2.attention.output.LayerNorm.weight, encoder.layer.2.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_147: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_23, encoder.layer.2.intermediate.dense.weight, encoder.layer.2.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_153: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_147, encoder.layer.2.output.dense.weight, encoder.layer.2.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_23_add_157: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_23],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_157: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_153, buffer_0__fused_op_23_add_157],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_157, lc.input_tensor.layernorm_158.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_24: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_158.1, layernorm_158.dc.reduce_sum.0.lc1, add_157],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_158.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_24, _fused_op_24],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_158.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_158.dc.multiply.4, lc.input_tensor.layernorm_158.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_25: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_158.6, layernorm_158.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_158.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_26: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_24, _fused_op_25, encoder.layer.2.output.LayerNorm.weight, encoder.layer.2.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_3_temporal_epoch_3:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_26_matmul_161: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_26_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_161: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_26_matmul_161, encoder.layer.3.attention.self.query.weight, encoder.layer.3.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_167: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_26_matmul_161, encoder.layer.3.attention.self.key.weight, encoder.layer.3.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_173: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_161, matmul_167],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_27: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_173, input_1_multiply_175, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_177.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_27, lc.input_tensor.softmax_177.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_181: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_26_matmul_161, encoder.layer.3.attention.self.value.weight, encoder.layer.3.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_27__fused_op_29: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_27], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_28: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_177.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_177.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_29: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_27__fused_op_29, _fused_op_28],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_188: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_29, matmul_181],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_192: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_188, encoder.layer.3.attention.output.dense.weight, encoder.layer.3.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_196: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_192, e2e__fused_op_26_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_196, lc.input_tensor.layernorm_197.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_30: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_197.1, layernorm_197.dc.reduce_sum.0.lc1, add_196], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_197.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_30, _fused_op_30], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_197.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_197.dc.multiply.4, lc.input_tensor.layernorm_197.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_31: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_197.6, layernorm_197.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_197.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_32: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_30, _fused_op_31, encoder.layer.3.attention.output.LayerNorm.weight, encoder.layer.3.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_200: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_32, encoder.layer.3.intermediate.dense.weight, encoder.layer.3.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_206: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_200, encoder.layer.3.output.dense.weight, encoder.layer.3.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_32_add_210: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_32],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_210: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_206, buffer_0__fused_op_32_add_210],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_210, lc.input_tensor.layernorm_211.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_33: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_211.1, layernorm_211.dc.reduce_sum.0.lc1, add_210],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_211.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_33, _fused_op_33],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_211.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_211.dc.multiply.4, lc.input_tensor.layernorm_211.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_34: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_211.6, layernorm_211.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_211.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_35: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_33, _fused_op_34, encoder.layer.3.output.LayerNorm.weight, encoder.layer.3.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_4_temporal_epoch_4:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_35_matmul_214: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_35_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_214: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_35_matmul_214, encoder.layer.4.attention.self.query.weight, encoder.layer.4.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_220: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_35_matmul_214, encoder.layer.4.attention.self.key.weight, encoder.layer.4.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_226: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_214, matmul_220],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_36: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_226, input_1_multiply_228, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_230.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_36, lc.input_tensor.softmax_230.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_234: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_35_matmul_214, encoder.layer.4.attention.self.value.weight, encoder.layer.4.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_36__fused_op_38: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_36], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_37: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_230.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_230.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_38: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_36__fused_op_38, _fused_op_37],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_241: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_38, matmul_234],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_245: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_241, encoder.layer.4.attention.output.dense.weight, encoder.layer.4.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_249: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_245, e2e__fused_op_35_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_249, lc.input_tensor.layernorm_250.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_39: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_250.1, layernorm_250.dc.reduce_sum.0.lc1, add_249], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_250.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_39, _fused_op_39], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_250.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_250.dc.multiply.4, lc.input_tensor.layernorm_250.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_40: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_250.6, layernorm_250.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_250.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_41: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_39, _fused_op_40, encoder.layer.4.attention.output.LayerNorm.weight, encoder.layer.4.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_253: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_41, encoder.layer.4.intermediate.dense.weight, encoder.layer.4.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_259: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_253, encoder.layer.4.output.dense.weight, encoder.layer.4.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_41_add_263: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_41],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_263: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_259, buffer_0__fused_op_41_add_263],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_263, lc.input_tensor.layernorm_264.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_42: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_264.1, layernorm_264.dc.reduce_sum.0.lc1, add_263],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_264.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_42, _fused_op_42],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_264.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_264.dc.multiply.4, lc.input_tensor.layernorm_264.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_43: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_264.6, layernorm_264.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_264.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_44: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_42, _fused_op_43, encoder.layer.4.output.LayerNorm.weight, encoder.layer.4.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_5_temporal_epoch_5:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_44_matmul_267: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_44_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_267: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_44_matmul_267, encoder.layer.5.attention.self.query.weight, encoder.layer.5.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_273: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_44_matmul_267, encoder.layer.5.attention.self.key.weight, encoder.layer.5.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_279: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_267, matmul_273],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_45: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_279, input_1_multiply_281, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_283.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_45, lc.input_tensor.softmax_283.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_287: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_44_matmul_267, encoder.layer.5.attention.self.value.weight, encoder.layer.5.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_45__fused_op_47: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_45], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_46: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_283.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_283.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_47: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_45__fused_op_47, _fused_op_46],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_294: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_47, matmul_287],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_298: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_294, encoder.layer.5.attention.output.dense.weight, encoder.layer.5.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_302: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_298, e2e__fused_op_44_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_302, lc.input_tensor.layernorm_303.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_48: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_303.1, layernorm_303.dc.reduce_sum.0.lc1, add_302], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_303.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_48, _fused_op_48], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_303.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_303.dc.multiply.4, lc.input_tensor.layernorm_303.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_49: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_303.6, layernorm_303.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_303.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_50: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_48, _fused_op_49, encoder.layer.5.attention.output.LayerNorm.weight, encoder.layer.5.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_306: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_50, encoder.layer.5.intermediate.dense.weight, encoder.layer.5.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_312: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_306, encoder.layer.5.output.dense.weight, encoder.layer.5.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_50_add_316: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_50],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_316: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_312, buffer_0__fused_op_50_add_316],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_316, lc.input_tensor.layernorm_317.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_51: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_317.1, layernorm_317.dc.reduce_sum.0.lc1, add_316],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_317.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_51, _fused_op_51],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_317.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_317.dc.multiply.4, lc.input_tensor.layernorm_317.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_52: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_317.6, layernorm_317.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_317.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_53: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_51, _fused_op_52, encoder.layer.5.output.LayerNorm.weight, encoder.layer.5.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_6_temporal_epoch_6:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_53_matmul_320: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_53_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_320: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_53_matmul_320, encoder.layer.6.attention.self.query.weight, encoder.layer.6.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_326: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_53_matmul_320, encoder.layer.6.attention.self.key.weight, encoder.layer.6.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_332: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_320, matmul_326],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_54: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_332, input_1_multiply_334, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_336.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_54, lc.input_tensor.softmax_336.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_340: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_53_matmul_320, encoder.layer.6.attention.self.value.weight, encoder.layer.6.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_54__fused_op_56: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_54], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_55: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_336.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_336.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_56: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_54__fused_op_56, _fused_op_55],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_347: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_56, matmul_340],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_351: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_347, encoder.layer.6.attention.output.dense.weight, encoder.layer.6.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_355: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_351, e2e__fused_op_53_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_355, lc.input_tensor.layernorm_356.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_57: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_356.1, layernorm_356.dc.reduce_sum.0.lc1, add_355], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_356.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_57, _fused_op_57], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_356.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_356.dc.multiply.4, lc.input_tensor.layernorm_356.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_58: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_356.6, layernorm_356.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_356.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_59: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_57, _fused_op_58, encoder.layer.6.attention.output.LayerNorm.weight, encoder.layer.6.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_359: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_59, encoder.layer.6.intermediate.dense.weight, encoder.layer.6.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_365: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_359, encoder.layer.6.output.dense.weight, encoder.layer.6.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_59_add_369: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_59],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_369: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_365, buffer_0__fused_op_59_add_369],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_369, lc.input_tensor.layernorm_370.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_60: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_370.1, layernorm_370.dc.reduce_sum.0.lc1, add_369],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_370.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_60, _fused_op_60],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_370.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_370.dc.multiply.4, lc.input_tensor.layernorm_370.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_61: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_370.6, layernorm_370.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_370.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_62: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_60, _fused_op_61, encoder.layer.6.output.LayerNorm.weight, encoder.layer.6.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_7_temporal_epoch_7:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_62_matmul_373: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_62_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_373: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_62_matmul_373, encoder.layer.7.attention.self.query.weight, encoder.layer.7.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_379: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_62_matmul_373, encoder.layer.7.attention.self.key.weight, encoder.layer.7.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_385: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_373, matmul_379],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_63: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_385, input_1_multiply_387, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_389.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_63, lc.input_tensor.softmax_389.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_393: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_62_matmul_373, encoder.layer.7.attention.self.value.weight, encoder.layer.7.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_63__fused_op_65: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_63], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_64: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_389.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_389.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_65: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_63__fused_op_65, _fused_op_64],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_400: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_65, matmul_393],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_404: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_400, encoder.layer.7.attention.output.dense.weight, encoder.layer.7.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_408: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_404, e2e__fused_op_62_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_408, lc.input_tensor.layernorm_409.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_66: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_409.1, layernorm_409.dc.reduce_sum.0.lc1, add_408], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_409.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_66, _fused_op_66], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_409.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_409.dc.multiply.4, lc.input_tensor.layernorm_409.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_67: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_409.6, layernorm_409.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_409.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_68: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_66, _fused_op_67, encoder.layer.7.attention.output.LayerNorm.weight, encoder.layer.7.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_412: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_68, encoder.layer.7.intermediate.dense.weight, encoder.layer.7.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_418: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_412, encoder.layer.7.output.dense.weight, encoder.layer.7.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_68_add_422: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_68],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_422: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_418, buffer_0__fused_op_68_add_422],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_422, lc.input_tensor.layernorm_423.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_69: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_423.1, layernorm_423.dc.reduce_sum.0.lc1, add_422],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_423.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_69, _fused_op_69],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_423.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_423.dc.multiply.4, lc.input_tensor.layernorm_423.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_70: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_423.6, layernorm_423.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_423.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_71: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_69, _fused_op_70, encoder.layer.7.output.LayerNorm.weight, encoder.layer.7.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_8_temporal_epoch_8:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_71_matmul_426: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_71_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_426: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_71_matmul_426, encoder.layer.8.attention.self.query.weight, encoder.layer.8.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_432: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_71_matmul_426, encoder.layer.8.attention.self.key.weight, encoder.layer.8.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_438: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_426, matmul_432],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_72: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_438, input_1_multiply_440, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_442.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_72, lc.input_tensor.softmax_442.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_446: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_71_matmul_426, encoder.layer.8.attention.self.value.weight, encoder.layer.8.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_72__fused_op_74: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_72], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_73: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_442.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_442.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_74: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_72__fused_op_74, _fused_op_73],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_453: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_74, matmul_446],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_457: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_453, encoder.layer.8.attention.output.dense.weight, encoder.layer.8.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_461: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_457, e2e__fused_op_71_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_461, lc.input_tensor.layernorm_462.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_75: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_462.1, layernorm_462.dc.reduce_sum.0.lc1, add_461], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_462.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_75, _fused_op_75], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_462.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_462.dc.multiply.4, lc.input_tensor.layernorm_462.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_76: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_462.6, layernorm_462.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_462.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_77: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_75, _fused_op_76, encoder.layer.8.attention.output.LayerNorm.weight, encoder.layer.8.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_465: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_77, encoder.layer.8.intermediate.dense.weight, encoder.layer.8.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_471: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_465, encoder.layer.8.output.dense.weight, encoder.layer.8.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_77_add_475: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_77],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_475: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_471, buffer_0__fused_op_77_add_475],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_475, lc.input_tensor.layernorm_476.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_78: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_476.1, layernorm_476.dc.reduce_sum.0.lc1, add_475],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_476.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_78, _fused_op_78],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_476.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_476.dc.multiply.4, lc.input_tensor.layernorm_476.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_79: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_476.6, layernorm_476.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_476.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_80: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_78, _fused_op_79, encoder.layer.8.output.LayerNorm.weight, encoder.layer.8.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_9_temporal_epoch_9:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_80_matmul_479: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_80_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_479: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_80_matmul_479, encoder.layer.9.attention.self.query.weight, encoder.layer.9.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_485: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_80_matmul_479, encoder.layer.9.attention.self.key.weight, encoder.layer.9.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_491: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_479, matmul_485],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_81: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_491, input_1_multiply_493, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_495.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_81, lc.input_tensor.softmax_495.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_499: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_80_matmul_479, encoder.layer.9.attention.self.value.weight, encoder.layer.9.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_81__fused_op_83: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_81], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_82: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_495.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_495.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_83: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_81__fused_op_83, _fused_op_82],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_506: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_83, matmul_499],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_510: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_506, encoder.layer.9.attention.output.dense.weight, encoder.layer.9.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_514: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_510, e2e__fused_op_80_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_514, lc.input_tensor.layernorm_515.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_84: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_515.1, layernorm_515.dc.reduce_sum.0.lc1, add_514], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_515.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_84, _fused_op_84], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_515.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_515.dc.multiply.4, lc.input_tensor.layernorm_515.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_85: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_515.6, layernorm_515.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_515.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_86: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_84, _fused_op_85, encoder.layer.9.attention.output.LayerNorm.weight, encoder.layer.9.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_518: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_86, encoder.layer.9.intermediate.dense.weight, encoder.layer.9.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_524: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_518, encoder.layer.9.output.dense.weight, encoder.layer.9.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_86_add_528: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_86],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_528: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_524, buffer_0__fused_op_86_add_528],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_528, lc.input_tensor.layernorm_529.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_87: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_529.1, layernorm_529.dc.reduce_sum.0.lc1, add_528],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_529.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_87, _fused_op_87],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_529.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_529.dc.multiply.4, lc.input_tensor.layernorm_529.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_88: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_529.6, layernorm_529.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_529.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_89: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_87, _fused_op_88, encoder.layer.9.output.LayerNorm.weight, encoder.layer.9.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_10_temporal_epoch_10:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_89_matmul_532: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_89_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_532: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_89_matmul_532, encoder.layer.10.attention.self.query.weight, encoder.layer.10.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_538: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_89_matmul_532, encoder.layer.10.attention.self.key.weight, encoder.layer.10.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_544: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_532, matmul_538],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_90: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_544, input_1_multiply_546, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_548.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_90, lc.input_tensor.softmax_548.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_552: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_89_matmul_532, encoder.layer.10.attention.self.value.weight, encoder.layer.10.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_90__fused_op_92: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_90], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_91: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_548.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_548.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_92: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_90__fused_op_92, _fused_op_91],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_559: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_92, matmul_552],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_563: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_559, encoder.layer.10.attention.output.dense.weight, encoder.layer.10.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_567: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_563, e2e__fused_op_89_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_567, lc.input_tensor.layernorm_568.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_93: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_568.1, layernorm_568.dc.reduce_sum.0.lc1, add_567], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_568.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_93, _fused_op_93], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_568.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_568.dc.multiply.4, lc.input_tensor.layernorm_568.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_94: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_568.6, layernorm_568.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_568.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_95: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_93, _fused_op_94, encoder.layer.10.attention.output.LayerNorm.weight, encoder.layer.10.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_571: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_95, encoder.layer.10.intermediate.dense.weight, encoder.layer.10.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_577: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_571, encoder.layer.10.output.dense.weight, encoder.layer.10.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_95_add_581: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_95],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_581: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_577, buffer_0__fused_op_95_add_581],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_581, lc.input_tensor.layernorm_582.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_96: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_582.1, layernorm_582.dc.reduce_sum.0.lc1, add_581],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_582.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_96, _fused_op_96],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_582.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_582.dc.multiply.4, lc.input_tensor.layernorm_582.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_97: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_582.6, layernorm_582.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_582.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_98: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_96, _fused_op_97, encoder.layer.10.output.LayerNorm.weight, encoder.layer.10.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_11_temporal_epoch_11:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_98_matmul_585: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_98_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_585: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_98_matmul_585, encoder.layer.11.attention.self.query.weight, encoder.layer.11.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_591: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_98_matmul_585, encoder.layer.11.attention.self.key.weight, encoder.layer.11.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_597: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_585, matmul_591],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_99: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_597, input_1_multiply_599, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_601.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_99, lc.input_tensor.softmax_601.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_605: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_98_matmul_585, encoder.layer.11.attention.self.value.weight, encoder.layer.11.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_99__fused_op_101: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_99], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_100: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_601.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_601.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_101: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_99__fused_op_101, _fused_op_100],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_612: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_101, matmul_605],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_616: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_612, encoder.layer.11.attention.output.dense.weight, encoder.layer.11.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_620: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_616, e2e__fused_op_98_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_620, lc.input_tensor.layernorm_621.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_102: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_621.1, layernorm_621.dc.reduce_sum.0.lc1, add_620], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_621.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_102, _fused_op_102], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_621.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_621.dc.multiply.4, lc.input_tensor.layernorm_621.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_103: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_621.6, layernorm_621.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_621.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_104: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_102, _fused_op_103, encoder.layer.11.attention.output.LayerNorm.weight, encoder.layer.11.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_624: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_104, encoder.layer.11.intermediate.dense.weight, encoder.layer.11.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_630: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_624, encoder.layer.11.output.dense.weight, encoder.layer.11.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_104_add_634: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_104],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_634: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_630, buffer_0__fused_op_104_add_634],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_634, lc.input_tensor.layernorm_635.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_105: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_635.1, layernorm_635.dc.reduce_sum.0.lc1, add_634],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_635.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_105, _fused_op_105],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_635.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_635.dc.multiply.4, lc.input_tensor.layernorm_635.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_106: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_635.6, layernorm_635.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_635.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_107: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_105, _fused_op_106, encoder.layer.11.output.LayerNorm.weight, encoder.layer.11.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_12_temporal_epoch_12:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_107_matmul_638: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_107_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_638: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_107_matmul_638, encoder.layer.12.attention.self.query.weight, encoder.layer.12.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_644: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_107_matmul_638, encoder.layer.12.attention.self.key.weight, encoder.layer.12.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_650: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_638, matmul_644],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_108: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_650, input_1_multiply_652, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_654.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_108, lc.input_tensor.softmax_654.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_658: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_107_matmul_638, encoder.layer.12.attention.self.value.weight, encoder.layer.12.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_108__fused_op_110: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_108], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_109: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_654.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_654.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_110: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_108__fused_op_110, _fused_op_109],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_665: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_110, matmul_658],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_669: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_665, encoder.layer.12.attention.output.dense.weight, encoder.layer.12.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_673: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_669, e2e__fused_op_107_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_673, lc.input_tensor.layernorm_674.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_111: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_674.1, layernorm_674.dc.reduce_sum.0.lc1, add_673], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_674.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_111, _fused_op_111], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_674.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_674.dc.multiply.4, lc.input_tensor.layernorm_674.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_112: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_674.6, layernorm_674.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_674.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_113: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_111, _fused_op_112, encoder.layer.12.attention.output.LayerNorm.weight, encoder.layer.12.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_677: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_113, encoder.layer.12.intermediate.dense.weight, encoder.layer.12.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_683: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_677, encoder.layer.12.output.dense.weight, encoder.layer.12.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_113_add_687: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_113],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_687: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_683, buffer_0__fused_op_113_add_687],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_687, lc.input_tensor.layernorm_688.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_114: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_688.1, layernorm_688.dc.reduce_sum.0.lc1, add_687],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_688.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_114, _fused_op_114],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_688.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_688.dc.multiply.4, lc.input_tensor.layernorm_688.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_115: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_688.6, layernorm_688.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_688.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_116: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_114, _fused_op_115, encoder.layer.12.output.LayerNorm.weight, encoder.layer.12.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_13_temporal_epoch_13:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_116_matmul_691: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_116_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_691: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_116_matmul_691, encoder.layer.13.attention.self.query.weight, encoder.layer.13.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_697: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_116_matmul_691, encoder.layer.13.attention.self.key.weight, encoder.layer.13.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_703: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_691, matmul_697],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_117: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_703, input_1_multiply_705, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_707.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_117, lc.input_tensor.softmax_707.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_711: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_116_matmul_691, encoder.layer.13.attention.self.value.weight, encoder.layer.13.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_117__fused_op_119: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_117], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_118: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_707.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_707.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_119: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_117__fused_op_119, _fused_op_118],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_718: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_119, matmul_711],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_722: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_718, encoder.layer.13.attention.output.dense.weight, encoder.layer.13.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_726: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_722, e2e__fused_op_116_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_726, lc.input_tensor.layernorm_727.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_120: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_727.1, layernorm_727.dc.reduce_sum.0.lc1, add_726], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_727.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_120, _fused_op_120], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_727.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_727.dc.multiply.4, lc.input_tensor.layernorm_727.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_121: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_727.6, layernorm_727.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_727.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_122: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_120, _fused_op_121, encoder.layer.13.attention.output.LayerNorm.weight, encoder.layer.13.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_730: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_122, encoder.layer.13.intermediate.dense.weight, encoder.layer.13.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_736: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_730, encoder.layer.13.output.dense.weight, encoder.layer.13.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_122_add_740: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_122],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_740: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_736, buffer_0__fused_op_122_add_740],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_740, lc.input_tensor.layernorm_741.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_123: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_741.1, layernorm_741.dc.reduce_sum.0.lc1, add_740],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_741.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_123, _fused_op_123],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_741.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_741.dc.multiply.4, lc.input_tensor.layernorm_741.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_124: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_741.6, layernorm_741.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_741.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_125: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_123, _fused_op_124, encoder.layer.13.output.LayerNorm.weight, encoder.layer.13.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_14_temporal_epoch_14:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_125_matmul_744: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_125_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_744: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_125_matmul_744, encoder.layer.14.attention.self.query.weight, encoder.layer.14.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_750: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_125_matmul_744, encoder.layer.14.attention.self.key.weight, encoder.layer.14.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_756: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_744, matmul_750],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_126: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_756, input_1_multiply_758, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_760.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_126, lc.input_tensor.softmax_760.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_764: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_125_matmul_744, encoder.layer.14.attention.self.value.weight, encoder.layer.14.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_126__fused_op_128: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_126], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_127: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_760.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_760.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_128: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_126__fused_op_128, _fused_op_127],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_771: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_128, matmul_764],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_775: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_771, encoder.layer.14.attention.output.dense.weight, encoder.layer.14.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_779: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_775, e2e__fused_op_125_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_779, lc.input_tensor.layernorm_780.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_129: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_780.1, layernorm_780.dc.reduce_sum.0.lc1, add_779], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_780.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_129, _fused_op_129], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_780.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_780.dc.multiply.4, lc.input_tensor.layernorm_780.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_130: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_780.6, layernorm_780.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_780.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_131: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_129, _fused_op_130, encoder.layer.14.attention.output.LayerNorm.weight, encoder.layer.14.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_783: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_131, encoder.layer.14.intermediate.dense.weight, encoder.layer.14.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_789: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_783, encoder.layer.14.output.dense.weight, encoder.layer.14.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_131_add_793: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_131],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_793: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_789, buffer_0__fused_op_131_add_793],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_793, lc.input_tensor.layernorm_794.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_132: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_794.1, layernorm_794.dc.reduce_sum.0.lc1, add_793],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_794.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_132, _fused_op_132],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_794.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_794.dc.multiply.4, lc.input_tensor.layernorm_794.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_133: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_794.6, layernorm_794.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_794.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_134: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_132, _fused_op_133, encoder.layer.14.output.LayerNorm.weight, encoder.layer.14.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_15_temporal_epoch_15:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_134_matmul_797: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_134_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_797: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_134_matmul_797, encoder.layer.15.attention.self.query.weight, encoder.layer.15.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_803: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_134_matmul_797, encoder.layer.15.attention.self.key.weight, encoder.layer.15.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_809: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_797, matmul_803],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_135: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_809, input_1_multiply_811, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_813.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_135, lc.input_tensor.softmax_813.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_817: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_134_matmul_797, encoder.layer.15.attention.self.value.weight, encoder.layer.15.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_135__fused_op_137: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_135], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_136: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_813.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_813.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_137: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_135__fused_op_137, _fused_op_136],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_824: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_137, matmul_817],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_828: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_824, encoder.layer.15.attention.output.dense.weight, encoder.layer.15.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_832: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_828, e2e__fused_op_134_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_832, lc.input_tensor.layernorm_833.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_138: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_833.1, layernorm_833.dc.reduce_sum.0.lc1, add_832], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_833.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_138, _fused_op_138], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_833.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_833.dc.multiply.4, lc.input_tensor.layernorm_833.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_139: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_833.6, layernorm_833.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_833.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_140: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_138, _fused_op_139, encoder.layer.15.attention.output.LayerNorm.weight, encoder.layer.15.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_836: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_140, encoder.layer.15.intermediate.dense.weight, encoder.layer.15.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_842: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_836, encoder.layer.15.output.dense.weight, encoder.layer.15.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_140_add_846: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_140],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_846: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_842, buffer_0__fused_op_140_add_846],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_846, lc.input_tensor.layernorm_847.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_141: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_847.1, layernorm_847.dc.reduce_sum.0.lc1, add_846],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_847.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_141, _fused_op_141],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_847.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_847.dc.multiply.4, lc.input_tensor.layernorm_847.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_142: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_847.6, layernorm_847.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_847.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_143: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_141, _fused_op_142, encoder.layer.15.output.LayerNorm.weight, encoder.layer.15.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_16_temporal_epoch_16:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_143_matmul_850: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_143_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_850: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_143_matmul_850, encoder.layer.16.attention.self.query.weight, encoder.layer.16.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_856: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_143_matmul_850, encoder.layer.16.attention.self.key.weight, encoder.layer.16.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_862: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_850, matmul_856],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_144: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_862, input_1_multiply_864, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_866.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_144, lc.input_tensor.softmax_866.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_870: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_143_matmul_850, encoder.layer.16.attention.self.value.weight, encoder.layer.16.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_144__fused_op_146: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_144], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_145: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_866.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_866.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_146: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_144__fused_op_146, _fused_op_145],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_877: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_146, matmul_870],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_881: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_877, encoder.layer.16.attention.output.dense.weight, encoder.layer.16.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_885: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_881, e2e__fused_op_143_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_885, lc.input_tensor.layernorm_886.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_147: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_886.1, layernorm_886.dc.reduce_sum.0.lc1, add_885], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_886.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_147, _fused_op_147], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_886.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_886.dc.multiply.4, lc.input_tensor.layernorm_886.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_148: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_886.6, layernorm_886.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_886.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_149: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_147, _fused_op_148, encoder.layer.16.attention.output.LayerNorm.weight, encoder.layer.16.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_889: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_149, encoder.layer.16.intermediate.dense.weight, encoder.layer.16.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_895: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_889, encoder.layer.16.output.dense.weight, encoder.layer.16.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_149_add_899: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_149],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_899: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_895, buffer_0__fused_op_149_add_899],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_899, lc.input_tensor.layernorm_900.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_150: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_900.1, layernorm_900.dc.reduce_sum.0.lc1, add_899],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_900.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_150, _fused_op_150],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_900.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_900.dc.multiply.4, lc.input_tensor.layernorm_900.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_151: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_900.6, layernorm_900.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_900.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_152: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_150, _fused_op_151, encoder.layer.16.output.LayerNorm.weight, encoder.layer.16.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_17_temporal_epoch_17:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_152_matmul_903: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_152_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_903: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_152_matmul_903, encoder.layer.17.attention.self.query.weight, encoder.layer.17.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_909: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_152_matmul_903, encoder.layer.17.attention.self.key.weight, encoder.layer.17.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_915: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_903, matmul_909],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_153: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_915, input_1_multiply_917, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_919.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_153, lc.input_tensor.softmax_919.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_923: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_152_matmul_903, encoder.layer.17.attention.self.value.weight, encoder.layer.17.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_153__fused_op_155: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_153], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_154: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_919.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_919.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_155: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_153__fused_op_155, _fused_op_154],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_930: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_155, matmul_923],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_934: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_930, encoder.layer.17.attention.output.dense.weight, encoder.layer.17.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_938: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_934, e2e__fused_op_152_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_938, lc.input_tensor.layernorm_939.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_156: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_939.1, layernorm_939.dc.reduce_sum.0.lc1, add_938], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_939.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_156, _fused_op_156], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_939.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_939.dc.multiply.4, lc.input_tensor.layernorm_939.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_157: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_939.6, layernorm_939.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_939.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_158: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_156, _fused_op_157, encoder.layer.17.attention.output.LayerNorm.weight, encoder.layer.17.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_942: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_158, encoder.layer.17.intermediate.dense.weight, encoder.layer.17.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_948: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_942, encoder.layer.17.output.dense.weight, encoder.layer.17.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_158_add_952: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_158],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_952: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_948, buffer_0__fused_op_158_add_952],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_952, lc.input_tensor.layernorm_953.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_159: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_953.1, layernorm_953.dc.reduce_sum.0.lc1, add_952],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_953.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_159, _fused_op_159],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_953.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_953.dc.multiply.4, lc.input_tensor.layernorm_953.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_160: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_953.6, layernorm_953.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_953.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_161: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_159, _fused_op_160, encoder.layer.17.output.LayerNorm.weight, encoder.layer.17.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_18_temporal_epoch_18:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_161_matmul_956: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_161_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_956: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_161_matmul_956, encoder.layer.18.attention.self.query.weight, encoder.layer.18.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_962: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_161_matmul_956, encoder.layer.18.attention.self.key.weight, encoder.layer.18.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_968: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_956, matmul_962],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_162: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_968, input_1_multiply_970, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_972.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_162, lc.input_tensor.softmax_972.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_976: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_161_matmul_956, encoder.layer.18.attention.self.value.weight, encoder.layer.18.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_162__fused_op_164: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_162], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_163: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_972.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_972.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_164: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_162__fused_op_164, _fused_op_163],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_983: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_164, matmul_976],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_987: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_983, encoder.layer.18.attention.output.dense.weight, encoder.layer.18.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_991: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_987, e2e__fused_op_161_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_991, lc.input_tensor.layernorm_992.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_165: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_992.1, layernorm_992.dc.reduce_sum.0.lc1, add_991], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_992.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_165, _fused_op_165], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_992.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_992.dc.multiply.4, lc.input_tensor.layernorm_992.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_166: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_992.6, layernorm_992.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_992.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_167: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_165, _fused_op_166, encoder.layer.18.attention.output.LayerNorm.weight, encoder.layer.18.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_995: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_167, encoder.layer.18.intermediate.dense.weight, encoder.layer.18.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_1001: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_995, encoder.layer.18.output.dense.weight, encoder.layer.18.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_167_add_1005: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_167],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1005: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_1001, buffer_0__fused_op_167_add_1005],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_1005, lc.input_tensor.layernorm_1006.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_168: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1006.1, layernorm_1006.dc.reduce_sum.0.lc1, add_1005],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1006.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_168, _fused_op_168],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1006.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1006.dc.multiply.4, lc.input_tensor.layernorm_1006.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_169: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1006.6, layernorm_1006.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1006.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_170: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_168, _fused_op_169, encoder.layer.18.output.LayerNorm.weight, encoder.layer.18.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_19_temporal_epoch_19:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_170_matmul_1009: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_170_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1009: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_170_matmul_1009, encoder.layer.19.attention.self.query.weight, encoder.layer.19.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1015: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_170_matmul_1009, encoder.layer.19.attention.self.key.weight, encoder.layer.19.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1021: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_1009, matmul_1015],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_171: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_1021, input_1_multiply_1023, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_1025.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_171, lc.input_tensor.softmax_1025.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_1029: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_170_matmul_1009, encoder.layer.19.attention.self.value.weight, encoder.layer.19.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_171__fused_op_173: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_171], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_172: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_1025.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_1025.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_173: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_171__fused_op_173, _fused_op_172],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1036: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_173, matmul_1029],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_1040: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_1036, encoder.layer.19.attention.output.dense.weight, encoder.layer.19.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_1044: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1040, e2e__fused_op_170_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_1044, lc.input_tensor.layernorm_1045.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_174: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1045.1, layernorm_1045.dc.reduce_sum.0.lc1, add_1044], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1045.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_174, _fused_op_174], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1045.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_1045.dc.multiply.4, lc.input_tensor.layernorm_1045.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_175: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1045.6, layernorm_1045.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1045.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_176: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_174, _fused_op_175, encoder.layer.19.attention.output.LayerNorm.weight, encoder.layer.19.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_1048: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_176, encoder.layer.19.intermediate.dense.weight, encoder.layer.19.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_1054: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_1048, encoder.layer.19.output.dense.weight, encoder.layer.19.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_176_add_1058: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_176],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1058: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_1054, buffer_0__fused_op_176_add_1058],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_1058, lc.input_tensor.layernorm_1059.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_177: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1059.1, layernorm_1059.dc.reduce_sum.0.lc1, add_1058],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1059.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_177, _fused_op_177],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1059.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1059.dc.multiply.4, lc.input_tensor.layernorm_1059.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_178: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1059.6, layernorm_1059.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1059.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_179: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_177, _fused_op_178, encoder.layer.19.output.LayerNorm.weight, encoder.layer.19.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_20_temporal_epoch_20:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_179_matmul_1062: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_179_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1062: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_179_matmul_1062, encoder.layer.20.attention.self.query.weight, encoder.layer.20.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1068: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_179_matmul_1062, encoder.layer.20.attention.self.key.weight, encoder.layer.20.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1074: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_1062, matmul_1068],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_180: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_1074, input_1_multiply_1076, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_1078.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_180, lc.input_tensor.softmax_1078.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_1082: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_179_matmul_1062, encoder.layer.20.attention.self.value.weight, encoder.layer.20.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_180__fused_op_182: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_180], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_181: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_1078.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_1078.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_182: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_180__fused_op_182, _fused_op_181],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1089: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_182, matmul_1082],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_1093: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_1089, encoder.layer.20.attention.output.dense.weight, encoder.layer.20.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_1097: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1093, e2e__fused_op_179_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_1097, lc.input_tensor.layernorm_1098.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_183: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1098.1, layernorm_1098.dc.reduce_sum.0.lc1, add_1097], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1098.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_183, _fused_op_183], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1098.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_1098.dc.multiply.4, lc.input_tensor.layernorm_1098.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_184: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1098.6, layernorm_1098.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1098.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_185: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_183, _fused_op_184, encoder.layer.20.attention.output.LayerNorm.weight, encoder.layer.20.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_1101: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_185, encoder.layer.20.intermediate.dense.weight, encoder.layer.20.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_1107: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_1101, encoder.layer.20.output.dense.weight, encoder.layer.20.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_185_add_1111: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_185],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1111: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_1107, buffer_0__fused_op_185_add_1111],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_1111, lc.input_tensor.layernorm_1112.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_186: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1112.1, layernorm_1112.dc.reduce_sum.0.lc1, add_1111],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1112.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_186, _fused_op_186],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1112.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1112.dc.multiply.4, lc.input_tensor.layernorm_1112.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_187: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1112.6, layernorm_1112.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1112.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_188: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_186, _fused_op_187, encoder.layer.20.output.LayerNorm.weight, encoder.layer.20.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_21_temporal_epoch_21:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_188_matmul_1115: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_188_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1115: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_188_matmul_1115, encoder.layer.21.attention.self.query.weight, encoder.layer.21.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1121: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_188_matmul_1115, encoder.layer.21.attention.self.key.weight, encoder.layer.21.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1127: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_1115, matmul_1121],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_189: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_1127, input_1_multiply_1129, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_1131.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_189, lc.input_tensor.softmax_1131.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_1135: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_188_matmul_1115, encoder.layer.21.attention.self.value.weight, encoder.layer.21.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_189__fused_op_191: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_189], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_190: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_1131.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_1131.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_191: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_189__fused_op_191, _fused_op_190],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1142: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_191, matmul_1135],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_1146: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_1142, encoder.layer.21.attention.output.dense.weight, encoder.layer.21.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_1150: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1146, e2e__fused_op_188_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_1150, lc.input_tensor.layernorm_1151.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_192: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1151.1, layernorm_1151.dc.reduce_sum.0.lc1, add_1150], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1151.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_192, _fused_op_192], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1151.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_1151.dc.multiply.4, lc.input_tensor.layernorm_1151.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_193: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1151.6, layernorm_1151.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1151.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_194: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_192, _fused_op_193, encoder.layer.21.attention.output.LayerNorm.weight, encoder.layer.21.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_1154: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_194, encoder.layer.21.intermediate.dense.weight, encoder.layer.21.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_1160: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_1154, encoder.layer.21.output.dense.weight, encoder.layer.21.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_194_add_1164: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_194],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1164: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_1160, buffer_0__fused_op_194_add_1164],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_1164, lc.input_tensor.layernorm_1165.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_195: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1165.1, layernorm_1165.dc.reduce_sum.0.lc1, add_1164],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1165.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_195, _fused_op_195],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1165.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1165.dc.multiply.4, lc.input_tensor.layernorm_1165.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_196: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1165.6, layernorm_1165.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1165.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_197: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_195, _fused_op_196, encoder.layer.21.output.LayerNorm.weight, encoder.layer.21.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_22_temporal_epoch_22:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_197_matmul_1168: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_197_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1168: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_197_matmul_1168, encoder.layer.22.attention.self.query.weight, encoder.layer.22.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1174: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_197_matmul_1168, encoder.layer.22.attention.self.key.weight, encoder.layer.22.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1180: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_1168, matmul_1174],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_198: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_1180, input_1_multiply_1182, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_1184.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_198, lc.input_tensor.softmax_1184.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_1188: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_197_matmul_1168, encoder.layer.22.attention.self.value.weight, encoder.layer.22.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_198__fused_op_200: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_198], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_199: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_1184.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_1184.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_200: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_198__fused_op_200, _fused_op_199],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1195: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_200, matmul_1188],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_1199: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_1195, encoder.layer.22.attention.output.dense.weight, encoder.layer.22.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_1203: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1199, e2e__fused_op_197_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_1203, lc.input_tensor.layernorm_1204.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_201: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1204.1, layernorm_1204.dc.reduce_sum.0.lc1, add_1203], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1204.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_201, _fused_op_201], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1204.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_1204.dc.multiply.4, lc.input_tensor.layernorm_1204.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_202: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1204.6, layernorm_1204.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1204.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_203: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_201, _fused_op_202, encoder.layer.22.attention.output.LayerNorm.weight, encoder.layer.22.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_1207: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_203, encoder.layer.22.intermediate.dense.weight, encoder.layer.22.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_1213: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_1207, encoder.layer.22.output.dense.weight, encoder.layer.22.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_203_add_1217: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_203],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1217: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_1213, buffer_0__fused_op_203_add_1217],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_1217, lc.input_tensor.layernorm_1218.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_204: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1218.1, layernorm_1218.dc.reduce_sum.0.lc1, add_1217],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1218.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_204, _fused_op_204],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1218.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1218.dc.multiply.4, lc.input_tensor.layernorm_1218.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_205: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1218.6, layernorm_1218.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1218.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_206: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_204, _fused_op_205, encoder.layer.22.output.LayerNorm.weight, encoder.layer.22.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_23_temporal_epoch_23:
    target_device: 0
    input_count: 256
    buffer_0__fused_op_206_matmul_1221: {type: nop, grid_loc: [1, 2], grid_size: [1, 1], inputs: [e2e__fused_op_206_0],
         t: 1, mblock: [6, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [88], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    matmul_1221: {type: matmul, grid_loc: [0, 0], grid_size: [3, 1], inputs: [buffer_0__fused_op_206_matmul_1221, encoder.layer.23.attention.self.query.weight, encoder.layer.23.attention.self.query.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1227: {type: matmul, grid_loc: [0, 1], grid_size: [3, 1], inputs: [buffer_0__fused_op_206_matmul_1221, encoder.layer.23.attention.self.key.weight, encoder.layer.23.attention.self.key.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    matmul_1233: {type: matmul, grid_loc: [0, 2], grid_size: [1, 1], inputs: [matmul_1221, matmul_1227],
         t: 16, mblock: [6, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [transpose, vslice: 16], input_0_tms: [hslice: 16],
         attributes: {m_k: 1, min_buffer_input: 0, u_kt: 2}}
    _fused_op_207: {type: fused_op, grid_loc: [0, 3], grid_size: [3, 3], inputs: [matmul_1233, input_1_multiply_1235, attention_mask],
         t: 16, mblock: [2, 1], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 48], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_2_tms: [broadcast: {r: 12}, broadcast: {z: 16}], input_1_tms: [broadcast: {c: 12}, broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {approximate_mode: true, fused_op_id: 0, kernel_broadcast: {input_1: 1}}}
    softmax_1237.dc.reduce_sum.1.lc1: {type: matmul, grid_loc: [0, 6], grid_size: [2, 1], inputs: [_fused_op_207, lc.input_tensor.softmax_1237.dc.reduce_sum.1.0],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Bfp8_b, math_fidelity: LoFi,
         input_1_tms: [broadcast: {r: 12}, broadcast: {z: 16}],
         attributes: {kernel_broadcast: {input_1: 1}, m_k: 12, min_buffer_input: 0, u_kt: 1}}
    matmul_1241: {type: matmul, grid_loc: [4, 5], grid_size: [2, 2], inputs: [buffer_0__fused_op_206_matmul_1221, encoder.layer.23.attention.self.value.weight, encoder.layer.23.attention.self.value.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [408, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 8, min_buffer_input: 1, u_kt: 4}}
    buffer_0__fused_op_207__fused_op_209: {type: nop, grid_loc: [3, 3], grid_size: [2, 1], inputs: [_fused_op_207], grid_transpose: true,
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [776], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    _fused_op_208: {type: fused_op, grid_loc: [2, 6], grid_size: [2, 1], inputs: [softmax_1237.dc.reduce_sum.1.lc1, dc.input_tensor.softmax_1237.2],
         t: 16, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 1}}
    _fused_op_209: {type: fused_op, grid_loc: [0, 7], grid_size: [2, 1], inputs: [buffer_0__fused_op_207__fused_op_209, _fused_op_208],
         t: 16, mblock: [3, 3], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [752, 0], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 12}],
         attributes: {approximate_mode: true, fused_op_id: 2}}
    matmul_1248: {type: matmul, grid_loc: [2, 7], grid_size: [1, 1], inputs: [_fused_op_209, matmul_1241],
         t: 16, mblock: [6, 1], ublock: [2, 2], tile_dim: [32, 32], buf_size_mb: 32, input_buf_min_size_tiles: [0, 136], input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_1_tms: [hslice: 16],
         attributes: {m_k: 3, min_buffer_input: 0, u_kt: 4}}
    matmul_1252: {type: matmul, grid_loc: [3, 0], grid_size: [2, 2], inputs: [matmul_1248, encoder.layer.23.attention.output.dense.weight, encoder.layer.23.attention.output.dense.bias],
         t: 1, mblock: [3, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}], input_0_tms: [hstack: 16],
         attributes: {bias: true, kernel_broadcast: {input_2: 32}, m_k: 16, min_buffer_input: 1, u_kt: 2}}
    add_1256: {type: add, grid_loc: [2, 2], grid_size: [2, 1], inputs: [matmul_1252, e2e__fused_op_206_0],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 88], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [4, 2], grid_size: [1, 1], inputs: [add_1256, lc.input_tensor.layernorm_1257.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_210: {type: fused_op, grid_loc: [4, 3], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1257.1, layernorm_1257.dc.reduce_sum.0.lc1, add_1256], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1257.dc.multiply.4: {type: multiply, grid_loc: [5, 0], grid_size: [2, 1], inputs: [_fused_op_210, _fused_op_210], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1257.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [5, 2], grid_size: [1, 1], inputs: [layernorm_1257.dc.multiply.4, lc.input_tensor.layernorm_1257.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_211: {type: fused_op, grid_loc: [5, 7], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1257.6, layernorm_1257.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1257.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_212: {type: fused_op, grid_loc: [5, 3], grid_size: [2, 1], inputs: [_fused_op_210, _fused_op_211, encoder.layer.23.attention.output.LayerNorm.weight, encoder.layer.23.attention.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}
    matmul_1260: {type: matmul, grid_loc: [6, 0], grid_size: [3, 4], inputs: [_fused_op_212, encoder.layer.23.intermediate.dense.weight, encoder.layer.23.intermediate.dense.bias],
         t: 1, mblock: [2, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 64}, m_k: 16, min_buffer_input: 1, sfpu_op: gelu, u_kt: 2}}
    matmul_1266: {type: matmul, grid_loc: [6, 4], grid_size: [2, 4], inputs: [matmul_1260, encoder.layer.23.output.dense.weight, encoder.layer.23.output.dense.bias],
         t: 1, mblock: [3, 2], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 16}, m_k: 16, min_buffer_input: 1, u_kt: 8}}
    buffer_0__fused_op_212_add_1270: {type: nop, grid_loc: [8, 6], grid_size: [1, 2], inputs: [_fused_op_212],
         t: 1, mblock: [6, 4], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [536], input_dram_io_buf_size_tiles: [0], ublock_order: c, in_df: [Bfp8_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    add_1270: {type: add, grid_loc: [3, 7], grid_size: [2, 1], inputs: [matmul_1266, buffer_0__fused_op_212_add_1270],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 192], input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_sum.0.lc1: {type: matmul, grid_loc: [9, 6], grid_size: [1, 1], inputs: [add_1270, lc.input_tensor.layernorm_1271.dc.reduce_sum.0.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_213: {type: fused_op, grid_loc: [8, 5], grid_size: [2, 1], inputs: [dc.input_tensor.layernorm_1271.1, layernorm_1271.dc.reduce_sum.0.lc1, add_1270],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [0, 0, 64], input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 3}}
    layernorm_1271.dc.multiply.4: {type: multiply, grid_loc: [8, 4], grid_size: [2, 1], inputs: [_fused_op_213, _fused_op_213],
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}
    layernorm_1271.dc.reduce_sum.5.lc1: {type: matmul, grid_loc: [9, 3], grid_size: [1, 1], inputs: [layernorm_1271.dc.multiply.4, lc.input_tensor.layernorm_1271.dc.reduce_sum.5.0],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0], ublock_order: r, in_df: [Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_1_tms: [broadcast: {r: 32}],
         attributes: {kernel_broadcast: {input_1: 1}, l1_acc: true, m_k: 32, min_buffer_input: 0, u_kt: 1}}
    _fused_op_214: {type: fused_op, grid_loc: [9, 2], grid_size: [1, 1], inputs: [dc.input_tensor.layernorm_1271.6, layernorm_1271.dc.reduce_sum.5.lc1, dc.input_tensor.layernorm_1271.8],
         t: 1, mblock: [6, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         attributes: {approximate_mode: true, fused_op_id: 4}}
    _fused_op_215: {type: fused_op, grid_loc: [9, 0], grid_size: [2, 1], inputs: [_fused_op_213, _fused_op_214, encoder.layer.23.output.LayerNorm.weight, encoder.layer.23.output.LayerNorm.bias], grid_transpose: true,
         t: 1, mblock: [3, 8], ublock: [2, 4], tile_dim: [32, 32], buf_size_mb: 2, input_buf_min_size_tiles: [328, 0, 0, 0], input_dram_io_buf_size_tiles: [0, 0, 0, 0], ublock_order: c, in_df: [Bfp8_b, Float16_b, Float16_b, Float16_b], out_df: Bfp8_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3,
         input_3_tms: [broadcast: {r: 12}], input_2_tms: [broadcast: {r: 12}], input_1_tms: [broadcast: {c: 32}],
         attributes: {approximate_mode: true, fused_op_id: 5}}

  fwd_0_24_temporal_epoch_24:
    target_device: 0
    input_count: 256
    matmul_1274: {type: matmul, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e__fused_op_215_0, lm_head.weight, lm_head.bias],
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [90, 0, 0], ublock_order: r, in_df: [Bfp8_b, Bfp8_b, Bfp8_b], out_df: Bfp8_b, intermed_df: Bfp8_b, acc_df: Float16_b, math_fidelity: LoFi,
         input_2_tms: [broadcast: {r: 12}],
         attributes: {bias: true, kernel_broadcast: {input_2: 1}, m_k: 32, min_buffer_input: 0, u_kt: 1}}

  fwd_0_25_temporal_epoch_25:
    target_device: 0
    input_count: 256
    matmul_1274_output_nop_0: {type: nop, grid_loc: [0, 0], grid_size: [2, 1], inputs: [e2e_matmul_1274_0], untilize_output: true,
         t: 1, mblock: [3, 1], ublock: [2, 1], tile_dim: [32, 32], buf_size_mb: 2, input_dram_io_buf_size_tiles: [90], ublock_order: r, in_df: [Bfp8_b], out_df: Float16_b, intermed_df: Float16_b, acc_df: Float16_b, math_fidelity: HiFi3}


programs:
  - run_fwd_0:
    - param: [$p_loop_count]
    - var: {$lptr_q25: 0, $gptr_q23: 0, $gptr_q19: 0, $lptr_q19: 0, $lptr_q13: 0, $gptr_q11: 0, $lptr_q15: 0, $lptr_q11: 0, $gptr_q9: 0, $gptr_q25: 0, $lptr_q9: 0, $gptr_q5: 0, $gptr_q21: 0, $lptr_q5: 0, $lptr_q21: 0, $gptr_q3: 0, $lptr_q3: 0, $gptr_q43: 0, $lptr_q49: 0, $lptr_q23: 0, $lptr_q43: 0, $lptr_q45: 0, $gptr_q47: 0, $lptr_q48: 0, $lptr_q29: 0, $lptr_q47: 0, $gptr_q15: 0, $c_microbatch_size: 256, $c_zero: 0, $gptr_q48: 0, $lptr_q27: 0, $lptr_q41: 0, $lptr_q37: 0, $c_one: 1, $gptr_q41: 0, $gptr_q39: 0, $gptr_q37: 0, $gptr_q17: 0, $gptr_q45: 0, $lptr_q35: 0, $lptr_q17: 0, $gptr_q7: 0, $lptr_q7: 0, $lptr_q33: 0, $gptr_q13: 0, $gptr_q29: 0, $gptr_q33: 0, $gptr_q49: 0, $lptr_q39: 0, $lptr_q31: 0, $gptr_q35: 0, $gptr_q31: 0, $gptr_q27: 0}
    - staticvar: {$gptr_q18_shadow: 0, $gptr_q12_shadow: 0, $gptr_q10_shadow: 0, $gptr_q8_shadow: 0, $gptr_q6_shadow: 0, $gptr_q4_shadow: 0, $gptr_q2_shadow: 0, $gptr_q1_shadow: 0, $gptr_q26_shadow: 0, $gptr_q2: 0, $lptr_q2: 0, $gptr_q40: 0, $gptr_q1: 0, $lptr_q26: 0, $lptr_q16: 0, $gptr_q34_shadow: 0, $lptr_q28: 0, $lptr_q30: 0, $gptr_q32: 0, $gptr_q30: 0, $lptr_q32: 0, $lptr_q46: 0, $gptr_q20: 0, $lptr_q24: 0, $gptr_q28: 0, $gptr_q28_shadow: 0, $gptr_q36: 0, $lptr_q4: 0, $gptr_q42: 0, $lptr_q14: 0, $gptr_q18: 0, $gptr_q34: 0, $gptr_q44: 0, $lptr_q0: 0, $lptr_q10: 0, $gptr_q26: 0, $gptr_q16_shadow: 0, $gptr_q32_shadow: 0, $lptr_q42: 0, $gptr_q38: 0, $lptr_q38: 0, $gptr_q0: 0, $lptr_q36: 0, $gptr_q24_shadow: 0, $gptr_q10: 0, $lptr_q34: 0, $lptr_q44: 0, $gptr_q4: 0, $gptr_q6: 0, $gptr_q14_shadow: 0, $lptr_q6: 0, $lptr_q8: 0, $gptr_q22_shadow: 0, $lptr_q40: 0, $gptr_q8: 0, $gptr_q14: 0, $gptr_q46: 0, $lptr_q22: 0, $lptr_q12: 0, $gptr_q12: 0, $gptr_q30_shadow: 0, $gptr_q16: 0, $lptr_q1: 0, $gptr_q22: 0, $lptr_q20: 0, $lptr_q18: 0, $gptr_q24: 0, $gptr_q20_shadow: 0, $gptr_q44_shadow: 0, $gptr_q36_shadow: 0, $gptr_q42_shadow: 0, $gptr_q40_shadow: 0, $gptr_q38_shadow: 0}
    - loop: $p_loop_count
    -   varinst: [$gptr_q44, set, $gptr_q44_shadow]
    -   varinst: [$gptr_q42, set, $gptr_q42_shadow]
    -   varinst: [$gptr_q40, set, $gptr_q40_shadow]
    -   varinst: [$gptr_q38, set, $gptr_q38_shadow]
    -   varinst: [$gptr_q36, set, $gptr_q36_shadow]
    -   varinst: [$gptr_q34, set, $gptr_q34_shadow]
    -   varinst: [$gptr_q32, set, $gptr_q32_shadow]
    -   varinst: [$gptr_q30, set, $gptr_q30_shadow]
    -   varinst: [$gptr_q28, set, $gptr_q28_shadow]
    -   varinst: [$gptr_q26, set, $gptr_q26_shadow]
    -   varinst: [$gptr_q1, set, $gptr_q1_shadow]
    -   varinst: [$gptr_q2, set, $gptr_q2_shadow]
    -   varinst: [$gptr_q4, set, $gptr_q4_shadow]
    -   varinst: [$gptr_q6, set, $gptr_q6_shadow]
    -   varinst: [$gptr_q8, set, $gptr_q8_shadow]
    -   varinst: [$gptr_q10, set, $gptr_q10_shadow]
    -   varinst: [$gptr_q12, set, $gptr_q12_shadow]
    -   varinst: [$gptr_q14, set, $gptr_q14_shadow]
    -   varinst: [$gptr_q16, set, $gptr_q16_shadow]
    -   varinst: [$gptr_q18, set, $gptr_q18_shadow]
    -   varinst: [$gptr_q20, set, $gptr_q20_shadow]
    -   varinst: [$gptr_q22, set, $gptr_q22_shadow]
    -   varinst: [$gptr_q24, set, $gptr_q24_shadow]
    -   allocate_queue: [e2e__fused_op_8_0]
    -   execute: {graph_name: fwd_0_0_temporal_epoch_0, queue_settings: {
               input_1: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q0, rd_ptr_global: $gptr_q0},
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q1, rd_ptr_global: $gptr_q1},
               encoder.layer.0.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_16: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_18.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_18.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.0.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_38.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_38.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.0.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_52.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_52.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.0.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.0.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   varinst: [$gptr_q0, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q1_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q0, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q1, incwrap, $c_microbatch_size, 1024]
    -   allocate_queue: [e2e__fused_op_17_0]
    -   execute: {graph_name: fwd_0_1_temporal_epoch_1, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q2, rd_ptr_global: $gptr_q2},
               e2e__fused_op_8_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q3, rd_ptr_global: $gptr_q3},
               encoder.layer.1.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_69: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_71.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_71.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.1.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_91.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_91.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.1.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_105.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_105.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.1.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.1.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_8_0]
    -   varinst: [$gptr_q2_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q3, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q2, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q3, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_26_0]
    -   execute: {graph_name: fwd_0_2_temporal_epoch_2, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q4, rd_ptr_global: $gptr_q4},
               e2e__fused_op_17_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q5, rd_ptr_global: $gptr_q5},
               encoder.layer.2.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_122: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_124.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_124.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.2.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_144.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_144.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.2.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_158.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_158.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.2.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.2.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_17_0]
    -   varinst: [$gptr_q4_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q5, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q4, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q5, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_35_0]
    -   execute: {graph_name: fwd_0_3_temporal_epoch_3, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q6, rd_ptr_global: $gptr_q6},
               e2e__fused_op_26_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q7, rd_ptr_global: $gptr_q7},
               encoder.layer.3.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_175: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_177.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_177.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.3.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_197.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_197.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.3.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_211.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_211.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.3.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.3.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_26_0]
    -   varinst: [$gptr_q6_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q7, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q6, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q7, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_44_0]
    -   execute: {graph_name: fwd_0_4_temporal_epoch_4, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q8, rd_ptr_global: $gptr_q8},
               e2e__fused_op_35_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q9, rd_ptr_global: $gptr_q9},
               encoder.layer.4.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_228: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_230.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_230.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.4.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_250.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_250.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.4.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_264.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_264.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.4.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.4.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_35_0]
    -   varinst: [$gptr_q8_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q9, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q8, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q9, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_53_0]
    -   execute: {graph_name: fwd_0_5_temporal_epoch_5, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q10, rd_ptr_global: $gptr_q10},
               e2e__fused_op_44_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q11, rd_ptr_global: $gptr_q11},
               encoder.layer.5.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_281: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_283.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_283.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.5.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_303.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_303.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.5.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_317.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_317.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.5.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.5.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_44_0]
    -   varinst: [$gptr_q10_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q11, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q10, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q11, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_62_0]
    -   execute: {graph_name: fwd_0_6_temporal_epoch_6, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q12, rd_ptr_global: $gptr_q12},
               e2e__fused_op_53_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q13, rd_ptr_global: $gptr_q13},
               encoder.layer.6.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_334: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_336.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_336.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.6.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_356.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_356.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.6.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_370.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_370.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.6.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.6.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_53_0]
    -   varinst: [$gptr_q12_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q13, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q12, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q13, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_71_0]
    -   execute: {graph_name: fwd_0_7_temporal_epoch_7, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q14, rd_ptr_global: $gptr_q14},
               e2e__fused_op_62_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q15, rd_ptr_global: $gptr_q15},
               encoder.layer.7.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_387: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_389.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_389.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.7.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_409.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_409.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.7.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_423.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_423.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.7.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.7.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_62_0]
    -   varinst: [$gptr_q14_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q15, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q14, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q15, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_80_0]
    -   execute: {graph_name: fwd_0_8_temporal_epoch_8, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q16, rd_ptr_global: $gptr_q16},
               e2e__fused_op_71_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q17, rd_ptr_global: $gptr_q17},
               encoder.layer.8.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_440: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_442.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_442.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.8.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_462.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_462.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.8.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_476.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_476.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.8.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.8.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_71_0]
    -   varinst: [$gptr_q16_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q17, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q16, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q17, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_89_0]
    -   execute: {graph_name: fwd_0_9_temporal_epoch_9, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q18, rd_ptr_global: $gptr_q18},
               e2e__fused_op_80_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q19, rd_ptr_global: $gptr_q19},
               encoder.layer.9.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_493: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_495.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_495.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.9.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_515.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_515.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.9.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_529.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_529.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.9.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.9.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_80_0]
    -   varinst: [$gptr_q18_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q19, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q18, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q19, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_98_0]
    -   execute: {graph_name: fwd_0_10_temporal_epoch_10, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q20, rd_ptr_global: $gptr_q20},
               e2e__fused_op_89_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q21, rd_ptr_global: $gptr_q21},
               encoder.layer.10.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_546: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_548.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_548.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.10.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_568.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_568.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.10.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_582.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_582.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.10.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.10.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_89_0]
    -   varinst: [$gptr_q20_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q21, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q20, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q21, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_107_0]
    -   execute: {graph_name: fwd_0_11_temporal_epoch_11, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q22, rd_ptr_global: $gptr_q22},
               e2e__fused_op_98_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q23, rd_ptr_global: $gptr_q23},
               encoder.layer.11.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_599: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_601.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_601.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.11.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_621.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_621.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.11.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_635.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_635.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.11.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.11.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_98_0]
    -   varinst: [$gptr_q22_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q23, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q22, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q23, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_116_0]
    -   execute: {graph_name: fwd_0_12_temporal_epoch_12, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q24, rd_ptr_global: $gptr_q24},
               e2e__fused_op_107_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q25, rd_ptr_global: $gptr_q25},
               encoder.layer.12.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_652: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_654.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_654.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.12.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_674.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_674.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.12.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_688.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_688.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.12.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.12.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_107_0]
    -   varinst: [$gptr_q24_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q25, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q24, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q25, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_125_0]
    -   execute: {graph_name: fwd_0_13_temporal_epoch_13, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q26, rd_ptr_global: $gptr_q26},
               e2e__fused_op_116_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q27, rd_ptr_global: $gptr_q27},
               encoder.layer.13.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_705: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_707.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_707.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.13.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_727.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_727.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.13.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_741.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_741.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.13.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.13.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_116_0]
    -   varinst: [$gptr_q26_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q27, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q26, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q27, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_134_0]
    -   execute: {graph_name: fwd_0_14_temporal_epoch_14, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q28, rd_ptr_global: $gptr_q28},
               e2e__fused_op_125_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q29, rd_ptr_global: $gptr_q29},
               encoder.layer.14.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_758: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_760.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_760.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.14.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_780.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_780.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.14.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_794.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_794.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.14.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.14.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_125_0]
    -   varinst: [$gptr_q28_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q29, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q28, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q29, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_143_0]
    -   execute: {graph_name: fwd_0_15_temporal_epoch_15, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q30, rd_ptr_global: $gptr_q30},
               e2e__fused_op_134_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q31, rd_ptr_global: $gptr_q31},
               encoder.layer.15.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_811: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_813.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_813.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.15.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_833.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_833.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.15.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_847.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_847.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.15.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.15.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_134_0]
    -   varinst: [$gptr_q30_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q31, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q30, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q31, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_152_0]
    -   execute: {graph_name: fwd_0_16_temporal_epoch_16, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q32, rd_ptr_global: $gptr_q32},
               e2e__fused_op_143_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q33, rd_ptr_global: $gptr_q33},
               encoder.layer.16.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_864: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_866.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_866.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.16.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_886.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_886.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.16.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_900.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_900.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.16.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.16.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_143_0]
    -   varinst: [$gptr_q32_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q33, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q32, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q33, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_161_0]
    -   execute: {graph_name: fwd_0_17_temporal_epoch_17, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q34, rd_ptr_global: $gptr_q34},
               e2e__fused_op_152_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q35, rd_ptr_global: $gptr_q35},
               encoder.layer.17.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_917: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_919.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_919.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.17.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_939.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_939.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.17.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_953.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_953.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.17.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.17.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_152_0]
    -   varinst: [$gptr_q34_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q35, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q34, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q35, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_170_0]
    -   execute: {graph_name: fwd_0_18_temporal_epoch_18, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q36, rd_ptr_global: $gptr_q36},
               e2e__fused_op_161_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q37, rd_ptr_global: $gptr_q37},
               encoder.layer.18.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_970: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_972.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_972.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.18.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_992.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_992.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.18.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1006.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1006.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.18.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.18.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_161_0]
    -   varinst: [$gptr_q36_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q37, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q36, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q37, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_179_0]
    -   execute: {graph_name: fwd_0_19_temporal_epoch_19, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q38, rd_ptr_global: $gptr_q38},
               e2e__fused_op_170_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q39, rd_ptr_global: $gptr_q39},
               encoder.layer.19.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1023: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1025.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1025.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.19.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1045.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1045.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.19.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1059.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1059.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.19.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.19.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_170_0]
    -   varinst: [$gptr_q38_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q39, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q38, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q39, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_188_0]
    -   execute: {graph_name: fwd_0_20_temporal_epoch_20, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q40, rd_ptr_global: $gptr_q40},
               e2e__fused_op_179_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q41, rd_ptr_global: $gptr_q41},
               encoder.layer.20.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1076: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1078.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1078.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.20.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1098.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1098.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.20.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1112.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1112.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.20.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.20.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_179_0]
    -   varinst: [$gptr_q40_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q41, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q40, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q41, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_197_0]
    -   execute: {graph_name: fwd_0_21_temporal_epoch_21, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q42, rd_ptr_global: $gptr_q42},
               e2e__fused_op_188_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q43, rd_ptr_global: $gptr_q43},
               encoder.layer.21.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1129: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1131.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1131.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.21.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1151.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1151.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.21.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1165.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1165.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.21.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.21.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_188_0]
    -   varinst: [$gptr_q42_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q43, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q42, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q43, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_206_0]
    -   execute: {graph_name: fwd_0_22_temporal_epoch_22, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q44, rd_ptr_global: $gptr_q44},
               e2e__fused_op_197_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q45, rd_ptr_global: $gptr_q45},
               encoder.layer.22.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1182: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1184.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1184.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.22.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1204.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1204.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.22.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1218.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1218.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.22.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.22.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_197_0]
    -   varinst: [$gptr_q44_shadow, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q45, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q44, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q45, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e__fused_op_215_0]
    -   execute: {graph_name: fwd_0_23_temporal_epoch_23, queue_settings: {
               attention_mask: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q46, rd_ptr_global: $gptr_q46},
               e2e__fused_op_206_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q47, rd_ptr_global: $gptr_q47},
               encoder.layer.23.attention.self.query.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.attention.self.query.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.attention.self.key.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.attention.self.key.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               input_1_multiply_1235: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.softmax_1237.dc.reduce_sum.1.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.softmax_1237.2: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.23.attention.self.value.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.attention.self.value.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.attention.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.attention.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1257.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1257.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.23.attention.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.attention.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.intermediate.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.intermediate.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.output.dense.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.output.dense.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_sum.0.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.1: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               lc.input_tensor.layernorm_1271.dc.reduce_sum.5.0: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.6: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               dc.input_tensor.layernorm_1271.8: {prologue: true, epilogue: false, zero: False, rd_ptr_local: $c_zero, rd_ptr_global: $c_zero},
               encoder.layer.23.output.LayerNorm.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               encoder.layer.23.output.LayerNorm.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_206_0]
    -   varinst: [$gptr_q46, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$gptr_q47, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q46, incwrap, $c_microbatch_size, 1024]
    -   varinst: [$lptr_q47, incwrap, $c_microbatch_size, 512]
    -   allocate_queue: [e2e_matmul_1274_0]
    -   execute: {graph_name: fwd_0_24_temporal_epoch_24, queue_settings: {
               e2e__fused_op_215_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q48, rd_ptr_global: $gptr_q48},
               lm_head.weight: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero},
               lm_head.bias: {prologue: true, epilogue: false, zero: False, rd_ptr_global: $c_zero, wr_ptr_global: $c_zero}} }
    -   deallocate_queue: [e2e__fused_op_215_0]
    -   varinst: [$gptr_q48, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q48, incwrap, $c_microbatch_size, 512]
    -   execute: {graph_name: fwd_0_25_temporal_epoch_25, queue_settings: {
               e2e_matmul_1274_0: {prologue: false, epilogue: false, zero: False, rd_ptr_local: $lptr_q49, rd_ptr_global: $gptr_q49}} }
    -   deallocate_queue: [e2e_matmul_1274_0]
    -   varinst: [$gptr_q49, incwrap, $c_microbatch_size, 512]
    -   varinst: [$lptr_q49, incwrap, $c_microbatch_size, 512]
    - endloop


fused_ops:
  0: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - multiply_16.0: { type: multiply, inputs: [input0, input1], mblock: [2, 1], ublock: [2, 4], output: dest}
        - add_17.0: { type: add, inputs: [dest, input2], input_1_tms: [tile_broadcast: r], mblock: [2, 1], ublock: [2, 4], output: dest}
        - softmax_18.dc.exp.0.0: { type: exp, inputs: [dest], mblock: [2, 1], ublock: [2, 4], output: output}
  1: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.add.3.0: { type: add, inputs: [input0, input1], mblock: [3, 1], ublock: [2, 1], output: dest}
        - softmax_18.dc.reciprocal.4.0: { type: reciprocal, inputs: [dest], mblock: [3, 1], ublock: [2, 1], output: output}
  2: 
    inputs: 2
    intermediates: 0
    schedules: 
      -
        - softmax_18.dc.multiply.5.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [3, 3], ublock: [2, 4], output: output}
  3: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.2.0: { type: multiply, inputs: [input0, input1], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.subtract.3.0: { type: subtract, inputs: [input2, dest], mblock: [3, 8], ublock: [2, 4], output: output}
  4: 
    inputs: 3
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.7.0: { type: multiply, inputs: [input0, input1], mblock: [6, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.add.9.0: { type: add, inputs: [dest, input2], mblock: [6, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.sqrt.10.0: { type: sqrt, inputs: [dest], mblock: [6, 1], ublock: [2, 1], output: dest}
        - layernorm_38.dc.reciprocal.11.0: { type: reciprocal, inputs: [dest], mblock: [6, 1], ublock: [2, 1], output: output}
  5: 
    inputs: 4
    intermediates: 0
    schedules: 
      -
        - layernorm_38.dc.multiply.12.0: { type: multiply, inputs: [input0, input1], input_1_tms: [tile_broadcast: c], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.multiply.13.0: { type: multiply, inputs: [dest, input2], mblock: [3, 8], ublock: [2, 4], output: dest}
        - layernorm_38.dc.add.14.0: { type: add, inputs: [dest, input3], mblock: [3, 8], ublock: [2, 4], output: output}
